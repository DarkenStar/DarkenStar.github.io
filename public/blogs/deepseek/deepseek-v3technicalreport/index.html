<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DeepSeek-V3 Technical Report | WITHER</title>
<meta name="keywords" content="DeepSeek">
<meta name="description" content="Paper Reading of DeepSeek-V3 Technical Report">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="DeepSeek-V3 Technical Report">
  <meta property="og:description" content="Paper Reading of DeepSeek-V3 Technical Report">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-06-20T16:37:06+08:00">
    <meta property="article:modified_time" content="2025-06-22T17:53:30+08:00">
    <meta property="article:tag" content="DeepSeek">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeepSeek-V3 Technical Report">
<meta name="twitter:description" content="Paper Reading of DeepSeek-V3 Technical Report">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DeepSeek Related",
      "item": "http://localhost:1313/blogs/deepseek/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "DeepSeek-V3 Technical Report",
      "item": "http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DeepSeek-V3 Technical Report",
  "name": "DeepSeek-V3 Technical Report",
  "description": "Paper Reading of DeepSeek-V3 Technical Report",
  "keywords": [
    "DeepSeek"
  ],
  "articleBody": "Abstract DeepSeek-V3 (671B) 是 MoE 模型，每个 token 会激活 37B 的参数。采用 Multi-head Latent Attention (MLA) 和自创的 DeepSeek MoE 结构，在这两篇文章中已经做过讲解。同时采用了 auxiliary-loss-free 策略来实现专家负载平衡并且使用了 Multi-token Prediction (MTP) 来加速训练。整个预训练数据集一共有 14.8T tokens，通过 Suprvised Fine-Tuning (SFT) 和 强化学习来加强性能。训练时长为 2.788M H800 GPU 小时。\n1. Introduction 模型架构创新:\nMulti-head Latent Attention (MLA): 加速推理。 DeepSeek MoE: 减少训练开销。 增强模型能力的策略:\nauxiliary-loss-free: 实现负载平衡。 Multi-token Prediction (MTP): 增强模型表现。 提高训练效率的方法:\nFP8 混合精度训练: 加速训练和减少 GPU 内存使用。 DualPipe 流水线并行算法: 减少气泡并且在训练时候通过计算掩盖了大部分通信。 新的节点间 All-to-all 通信算子: 更好地利用 InfiniBand (IB) and NVLink 带宽。 优化了内存后 DeepSeek-V3 训练没有使用 TP. 训练过程:\npre-training: 在 14.8T tokens 上进行。 stage 1: 扩展最大上下文长度到 32K. stage 2: 扩展最大上下文长度到 128K. post-training: 使用 Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) 并且蒸馏了 DeepSeek-R1 系列模型来获得推理能力。 DeepSeek-V3 上训练每 1T token 只需要180K H800 GPU小时，即在 2048 个 H800 GPU 的集群上需要 3.7 天。\n2. Architecture Illustration of the Basic Architecture of DeepSeek-V3\n2.1 MLA 在相关文章中已经介绍。\n2.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing $$\r\\begin{align}\r\\mathbf{h}'_t \u0026= \\mathbf{u}_t + \\sum_{i=1}^{N_s} \\text{FFN}_i^{(s)} (\\mathbf{u}_t) + \\sum_{i=1}^{N_r} g_{i,t} \\text{FFN}_i^{(r)} (\\mathbf{u}_t), \\\\\rg_{i,t} \u0026= \\frac{g'_{i,t}}{\\sum_{j=1}^{N_r} g'_{j,t}}, \\\\\rg'_{i,t} \u0026= \\begin{cases} s_{i,t}, \u0026 s_{i,t} \\in \\text{Topk}(\\{s_{j,t}|1 \\le j \\le N_r\\}, K_r), \\\\ 0, \u0026 \\text{otherwise}, \\end{cases} \\\\\rs_{i,t} \u0026= \\text{Sigmoid}(\\mathbf{u}_t^T \\mathbf{e}_i),\r\\end{align}\r$$Basic Architecture of DeepSeekMoE. 在相关文章中已经介绍，V3 和 V2 不同之处在于使用sigmoid函数来计算亲和分数，并在归一化所有选定的亲和分数之间来产生门控制值。\nAuxiliary-Loss-Free Load Balancing. 为每个专家引入一个偏差项 $b_i$，并将其与相应的亲和力分数$s_{i,t}$ 相加，以确定 top-K 路由\n$$\rg'_{i,t} = \\begin{cases} s_{i,t}, \u0026 s_{i,t} + b_i \\in \\text{Topk}(\\{s_{j,t} + b_j | 1 \\le j \\le N_r\\}, K_r), \\\\ 0, \u0026 \\text{otherwise}. \\end{cases} \\tag{16}\r$$这个偏置项仅用于路由，用于和 FFN 输出相乘的门控值还是来自于原先的原先的 $s_{i,t}$. 在每一步结束时，如果其对应的专家过载， DeepSeek-V3 将偏差项减少 $\\gamma$ (一个超参数，被称作 bias update speed)，如果其对应的专家负载不足， DeepSeek-V3 将偏差项增加 $\\gamma$.\nV3 使用 sequence-wise balance loss，类似于 V2 中 Expert-Level Balance Loss。 不同之处在于使用归一化的亲和分数。\n$$\r\\begin{align}\r\\mathcal{L}_{\\text{Bal}} \u0026= \\alpha \\sum_{i=1}^{N_r} f_i P_i, \\\\\rf_i \u0026= \\frac{N_r}{K_r T} \\sum_{t=1}^{T} 1 (s_{i,t} \\in \\text{Topk}(\\{s_{j,t}|1 \\le j \\le N_r\\}, K_r)), \\\\\rs'_{i,t} \u0026= \\frac{s_{i,t}}{\\sum_{j=1}^{N_r} s_{j,t}}, \\\\\rP_i \u0026= \\frac{1}{T} \\sum_{t=1}^{T} s'_{i,t} \\end{align}\r$$Node-Limited Routing. 对于每个 token 计算每个节点计算亲和度分数前 $\\frac {K_r}M$ 的专家求和，选取前 $M$ 个作为路由节点。\nNo Token-Dropping. 训练和推理中均不采用。Token Dropping指的是在 MoE 当路由到某个专家的 Token 数量超过了该专家的处理容量时，系统会故意丢跳过那些超出容量的 token，不让它们被这个专家处理。这些 token 通常会通过一个残差连接，将其输入时的状态直接传递到下一层。\n2.3 Multi-Token Prediction Illustration of Multi-Token Prediction (MTP) implementation\n与 Gloeckle 等人使用独立的输出头并行预测 D 个额外 token 不同， DeepSeek-V3 顺序预测额外 token 并在每个预测深度保持完整的因果链。\nMTP Modules. DeepSeek-V3 使用 D 个顺序模块来预测 D 个额外 token。第 k 个 MTP 模块由一个共享嵌入层 Emb(·)、一个共享输出头 OutHead(·)、一个 Transformer 块 TRM_k(·) 和一个投影矩阵 $M_k \\in \\mathbb{R}^{d \\times 2d}$ 组成。对于第 i 个输入 token $t_i$，在第 k 个预测深度， DeepSeek-V3 首先通过线性映射结合第 i 个 token 在第 (k−1) 个深度上的表示 $h_i^{k-1} \\in \\mathbb{R}^d$ 和第 (i+k) 个 token 的嵌入 $\\text{Emb}(t_{i+k}) \\in \\mathbb{R}^d$\n$$\rh_t^k = M_k[\\text{RMSNorm}(h_t^{k-1}); \\text{RMSNorm}(\\text{Emb}(t_{i+k}))], \\tag{21}\r$$其中 [·;·] 表示拼接操作。特别地，当 k = 1 时，$h_t^{k-1}$ 指的是由主模型给出的表示。请注意，对于每个 MTP 模块，其嵌入层与主模型共享。合并后的 $h_t^k$ 作为第 k 个深度上 Transformer 块的输入，以在当前深度生成输出表示 $h_t^k$: $$\rh_{1:T-k}^k = \\text{TRM}_k(h_{1:T-k}^k), \\tag{22}\r$$ 其中 T 代表输入序列长度，而 $_{1:T-k}$ 表示切片操作 (包含左右边界)。最后，将 $h_T^k$ 作为输入，共享输出头将计算第 k 个额外预测 token 的概率分布 $p_{t+k+1}^k \\in \\mathbb{R}^V$，其中 $V$ 是词汇表的大小: $$\rp_{t+k+1}^k = \\text{OutHead}(h_T^k). \\tag{23}\r$$ 输出头 OutHead(·) 将表示线性映射到 logits，随后应用 Softmax 函数来计算第 k 个额外 token 的预测概率。此外，对于每个 MTP 模块，其输出头与主模型共享。DeepSeek-V3 维持预测因果链的原则与 EAGLE (Li et al., 2024b) 的原则相似，但其主要目标是推测解码 (Leviathan et al., 2023; Xia et al., 2023)，DeepSeek-V3 而 利用 MTP 来改进训练。\nMTP Training Objective. 对于每个预测深度，计算一个交叉熵损失 $\\mathcal{L}_{\\text{MTP}}^k$：\n$$\r\\mathcal{L}_{\\text{MTP}}^k = \\text{CrossEntropy}(P_{2+k:T+1}^k, t_{2+k:T+1}) = -\\frac{1}{T}\\sum_{i=2+k}^{T+1} \\log p_i^k[t_i], \\tag{24}\r$$ $T$: 输入序列长度 $t_i$: 第 i 个位置的真实 (ground-truth) token $p_i^k[t_i]$: 代表第 $k$ 个 MTP 模块对于第 $i$ 个位置的预测中，赋给真实正确 token $t_i$** 的概率。 最后计算所有深度的 MTP 损失的平均值，并乘以一个加权因子 $\\lambda$ 来获得总体的 MTP 损失 $\\mathcal{L}_{\\text{MTP}}$，作为 DeepSeek-V3 的一个额外训练目标：\n$$\r\\mathcal{L}_{\\text{MTP}} = \\frac{\\lambda}{D} \\sum_{k=1}^{D} \\mathcal{L}_{\\text{MTP}}^k. \\tag{25}\r$$MTP in Inference. MTP 策略主要旨在提升主模型的性能，因此在推理过程中可以直接丢弃 MTP 模块，主模型可以独立且正常地运作。此外，也可以重新利用这些 MTP 模块进行推测解码 (speculative decoding) ，以进一步改善生成延迟。\n3. Infrastructure 3.1 Compute Clusters DeepSeek-V3 在 2048 NVIDIA H800 GPU 组成的集群上训练。每个节点有 8 张通过 NVLink 和 NVSwitch 连接的 H800. 节点之间通过 InfiniBand (IB) 连接。\n3.2 Training Framework DeepSeek-V3 使用 16-way Pipeline Parallelism (PP), 横跨 8 个节点间的 64-way Expert Parallelism (EP) 以及 ZeRO-1 Data Parallelism (DP). 训练期间不使用 Tensor Parallelism (TP).\n3.2.1 DualPipe and Computation-Communication Overlap DeepSeek-V3 中专家并行导致的跨节点 All-to-all 通信所对应的计算通信比接近 1:1，效率很低。\nDualPipe 的核心思想是在一对独立的 forward \u0026 backword chunk 内部重叠计算和通信。具体来说，将每个 chunk 分为四个部分: attention, all-to-all dispatch， MLP 和 all-to-all combine. 特别地，对于 backword chunk, attention 和 MLP 都像在 ZeroBubble (Qi et al., 2023b) 中一样，被进一步拆分为两个部分：针对输入的反向传播和针对权重的反向传播。此外，还有一个流水线并行通信组件。如下图所示，对于一对 forward \u0026 backword chunk，重排这些组件，并手动调整专用于通信与计算的 GPU SM 的比例。通过这种重叠策略，可以确保 all-to-all 和 PP 通信在执行期间都能够被完全隐藏。\nOverlapping Strategy for a Pair of Individual Forward and Backward Chunks\n基于这种高效的重叠策略，完整的 DualPipe 调度方案如下图所示。它采用了一种双向流水线调度，即同时从流水线的两端送入 micro-batches，从而使得一大部分通信可以被完全重叠。这种重叠还确保随着模型规模的进一步扩大，只要保持恒定的计算与通信比率，仍然可以在节点间使用细粒度的专家 (fine-grained experts)，同时实现接近于零的all-to-all通信开销。具体的分析见相关文章。\nDualPipe Schedule\n3.2.2 Efficient Implementation of Cross-Node All-to-All Communication DeepSeek-V3 定制了高效的跨节点 All-to-all 通信内核，以节省专用于通信的 SM 数量。内核的实现与MoE门控算法和 DeepSeek-V3 集群的网络拓扑共同设计。集群中跨节点 GPU 通过 IB(50 GB/s) 全连接，节点内通信通过 NVLink(160GB/s) 处理。为了有效地利用 IB 和 NVLink 的不同带宽，每个 token 限制最多被 dispatch 到 4 个节点以减少 IB 流量。\n经过测试每个 token 在每个节点平均选择 3.2 个专家的同时不会产生额外的 NVLink 通信开销。意味着虽然 DeepSeek-V3 虽然实际上只选择 8 个路由专家，但它可以在保持相同通信成本的情况下最多选择 13 个专家 (4 节点x 3.2 专家/节点). 在这种通信策略下，仅 20 个 SMs 就足以充分利用 IB 和 NVLink 的带宽。详细地说，DeepSeek-V3 采用了 warp specialization 技术，并将 20 个 SMs 划分为 10 个通信通道。在 dispatch 过程中的通信链路为 (1)IB发送，(2) IB-to-NVLink 转发，(3) NVLink 接收由各自的 warp 处理。combine 过程则是相反的通信链路。\n3.2.3 Extremely Memory Saving with Minimal Overhead DeepSeek-V3 采取了如下技术来减少训练过程中的内存占用。\n重计算 RMSNorm 和 MLA 升维投影。 Exponential Moving Average (EMA) 参数被存放在 CPU 中并且异步更新。 MTP 的 Embedding 和输出头在 PP rank 相同的设备上是共享的。 3.3 FP8 Training 低精度计算在累加过程中容易出现的问题有:\n溢出 (Overflow): 当许多数字相加时，它们的和很容易会超出 FP8 格式所能表示的最大值。 精度损失 (Precision Loss/Underflow): 在累加过程中，如果一个很大的中间和与一个很小的乘积相加，这个很小的乘积可能会因为精度限制而被吞掉，直接变成零，对最终结果毫无贡献。 DeepSeek-V3 引入了一种细粒度的量化策略: $1\\times N_c$ 元素的 tile 分组或 $N_cN_c\\times N_c$ 元素的 block 分组。并且在其设计的高精度累加过程过程中，相关的反量化开销在很大程度上得到了缓解。此外，为了进一步减少 MoE 训练中的内存和通信开销，DeepSeek-V3 用 FP8 格式缓存和 dispatch 激活，以 BF16 格式存储低精度优化器状态。相较于 BF16 baseline, FP8 训练的相对误差低于 0.25%.\n3.3.1 Mixed Precision Framework 如图中所示 Fprop(forward pass), Dgrad(activation backward pass) 以及 Wgrad(weight backward pass) GEMM 操作的输入是 FP8 格式，输出为 BF16 或者 FP32 格式。以 FP8 格式进行 Wgrad 允许激活也以 FP8 格式进行存储，减少了内存占用。\nThe Overall Mixed Precision Framework with FP8 Data Format\n一些低开销的算子可以使用更高精度并且对训练开销的影响可以忽略不计。DeepSeek-V3 对这些模块使用原格式进行运算：Embedding，输出头，MoE 门控，归一化操作以及 Attention 操作。同时为了数值稳定性，以更高精度存储 master weights(FP32), weight gradients(FP32) \u0026 optimizer states(BF16). 这些高精度部分带来的内存开销可以被 DP 减轻。\n3.3.2 Improved Precision from Quantization and Multiplication DeepSeek-V3 使用了如下技术来提高低精度训练的准确性:\nAs a standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes lowprecision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy.\nFine-Grained Quantization. 如下图 所示 DeepSeek-V3 采取更细粒度的方式对输入进行缩放到 FP8 的表示范围: (1) 对于激活以 1x128 tile 进行分组和缩放 (每个 token 的 128 通道为一组); (2) 对于权重以 128x128 进行分组和缩放 (每 128 个输入和输出通道为一组). 虽然原生的 FP8 GEMM 不支持对 reduction 维度进行按组缩放，但可以和下面介绍的 FP32 累加策略进行配合使用。\nFine-grained Quantization\nIncreasing Accumulation Precision. NVIDIA H800 GPU 上的 FP8 GEMM 累加精度被限制在 14 bits (远低于 FP32). 为在低精度计算中确保最终的数值精度，DeepSeek-V3 采用了一种结合 Tensor Cores 与 CUDA Cores 的混合计算流程。首先利用 Tensor Cores 的高吞吐量特性来执行 MMA (Matrix Multiply-Accumulate) 运算，中间结果在硬件原生的有限位宽累加器中进行阶段性累加。当累加操作进行 $N_c$ 次后，所产生的部分和将被立即复制到 CUDA Cores 上的 FP32 寄存器中，并与各自对应的细粒度量化缩放因子相乘，从而在执行全精度 FP32 最终累加的同时，高效地完成了反量化操作。这样能将反量化开销无缝融入到高精度累加步骤中，从而以最小的性能代价保证了最终结果的精确性。\nIncreasing Accumulation Precision\n在 H800 架构上，典型的情况是两个 WGMMA 同时存在，当一个 warp group 执行 promotion 到 CUDA Core 操作时，另一个 warp group 能够执行 MMA 操作。实验中取 $N_c=128$，对应于 4 个 WGMMA.\nMantissa over Exponents. 对所有高精度的张量使用 4EM3 格式。\nHow to Compute Float Point Value 一个常规浮点数 (即非零、非无穷大等特殊值) 的计算公式为：\n$$\r\\text{Value} = (-1)^S \\times (1.M)_{\\text{binary}} \\times 2^{(E_{\\text{decimal}} - \\text{Bias})}\r$$要理解这个公式， DeepSeek-V3 需要拆解里面的三个关键部分：符号、尾数和指数。\n确定符号 (Sign)\nS = 0，则数值为正，$(-1)^0 = 1$ S = 1，则数值为负，$(-1)^1 = -1$ 计算尾数的值 (Mantissa)\n不能直接使用 M 的二进制值。在常规浮点数中，标准规定尾数部分永远以 1. 开头。这样，这个 1 就不需要实际存储，从而可以节省一个比特位来提高精度。因此，尾数的实际值是 (1.M) 的二进制形式。\n假设尾数位是 $m_1 m_2 m_3 \\dots$，其代表的小数值为\n$$\rm_1 \\times 2^{-1} + m_2 \\times 2^{-2} + m_3 \\times 2^{-3} + \\dots\r$$.\n最终尾数项的值为 $1 + (m_1 \\times 2^{-1} + m_2 \\times 2^{-2} + m_3 \\times 2^{-3} + \\dots)$.\n计算指数的值 (Exponent) 指数部分也不能直接使用。为了能表示正、负指数，引入了偏置值 (Bias). 首先从 E 的二进制值计算出其十进制值 $E_{\\text{decimal}}$ 后减去 Bias($2^{k-1} - 1$). 其中 k 是指数位的比特数。\n对于 E4M3 (k=4)，Bias = $2^{(4-1)} - 1 = 2^3 - 1 = 7$. 对于 E5M2 (k=5)，Bias = $2^{(5-1)} - 1 = 2^4 - 1 = 15$. 特殊值说明 当指数 E 全为 0或全为 1 时，代表的是一些特殊值，计算规则也不同: E 全为 0: 如果 M 也全为 0，代表零 (Zero). 如果 M 不为 0，代表非规格化数 (Subnormal Numbers)，计算公式变为 $(-1)^S \\times (0.M) \\times 2^{(1 - \\text{Bias})}$，此时没有隐含的 1. E 全为 1: 如果 M 全为0，代表无穷大 (Infinity)。 如果 M 不为0，代表NaN(Not a Number). Online Quantization. 采用 online 方式计算每个 1x128 激活 tile 和 128x128 权重 block 的最大绝对值。\n3.3.3 Low-Precision Storage and Communication Low-Precision Optimizer States. 用 BF16 格式存储 AdamW 优化器的一阶和二阶动量。优化器存储的 master weights 和 betch 的累加梯度仍以 FP32 格式存储。\nLow-Precision Activation. 大部分激活以 FP8 格式存储，但以下这些是例外。\nInputs of the Linear after the attention operator. 这些激活会在反向传播过程中作为 attention 的输入，对精度比较敏感，因此采用 E5M6 格式存储。量化过程的缩放因子被限制为 2 的整数次幂。 Inputs of the SwiGLU operator in MoE. 以 FP8 格式存储 SwiGLU 的输入然后再反向传播中重计算。 Low-Precision Communication. 在 dispatch 之前对 MoE up-projections 的输入进行 FP8 量化。专家接收到 FP8 数据后，可以直接进行兼容的 FP8 前向传播。量化过程的缩放因子被限制为 2 的整数次幂。在反向传播进入 MoE down-projections 之前同样使用该策略。前向传播和反向传播 combine 后的结果以 FP16 格式存储。\n3.4 Inference and Deployment 为了同时保证 Service-Level Objective (SLO) 和高吞吐量, prefilling 和 decoding 阶段采用了不同的部署策略。\nprefilling 阶段的部署单元为 4 个节点 (32 GPUs). 并行策略如下\nattention part: 采用带有 Sequence Parallel (SP) 的 4-way Tensor Parallel (TP4)，并且和 8-way Data Parallelism (DP8) 一起使用。 MoE part: 采用 32-way Expert Parallelism (EP32), shallow layer 不使用 TP. 其他部署细节:\nredundant experts: 部署 32 高负载的专家 (每十分钟统计一次进行调整) 副本。每个 GPU 除了有自己的 8 个专家之外还有 1 个高负载专家。 同时处理两个计算量差不多的 micro-batches，来掩盖 All-to-all 和 TP 的通信。即将一个 micro-batch 的 attention+MoE 和另一个 batch 的 dispatch+combine 重叠。 dynamic redundancy: 每个 GPU 上放置 16 个专家，但每次只有 9 个被激活。 decoding 阶段的部署单元为 40 个节点 (320 GPUs). 并行策略如下\nattention part: 采用带有 SP 的 TP4，并且和 DP80 一起使用。 MoE part: 采用 EP320. 256 GPU 被用来放置路由专家，64 GPU 被用来放置共享专家和冗余专家。 All-to-all 通过 IB 进行点对点直接传输。同时利用 IBGDA 技术让网卡直接读写 GPU 内存。系统会根据流量统计周期性地判断哪些常规路由专家是当前最热门的，然后动态地让那 64 个GPU去扮演这些热门专家的副本。因为每个 GPU 只被放置一个专家，所以当需要更改冗余策略时系统只需要改变路由逻辑，不需要在物理上移动或重新加载模型权重。\n在 decoding 过程中 attention 会耗费更多时间。因此将一个 micro-batch 的 attention 和另一个的 dispatch+MoE+combine 重叠。decoding 阶段每个 GPU 只需要加载一个专家的参数，因此可以分配更多的 SM 给 attention 部分来加速其计算。\n3.5 Suggestions on Hardware Design 基于 All-to-all 实现和 FP8 训练框架，DeepSeek-V3 对 AI 硬件厂商提出了一些建议。\n3.5.1 Communication Hardware 当前通信算子的实现依赖于 SM，DeepSeek-V3 使用了 20 个 H800 SMs (一共 132 个) 用于通信，但使用 SM 进行通信会导致 tensor core 利用率很低。\n当前 SM 主要在 All-to-all 通信中执行以下任务:\nIB 和 NVLink 域之间的数据转发，将目的地为同一节点内多个不同 GPU 的流量，首先汇聚到单个代理GPU上。 在 RDMA 缓冲区 (已注册的 GPU 内存区域) 与模型的输入/输出缓冲区之间进行数据搬运。 为 All-to-all 通信的 combine 阶段执行 reduce 操作。 在需要跨越 IB 和 NVLink 网络域、向多个不同专家进行分块数据传输在一个 GPU上 的tokens，其中一些可能要去当前节点内的专家 (通过NVLink)，另一些则要去其他节点上的专家 (通过IB). 在发送之前，GPU必须在自己的内存里进行一次数据重排，把所有目的地是专家 A 的 tokens 打包成一个连续的内存块，所有去专家 B 的 tokens 打包成另一个内存块。\r的过程中，管理细粒度的内存布局。 3.5.2 Compute Hardware Higher FP8 GEMM Accumulation Precision in Tensor Cores. 在目前 NVIDIA Hopper 架构的 Tensor Core 实现中，FP8 GEMM 的累积精度有限。在根据最大指数右移对齐 32 个尾数乘积后，Tensor Core 只使用每个尾数乘积的最高 14 位进行加法，并截断超过此范围的位。将加法结果累加到寄存器中也采用 14 位精度。\nSupport for Tile- and Block-Wise Quantization. 目前的 GPU 只支持逐张量量化，缺乏对细粒度量化的原生支持，比如 DeepSeek 的 tile 量化和 block 量化。在当前的实现中，当累加 $N_c$ 次时，部分结果将从 Tensor Core 复制到 CUDA Core，乘以缩放因子，并累加到 CUDA Core 上的FP32 寄存器。尽管与精确的 FP32 累加策略相结合，反量化开销显着减轻，但 Tensor Core 和 CUDA Core 之间频繁的数据移动仍然限制了计算效率。\nSupport for Online Quantization. 当前情况下需要从 HBM 中读取 128 个 BF16 激活值 (之前计算的输出) 进行量化，然后将量化后的 FP8 值写回 HBM，然后再次读取以进行 MMA.\nSupport for Transposed GEMM Operations. 在当前工作流程中，前向传播的激活被量化为 1x128 FP8 tile 并存储。在反向传播中，矩阵需要被读出、反量化、转置、重新量化为 128x1 tile，并存储在 HBM 中。\nDeepSeek-V3 的预训练阶段围绕着高质量的数据构建、精心设计的超参数、长上下文扩展以及全面的性能评测展开。\n4. Pretraining 4.1 Data Construction 训练语料: 模型在一个包含 14.8T 高质量、多样化 token 的语料库上进行预训练。 与 DeepSeek-V2 相比，新语料提升了数学和编程相关样本的比例，并扩展了除中英文之外的多语言覆盖范围。 数据处理流程经过优化，旨在最小化冗余，同时保持语料的多样性。 FIM 策略: 模型训练中采用了 FIM (Fill-in-Middle) 策略，该策略被证明在不损害常规“下一词预测”能力的同时，赋予了模型根据上下文准确预测中间文本的能力。 FIM 策略在文档层面以 10% 的应用率实施，并采用 Prefix-Suffix-Middle (PSM) 框架构建数据格式。 分词器: 分词器采用 Byte-level BPE，词汇表大小扩展至 128K. 为了优化多语言压缩效率，对预分词器和训练数据进行了修改。 为了解决因合并标点和换行符可能导致的 token边界偏差，训练中会随机拆分一部分这类组合 token. 4.2 Hyper-Parameters 模型结构超参数: 总共有 61 层 Transformer，隐藏层维度为 7168. MLA 注意力头数 $n_h$ 为128，每个头的维度为 128. KV 压缩维度 $d_c$ 为 512，Query 压缩维度 $d_c^{'}$ 为1536. 除了前三层，其余所有 FFN 都被替换为 MoE 层。 每个 MoE 层包含 1个共享专家 和 256个路由专家。每个 token 会激活其中的 8个 路由专。 采用 MTP 策略，预测深度为 1，即除了下一个词，还会额外再预测一个词。 最终模型总参数量为 671B，每个 token 的激活参数量为 37B. 训练超参数: 优化器采用 AdamW，其中 $\\beta_{1}=0.9, \\beta_{2}=0.95$，权重衰减为 0.1. 预训练阶段的最大序列长度为 4K. 学习率调度：先在 2K 步内线性增长至 $2.2\\times10^{-4}$，保持该速率直到消耗10T token，然后在 4.3T token 内余弦衰减至 $2.2\\times10^{-5}$，最后在 500B token 的训练中进一步调整。 采用了批次大小调度策略，从 3072 逐步增加到15360. 路由机制被限制为每个 token 最多发送到 4 个节点，以平衡负载。 负载均衡策略主要采用 auxiliary-loss-free，偏置更新速率 $\\gamma$ 在前 14.3 Token 时为 0.001，后 500B token 时为 0.0. 对于序列级平衡损失 $\\alpha=0.00001$，以防止单一样本内的极端不平衡。 MTP loss 权重 $\\lambda$ 对于前 10T token 为 0.3，对于后 4.8T token 为 0.1. 4.3 Long Context Extension 扩展方法: 采用与 DeepSeek-V2 类似的方法，在预训练后应用 YaRN 技术进行上下文扩展。 扩展阶段: 分为两个阶段，分别将上下文窗口从 4K 扩展到 32K，再进一步扩展到 128K. 效果验证: 通过大海捞针 (Needle In A Haystack) 测试表明，模型在高达 128K 的完整上下文长度内均表现出色且稳定。 4.4 Evaluations 评测范围: 主要在中英文基准测试以及一个多语言基准上进行评测，与当前最先进的开源基础模型进行比较，如 DeepSeek-V2-Base, Qwen2.5 72B Base, 和 LLaMA-3.1 405B Base. 评测结果: DeepSeek-V3-Base 全面超越了 DeepSeek-V2-Base 和 Qwen2.5 72B Base，并在绝大多数基准上超过了 LLaMA-3.1 405B Base，成为当前最强的开源模型。 与拥有 11 倍激活参数量的 LLaMA-3.1 405B Base 相比，DeepSeek-V3-Base 在多语言、代码和数学基准上表现出好得多的性能。 在英语和中文语言基准上，DeepSeek-V3-Base 也展现出有竞争力或更好的性能。 训练效率: 得益于高效的架构和工程优化，DeepSeek-V3 的训练效率极高。每训练 1T token 仅需 180K H800 GPU 小时，远比训练 72B 或 405B 的密集模型便宜。 4.5 Discussion 本节通过一系列消融实验，深入探讨了模型采用的两个关键新策略的有效性，并对负载均衡的不同实现方式进行了对比分析。\n4.5.1 Ablation Studies for Multi-Token Prediction 实验设置: 在两个不同规模 (一个15.7B，一个228.7B) baseline MoE模型上进行验证。 对比模型在 baseline 模型的基础上增加了一个预测深度为 1 的MTP模块，其他设置 (如训练数据、架构) 保持不变。 为了保证公平比较，在推理阶段会丢弃MTP模块，因此对比模型的推理成本完全相同。 实验结论: 实验结果 (Table 4) 表明，MTP策略在绝大多数评测基准上都能稳定地提升模型性能。 例如，在大型模型上，HumanEval (代码生成) 和 GSM8K (数学推理) 等任务的性能得到了显著提升。 4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy 实验设置: 同样在两个不同规模 (一个小型15.7B，一个大型228.7B) baseline MoE 模型上进行验证。 baseline 模型完全依赖传统的辅助损失函数来促进专家负载均衡。 对比模型则移除了所有辅助损失，并引入了 Auxiliary-Loss-Free 的均衡策略，其他设置保持一致。 实验结论: 实验结果 (Table 5) 显示，Auxiliary-Loss-Free 策略在绝大多数评测基准上都取得了比纯辅助损失方法更好的模型性能。 在代码和数学等任务上，性能提升尤为明显。 4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance 核心区别: Auxiliary-Loss-Free 策略是在整个训练批次 (batch-wise) 上实现均衡，而传统的辅助损失则是在每个序列 (sequence-wise) 内部强制实现均衡。 理论优势: 批次级的均衡约束更为灵活，它不强制每个序列内部的专家使用频率都一样，从而允许专家更好地专精于特定领域 (如代码、数学等). 实验验证: 通过分析模型在不同领域数据上的专家负载，观测到 Auxiliary-Loss-Free 模型展现出了更强的专家特化模式。 进一步的实验表明，只要能实现相似水平的批次级负载均衡，无论是使用 Auxiliary-Loss-Free 方法还是新设计的批次级 Auxiliary-Loss-Free 方法，都能达到相似的优异模型性能，且均优于序列级辅助损失方法。 潜在挑战与解决方案: 批次级均衡可能面临两个挑战：单个序列或小批次内的负载不均，以及推理时因领域切换导致的负载不均。 第一个挑战通过使用大规模的专家并行和数据并行 (确保了每个微批次的规模足够大) 得以自然解决。 第二个挑战则通过在推理部署中采用冗余专家策略来克服。 5. Post-Training 后训练阶段旨在将预训练好的基础模型与人类偏好对齐，并进一步解锁其潜力。该阶段主要包括监督微调 (SFT) 和强化学习 (RL)，并涉及从 DeepSeek-R1 系列模型中蒸馏推理能力。\n5.1 Supervised Fine-Tuning, SFT 数据集构建: SFT 数据集包含 150 万个实例，涵盖多个领域。 推理数据: 对于数学、代码、逻辑等推理任务，利用内部的 DeepSeek-R1 模型生成数据。虽然 R1 生成的数据准确性高，但存在过度思考、格式不佳和长度过长等问题。为了平衡准确性与简洁性，SFT 训练中会混合使用原始应答和经过精心设计的系统提示词引导下的 R1 应答。 非推理数据: 对于创意写作、角色扮演等任务，使用 DeepSeek-V2.5 生成应答，并由人类标注员进行验证。 SFT 设置: 模型在 SFT 数据集上微调了 2 个 epoch. 学习率采用余弦衰减策略，从 $5\\times10^{-6}$ 降至 $1\\times10^{-6}$. 训练序列由多个样本打包而成，但采用样本掩码策略确保样本间相互隔离。 5.2 Reinforcement Learning 5.2.1 Reward Model RL 过程采用了 Rule-Based 模型和 Model-Based 的奖励模型。\nRule-Based RM: 用于有明确验证规则的问题，如数学题的确定性答案或代码题的单元测试结果。这种方式可靠性高，不易被模型钻空子。 Model-Based RM: 用于答案更开放、没有确定性对错的问题。该奖励模型由 DeepSeek-V3 的 SFT 版本训练而来，并通过包含思维链的偏好数据进行训练，以降低 reward hacking 的风险。 5.2.2 GRPO 采用了 GRPO (Group Relative Policy Optimization) 算法进行强化学习。 GRPO 的一个特点是它不需要一个与策略模型同等大小的 critic 模型，而是从一组采样输出的分数中估计 baseline. RL 过程融合了来自编码、数学、写作、角色扮演等不同领域的提示词，这不仅使模型与人类偏好更对齐，也提升了在 SFT 数据有限场景下的基准测试性能。 5.3 Evaluations 评测设置: 除了基础模型评测用的基准外，进一步在 IFEval, GPQA, LongBench v2, SWE-Bench Verified, Aider, Codeforces, AIME 2024 等更具挑战性的基准上进行评估。 对比的 baseline 模型包括其他强大的开源和闭源模型，如 Qwen2.5-72B-Inst, LLaMA-3.1-405B-Inst, Claude-3.5-Sonnet, 和 GPT-4o-0513。 Standard Evaluation: 评测结果 (Table 6) 显示，DeepSeek-V3 是表现最好的开源聊天模型。 在知识基准 (MMLU, MMLU-Pro, GPQA-Diamond) 上，其性能与顶级的闭源模型相当或相近。 在长上下文理解基准 (DROP, LongBench v2, FRAMES) 上，表现出顶级水平，例如在 DROP 上取得了 91.6 的 F1 分数，超越了所有其他模型。 在代码和数学基准上表现卓越，尤其是在 AIME, MATH-500 等高难度数学竞赛基准上，绝对得分领先第二名约 10%，优势巨大。 在中文基准上，如 C-SimpleQA，其表现也超越了包括 Qwen2.5 在内的其他模型。 Open-Ended Evaluation: 在 Arena-Hard 基准测试中，DeepSeek-V3 取得了超过 85% 的胜率，与顶级的 Claude-3.5-Sonnet-1022 表现持平，成为首个在该基准上突破 85% 的开源模型。 在 AlpacaEval 2.0 上，其表现同样出色，超越了所有对比的开源和闭源模型。 作为奖励模型的能力: 在 RewardBench 基准上评测其作为奖励模型的判断能力，结果显示 DeepSeek-V3 与最新版本的 GPT-4o 和 Claude-3.5-Sonnet 表现相当。 5.4 Discussion 从 DeepSeek-R1 蒸馏知识: 消融实验 (Table 9) 证明，从长思维链 (long-CoT) 模型 DeepSeek-R1 中蒸馏知识的策略非常有效，显著提升了模型在 LiveCodeBench 和 MATH-500 上的性能。 实验也揭示了一个权衡：蒸馏带来了性能提升，但也显著增加了回应的平均长度。因此，在 DeepSeek-V3 的开发中对蒸馏设置进行了仔细选择以求平衡。 Self-Rewarding: 在缺乏明确验证规则的通用场景中，模型开发采用了 constitutional AI 的方法，即使用 DeepSeek-V3 自身的投票评估结果作为反馈源来进行优化。 这种自奖励的范式产生了显著的对齐效果，并被认为是实现LLM自我改进的重要方向。 MTP 评测: 模型采用的 MTP 技术可以预测第 2 个token. 评测显示，这个额外预测的 token 的接受率在 85%-90%之间。 结合 speculative decoding 框架，这个高接受率使得模型的解码速度 (TPS) 提升了1.8倍. ",
  "wordCount" : "9396",
  "inLanguage": "en",
  "datePublished": "2025-06-20T16:37:06+08:00",
  "dateModified": "2025-06-22T17:53:30+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/deepseek/">DeepSeek Related</a></div>
    <h1 class="post-title entry-hint-parent">
      DeepSeek-V3 Technical Report
    </h1>
    <div class="post-description">
      Paper Reading of DeepSeek-V3 Technical Report
    </div>
    <div class="post-meta"><span title='2025-06-20 16:37:06 +0800 CST'>Jun-20-2025</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;9396 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                    <li>
                        <a href="#1-introduction" aria-label="1. Introduction">1. Introduction</a></li>
                    <li>
                        <a href="#2-architecture" aria-label="2. Architecture">2. Architecture</a><ul>
                            
                    <li>
                        <a href="#21-mla" aria-label="2.1 MLA">2.1 MLA</a></li>
                    <li>
                        <a href="#22-deepseekmoe-with-auxiliary-loss-free-load-balancing" aria-label="2.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing">2.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing</a></li>
                    <li>
                        <a href="#23-multi-token-prediction" aria-label="2.3 Multi-Token Prediction">2.3 Multi-Token Prediction</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-infrastructure" aria-label="3. Infrastructure">3. Infrastructure</a><ul>
                            
                    <li>
                        <a href="#31-compute-clusters" aria-label="3.1 Compute Clusters">3.1 Compute Clusters</a></li>
                    <li>
                        <a href="#32-training-framework" aria-label="3.2 Training Framework">3.2 Training Framework</a><ul>
                            
                    <li>
                        <a href="#321--dualpipe-and-computation-communication-overlap" aria-label="3.2.1  DualPipe and Computation-Communication Overlap">3.2.1  DualPipe and Computation-Communication Overlap</a></li>
                    <li>
                        <a href="#322-efficient-implementation-of-cross-node-all-to-all-communication" aria-label="3.2.2 Efficient Implementation of Cross-Node All-to-All Communication">3.2.2 Efficient Implementation of Cross-Node All-to-All Communication</a></li>
                    <li>
                        <a href="#323-extremely-memory-saving-with-minimal-overhead" aria-label="3.2.3 Extremely Memory Saving with Minimal Overhead">3.2.3 Extremely Memory Saving with Minimal Overhead</a></li></ul>
                    </li>
                    <li>
                        <a href="#33-fp8-training" aria-label="3.3 FP8 Training">3.3 FP8 Training</a><ul>
                            
                    <li>
                        <a href="#331-mixed-precision-framework" aria-label="3.3.1 Mixed Precision Framework">3.3.1 Mixed Precision Framework</a></li>
                    <li>
                        <a href="#332-improved-precision-from-quantization-and-multiplication" aria-label="3.3.2 Improved Precision from Quantization and Multiplication">3.3.2 Improved Precision from Quantization and Multiplication</a></li>
                    <li>
                        <a href="#333-low-precision-storage-and-communication" aria-label="3.3.3 Low-Precision Storage and Communication">3.3.3 Low-Precision Storage and Communication</a></li></ul>
                    </li>
                    <li>
                        <a href="#34-inference-and-deployment" aria-label="3.4 Inference and Deployment">3.4 Inference and Deployment</a></li>
                    <li>
                        <a href="#35-suggestions-on-hardware-design" aria-label="3.5 Suggestions on Hardware Design">3.5 Suggestions on Hardware Design</a><ul>
                            
                    <li>
                        <a href="#351-communication-hardware" aria-label="3.5.1 Communication Hardware">3.5.1 Communication Hardware</a></li>
                    <li>
                        <a href="#352-compute-hardware" aria-label="3.5.2 Compute Hardware">3.5.2 Compute Hardware</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#4-pretraining" aria-label="4. Pretraining">4. Pretraining</a><ul>
                            
                    <li>
                        <a href="#41-data-construction" aria-label="4.1 Data Construction">4.1 Data Construction</a></li>
                    <li>
                        <a href="#42-hyper-parameters" aria-label="4.2 Hyper-Parameters">4.2 Hyper-Parameters</a></li>
                    <li>
                        <a href="#43-long-context-extension" aria-label="4.3 Long Context Extension">4.3 Long Context Extension</a></li>
                    <li>
                        <a href="#44-evaluations" aria-label="4.4 Evaluations">4.4 Evaluations</a></li>
                    <li>
                        <a href="#45-discussion" aria-label="4.5 Discussion">4.5 Discussion</a><ul>
                            
                    <li>
                        <a href="#451-ablation-studies-for-multi-token-prediction" aria-label="4.5.1 Ablation Studies for Multi-Token Prediction">4.5.1 Ablation Studies for Multi-Token Prediction</a></li>
                    <li>
                        <a href="#452-ablation-studies-for-the-auxiliary-loss-free-balancing-strategy" aria-label="4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy">4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy</a></li>
                    <li>
                        <a href="#453-batch-wise-load-balance-vs-sequence-wise-load-balance" aria-label="4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance">4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#5-post-training" aria-label="5. Post-Training">5. Post-Training</a><ul>
                            
                    <li>
                        <a href="#51-supervised-fine-tuning-sft" aria-label="5.1 Supervised Fine-Tuning, SFT">5.1 Supervised Fine-Tuning, SFT</a></li>
                    <li>
                        <a href="#52-reinforcement-learning" aria-label="5.2 Reinforcement Learning">5.2 Reinforcement Learning</a><ul>
                            
                    <li>
                        <a href="#521-reward-model" aria-label="5.2.1 Reward Model">5.2.1 Reward Model</a></li>
                    <li>
                        <a href="#522-grpo" aria-label="5.2.2 GRPO">5.2.2 GRPO</a></li></ul>
                    </li>
                    <li>
                        <a href="#53-evaluations" aria-label="5.3 Evaluations">5.3 Evaluations</a></li>
                    <li>
                        <a href="#54-discussion" aria-label="5.4 Discussion">5.4 Discussion</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<p>DeepSeek-V3 (671B) 是 MoE 模型，每个 token 会激活 37B 的参数。采用 Multi-head Latent Attention (MLA) 和自创的 DeepSeek MoE 结构，在这两篇文章中已经做过讲解。同时采用了 auxiliary-loss-free 策略来实现专家负载平衡并且使用了 Multi-token Prediction (MTP) 来加速训练。整个预训练数据集一共有 14.8T tokens，通过 Suprvised Fine-Tuning (SFT) 和 强化学习来加强性能。训练时长为 2.788M H800 GPU 小时。</p>
<h1 id="1-introduction">1. Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h1>
<p><strong>模型架构创新:</strong></p>
<ul>
<li>Multi-head Latent Attention (MLA): 加速推理。</li>
<li>DeepSeek MoE: 减少训练开销。</li>
</ul>
<p><strong>增强模型能力的策略:</strong></p>
<ul>
<li>auxiliary-loss-free: 实现负载平衡。</li>
<li>Multi-token Prediction (MTP): 增强模型表现。</li>
</ul>
<p><strong>提高训练效率的方法:</strong></p>
<ul>
<li>FP8 混合精度训练: 加速训练和减少 GPU 内存使用。</li>
<li>DualPipe 流水线并行算法: 减少气泡并且在训练时候通过计算掩盖了大部分通信。</li>
<li>新的节点间 All-to-all 通信算子: 更好地利用 InfiniBand (IB) and NVLink 带宽。</li>
<li>优化了内存后 DeepSeek-V3 训练没有使用 TP.</li>
</ul>
<p><strong>训练过程:</strong></p>
<ul>
<li>pre-training: 在 14.8T tokens 上进行。</li>
<li>stage 1: 扩展最大上下文长度到 32K.</li>
<li>stage 2: 扩展最大上下文长度到 128K.</li>
<li>post-training: 使用 Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) 并且蒸馏了 DeepSeek-R1 系列模型来获得推理能力。</li>
</ul>
<p>DeepSeek-V3 上训练每 1T token 只需要180K H800 GPU小时，即在 2048 个 H800 GPU 的集群上需要 3.7 天。</p>
<h1 id="2-architecture">2. Architecture<a hidden class="anchor" aria-hidden="true" href="#2-architecture">#</a></h1>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB4af4bca65c0e55a9290c2e4d808cb6b2?method=download&amp;shareKey=d8c9ce5c9b545f9d954d769f4e520fed" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB4af4bca65c0e55a9290c2e4d808cb6b2?method=download&amp;shareKey=d8c9ce5c9b545f9d954d769f4e520fed" alt="Illustration of the Basic Architecture of DeepSeek-V3">
    </a><figcaption>Illustration of the Basic Architecture of DeepSeek-V3</figcaption></figure></p>
<h2 id="21-mla">2.1 MLA<a hidden class="anchor" aria-hidden="true" href="#21-mla">#</a></h2>
<p>在相关文章中已经介绍。</p>
<h2 id="22-deepseekmoe-with-auxiliary-loss-free-load-balancing">2.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing<a hidden class="anchor" aria-hidden="true" href="#22-deepseekmoe-with-auxiliary-loss-free-load-balancing">#</a></h2>
$$
\begin{align}
\mathbf{h}'_t &= \mathbf{u}_t + \sum_{i=1}^{N_s} \text{FFN}_i^{(s)} (\mathbf{u}_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}_i^{(r)} (\mathbf{u}_t),  \\
g_{i,t} &= \frac{g'_{i,t}}{\sum_{j=1}^{N_r} g'_{j,t}},  \\
g'_{i,t} &= \begin{cases} s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t}|1 \le j \le N_r\}, K_r), \\ 0, & \text{otherwise}, \end{cases} \\
s_{i,t} &= \text{Sigmoid}(\mathbf{u}_t^T \mathbf{e}_i),
\end{align}
$$<p><strong>Basic Architecture of DeepSeekMoE.</strong> 在相关文章中已经介绍，V3 和 V2 不同之处在于使用sigmoid函数来计算亲和分数，并在归一化所有选定的亲和分数之间来产生门控制值。</p>
<p><strong>Auxiliary-Loss-Free Load Balancing.</strong> 为每个专家引入一个偏差项 $b_i$，并将其与相应的亲和力分数$s_{i,t}$ 相加，以确定 top-K 路由</p>
$$
g'_{i,t} = \begin{cases} s_{i,t}, & s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \le j \le N_r\}, K_r), \\ 0, & \text{otherwise}. \end{cases} \tag{16}
$$<p><strong>这个偏置项仅用于路由</strong>，用于和 FFN 输出相乘的门控值还是来自于原先的原先的 $s_{i,t}$. 在每一步结束时，如果其对应的专家过载， DeepSeek-V3 将偏差项减少 $\gamma$ (一个超参数，被称作 bias update speed)，如果其对应的专家负载不足， DeepSeek-V3 将偏差项增加 $\gamma$.</p>
<p>V3 使用 sequence-wise balance loss，类似于 V2 中 Expert-Level Balance Loss。 不同之处在于使用归一化的亲和分数。</p>
$$
\begin{align}
\mathcal{L}_{\text{Bal}} &= \alpha \sum_{i=1}^{N_r} f_i P_i, \\
f_i &= \frac{N_r}{K_r T} \sum_{t=1}^{T} 1 (s_{i,t} \in \text{Topk}(\{s_{j,t}|1 \le j \le N_r\}, K_r)),  \\
s'_{i,t} &= \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}},  \\
P_i &= \frac{1}{T} \sum_{t=1}^{T} s'_{i,t} 
\end{align}
$$<p><strong>Node-Limited Routing.</strong> 对于每个 token 计算每个节点计算亲和度分数前 $\frac {K_r}M$ 的专家求和，选取前 $M$ 个作为路由节点。</p>
<p><strong>No Token-Dropping.</strong> 训练和推理中均不采用。Token Dropping指的是在 MoE 当路由到某个专家的 Token 数量超过了该专家的处理容量时，系统会故意丢跳过那些超出容量的 token，不让它们被这个专家处理。这些 token 通常会通过一个残差连接，将其输入时的状态直接传递到下一层。</p>
<h2 id="23-multi-token-prediction">2.3 Multi-Token Prediction<a hidden class="anchor" aria-hidden="true" href="#23-multi-token-prediction">#</a></h2>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBcc8ba55fb1b39401e8287ae38a50d829?method=download&amp;shareKey=3fd271dd7007a3fc5b6b0939e165869f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBcc8ba55fb1b39401e8287ae38a50d829?method=download&amp;shareKey=3fd271dd7007a3fc5b6b0939e165869f" alt="Illustration of Multi-Token Prediction (MTP) implementation">
    </a><figcaption>Illustration of Multi-Token Prediction (MTP) implementation</figcaption></figure></p>
<p>与 Gloeckle 等人使用独立的输出头并行预测 D 个额外 token 不同， DeepSeek-V3 顺序预测额外 token 并在每个预测深度保持完整的因果链。</p>
<p><strong>MTP Modules.</strong> DeepSeek-V3 使用 D 个顺序模块来预测 D 个额外 token。第 k 个 MTP 模块由一个共享嵌入层 Emb(·)、一个共享输出头 OutHead(·)、一个 Transformer 块 TRM_k(·) 和一个投影矩阵 $M_k \in \mathbb{R}^{d \times 2d}$ 组成。对于第 <em>i</em> 个输入 token  $t_i$，在第 <em>k</em> 个预测深度， DeepSeek-V3 首先通过线性映射<strong>结合第 <em>i</em> 个 token 在第 (k−1) 个深度上的表示 $h_i^{k-1} \in \mathbb{R}^d$ 和第 (i+k) 个 token 的嵌入 $\text{Emb}(t_{i+k}) \in \mathbb{R}^d$</strong></p>
$$
h_t^k = M_k[\text{RMSNorm}(h_t^{k-1}); \text{RMSNorm}(\text{Emb}(t_{i+k}))], \tag{21}
$$<p>其中 [·;·] 表示拼接操作。特别地，当 k = 1 时，$h_t^{k-1}$ 指的是由主模型给出的表示。请注意，对于每个 MTP 模块，其嵌入层与主模型共享。合并后的 $h_t^k$ 作为第 k 个深度上 Transformer 块的输入，以在当前深度生成输出表示 $h_t^k$:
</p>
$$
h_{1:T-k}^k = \text{TRM}_k(h_{1:T-k}^k), \tag{22}
$$<p>
其中 T 代表输入序列长度，而 $_{1:T-k}$ 表示切片操作 (包含左右边界)。最后，将 $h_T^k$ 作为输入，共享输出头将计算第 k 个额外预测 token 的概率分布 $p_{t+k+1}^k \in \mathbb{R}^V$，其中 $V$ 是词汇表的大小:
</p>
$$
p_{t+k+1}^k = \text{OutHead}(h_T^k). \tag{23}
$$<p>
输出头 OutHead(·) 将表示线性映射到 logits，随后应用 Softmax 函数来计算第 k 个额外 token 的预测概率。此外，对于每个 MTP 模块，其输出头与主模型共享。DeepSeek-V3 维持预测因果链的原则与 EAGLE (Li et al., 2024b) 的原则相似，但其主要目标是推测解码 (Leviathan et al., 2023; Xia et al., 2023)，DeepSeek-V3 而 利用 MTP 来改进训练。</p>
<p><strong>MTP Training Objective.</strong> 对于每个预测深度，计算一个交叉熵损失 $\mathcal{L}_{\text{MTP}}^k$：</p>
$$
\mathcal{L}_{\text{MTP}}^k = \text{CrossEntropy}(P_{2+k:T+1}^k, t_{2+k:T+1}) = -\frac{1}{T}\sum_{i=2+k}^{T+1} \log p_i^k[t_i], \tag{24}
$$<ul>
<li>$T$: 输入序列长度</li>
<li>$t_i$: 第 i 个位置的真实 (ground-truth) token</li>
<li>$p_i^k[t_i]$: 代表第 $k$ 个 MTP 模块对于第 $i$ 个位置的预测中，赋给真实正确 token $t_i$** 的概率。</li>
</ul>
<p>最后计算所有深度的 MTP 损失的平均值，并乘以一个加权因子 $\lambda$ 来获得总体的 MTP 损失 $\mathcal{L}_{\text{MTP}}$，作为 DeepSeek-V3 的一个额外训练目标：</p>
$$
\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{\text{MTP}}^k. \tag{25}
$$<p><strong>MTP in Inference.</strong> MTP 策略主要旨在提升主模型的性能，因此在推理过程中可以直接丢弃 MTP 模块，主模型可以独立且正常地运作。此外，也可以重新利用这些 MTP 模块进行推测解码 (speculative decoding) ，以进一步改善生成延迟。</p>
<h1 id="3-infrastructure">3. Infrastructure<a hidden class="anchor" aria-hidden="true" href="#3-infrastructure">#</a></h1>
<h2 id="31-compute-clusters">3.1 Compute Clusters<a hidden class="anchor" aria-hidden="true" href="#31-compute-clusters">#</a></h2>
<p>DeepSeek-V3 在 2048 NVIDIA H800 GPU 组成的集群上训练。每个节点有 8 张通过 NVLink 和 NVSwitch 连接的 H800. 节点之间通过 InfiniBand (IB) 连接。</p>
<h2 id="32-training-framework">3.2 Training Framework<a hidden class="anchor" aria-hidden="true" href="#32-training-framework">#</a></h2>
<p>DeepSeek-V3 使用 16-way Pipeline Parallelism (PP), 横跨 8 个节点间的 64-way Expert Parallelism (EP) 以及 ZeRO-1 Data Parallelism (DP). 训练期间不使用 Tensor Parallelism (TP).</p>
<h3 id="321--dualpipe-and-computation-communication-overlap">3.2.1  DualPipe and Computation-Communication Overlap<a hidden class="anchor" aria-hidden="true" href="#321--dualpipe-and-computation-communication-overlap">#</a></h3>
<p>DeepSeek-V3 中专家并行导致的跨节点 All-to-all 通信所对应的计算通信比接近 1:1，效率很低。</p>
<p>DualPipe 的核心思想是在一对独立的 forward &amp; backword chunk 内部重叠计算和通信。具体来说，将每个 chunk 分为四个部分: attention, all-to-all dispatch， MLP 和 all-to-all combine. 特别地，对于 backword chunk, attention 和 MLP 都像在 ZeroBubble (Qi et al., 2023b) 中一样，被进一步拆分为两个部分：针对输入的反向传播和针对权重的反向传播。此外，还有一个流水线并行通信组件。如下图所示，对于一对 forward &amp; backword chunk，重排这些组件，并手动调整专用于通信与计算的 GPU SM 的比例。通过这种重叠策略，可以确保 all-to-all 和 PP 通信在执行期间都能够被完全隐藏。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBf348c55ccc87c5e4e388f2df2f18fb76?method=download&amp;shareKey=849010c74a6772b18fff8ca8d5550e8c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBf348c55ccc87c5e4e388f2df2f18fb76?method=download&amp;shareKey=849010c74a6772b18fff8ca8d5550e8c" alt="Overlapping Strategy for a Pair of Individual Forward and Backward Chunks">
    </a><figcaption>Overlapping Strategy for a Pair of Individual Forward and Backward Chunks</figcaption></figure></p>
<p>基于这种高效的重叠策略，完整的 DualPipe 调度方案如下图所示。它采用了一种双向流水线调度，即同时从流水线的两端送入 micro-batches，从而使得一大部分通信可以被完全重叠。这种重叠还确保随着模型规模的进一步扩大，只要保持恒定的计算与通信比率，仍然可以在节点间使用细粒度的专家 (fine-grained experts)，同时实现接近于零的all-to-all通信开销。具体的分析见相关文章。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" alt="DualPipe Schedule">
    </a><figcaption>DualPipe Schedule</figcaption></figure></p>
<h3 id="322-efficient-implementation-of-cross-node-all-to-all-communication">3.2.2 Efficient Implementation of Cross-Node All-to-All Communication<a hidden class="anchor" aria-hidden="true" href="#322-efficient-implementation-of-cross-node-all-to-all-communication">#</a></h3>
<p>DeepSeek-V3 定制了高效的跨节点 All-to-all 通信内核，以节省专用于通信的 SM 数量。内核的实现与MoE门控算法和 DeepSeek-V3 集群的网络拓扑共同设计。集群中跨节点 GPU 通过 IB(50 GB/s) 全连接，节点内通信通过 NVLink(160GB/s) 处理。为了有效地利用 IB 和 NVLink 的不同带宽，每个 token 限制最多被 dispatch 到 4 个节点以减少 IB 流量。</p>
<p>经过测试每个 token 在每个节点平均选择 3.2 个专家的同时不会产生额外的 NVLink 通信开销。意味着虽然 DeepSeek-V3 虽然实际上只选择 8 个路由专家，但它可以在保持相同通信成本的情况下最多选择 13 个专家 (4 节点x 3.2 专家/节点). 在这种通信策略下，仅 20 个 SMs 就足以充分利用 IB 和 NVLink 的带宽。详细地说，DeepSeek-V3 采用了 warp specialization 技术，并将 20 个 SMs 划分为 10 个通信通道。在 dispatch 过程中的通信链路为 (1)IB发送，(2) IB-to-NVLink 转发，(3) NVLink 接收由各自的 warp 处理。combine 过程则是相反的通信链路。</p>
<h3 id="323-extremely-memory-saving-with-minimal-overhead">3.2.3 Extremely Memory Saving with Minimal Overhead<a hidden class="anchor" aria-hidden="true" href="#323-extremely-memory-saving-with-minimal-overhead">#</a></h3>
<p>DeepSeek-V3 采取了如下技术来减少训练过程中的内存占用。</p>
<ul>
<li>重计算 RMSNorm 和 MLA 升维投影。</li>
<li>Exponential Moving Average (EMA) 参数被存放在 CPU 中并且异步更新。</li>
<li>MTP 的 Embedding 和输出头在 PP rank 相同的设备上是共享的。</li>
</ul>
<h2 id="33-fp8-training">3.3 FP8 Training<a hidden class="anchor" aria-hidden="true" href="#33-fp8-training">#</a></h2>
<p>低精度计算在累加过程中容易出现的问题有:</p>
<ol>
<li>溢出 (Overflow): 当许多数字相加时，它们的和很容易会超出 FP8 格式所能表示的最大值。</li>
<li>精度损失 (Precision Loss/Underflow): 在累加过程中，如果一个很大的中间和与一个很小的乘积相加，这个很小的乘积可能会因为精度限制而被吞掉，直接变成零，对最终结果毫无贡献。</li>
</ol>
<p>DeepSeek-V3 引入了一种细粒度的量化策略: $1\times N_c$ 元素的 tile 分组或 $N_cN_c\times N_c$ 元素的 block 分组。并且在其设计的高精度累加过程过程中，相关的反量化开销在很大程度上得到了缓解。此外，为了进一步减少 MoE 训练中的内存和通信开销，DeepSeek-V3 用 FP8 格式缓存和 dispatch 激活，以 BF16 格式存储低精度优化器状态。相较于 BF16 baseline, FP8 训练的相对误差低于 0.25%.</p>
<h3 id="331-mixed-precision-framework">3.3.1 Mixed Precision Framework<a hidden class="anchor" aria-hidden="true" href="#331-mixed-precision-framework">#</a></h3>
<p>如图中所示 Fprop(forward pass), Dgrad(activation backward pass) 以及 Wgrad(weight backward pass) GEMM 操作的输入是 FP8 格式，输出为 BF16 或者 FP32 格式。以 FP8 格式进行 Wgrad 允许激活也以 FP8 格式进行存储，减少了内存占用。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBa301c0983af1cc29da1165ca160c0d3e?method=download&amp;shareKey=92f1e462ac7dc2e7fd40cb3dff511153" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBa301c0983af1cc29da1165ca160c0d3e?method=download&amp;shareKey=92f1e462ac7dc2e7fd40cb3dff511153" alt="The Overall Mixed Precision Framework with FP8 Data Format">
    </a><figcaption>The Overall Mixed Precision Framework with FP8 Data Format</figcaption></figure></p>
<p>一些低开销的算子可以使用更高精度并且对训练开销的影响可以忽略不计。DeepSeek-V3 对这些模块使用原格式进行运算：Embedding，输出头，MoE 门控，归一化操作以及 Attention 操作。同时为了数值稳定性，以更高精度存储 master weights(FP32), weight gradients(FP32) &amp; optimizer states(BF16). 这些高精度部分带来的内存开销可以被 DP 减轻。</p>
<h3 id="332-improved-precision-from-quantization-and-multiplication">3.3.2 Improved Precision from Quantization and Multiplication<a hidden class="anchor" aria-hidden="true" href="#332-improved-precision-from-quantization-and-multiplication">#</a></h3>
<p>DeepSeek-V3 使用了如下技术来提高低精度训练的准确性:</p>
<blockquote>
<p>As a standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes lowprecision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy.</p></blockquote>
<p><strong>Fine-Grained Quantization.</strong> 如下图 所示 DeepSeek-V3 采取更细粒度的方式对输入进行缩放到 FP8 的表示范围: (1) 对于激活以 1x128 tile 进行分组和缩放 (每个 token 的 128 通道为一组); (2) 对于权重以 128x128 进行分组和缩放 (每 128 个输入和输出通道为一组). 虽然原生的 FP8 GEMM 不支持对 reduction 维度进行按组缩放，但可以和下面介绍的 FP32 累加策略进行配合使用。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9db831a1951c8d26e446ee1588f8f55b?method=download&amp;shareKey=fab1571fc5238ec904095a783f855ef3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9db831a1951c8d26e446ee1588f8f55b?method=download&amp;shareKey=fab1571fc5238ec904095a783f855ef3" alt="Fine-grained Quantization">
    </a><figcaption>Fine-grained Quantization</figcaption></figure></p>
<p><strong>Increasing Accumulation Precision.</strong>  NVIDIA H800 GPU 上的 FP8 GEMM 累加精度被限制在 14 bits (远低于 FP32). 为在低精度计算中确保最终的数值精度，DeepSeek-V3 采用了一种结合 Tensor Cores 与 CUDA Cores 的混合计算流程。首先利用 Tensor Cores 的高吞吐量特性来执行 MMA (Matrix Multiply-Accumulate) 运算，中间结果在硬件原生的有限位宽累加器中进行阶段性累加。当累加操作进行 $N_c$ 次后，所产生的部分和将被立即复制到 CUDA Cores 上的 FP32 寄存器中，并与各自对应的细粒度量化缩放因子相乘，从而在执行全精度 FP32 最终累加的同时，高效地完成了反量化操作。这样能将反量化开销无缝融入到高精度累加步骤中，从而以最小的性能代价保证了最终结果的精确性。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB123812d18b874758b234db511dc40e22?method=download&amp;shareKey=982d72c6d03c9bf72d5461d643ad4c65" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB123812d18b874758b234db511dc40e22?method=download&amp;shareKey=982d72c6d03c9bf72d5461d643ad4c65" alt="Increasing Accumulation Precision">
    </a><figcaption>Increasing Accumulation Precision</figcaption></figure></p>
<p>在 H800 架构上，典型的情况是两个 WGMMA 同时存在，当一个 warp group 执行 promotion 到 CUDA Core 操作时，另一个 warp group 能够执行 MMA 操作。实验中取 $N_c=128$，对应于 4 个 WGMMA.</p>
<p><strong>Mantissa over Exponents.</strong> 对所有高精度的张量使用 4EM3 格式。</p>
<details class="custom-details">
    <summary class="custom-summary">How to Compute Float Point Value</summary>
    <div><p>一个常规浮点数 (即非零、非无穷大等特殊值) 的计算公式为：</p>
$$
\text{Value} = (-1)^S \times (1.M)_{\text{binary}} \times 2^{(E_{\text{decimal}} - \text{Bias})}
$$<p>要理解这个公式， DeepSeek-V3 需要拆解里面的三个关键部分：符号、尾数和指数。</p>
<hr>
<ol>
<li>
<p>确定符号 (Sign)</p>
<ul>
<li><code>S = 0</code>，则数值为正，$(-1)^0 = 1$</li>
<li><code>S = 1</code>，则数值为负，$(-1)^1 = -1$</li>
</ul>
</li>
<li>
<p>计算尾数的值 (Mantissa)</p>
</li>
</ol>
<p>不能直接使用 M 的二进制值。在常规浮点数中，标准规定尾数部分永远以 <code>1.</code> 开头。这样，这个 <code>1</code> 就不需要实际存储，从而可以节省一个比特位来提高精度。因此，尾数的实际值是 <code>(1.M)</code> 的二进制形式。</p>
<p>假设尾数位是 $m_1 m_2 m_3 \dots$，其代表的小数值为</p>
$$
m_1 \times 2^{-1} + m_2 \times 2^{-2} + m_3 \times 2^{-3} + \dots
$$<p>.</p>
<p>最终尾数项的值为 $1 + (m_1 \times 2^{-1} + m_2 \times 2^{-2} + m_3 \times 2^{-3} + \dots)$.</p>
<ol start="3">
<li>计算指数的值 (Exponent)</li>
</ol>
<p>指数部分也不能直接使用。为了能表示正、负指数，引入了偏置值 (Bias). 首先从 E 的二进制值计算出其十进制值 $E_{\text{decimal}}$ 后减去 Bias($2^{k-1} - 1$). 其中 k 是指数位的比特数。</p>
<ul>
<li>对于 <strong>E4M3</strong> (k=4)，Bias = $2^{(4-1)} - 1 = 2^3 - 1 = 7$.</li>
<li>对于 <strong>E5M2</strong> (k=5)，Bias = $2^{(5-1)} - 1 = 2^4 - 1 = 15$.</li>
</ul>
<ol start="4">
<li>特殊值说明
当指数 E 全为 0或全为 1 时，代表的是一些特殊值，计算规则也不同:</li>
</ol>
<ul>
<li>E 全为 0:
<ul>
<li>如果 M 也全为 0，代表零 (Zero).</li>
<li>如果 M 不为 0，代表<strong>非规格化数 (Subnormal Numbers)</strong>，计算公式变为 $(-1)^S \times (0.M) \times 2^{(1 - \text{Bias})}$，此时没有隐含的 1.</li>
</ul>
</li>
<li>E 全为 1:
<ul>
<li>如果 M 全为0，代表<strong>无穷大 (Infinity)</strong>。</li>
<li>如果 M 不为0，代表<strong>NaN(Not a Number)</strong>.</li>
</ul>
</li>
</ul>
</div>
</details><br>
<p><strong>Online Quantization.</strong> 采用 online 方式计算每个 1x128 激活 tile 和 128x128 权重 block 的最大绝对值。</p>
<h3 id="333-low-precision-storage-and-communication">3.3.3 Low-Precision Storage and Communication<a hidden class="anchor" aria-hidden="true" href="#333-low-precision-storage-and-communication">#</a></h3>
<p><strong>Low-Precision Optimizer States.</strong> 用 BF16 格式存储 AdamW 优化器的一阶和二阶动量。优化器存储的 master weights 和 betch 的累加梯度仍以 FP32 格式存储。</p>
<p><strong>Low-Precision Activation.</strong> 大部分激活以 FP8 格式存储，但以下这些是例外。</p>
<ul>
<li><strong>Inputs of the Linear after the attention operator.</strong> 这些激活会在反向传播过程中作为 attention 的输入，对精度比较敏感，因此采用 E5M6 格式存储。量化过程的缩放因子被限制为 2 的整数次幂。</li>
<li><strong>Inputs of the SwiGLU operator in MoE.</strong> 以 FP8 格式存储 SwiGLU 的输入然后再反向传播中重计算。</li>
</ul>
<p><strong>Low-Precision Communication.</strong> 在 dispatch 之前对  MoE up-projections 的输入进行 FP8 量化。专家接收到 FP8 数据后，可以直接进行兼容的 FP8 前向传播。量化过程的缩放因子被限制为 2 的整数次幂。在反向传播进入 MoE down-projections 之前同样使用该策略。前向传播和反向传播 combine 后的结果以 FP16 格式存储。</p>
<h2 id="34-inference-and-deployment">3.4 Inference and Deployment<a hidden class="anchor" aria-hidden="true" href="#34-inference-and-deployment">#</a></h2>
<p>为了同时保证 Service-Level Objective (SLO) 和高吞吐量, <em>prefilling</em> 和 <em>decoding</em> 阶段采用了不同的部署策略。</p>
<p>prefilling 阶段的部署单元为 4 个节点 (32 GPUs). 并行策略如下</p>
<ul>
<li>attention part: 采用带有 Sequence Parallel (SP) 的 4-way Tensor Parallel (TP4)，并且和 8-way Data Parallelism (DP8) 一起使用。</li>
<li>MoE part: 采用 32-way Expert Parallelism (EP32), shallow layer 不使用 TP.</li>
</ul>
<p>其他部署细节:</p>
<ul>
<li><em>redundant experts</em>: 部署 32 高负载的专家 (每十分钟统计一次进行调整) 副本。每个 GPU 除了有自己的 8 个专家之外还有 1 个高负载专家。</li>
<li>同时处理两个计算量差不多的 micro-batches，来掩盖 All-to-all 和 TP 的通信。即将一个 micro-batch 的 attention+MoE 和另一个 batch 的 dispatch+combine 重叠。</li>
<li><em>dynamic redundancy</em>: 每个 GPU 上放置 16 个专家，但每次只有 9 个被激活。</li>
</ul>
<p>decoding 阶段的部署单元为 40 个节点 (320 GPUs). 并行策略如下</p>
<ul>
<li>attention part: 采用带有 SP 的 TP4，并且和 DP80 一起使用。</li>
<li>MoE part: 采用 EP320. 256 GPU 被用来放置路由专家，64 GPU 被用来放置共享专家和冗余专家。</li>
</ul>
<p>All-to-all 通过 IB 进行点对点直接传输。同时利用 IBGDA 技术让网卡直接读写 GPU 内存。系统会根据流量统计周期性地判断哪些常规路由专家是当前最热门的，然后动态地让那 64 个GPU去扮演这些热门专家的副本。因为每个 GPU 只被放置一个专家，所以当需要更改冗余策略时系统只需要改变路由逻辑，不需要在物理上移动或重新加载模型权重。</p>
<p>在 decoding 过程中 attention 会耗费更多时间。因此将一个 micro-batch 的 attention 和另一个的 dispatch+MoE+combine 重叠。decoding 阶段每个 GPU 只需要加载一个专家的参数，因此可以分配更多的 SM 给 attention 部分来加速其计算。</p>
<h2 id="35-suggestions-on-hardware-design">3.5 Suggestions on Hardware Design<a hidden class="anchor" aria-hidden="true" href="#35-suggestions-on-hardware-design">#</a></h2>
<p>基于 All-to-all 实现和 FP8 训练框架，DeepSeek-V3 对 AI 硬件厂商提出了一些建议。</p>
<h3 id="351-communication-hardware">3.5.1 Communication Hardware<a hidden class="anchor" aria-hidden="true" href="#351-communication-hardware">#</a></h3>
<p>当前通信算子的实现依赖于 SM，DeepSeek-V3 使用了 20 个 H800 SMs (一共 132 个) 用于通信，但使用 SM 进行通信会导致 tensor core 利用率很低。</p>
<p>当前 SM 主要在 All-to-all 通信中执行以下任务:</p>
<ul>
<li>IB 和 NVLink 域之间的数据转发，将目的地为同一节点内多个不同 GPU 的流量，首先汇聚到单个代理GPU上。</li>
<li>在 RDMA 缓冲区 (已注册的 GPU 内存区域) 与模型的输入/输出缓冲区之间进行数据搬运。</li>
<li>为 All-to-all 通信的 combine 阶段执行 reduce 操作。</li>
<li>在需要跨越 IB 和 NVLink 网络域、向多个不同专家进行分块数据传输<span class="sidenote-number"><small class="sidenote">在一个 GPU上 的tokens，其中一些可能要去当前节点内的专家 (通过NVLink)，另一些则要去其他节点上的专家 (通过IB). 在发送之前，GPU必须在自己的内存里进行一次数据重排，把所有目的地是专家 A 的 tokens 打包成一个连续的内存块，所有去专家 B 的 tokens 打包成另一个内存块。</small></span>
的过程中，管理细粒度的内存布局。</li>
</ul>
<h3 id="352-compute-hardware">3.5.2 Compute Hardware<a hidden class="anchor" aria-hidden="true" href="#352-compute-hardware">#</a></h3>
<p><strong>Higher FP8 GEMM Accumulation Precision in Tensor Cores.</strong> 在目前 NVIDIA Hopper 架构的 Tensor Core 实现中，FP8 GEMM 的累积精度有限。在根据最大指数右移对齐 32 个尾数乘积后，Tensor Core 只使用每个尾数乘积的最高 14 位进行加法，并截断超过此范围的位。将加法结果累加到寄存器中也采用 14 位精度。</p>
<p><strong>Support for Tile- and Block-Wise Quantization.</strong> 目前的 GPU 只支持逐张量量化，缺乏对细粒度量化的原生支持，比如 DeepSeek 的 tile 量化和 block 量化。在当前的实现中，当累加 $N_c$ 次时，部分结果将从 Tensor Core 复制到 CUDA Core，乘以缩放因子，并累加到 CUDA Core 上的FP32 寄存器。尽管与精确的 FP32 累加策略相结合，反量化开销显着减轻，但 Tensor Core 和 CUDA Core 之间频繁的数据移动仍然限制了计算效率。</p>
<p><strong>Support for Online Quantization.</strong> 当前情况下需要从 HBM 中读取 128 个 BF16 激活值 (之前计算的输出) 进行量化，然后将量化后的 FP8 值写回 HBM，然后再次读取以进行 MMA.</p>
<p><strong>Support for Transposed GEMM Operations.</strong> 在当前工作流程中，前向传播的激活被量化为 1x128 FP8 tile 并存储。在反向传播中，矩阵需要被读出、反量化、转置、重新量化为 128x1 tile，并存储在 HBM 中。</p>
<p>DeepSeek-V3 的预训练阶段围绕着高质量的数据构建、精心设计的超参数、长上下文扩展以及全面的性能评测展开。</p>
<h1 id="4-pretraining">4. Pretraining<a hidden class="anchor" aria-hidden="true" href="#4-pretraining">#</a></h1>
<h2 id="41-data-construction">4.1 Data Construction<a hidden class="anchor" aria-hidden="true" href="#41-data-construction">#</a></h2>
<ul>
<li><strong>训练语料</strong>:
<ul>
<li>模型在一个包含 <strong>14.8T</strong> 高质量、多样化 token 的语料库上进行预训练。</li>
<li>与 DeepSeek-V2 相比，新语料提升了数学和编程相关样本的比例，并扩展了除中英文之外的多语言覆盖范围。</li>
<li>数据处理流程经过优化，旨在最小化冗余，同时保持语料的多样性。</li>
</ul>
</li>
<li><strong>FIM 策略</strong>:
<ul>
<li>模型训练中采用了 FIM (Fill-in-Middle) 策略，该策略被证明在不损害常规“下一词预测”能力的同时，赋予了模型根据上下文准确预测中间文本的能力。</li>
<li>FIM 策略在文档层面以 10% 的应用率实施，并采用 Prefix-Suffix-Middle (PSM) 框架构建数据格式。</li>
</ul>
</li>
<li><strong>分词器</strong>:
<ul>
<li>分词器采用 Byte-level BPE，词汇表大小扩展至 <strong>128K</strong>.</li>
<li>为了优化多语言压缩效率，对预分词器和训练数据进行了修改。</li>
<li>为了解决因合并标点和换行符可能导致的 token边界偏差，训练中会随机拆分一部分这类组合 token.</li>
</ul>
</li>
</ul>
<h2 id="42-hyper-parameters">4.2 Hyper-Parameters<a hidden class="anchor" aria-hidden="true" href="#42-hyper-parameters">#</a></h2>
<ul>
<li><strong>模型结构超参数</strong>:
<ul>
<li>总共有 61 层 Transformer，隐藏层维度为 7168.</li>
<li>MLA 注意力头数 $n_h$ 为128，每个头的维度为 128. KV 压缩维度 $d_c$ 为 512，Query 压缩维度 $d_c^{'}$ 为1536.</li>
<li>除了前三层，其余所有 FFN 都被替换为 MoE 层。</li>
<li>每个 MoE 层包含 1个共享专家 和 256个路由专家。每个 token 会激活其中的 8个 路由专。</li>
<li>采用 MTP 策略，预测深度为 1，即除了下一个词，还会额外再预测一个词。</li>
<li>最终模型总参数量为 671B，每个 token 的激活参数量为 37B.</li>
</ul>
</li>
<li><strong>训练超参数</strong>:
<ul>
<li>优化器采用 AdamW，其中 $\beta_{1}=0.9, \beta_{2}=0.95$，权重衰减为 0.1.</li>
<li>预训练阶段的最大序列长度为 4K.</li>
<li>学习率调度：先在 2K 步内线性增长至 $2.2\times10^{-4}$，保持该速率直到消耗10T token，然后在 4.3T token 内余弦衰减至 $2.2\times10^{-5}$，最后在 500B token 的训练中进一步调整。</li>
<li>采用了批次大小调度策略，从 3072 逐步增加到15360.</li>
<li>路由机制被限制为每个 token 最多发送到 4 个节点，以平衡负载。</li>
<li>负载均衡策略主要采用 auxiliary-loss-free，偏置更新速率 $\gamma$ 在前 14.3 Token 时为 0.001，后 500B token 时为 0.0.</li>
<li>对于序列级平衡损失 $\alpha=0.00001$，以防止单一样本内的极端不平衡。</li>
<li>MTP loss 权重 $\lambda$ 对于前 10T token 为 0.3，对于后 4.8T token 为 0.1.</li>
</ul>
</li>
</ul>
<h2 id="43-long-context-extension">4.3 Long Context Extension<a hidden class="anchor" aria-hidden="true" href="#43-long-context-extension">#</a></h2>
<ul>
<li><strong>扩展方法</strong>: 采用与 DeepSeek-V2 类似的方法，在预训练后应用 <strong>YaRN</strong> 技术进行上下文扩展。</li>
<li><strong>扩展阶段</strong>: 分为两个阶段，分别将上下文窗口从 4K 扩展到 32K，再进一步扩展到 128K.</li>
<li><strong>效果验证</strong>: 通过大海捞针 (Needle In A Haystack) 测试表明，模型在高达 128K 的完整上下文长度内均表现出色且稳定。</li>
</ul>
<h2 id="44-evaluations">4.4 Evaluations<a hidden class="anchor" aria-hidden="true" href="#44-evaluations">#</a></h2>
<ul>
<li><strong>评测范围</strong>: 主要在中英文基准测试以及一个多语言基准上进行评测，与当前最先进的开源基础模型进行比较，如 DeepSeek-V2-Base, Qwen2.5 72B Base, 和 LLaMA-3.1 405B Base.</li>
<li><strong>评测结果</strong>:
<ul>
<li>DeepSeek-V3-Base 全面超越了 DeepSeek-V2-Base 和 Qwen2.5 72B Base，并在绝大多数基准上超过了 LLaMA-3.1 405B Base，成为当前最强的开源模型。</li>
<li>与拥有 11 倍激活参数量的 LLaMA-3.1 405B Base 相比，DeepSeek-V3-Base 在多语言、代码和数学基准上表现出好得多的性能。</li>
<li>在英语和中文语言基准上，DeepSeek-V3-Base 也展现出有竞争力或更好的性能。</li>
</ul>
</li>
<li><strong>训练效率</strong>: 得益于高效的架构和工程优化，DeepSeek-V3 的训练效率极高。每训练 1T token 仅需 180K H800 GPU 小时，远比训练 72B 或 405B 的密集模型便宜。</li>
</ul>
<h2 id="45-discussion">4.5 Discussion<a hidden class="anchor" aria-hidden="true" href="#45-discussion">#</a></h2>
<p>本节通过一系列消融实验，深入探讨了模型采用的两个关键新策略的有效性，并对负载均衡的不同实现方式进行了对比分析。</p>
<h3 id="451-ablation-studies-for-multi-token-prediction">4.5.1 Ablation Studies for Multi-Token Prediction<a hidden class="anchor" aria-hidden="true" href="#451-ablation-studies-for-multi-token-prediction">#</a></h3>
<ul>
<li><strong>实验设置</strong>:
<ul>
<li>在两个不同规模 (一个15.7B，一个228.7B) baseline MoE模型上进行验证。</li>
<li>对比模型在 baseline 模型的基础上增加了一个预测深度为 1 的MTP模块，其他设置 (如训练数据、架构) 保持不变。</li>
<li>为了保证公平比较，在推理阶段会丢弃MTP模块，因此对比模型的推理成本完全相同。</li>
</ul>
</li>
<li><strong>实验结论</strong>:
<ul>
<li>实验结果 (Table 4) 表明，MTP策略在绝大多数评测基准上都能稳定地提升模型性能。</li>
<li>例如，在大型模型上，HumanEval (代码生成) 和 GSM8K (数学推理) 等任务的性能得到了显著提升。</li>
</ul>
</li>
</ul>
<h3 id="452-ablation-studies-for-the-auxiliary-loss-free-balancing-strategy">4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy<a hidden class="anchor" aria-hidden="true" href="#452-ablation-studies-for-the-auxiliary-loss-free-balancing-strategy">#</a></h3>
<ul>
<li><strong>实验设置</strong>:
<ul>
<li>同样在两个不同规模 (一个小型15.7B，一个大型228.7B) baseline MoE 模型上进行验证。</li>
<li>baseline 模型完全依赖传统的辅助损失函数来促进专家负载均衡。</li>
<li>对比模型则移除了所有辅助损失，并引入了 Auxiliary-Loss-Free 的均衡策略，其他设置保持一致。</li>
</ul>
</li>
<li><strong>实验结论</strong>:
<ul>
<li>实验结果 (Table 5) 显示，Auxiliary-Loss-Free 策略在绝大多数评测基准上都取得了比纯辅助损失方法更好的模型性能。</li>
<li>在代码和数学等任务上，性能提升尤为明显。</li>
</ul>
</li>
</ul>
<h3 id="453-batch-wise-load-balance-vs-sequence-wise-load-balance">4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance<a hidden class="anchor" aria-hidden="true" href="#453-batch-wise-load-balance-vs-sequence-wise-load-balance">#</a></h3>
<ul>
<li><strong>核心区别</strong>: Auxiliary-Loss-Free 策略是在整个训练批次 (batch-wise) 上实现均衡，而传统的辅助损失则是在每个序列 (sequence-wise) 内部强制实现均衡。</li>
<li><strong>理论优势</strong>: 批次级的均衡约束更为灵活，它不强制每个序列内部的专家使用频率都一样，从而允许专家更好地专精于特定领域 (如代码、数学等).</li>
<li><strong>实验验证</strong>:
<ul>
<li>通过分析模型在不同领域数据上的专家负载，观测到 Auxiliary-Loss-Free 模型展现出了更强的专家特化模式。</li>
<li>进一步的实验表明，只要能实现相似水平的批次级负载均衡，无论是使用 Auxiliary-Loss-Free 方法还是新设计的批次级 Auxiliary-Loss-Free 方法，都能达到相似的优异模型性能，且均优于序列级辅助损失方法。</li>
</ul>
</li>
<li><strong>潜在挑战与解决方案</strong>:
<ul>
<li>批次级均衡可能面临两个挑战：单个序列或小批次内的负载不均，以及推理时因领域切换导致的负载不均。</li>
<li>第一个挑战通过使用大规模的专家并行和数据并行 (确保了每个微批次的规模足够大) 得以自然解决。</li>
<li>第二个挑战则通过在推理部署中采用冗余专家策略来克服。</li>
</ul>
</li>
</ul>
<h1 id="5-post-training">5. Post-Training<a hidden class="anchor" aria-hidden="true" href="#5-post-training">#</a></h1>
<p>后训练阶段旨在将预训练好的基础模型与人类偏好对齐，并进一步解锁其潜力。该阶段主要包括监督微调 (SFT) 和强化学习 (RL)，并涉及从 DeepSeek-R1 系列模型中蒸馏推理能力。</p>
<h2 id="51-supervised-fine-tuning-sft">5.1 Supervised Fine-Tuning, SFT<a hidden class="anchor" aria-hidden="true" href="#51-supervised-fine-tuning-sft">#</a></h2>
<ul>
<li><strong>数据集构建</strong>: SFT 数据集包含 150 万个实例，涵盖多个领域。
<ul>
<li><strong>推理数据</strong>: 对于数学、代码、逻辑等推理任务，利用内部的 DeepSeek-R1 模型生成数据。虽然 R1 生成的数据准确性高，但存在过度思考、格式不佳和长度过长等问题。为了平衡准确性与简洁性，SFT 训练中会混合使用原始应答和经过精心设计的系统提示词引导下的 R1 应答。</li>
<li><strong>非推理数据</strong>: 对于创意写作、角色扮演等任务，使用 DeepSeek-V2.5 生成应答，并由人类标注员进行验证。</li>
</ul>
</li>
<li><strong>SFT 设置</strong>:
<ul>
<li>模型在 SFT 数据集上微调了 2 个 epoch.</li>
<li>学习率采用余弦衰减策略，从 $5\times10^{-6}$ 降至 $1\times10^{-6}$.</li>
<li>训练序列由多个样本打包而成，但采用样本掩码策略确保样本间相互隔离。</li>
</ul>
</li>
</ul>
<h2 id="52-reinforcement-learning">5.2 Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#52-reinforcement-learning">#</a></h2>
<h3 id="521-reward-model">5.2.1 Reward Model<a hidden class="anchor" aria-hidden="true" href="#521-reward-model">#</a></h3>
<p>RL 过程采用了 Rule-Based 模型和 Model-Based 的奖励模型。</p>
<ul>
<li><strong>Rule-Based RM</strong>: 用于有明确验证规则的问题，如数学题的确定性答案或代码题的单元测试结果。这种方式可靠性高，不易被模型钻空子。</li>
<li><strong>Model-Based RM</strong>: 用于答案更开放、没有确定性对错的问题。该奖励模型由 DeepSeek-V3 的 SFT 版本训练而来，并通过包含思维链的偏好数据进行训练，以降低 reward hacking 的风险。</li>
</ul>
<h3 id="522-grpo">5.2.2 GRPO<a hidden class="anchor" aria-hidden="true" href="#522-grpo">#</a></h3>
<ul>
<li>采用了 <strong>GRPO (Group Relative Policy Optimization)</strong> 算法进行强化学习。</li>
<li>GRPO 的一个特点是它不需要一个与策略模型同等大小的 critic 模型，而是从一组采样输出的分数中估计 baseline.</li>
<li>RL 过程融合了来自编码、数学、写作、角色扮演等不同领域的提示词，这不仅使模型与人类偏好更对齐，也提升了在 SFT 数据有限场景下的基准测试性能。</li>
</ul>
<h2 id="53-evaluations">5.3 Evaluations<a hidden class="anchor" aria-hidden="true" href="#53-evaluations">#</a></h2>
<ul>
<li><strong>评测设置</strong>:
<ul>
<li>除了基础模型评测用的基准外，进一步在 IFEval, GPQA, LongBench v2, SWE-Bench Verified, Aider, Codeforces, AIME 2024 等更具挑战性的基准上进行评估。</li>
<li>对比的 baseline 模型包括其他强大的开源和闭源模型，如 Qwen2.5-72B-Inst, LLaMA-3.1-405B-Inst, Claude-3.5-Sonnet, 和 GPT-4o-0513。</li>
</ul>
</li>
<li><strong>Standard Evaluation</strong>:
<ul>
<li>评测结果 (Table 6) 显示，DeepSeek-V3 是表现最好的开源聊天模型。</li>
<li>在知识基准 (MMLU, MMLU-Pro, GPQA-Diamond) 上，其性能与顶级的闭源模型相当或相近。</li>
<li>在长上下文理解基准 (DROP, LongBench v2, FRAMES) 上，表现出顶级水平，例如在 DROP 上取得了 91.6 的 F1 分数，超越了所有其他模型。</li>
<li>在代码和数学基准上表现卓越，尤其是在 AIME, MATH-500 等高难度数学竞赛基准上，绝对得分领先第二名约 10%，优势巨大。</li>
<li>在中文基准上，如 C-SimpleQA，其表现也超越了包括 Qwen2.5 在内的其他模型。</li>
</ul>
</li>
<li><strong>Open-Ended Evaluation</strong>:
<ul>
<li>在 Arena-Hard 基准测试中，DeepSeek-V3 取得了超过 85% 的胜率，与顶级的 Claude-3.5-Sonnet-1022 表现持平，成为首个在该基准上突破 85% 的开源模型。</li>
<li>在 AlpacaEval 2.0 上，其表现同样出色，超越了所有对比的开源和闭源模型。</li>
</ul>
</li>
<li><strong>作为奖励模型的能力</strong>:
<ul>
<li>在 RewardBench 基准上评测其作为奖励模型的判断能力，结果显示 DeepSeek-V3 与最新版本的 GPT-4o 和 Claude-3.5-Sonnet 表现相当。</li>
</ul>
</li>
</ul>
<h2 id="54-discussion">5.4 Discussion<a hidden class="anchor" aria-hidden="true" href="#54-discussion">#</a></h2>
<ul>
<li><strong>从 DeepSeek-R1 蒸馏知识</strong>:
<ul>
<li>消融实验 (Table 9) 证明，从长思维链 (long-CoT) 模型 DeepSeek-R1 中蒸馏知识的策略非常有效，显著提升了模型在 LiveCodeBench 和 MATH-500 上的性能。</li>
<li>实验也揭示了一个权衡：蒸馏带来了性能提升，但也显著增加了回应的平均长度。因此，在 DeepSeek-V3 的开发中对蒸馏设置进行了仔细选择以求平衡。</li>
</ul>
</li>
<li><strong>Self-Rewarding</strong>:
<ul>
<li>在缺乏明确验证规则的通用场景中，模型开发采用了 constitutional AI 的方法，即<strong>使用 DeepSeek-V3 自身的投票评估结果作为反馈源</strong>来进行优化。</li>
<li>这种自奖励的范式产生了显著的对齐效果，并被认为是实现LLM自我改进的重要方向。</li>
</ul>
</li>
<li><strong>MTP 评测</strong>:
<ul>
<li>模型采用的 MTP 技术可以预测第 2 个token.</li>
<li>评测显示，这个额外预测的 token 的接受率在 85%-90%之间。</li>
<li>结合 speculative decoding 框架，这个高接受率使得模型的解码速度 (TPS) 提升了1.8倍.</li>
</ul>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/deepseek/">DeepSeek</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/deepseek/dualpipe/">
    <span class="title">« Prev</span>
    <br>
    <span>DualPipe</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/deepseek/deepseekmoe/">
    <span class="title">Next »</span>
    <br>
    <span>DeepSeekMoE</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
