<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DeepSeekMLA | WITHER</title>
<meta name="keywords" content="MLA">
<meta name="description" content="Principle of DeepSeekV3 MLA">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/deepseek/deepseekmla/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/deepseek/deepseekmla/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/deepseek/deepseekmla/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="DeepSeekMLA">
  <meta property="og:description" content="Principle of DeepSeekV3 MLA">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-06-19T11:24:45+08:00">
    <meta property="article:modified_time" content="2025-06-22T17:53:30+08:00">
    <meta property="article:tag" content="DeepSeek">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeepSeekMLA">
<meta name="twitter:description" content="Principle of DeepSeekV3 MLA">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DeepSeek Related",
      "item": "http://localhost:1313/blogs/deepseek/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "DeepSeekMLA",
      "item": "http://localhost:1313/blogs/deepseek/deepseekmla/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DeepSeekMLA",
  "name": "DeepSeekMLA",
  "description": "Principle of DeepSeekV3 MLA",
  "keywords": [
    "MLA"
  ],
  "articleBody": "Preliminary: What is RoPE Introduction æ—‹è½¬ä½ç½®ç¼–ç  (RoPE) æ˜¯ä¸€ç§æ–°é¢–çš„ã€åŸºäºç›¸å¯¹ä½ç½®çš„ç¼–ç æ–¹æ³•ï¼Œå®ƒè¢«è®¾è®¡ç”¨äºæé«˜ Transformer æ¨¡å‹å¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„ç»å¯¹ä½ç½®ç¼–ç  (å¦‚æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ) æˆ–ç›´æ¥çš„ç›¸å¯¹ä½ç½®ç¼–ç  (å¦‚ T5 ä¸­ä½¿ç”¨çš„ç›¸å¯¹åç½®) ä¸åŒï¼ŒRoPE å°†ä½ç½®ä¿¡æ¯é›†æˆåˆ°è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ Q å’Œ K çš„è¡¨ç¤ºä¸­ï¼Œä½¿å¾— Q å’Œ K çš„ç‚¹ç§¯è‡ªç„¶åœ°ç¼–ç äº†ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚\nRoPE çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œé€šè¿‡å¯¹æŸ¥è¯¢å’Œé”®å‘é‡åº”ç”¨ç‰¹å®šçš„æ—‹è½¬æ“ä½œï¼Œä½¿å¾—ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ç»“æœåªä¾èµ–äºå®ƒä»¬ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹ä½ç½®ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç½®ä¿¡æ¯æ—¶æ›´åŠ é«˜æ•ˆã€‚\nRoPE çš„ä¸»è¦ä¼˜ç‚¹åŒ…æ‹¬ï¼š\nç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼š è‡ªç„¶åœ°å°†ç›¸å¯¹ä½ç½®ä¿¡æ¯èå…¥åˆ°æ³¨æ„åŠ›åˆ†æ•°ä¸­ã€‚ é•¿åºåˆ—å¤–æ¨èƒ½åŠ›ï¼š æé«˜äº†æ¨¡å‹åœ¨è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´é•¿åºåˆ—ä¸Šçš„æ€§èƒ½ã€‚ ä¸è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å…¼å®¹æ€§ï¼š æ— ç¼é›†æˆåˆ° QKV ç‚¹ç§¯æ³¨æ„åŠ›ä¸­ã€‚ ç®€å•ä¸”é«˜æ•ˆï¼š å®ç°ç›¸å¯¹ç®€å•ï¼Œä¸”ä¸ä¼šæ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚åº¦ã€‚ Formular RoPE çš„ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡å¯¹æŸ¥è¯¢ $q$ å’Œé”® $k$ åº”ç”¨ä¸€ä¸ªæ—‹è½¬çŸ©é˜µ $R_t$ (å–å†³äºå…¶ç»å¯¹ä½ç½® $t$) ï¼Œä½¿å¾—ç‚¹ç§¯ $q_m^T k_n$ èƒ½å¤Ÿé€šè¿‡æŸç§æ–¹å¼è½¬åŒ–ä¸ºåªä¾èµ–äºç›¸å¯¹ä½ç½® $m-n$ çš„å‡½æ•°ã€‚\nå¯¹äºä¸€ä¸ªå‘é‡ $x \\in \\mathbb{R}^d$ åœ¨ä½ç½® $m$ å¤„ï¼ŒRoPE çš„å˜æ¢å‡½æ•° $f(x, m)$ å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼š\nå¦‚æœå‘é‡ç»´åº¦æ˜¯å¶æ•° $d$ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†æˆ $d/2$ å¯¹ï¼Œæ¯å¯¹æ‰§è¡Œä¸€ä¸ªäºŒç»´æ—‹è½¬ã€‚ å¯¹äºå‘é‡ $x = [x_0, x_1, \\ldots, x_{d-1}]^T$ï¼ŒRoPE å¯¹å…¶æ¯ä¸ªç»´åº¦å¯¹ $(x_{2i}, x_{2i+1})$ åº”ç”¨æ—‹è½¬ï¼š\n$$\rf(x, m)_{2i} = x_{2i} \\cos(m\\theta_i) - x_{2i+1} \\sin(m\\theta_i) \\\\\rf(x, m)_{2i+1} = x_{2i} \\sin(m\\theta_i) + x_{2i+1} \\cos(m\\theta_i)\r$$å…¶ä¸­ $\\theta_i$ æ˜¯é¢„è®¾çš„é¢‘ç‡ï¼Œé€šå¸¸å®šä¹‰ä¸º $\\theta_i = 10000^{-2i/d}$. $i=0, \\dots, d/2 - 1$ æ˜¯ç»´åº¦å¯¹çš„ç´¢å¼•ã€‚\nç”¨çŸ©é˜µå½¢å¼è¡¨ç¤ºï¼š æˆ‘ä»¬å¯ä»¥å°†è¿™ç§æ—‹è½¬æ“ä½œè¡¨ç¤ºä¸ºä¸€ä¸ªç¨€ç–çš„å—å¯¹è§’çŸ©é˜µ $R_m^d$ï¼Œå…¶å½¢å¼ä¸ºï¼š $$R_m^d = \\begin{pmatrix}\r\\cos(m\\theta_0) \u0026 -\\sin(m\\theta_0) \u0026 0 \u0026 0 \u0026 \\cdots \\\\\r\\sin(m\\theta_0) \u0026 \\cos(m\\theta_0) \u0026 0 \u0026 0 \u0026 \\cdots \\\\\r0 \u0026 0 \u0026 \\cos(m\\theta_1) \u0026 -\\sin(m\\theta_1) \u0026 \\cdots \\\\\r0 \u0026 0 \u0026 \\sin(m\\theta_1) \u0026 \\cos(m\\theta_1) \u0026 \\cdots \\\\\r\\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots\r\\end{pmatrix}$$ é‚£ä¹ˆï¼Œç»è¿‡ RoPE ç¼–ç çš„æŸ¥è¯¢å’Œé”®å¯ä»¥è¡¨ç¤ºä¸ºï¼š $$\\mathbf{q}_m = R_m^d \\mathbf{q}$$ $$\\mathbf{k}_n = R_n^d \\mathbf{k}$$ å…¶ä¸­ $\\mathbf{q}$ å’Œ $\\mathbf{k}$ æ˜¯åŸå§‹çš„æŸ¥è¯¢å’Œé”®å‘é‡ (ä¸å«ä½ç½®ä¿¡æ¯) ï¼Œ$\\mathbf{q}_m$ å’Œ $\\mathbf{k}_n$ æ˜¯ç»è¿‡ RoPE å¤„ç†åçš„æŸ¥è¯¢å’Œé”®å‘é‡ã€‚\nRoPE çš„å…³é”®ç‰¹æ€§ï¼šç‚¹ç§¯ä¸ç›¸å¯¹ä½ç½® ç»è¿‡ RoPE å˜æ¢åï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ç‚¹ç§¯å¯ä»¥åˆ†è§£ä¸ºï¼š $$\\mathbf{q}_m^T \\mathbf{k}_n = (R_m^d \\mathbf{q})^T (R_n^d \\mathbf{k})$$ ç”±äº $R_m^d$ æ˜¯æ­£äº¤çŸ©é˜µï¼Œå…¶é€†çŸ©é˜µç­‰äºå…¶è½¬ç½®ï¼Œå³ $(R_m^d)^T = (R_m^d)^{-1} = R_{-m}^d$. å› æ­¤æœ‰ $$\\mathbf{q}_m^T \\mathbf{k}_n = \\mathbf{q}^T (R_m^d)^T R_n^d \\mathbf{k} = \\mathbf{q}^T R_{-m}^d R_n^d \\mathbf{k} = \\mathbf{q}^T R_{n-m}^d \\mathbf{k}$$ è¿™ä¸ªæœ€ç»ˆç»“æœ $\\mathbf{q}^T R_{n-m}^d \\mathbf{k}$ è¡¨æ˜ï¼Œä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯åªä¾èµ–äºå®ƒä»¬çš„ç›¸å¯¹ä½ç½®å·® $n-m$ï¼Œè€Œä¸å®ƒä»¬çš„ç»å¯¹ä½ç½® $n$ å’Œ $m$ æ— å…³ã€‚è¿™å°±æ˜¯ RoPE èƒ½å¤Ÿç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„æ•°å­¦åŸºç¡€ã€‚\nWorkflow Notation $d$: embedding ç»´åº¦ $d_h$: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ $\\mathbf{h}_t\\in\\mathbb{R}^d$: æŸä¸ª attention å±‚ç¬¬ t ä¸ª token çš„è¾“å…¥ã€‚ KV Compression $$\r\\textcolor{red}{c_t^{KV}} = W^{DKV}h_t \\tag{1}\r$$ $$\r[k_{t,1}^{C}, k_{t,2}^{C}, \\ldots, k_{t,n_h}^{C}] = k_t^C = W^{UK}c_t^{KV} \\tag{2}\r$$ $$\r\\textcolor{red}{k_t^R} = \\text{RoPE}(W^{KR}h_t) \\tag{3}\r$$ $$\rk_{t,i} = [k_{t,i}^C, k_{t}^R] \\tag{4}\r$$ $$\r[v_{t,1}^C, v_{t,2}^C, \\ldots, v_{t,n_h}^C] = v_t^C = W^{UV}c_t^{KV} \\tag{5}\r$$ $c_t^{KV} \\in \\mathbb{R}^{d_c}$: å‹ç¼©åçš„ KV æ½œåœ¨å‘é‡ã€‚ $d_c (\\ll d_h n_h)$: KV å‹ç¼©åˆ°çš„ç»´åº¦ã€‚ $W^{DKV} \\in \\mathbb{R}^{d_c \\times d}$: KV é™ç»´æŠ•å½±çŸ©é˜µã€‚ $W^{UK}, W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}$ åˆ†åˆ«æ˜¯ K \u0026 V çš„å‡ç»´æŠ•å½±çŸ©é˜µã€‚ $W^{KR} \\in \\mathbb{R}^{d_h^R \\times d}$: ç”¨äºç”Ÿæˆæºå¸¦ RoPE çš„è§£è€¦é”®çš„çŸ©é˜µ (Su et al., 2024) çº¢è‰²çš„æ˜¯éœ€è¦ç¼“å­˜çš„å‘é‡ï¼Œåç»­è¯´æ˜åŸå› ã€‚æ³¨æ„åˆ°å¯¹ K è¿›è¡Œ RoPE ä¹‹å‰æ˜¯å¯¹è¾“å…¥å‘é‡ä¹˜ä»¥äº†ä¸ªæŠ•å½±å†è¿›è¡Œçš„ã€‚è€Œä¸” K çš„æ¯ä¸ªæ³¨æ„åŠ›å¤´è¢«æ‹¼æ¥çš„éƒ½æ˜¯åŒä¸€ä¸ª $k_{t}^R$ï¼Œæœ‰ç‚¹ç±»ä¼¼äº MQA.\nQ Compression $$c_t^Q = W^{DQ}h_t \\tag{6}$$ $$[q_{t,1}^C, q_{t,2}^C, \\ldots, q_{t,n_h}^C] = q_t^C = W^{UQ}c_t^Q \\tag{7}$$ $$[q_{t,1}^R, q_{t,2}^R, \\ldots, q_{t,n_h}^R] = q_t^R = \\text{RoPE}(W^{QR}q_t^C) \\tag{8}$$ $$q_{t,i} = [q_{t,i}^C, q_{t,i}^R] \\tag{9}$$ $c_t^Q \\in \\mathbb{R}^{d_c'}$: Q å‹ç¼©åçš„æ½œåœ¨å‘é‡ã€‚ $d_c'(\\ll d_h n_h)$ è¡¨ç¤º Q å‹ç¼©åçš„ç»´åº¦ã€‚ $W^{DQ} \\in \\mathbb{R}^{d_c' \\times d}, W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c'}$: åˆ†åˆ«æ˜¯ Q çš„é™ç»´å’Œå‡ç»´çŸ©é˜µã€‚ $W^{QR} \\in \\mathbb{R}^{d_h^R n_h \\times d_c'}$ æ˜¯ç”¨äºç”Ÿæˆæºå¸¦ RoPE çš„è§£è€¦ Q çš„çŸ©é˜µã€‚ æ³¨æ„åˆ°å¯¹ Q çš„ RoPE æ˜¯åœ¨å‹ç¼©åè¿›è¡Œçš„ï¼Œå³ä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½ç”Ÿæˆäº†ä¸€ä¸ªä½ç½®ç¼–ç ä¿¡æ¯åè¿›è¡Œæ‹¼æ¥ã€‚\nAttention Computation æœ€ç»ˆ $q_{t,i}$, $k_{j,i}$, $v_{j,i}^C$ è¢«ç»„åˆèµ·æ¥ä»¥ç”Ÿæˆæœ€ç»ˆçš„æ³¨æ„åŠ›è¾“å‡º $u_t$\n$$\\mathbf{o}_{t,i} = \\sum_{j=1}^{t} \\text{Softmax}\\left(\\frac{q_{t,i}^T \\mathbf{k}_{j,i}}{\\sqrt{d_h + d_R}}\\right)v_{j,i}^C \\tag{10}$$ $$\\mathbf{u}_t = W^O[\\mathbf{o}_{t,1}, \\mathbf{o}_{t,2}, \\ldots, \\mathbf{o}_{t,n_h}] \\tag{11}$$ $W^O \\in \\mathbb{R}^{d \\times d_h n_h}$: è¾“å‡ºæŠ•å½±çŸ©é˜µã€‚ Why Decoupled RoPE å‡è®¾ä¸åŠ  RoPE çš„æƒ…å†µä¸‹è¿›è¡Œ $q_{t,i}$, $k_{j,i}$ çš„å†…ç§¯åˆ™æœ‰\n$$\rq_{t,i}^{T}\\times k_{j,i}=(W_{(i)}^{UQ}c_{t}^{Q})^{T}\\times W_{(i)}^{UK}c_{j}^{KV}=(c_{t}^{Q})^{T}\\times(W_{(i)}^{UQ})^{T}W_{(i)}^{UK}\\times c_{j}^{KV} \\tag{12}\r$$RoPE é€šè¿‡å¯¹å‘é‡åº”ç”¨ä¸€ä¸ªä½ç½®ä¾èµ–çš„æ—‹è½¬å˜æ¢æ¥æ³¨å…¥ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚å¯¹äºä¸€ä¸ªå‘é‡ $X$ åœ¨ä½ç½® $t$ï¼ŒRoPE å¯ä»¥è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªæ—‹è½¬çŸ©é˜µ $R_t$ ä¹˜ä»¥ $X$ï¼š $$\\text{RoPE}(X, t) = R_t X$$ è¿™é‡Œçš„ $R_t$ æ˜¯ä¸€ä¸ªæ­£äº¤æ—‹è½¬çŸ©é˜µï¼Œå®ƒå–å†³äºä½ç½® $t$.\nå¦‚æœç›´æ¥å¯¹å‹ç¼©å $k_t^C$ çš„ ä½¿ç”¨ RoPE é‚£ä¹ˆæƒ…å†µä¼šå˜æˆ\n$$\r\\begin{aligned}\rq_{t,i}^{T}\\times k_{j,i}\u0026=(\\mathcal{R}_{t}W_{(i)}^{UQ}c_{t}^{Q})^{T}\\times\\mathcal{R}_{j}W_{(i)}^{UK}c_{j}^{KV} \\\\\r\u0026=(c_{t}^{Q})^{T}\\times(W_{(i)}^{UQ})^{T}\\mathcal{R}_{t}^{T}\\mathcal{R}_{j}W_{(i)}^{UK}\\times c_{j}^{KV}\\\\\r\u0026=(c_{t}^{Q})^{T}\\times(W_{(i)}^{UQ})^{T}\\mathcal{R}_{t-j}W_{(i)}^{UK}\\times c_{j}^{KV}\r\\end{aligned} \\tag{13}\r$$ä¸­é—´çš„çŸ©é˜µä¸ç›¸å¯¹ä½ç½®æœ‰å…³ï¼Œæ— æ³•æå‰è®¡ç®—å‡ºæ¥ã€‚å› æ­¤æ–‡ä¸­å°±æ˜¯å¯¹æ‰€æœ‰å¤´éƒ½ä½¿ç”¨åŒä¸€ä¸ª k å’Œè®¡ç®— RoPE. æ‹¼æ¥åçš„å‘é‡å†è®¡ç®—æ—¶\n$$\rq_{t,i}^T\\times k_{j,i}=[q_{t,i}^C;q_{t,i}^R]^T\\times[k_{j,i}^C;k_t^R]=(q_{t,i}^C,k_{j,i}^C)+(q_{t,i}^R,k_t^R) \\tag{14}\r$$å‰ä¸€éƒ¨åˆ†æŒ‰ç…§å…¬å¼ (12) è¿›è¡Œè®¡ç®—ï¼Œåä¸€éƒ¨åˆ†æŒ‰ç…§ MQA æ–¹å¼è®¡ç®—ã€‚å› æ­¤åªç”¨ç¼“å­˜ $c_t^{KV}$ å’Œ $k_t^R$.\nSource Code DeepSeek-V3 MLA å¯¹åº”çš„æºç ä½ç½®\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class MLA(nn.Module): \"\"\" Multi-Head Latent Attention (MLA) Layer. Attributes: dim (int): Dimensionality of the input features. n_heads (int): Number of attention heads. n_local_heads (int): Number of local attention heads for distributed systems. q_lora_rank (int): Rank for low-rank query projection. kv_lora_rank (int): Rank for low-rank key/value projection. qk_nope_head_dim (int): Dimensionality of non-positional query/key projections. qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections. qk_head_dim (int): Total dimensionality of query/key projections. v_head_dim (int): Dimensionality of value projections. softmax_scale (float): Scaling factor for softmax in attention computation. \"\"\" def __init__(self, args: ModelArgs): super().__init__() self.dim = args.dim self.n_heads = args.n_heads # è®¡ç®—å½“å‰è¿›ç¨‹ï¼ˆå¡ï¼‰è´Ÿè´£çš„æ³¨æ„åŠ›å¤´æ•°é‡ï¼Œç”¨äºæ¨¡å‹å¹¶è¡Œ self.n_local_heads = args.n_heads // world_size self.q_lora_rank = args.q_lora_rank self.kv_lora_rank = args.kv_lora_rank self.qk_nope_head_dim = args.qk_nope_head_dim self.qk_rope_head_dim = args.qk_rope_head_dim # QK å¤´æ€»ç»´åº¦ = é RoPE éƒ¨åˆ† + RoPE éƒ¨åˆ† self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim self.v_head_dim = args.v_head_dim # æŸ¥è¯¢æŠ•å½± (wq) çš„ LoRA å®ç° if self.q_lora_rank == 0: # å¦‚æœ q_lora_rank ä¸º 0ï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ LoRAï¼Œç›´æ¥è¿›è¡Œå…¨ç§©æŠ•å½± # å°† dim ç»´åº¦çš„è¾“å…¥æŠ•å½±åˆ° n_heads * qk_head_dim ç»´åº¦ self.wq = ColumnParallelLinear(self.dim, self.n_heads * self.qk_head_dim) else: # å¦‚æœ q_lora_rank \u003e 0ï¼Œä½¿ç”¨ LoRA ç»“æ„è¿›è¡Œä½ç§©æŠ•å½± # wq_a: dim -\u003e q_lora_rank (ä½ç§©æŠ•å½±çš„ç¬¬ä¸€æ­¥) self.wq_a = Linear(self.dim, self.q_lora_rank) # q_norm: RMSNorm åº”ç”¨äºä½ç§©ç»´åº¦ self.q_norm = RMSNorm(self.q_lora_rank) # wq_b: q_lora_rank -\u003e n_heads * qk_head_dim (ä½ç§©æŠ•å½±çš„ç¬¬äºŒæ­¥) self.wq_b = ColumnParallelLinear(self.q_lora_rank, self.n_heads * self.qk_head_dim) # é”®å€¼æŠ•å½± (wkv) çš„ LoRA å®ç° # wkv_a: dim -\u003e kv_lora_rank + qk_rope_head_dim # å¯¹åº”å›¾ä¸­çš„ W^{DKV} æŠ•å½±åˆ°ä½ç§© KV æ½œåœ¨ç©ºé—´ (kv_lora_rank) å’Œè§£è€¦çš„ RoPE é”® (qk_rope_head_dim) # è¿™é‡Œçš„ kv_lora_rank å¯¹åº”å…¬å¼ä¸­çš„ d_c # è¿™é‡Œçš„ qk_rope_head_dim å¯¹åº”å…¬å¼ä¸­çš„ d_h self.wkv_a = Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim) # kv_norm: RMSNorm åº”ç”¨äºä½ç§©ç»´åº¦ self.kv_norm = RMSNorm(self.kv_lora_rank) # wkv_b: kv_lora_rank -\u003e n_heads * (qk_nope_head_dim + v_head_dim) # å¯¹åº”å›¾ä¸­çš„ W^{UK} å’Œ W^{UV} çš„ç»„åˆ # å®ƒå°†å‹ç¼©åçš„ KV æ½œåœ¨å‘é‡ (kv_lora_rank) æŠ•å½±å›é RoPE é”® (qk_nope_head_dim) å’Œå€¼ (v_head_dim) çš„é«˜ç»´åº¦ç©ºé—´ self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim)) # è¾“å‡ºæŠ•å½± (wo) self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim) # Softmax ç¼©æ”¾å› å­ï¼Œç”¨äºæ³¨æ„åŠ›åˆ†æ•°çš„ç¼©æ”¾ï¼Œé˜²æ­¢å†…ç§¯è¿‡å¤§ self.softmax_scale = self.qk_head_dim ** -0.5 # å¦‚æœåºåˆ—é•¿åº¦è¶…è¿‡åŸå§‹è®­ç»ƒé•¿åº¦ï¼Œæ ¹æ® RopeFactor è¿›è¡Œé¢å¤–ç¼©æ”¾ï¼Œç”¨äºå¤„ç†é•¿åºåˆ—å¤–æ¨é—®é¢˜ if args.max_seq_len \u003e args.original_seq_len: mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0 self.softmax_scale = self.softmax_scale * mscale * mscale # æ ¹æ®æ³¨æ„åŠ›å®ç°æ–¹å¼ï¼ˆnaive æˆ– optimizedï¼‰é€‰æ‹©ä¸åŒçš„ KV ç¼“å­˜ç»“æ„ if attn_impl == \"naive\": # naive å®ç°ç›´æ¥ç¼“å­˜å®Œæ•´é”® K å’Œå€¼ V # k_cache: (max_batch_size, max_seq_len, n_local_heads, qk_head_dim) self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim), persistent=False) # v_cache: (max_batch_size, max_seq_len, n_local_heads, v_head_dim) self.register_buffer(\"v_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim), persistent=False) else: # optimized å®ç°ç¼“å­˜å‹ç¼©åçš„ KV æ½œåœ¨å‘é‡å’Œè§£è€¦çš„ RoPE é”® # kv_cache: (max_batch_size, max_seq_len, kv_lora_rank) - å¯¹åº”è®ºæ–‡ä¸­çš„ c_t^{KV} self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False) # pe_cache: (max_batch_size, max_seq_len, qk_rope_head_dim) - å¯¹åº”è®ºæ–‡ä¸­çš„ k_t^R self.register_buffer(\"pe_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False) def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]): \"\"\" Forward pass for the Multi-Head Latent Attention (MLA) Layer. Args: x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim). start_pos (int): Starting position in the sequence for caching. freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings. mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention. Returns: torch.Tensor: Output tensor with the same shape as the input. \"\"\" bsz, seqlen, _ = x.size() end_pos = start_pos + seqlen # 1. æŸ¥è¯¢ (Q) çš„ç”Ÿæˆ if self.q_lora_rank == 0: # å…¨ç§©æŠ•å½± q = self.wq(x) else: # LoRA æŠ•å½±ï¼šx -\u003e wq_a -\u003e q_norm -\u003e wq_b q = self.wq_b(self.q_norm(self.wq_a(x))) # reshape Q q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim) # åˆ†ç¦» Q çš„é RoPE éƒ¨åˆ†å’Œ RoPE éƒ¨åˆ† # q_nope å¯¹åº”è®ºæ–‡ä¸­çš„ q_{t,i}^C (éä½ç½®ä¿¡æ¯æŸ¥è¯¢) # q_pe å¯¹åº”è®ºæ–‡ä¸­çš„ q_{t,i}^R (æºå¸¦ RoPE çš„è§£è€¦æŸ¥è¯¢) q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1) # å¯¹ Q çš„ RoPE éƒ¨åˆ†åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç  # å¯¹åº”è®ºæ–‡ä¸­çš„ q_t^R = RoPE(W^{QR}c_t^Q) çš„ RoPE éƒ¨åˆ† q_pe = apply_rotary_emb(q_pe, freqs_cis) # 2. é”®å€¼ (KV) çš„ç”Ÿæˆ # å°†è¾“å…¥ x æŠ•å½±åˆ°ä½ç§© KV æ½œåœ¨ç©ºé—´å’Œè§£è€¦çš„ RoPE é”® # å¯¹åº”è®ºæ–‡ä¸­çš„ c_t^{KV} å’Œ k_t^R kv = self.wkv_a(x) # åˆ†ç¦»å‡º KV æ½œåœ¨å‘é‡å’Œè§£è€¦çš„ RoPE é”® # kv å¯¹åº”è®ºæ–‡ä¸­çš„ c_t^{KV} # k_pe å¯¹åº”è®ºæ–‡ä¸­çš„ k_t^R (RoPE è§£è€¦é”®) kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1) # å¯¹ K çš„ RoPE éƒ¨åˆ†åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç  # æ³¨æ„ k_pe.unsqueeze(2) æ˜¯å› ä¸º apply_rotary_emb æœŸæœ› (..., seq_len, head_dim) ç»“æ„ # è¿™é‡Œçš„ k_pe å¯èƒ½æ˜¯ (bsz, seqlen, qk_rope_head_dim)ï¼Œéœ€è¦æ·»åŠ ä¸€ä¸ª head ç»´åº¦ k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis) # 3. æ³¨æ„åŠ›è®¡ç®—ï¼šæ ¹æ®å®ç°æ–¹å¼ (naive æˆ– optimized) if attn_impl == \"naive\": # Naive å®ç°ç›´æ¥æ‹¼æ¥ Q çš„ RoPE å’Œé RoPE éƒ¨åˆ† q = torch.cat([q_nope, q_pe], dim=-1) # Q æ¢å¤ä¸º (bsz, seqlen, n_local_heads, qk_head_dim) # å¯¹ KV æ½œåœ¨å‘é‡åº”ç”¨å½’ä¸€åŒ–ï¼Œå¹¶è¿›è¡Œç¬¬äºŒé˜¶æ®µæŠ•å½± # å¯¹åº”è®ºæ–‡ä¸­å°† c_t^{KV} æŠ•å½±åˆ°é RoPE é”®å’Œå€¼çš„éƒ¨åˆ† (k_t^C å’Œ v_t^C) kv = self.wkv_b(self.kv_norm(kv)) # å°† KV ç»“æœé‡å¡‘ä¸º (batch_size, seq_len, n_local_heads, qk_nope_head_dim + v_head_dim) kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim) # åˆ†ç¦»å‡ºé RoPE é”®å’Œå€¼ k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1) # æ‹¼æ¥é RoPE é”®å’Œ RoPE é”®ï¼Œç»„æˆå®Œæ•´çš„é”® K # k_pe ä¹‹å‰æ˜¯ (bsz, seqlen, 1, qk_rope_head_dim)ï¼Œéœ€è¦ expand åˆ° n_local_heads k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1) # æ›´æ–° K å’Œ V ç¼“å­˜ (åœ¨æ¨ç†æ—¶ç”¨äºè‡ªå›å½’ç”Ÿæˆ) self.k_cache[:bsz, start_pos:end_pos] = k self.v_cache[:bsz, start_pos:end_pos] = v # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Q @ K^T) # scores: (batch_size, q_seq_len, n_local_heads, k_seq_len) # ä½¿ç”¨æ•´ä¸ªç¼“å­˜ä¸­çš„é”®è¿›è¡Œè®¡ç®— scores = torch.einsum(\"bshd,bthd-\u003ebsht\", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale else: # optimized å®ç° # è·å– wkv_b æƒé‡ï¼Œå¦‚æœä½¿ç”¨äº†é‡åŒ–åˆ™è¿›è¡Œåé‡åŒ– wkv_b = self.wkv_b.weight if self.wkv_b.scale is None else weight_dequant(self.wkv_b.weight, self.wkv_b.scale, block_size) # å°† wkv_b é‡å¡‘ä¸º (n_local_heads, head_dim, kv_lora_rank) ä»¥ä¾¿è¿›è¡Œé€å¤´çš„æ“ä½œ wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank) # (n_heads, (qk_nope+v), kv_rank) # è®¡ç®— Q_nope ä¸ K_nope çš„ç‚¹ç§¯ (é€šè¿‡ kv ç¼“å­˜) # q_nope: (bsz, seqlen, n_local_heads, qk_nope_head_dim) # wkv_b[:, :self.qk_nope_head_dim] æ˜¯ W^{UK} çš„éƒ¨åˆ† # è¿™å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(q_{t,i}^C @ c_{j,i}^{KV}) çš„ç¬¬ä¸€é¡¹ q_nope = torch.einsum(\"bshd,hdc-\u003ebshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim]) # æ›´æ–° KV ç¼“å­˜ (kv_cache å¯¹åº”è®ºæ–‡ä¸­çš„ c_t^{KV}) self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv) # æ›´æ–° PE ç¼“å­˜ (pe_cache å¯¹åº”è®ºæ–‡ä¸­çš„ k_t^R) # k_pe ä¹‹å‰æ˜¯ (bsz, seqlen, 1, qk_rope_head_dim)ï¼Œsqueeze æ‰é‚£ä¸ª 1 ç»´åº¦ self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2) # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° # ç¬¬ä¸€é¡¹: é RoPE æŸ¥è¯¢ q_nope ä¸ç¼“å­˜çš„ kv_cache (å‹ç¼©é”®) çš„ç‚¹ç§¯ # å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(q_{t,i}^C @ c_{j,i}^{KV}) çš„ç¬¬ä¸€éƒ¨åˆ† scores = torch.einsum(\"bshc,btc-\u003ebsht\", q_nope, self.kv_cache[:bsz, :end_pos]) + \\ # ç¬¬äºŒé¡¹: RoPE æŸ¥è¯¢ q_pe ä¸ç¼“å­˜çš„ pe_cache (è§£è€¦ RoPE é”®) çš„ç‚¹ç§¯ # å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(q_{t,i}^R @ k_{j,i}^R) çš„ç¬¬äºŒéƒ¨åˆ† torch.einsum(\"bshr,btr-\u003ebsht\", q_pe, self.pe_cache[:bsz, :end_pos]) scores *= self.softmax_scale # åº”ç”¨ç¼©æ”¾å› å­ # åº”ç”¨æ³¨æ„åŠ›æ©ç  (å¦‚å› æœæ©ç ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯) if mask is not None: scores += mask.unsqueeze(1) # unsqueeze(1) å¹¿æ’­åˆ° heads ç»´åº¦ # å¯¹åˆ†æ•°åº”ç”¨ Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡ scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x) # 4. å€¼ (V) çš„åŠ æƒæ±‚å’Œ if attn_impl == \"naive\": # Naive å®ç°ç›´æ¥ä¸ V ç¼“å­˜è¿›è¡Œç‚¹ç§¯ # å¯¹åº”è®ºæ–‡ä¸­çš„ sum(Softmax(...) * v_{j,i}^C) x = torch.einsum(\"bsht,bthd-\u003ebshd\", scores, self.v_cache[:bsz, :end_pos]) else: # optimized å®ç° # optimized å®ç°é€šè¿‡ wkv_b çš„å€¼éƒ¨åˆ†å°†åŠ æƒåçš„å‹ç¼© KV è¿˜åŸä¸º V # ç¬¬ä¸€æ­¥: å°†æ³¨æ„åŠ›æƒé‡ä¸ç¼“å­˜çš„ kv_cache (å‹ç¼©å€¼) è¿›è¡Œç‚¹ç§¯ # å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(...) * c_{j,i}^{KV} çš„ç¬¬ä¸€éƒ¨åˆ† x = torch.einsum(\"bsht,btc-\u003ebshc\", scores, self.kv_cache[:bsz, :end_pos]) # ç¬¬äºŒæ­¥: å°†åŠ æƒåçš„å‹ç¼©å€¼é€šè¿‡ wkv_b çš„å€¼æŠ•å½±éƒ¨åˆ†è¿˜åŸä¸ºæœ€ç»ˆçš„å€¼å‘é‡ # wkv_b[:, -self.v_head_dim:] æ˜¯ W^{UV} çš„éƒ¨åˆ† # å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(...) * v_{j,i}^C çš„ç¬¬äºŒéƒ¨åˆ† x = torch.einsum(\"bshc,hdc-\u003ebshd\", x, wkv_b[:, -self.v_head_dim:]) # å°†æ‰€æœ‰å¤´çš„ç»“æœå±•å¹³å¹¶è¿›è¡Œæœ€ç»ˆçš„è¾“å‡ºæŠ•å½± x = self.wo(x.flatten(2)) # x.flatten(2) å°† (bsz, seqlen, n_local_heads, v_head_dim) å±•å¹³ä¸º (bsz, seqlen, n_local_heads * v_head_dim) return x ",
  "wordCount" : "3711",
  "inLanguage": "en",
  "datePublished": "2025-06-19T11:24:45+08:00",
  "dateModified": "2025-06-22T17:53:30+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/deepseek/deepseekmla/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="ç®€ä½“ä¸­æ–‡"
                            aria-label="ç®€ä½“ä¸­æ–‡">ç®€ä½“ä¸­æ–‡</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="ğŸ  Home">
                    <span>ğŸ  Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="ğŸ™‹ğŸ»â€â™‚ï¸ Me">
                    <span>ğŸ™‹ğŸ»â€â™‚ï¸ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="ğŸ“š Blogs">
                    <span>ğŸ“š Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="ğŸ§© Categories">
                    <span>ğŸ§© Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="ğŸ”– Tags">
                    <span>ğŸ”– Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="â± Archive">
                    <span>â± Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="ğŸ” Search (Alt &#43; /)" accesskey=/>
                    <span>ğŸ” Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="ğŸ¤ Friends">
                    <span>ğŸ¤ Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/blogs/deepseek/">DeepSeek Related</a></div>
    <h1 class="post-title entry-hint-parent">
      DeepSeekMLA
    </h1>
    <div class="post-description">
      Principle of DeepSeekV3 MLA
    </div>
    <div class="post-meta"><span title='2025-06-19 11:24:45 +0800 CST'>Jun-19-2025</span>&nbsp;Â·&nbsp;8 min&nbsp;Â·&nbsp;3711 words&nbsp;Â·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#preliminary-what-is-rope" aria-label="Preliminary: What is RoPE">Preliminary: What is RoPE</a><ul>
                            
                    <li>
                        <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                    <li>
                        <a href="#formular" aria-label="Formular">Formular</a></li></ul>
                    </li>
                    <li>
                        <a href="#workflow" aria-label="Workflow">Workflow</a><ul>
                            
                    <li>
                        <a href="#notation" aria-label="Notation">Notation</a></li>
                    <li>
                        <a href="#kv-compression" aria-label="KV Compression">KV Compression</a></li>
                    <li>
                        <a href="#q-compression" aria-label="Q Compression">Q Compression</a></li>
                    <li>
                        <a href="#attention-computation" aria-label="Attention Computation">Attention Computation</a></li></ul>
                    </li>
                    <li>
                        <a href="#why-decoupled-rope" aria-label="Why Decoupled RoPE">Why Decoupled RoPE</a></li>
                    <li>
                        <a href="#source-code" aria-label="Source Code">Source Code</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="preliminary-what-is-rope">Preliminary: What is RoPE<a hidden class="anchor" aria-hidden="true" href="#preliminary-what-is-rope">#</a></h1>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>æ—‹è½¬ä½ç½®ç¼–ç  (RoPE) æ˜¯ä¸€ç§æ–°é¢–çš„ã€åŸºäºç›¸å¯¹ä½ç½®çš„ç¼–ç æ–¹æ³•ï¼Œå®ƒè¢«è®¾è®¡ç”¨äºæé«˜ Transformer æ¨¡å‹å¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„ç»å¯¹ä½ç½®ç¼–ç  (å¦‚æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç ) æˆ–ç›´æ¥çš„ç›¸å¯¹ä½ç½®ç¼–ç  (å¦‚ T5 ä¸­ä½¿ç”¨çš„ç›¸å¯¹åç½®) ä¸åŒï¼ŒRoPE å°†ä½ç½®ä¿¡æ¯é›†æˆåˆ°è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ Q å’Œ K çš„è¡¨ç¤ºä¸­ï¼Œä½¿å¾— Q å’Œ K çš„ç‚¹ç§¯è‡ªç„¶åœ°ç¼–ç äº†<strong>ç›¸å¯¹ä½ç½®ä¿¡æ¯</strong>ã€‚</p>
<p>RoPE çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œé€šè¿‡å¯¹æŸ¥è¯¢å’Œé”®å‘é‡åº”ç”¨ç‰¹å®šçš„æ—‹è½¬æ“ä½œï¼Œä½¿å¾—ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯ç»“æœåªä¾èµ–äºå®ƒä»¬ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹ä½ç½®ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç½®ä¿¡æ¯æ—¶æ›´åŠ é«˜æ•ˆã€‚</p>
<p><strong>RoPE çš„ä¸»è¦ä¼˜ç‚¹åŒ…æ‹¬ï¼š</strong></p>
<ul>
<li><strong>ç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼š</strong> è‡ªç„¶åœ°å°†ç›¸å¯¹ä½ç½®ä¿¡æ¯èå…¥åˆ°æ³¨æ„åŠ›åˆ†æ•°ä¸­ã€‚</li>
<li><strong>é•¿åºåˆ—å¤–æ¨èƒ½åŠ›ï¼š</strong> æé«˜äº†æ¨¡å‹åœ¨è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´é•¿åºåˆ—ä¸Šçš„æ€§èƒ½ã€‚</li>
<li><strong>ä¸è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å…¼å®¹æ€§ï¼š</strong> æ— ç¼é›†æˆåˆ° QKV ç‚¹ç§¯æ³¨æ„åŠ›ä¸­ã€‚</li>
<li><strong>ç®€å•ä¸”é«˜æ•ˆï¼š</strong> å®ç°ç›¸å¯¹ç®€å•ï¼Œä¸”ä¸ä¼šæ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚åº¦ã€‚</li>
</ul>
<h2 id="formular">Formular<a hidden class="anchor" aria-hidden="true" href="#formular">#</a></h2>
<p>RoPE çš„ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡å¯¹æŸ¥è¯¢ $q$ å’Œé”® $k$ åº”ç”¨ä¸€ä¸ªæ—‹è½¬çŸ©é˜µ $R_t$ (å–å†³äºå…¶ç»å¯¹ä½ç½® $t$) ï¼Œä½¿å¾—ç‚¹ç§¯ $q_m^T k_n$ èƒ½å¤Ÿé€šè¿‡æŸç§æ–¹å¼è½¬åŒ–ä¸ºåªä¾èµ–äºç›¸å¯¹ä½ç½® $m-n$ çš„å‡½æ•°ã€‚</p>
<p>å¯¹äºä¸€ä¸ªå‘é‡ $x \in \mathbb{R}^d$ åœ¨ä½ç½® $m$ å¤„ï¼ŒRoPE çš„å˜æ¢å‡½æ•° $f(x, m)$ å¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼š</p>
<p>å¦‚æœå‘é‡ç»´åº¦æ˜¯å¶æ•° $d$ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†æˆ $d/2$ å¯¹ï¼Œæ¯å¯¹æ‰§è¡Œä¸€ä¸ªäºŒç»´æ—‹è½¬ã€‚
å¯¹äºå‘é‡ $x = [x_0, x_1, \ldots, x_{d-1}]^T$ï¼ŒRoPE å¯¹å…¶æ¯ä¸ªç»´åº¦å¯¹ $(x_{2i}, x_{2i+1})$ åº”ç”¨æ—‹è½¬ï¼š</p>
$$
f(x, m)_{2i} = x_{2i} \cos(m\theta_i) - x_{2i+1} \sin(m\theta_i) \\
f(x, m)_{2i+1} = x_{2i} \sin(m\theta_i) + x_{2i+1} \cos(m\theta_i)
$$<p>å…¶ä¸­ $\theta_i$ æ˜¯é¢„è®¾çš„é¢‘ç‡ï¼Œé€šå¸¸å®šä¹‰ä¸º $\theta_i = 10000^{-2i/d}$. $i=0, \dots, d/2 - 1$ æ˜¯ç»´åº¦å¯¹çš„ç´¢å¼•ã€‚</p>
<p><strong>ç”¨çŸ©é˜µå½¢å¼è¡¨ç¤ºï¼š</strong>
æˆ‘ä»¬å¯ä»¥å°†è¿™ç§æ—‹è½¬æ“ä½œè¡¨ç¤ºä¸ºä¸€ä¸ªç¨€ç–çš„å—å¯¹è§’çŸ©é˜µ $R_m^d$ï¼Œå…¶å½¢å¼ä¸ºï¼š
</p>
$$R_m^d = \begin{pmatrix}
\cos(m\theta_0) & -\sin(m\theta_0) & 0 & 0 & \cdots \\
\sin(m\theta_0) & \cos(m\theta_0) & 0 & 0 & \cdots \\
0 & 0 & \cos(m\theta_1) & -\sin(m\theta_1) & \cdots \\
0 & 0 & \sin(m\theta_1) & \cos(m\theta_1) & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}$$<p>
é‚£ä¹ˆï¼Œç»è¿‡ RoPE ç¼–ç çš„æŸ¥è¯¢å’Œé”®å¯ä»¥è¡¨ç¤ºä¸ºï¼š
</p>
$$\mathbf{q}_m = R_m^d \mathbf{q}$$<p>
</p>
$$\mathbf{k}_n = R_n^d \mathbf{k}$$<p>
å…¶ä¸­ $\mathbf{q}$ å’Œ $\mathbf{k}$ æ˜¯åŸå§‹çš„æŸ¥è¯¢å’Œé”®å‘é‡ (ä¸å«ä½ç½®ä¿¡æ¯) ï¼Œ$\mathbf{q}_m$ å’Œ $\mathbf{k}_n$ æ˜¯ç»è¿‡ RoPE å¤„ç†åçš„æŸ¥è¯¢å’Œé”®å‘é‡ã€‚</p>
<p><strong>RoPE çš„å…³é”®ç‰¹æ€§ï¼šç‚¹ç§¯ä¸ç›¸å¯¹ä½ç½®</strong>
ç»è¿‡ RoPE å˜æ¢åï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ç‚¹ç§¯å¯ä»¥åˆ†è§£ä¸ºï¼š
</p>
$$\mathbf{q}_m^T \mathbf{k}_n = (R_m^d \mathbf{q})^T (R_n^d \mathbf{k})$$<p>
ç”±äº $R_m^d$ æ˜¯æ­£äº¤çŸ©é˜µï¼Œå…¶é€†çŸ©é˜µç­‰äºå…¶è½¬ç½®ï¼Œå³ $(R_m^d)^T = (R_m^d)^{-1} = R_{-m}^d$. å› æ­¤æœ‰
</p>
$$\mathbf{q}_m^T \mathbf{k}_n = \mathbf{q}^T (R_m^d)^T R_n^d \mathbf{k} = \mathbf{q}^T R_{-m}^d R_n^d \mathbf{k} = \mathbf{q}^T R_{n-m}^d \mathbf{k}$$<p>
è¿™ä¸ªæœ€ç»ˆç»“æœ $\mathbf{q}^T R_{n-m}^d \mathbf{k}$ è¡¨æ˜ï¼Œä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯åªä¾èµ–äºå®ƒä»¬çš„<strong>ç›¸å¯¹ä½ç½®å·® $n-m$</strong>ï¼Œè€Œä¸å®ƒä»¬çš„ç»å¯¹ä½ç½® $n$ å’Œ $m$ æ— å…³ã€‚è¿™å°±æ˜¯ RoPE èƒ½å¤Ÿç¼–ç ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„æ•°å­¦åŸºç¡€ã€‚</p>
<h1 id="workflow">Workflow<a hidden class="anchor" aria-hidden="true" href="#workflow">#</a></h1>
<h2 id="notation">Notation<a hidden class="anchor" aria-hidden="true" href="#notation">#</a></h2>
<ul>
<li>$d$: embedding ç»´åº¦</li>
<li>$d_h$: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦</li>
<li>$\mathbf{h}_t\in\mathbb{R}^d$: æŸä¸ª attention å±‚ç¬¬ t ä¸ª token çš„è¾“å…¥ã€‚</li>
</ul>
<h2 id="kv-compression">KV Compression<a hidden class="anchor" aria-hidden="true" href="#kv-compression">#</a></h2>
$$
\textcolor{red}{c_t^{KV}} = W^{DKV}h_t  \tag{1}
$$<p>
</p>
$$
[k_{t,1}^{C}, k_{t,2}^{C}, \ldots, k_{t,n_h}^{C}] = k_t^C = W^{UK}c_t^{KV}  \tag{2}
$$<p>
</p>
$$
\textcolor{red}{k_t^R} = \text{RoPE}(W^{KR}h_t)  \tag{3}
$$<p>
</p>
$$
k_{t,i} = [k_{t,i}^C, k_{t}^R] \tag{4}
$$<p>
</p>
$$
[v_{t,1}^C, v_{t,2}^C, \ldots, v_{t,n_h}^C] = v_t^C = W^{UV}c_t^{KV} \tag{5}
$$<ul>
<li>$c_t^{KV} \in \mathbb{R}^{d_c}$: å‹ç¼©åçš„ KV æ½œåœ¨å‘é‡ã€‚</li>
<li>$d_c (\ll d_h n_h)$: KV å‹ç¼©åˆ°çš„ç»´åº¦ã€‚</li>
<li>$W^{DKV} \in \mathbb{R}^{d_c \times d}$: KV é™ç»´æŠ•å½±çŸ©é˜µã€‚</li>
<li>$W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ åˆ†åˆ«æ˜¯ K &amp; V çš„å‡ç»´æŠ•å½±çŸ©é˜µã€‚</li>
<li>$W^{KR} \in \mathbb{R}^{d_h^R \times d}$: ç”¨äºç”Ÿæˆæºå¸¦ RoPE çš„è§£è€¦é”®çš„çŸ©é˜µ (Su et al., 2024)</li>
</ul>
<p>çº¢è‰²çš„æ˜¯éœ€è¦ç¼“å­˜çš„å‘é‡ï¼Œåç»­è¯´æ˜åŸå› ã€‚æ³¨æ„åˆ°å¯¹ K è¿›è¡Œ RoPE ä¹‹å‰æ˜¯å¯¹è¾“å…¥å‘é‡ä¹˜ä»¥äº†ä¸ªæŠ•å½±å†è¿›è¡Œçš„ã€‚è€Œä¸” K çš„æ¯ä¸ªæ³¨æ„åŠ›å¤´è¢«æ‹¼æ¥çš„éƒ½æ˜¯åŒä¸€ä¸ª $k_{t}^R$ï¼Œæœ‰ç‚¹ç±»ä¼¼äº MQA.</p>
<h2 id="q-compression">Q Compression<a hidden class="anchor" aria-hidden="true" href="#q-compression">#</a></h2>
$$c_t^Q = W^{DQ}h_t \tag{6}$$<p>
</p>
$$[q_{t,1}^C, q_{t,2}^C, \ldots, q_{t,n_h}^C] = q_t^C = W^{UQ}c_t^Q \tag{7}$$<p>
</p>
$$[q_{t,1}^R, q_{t,2}^R, \ldots, q_{t,n_h}^R] = q_t^R = \text{RoPE}(W^{QR}q_t^C) \tag{8}$$<p>
</p>
$$q_{t,i} = [q_{t,i}^C, q_{t,i}^R] \tag{9}$$<ul>
<li>$c_t^Q \in \mathbb{R}^{d_c'}$: Q å‹ç¼©åçš„æ½œåœ¨å‘é‡ã€‚</li>
<li>$d_c'(\ll d_h n_h)$ è¡¨ç¤º Q å‹ç¼©åçš„ç»´åº¦ã€‚</li>
<li>$W^{DQ} \in \mathbb{R}^{d_c' \times d}, W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}$: åˆ†åˆ«æ˜¯ Q çš„é™ç»´å’Œå‡ç»´çŸ©é˜µã€‚</li>
<li>$W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c'}$ æ˜¯ç”¨äºç”Ÿæˆæºå¸¦ RoPE çš„è§£è€¦ Q çš„çŸ©é˜µã€‚</li>
</ul>
<p>æ³¨æ„åˆ°å¯¹ Q çš„ RoPE æ˜¯åœ¨å‹ç¼©åè¿›è¡Œçš„ï¼Œå³ä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´éƒ½ç”Ÿæˆäº†ä¸€ä¸ªä½ç½®ç¼–ç ä¿¡æ¯åè¿›è¡Œæ‹¼æ¥ã€‚</p>
<h2 id="attention-computation">Attention Computation<a hidden class="anchor" aria-hidden="true" href="#attention-computation">#</a></h2>
<p>æœ€ç»ˆ $q_{t,i}$, $k_{j,i}$, $v_{j,i}^C$ è¢«ç»„åˆèµ·æ¥ä»¥ç”Ÿæˆæœ€ç»ˆçš„æ³¨æ„åŠ›è¾“å‡º $u_t$</p>
$$\mathbf{o}_{t,i} = \sum_{j=1}^{t} \text{Softmax}\left(\frac{q_{t,i}^T \mathbf{k}_{j,i}}{\sqrt{d_h + d_R}}\right)v_{j,i}^C \tag{10}$$<p>
</p>
$$\mathbf{u}_t = W^O[\mathbf{o}_{t,1}, \mathbf{o}_{t,2}, \ldots, \mathbf{o}_{t,n_h}] \tag{11}$$<ul>
<li>$W^O \in \mathbb{R}^{d \times d_h n_h}$: è¾“å‡ºæŠ•å½±çŸ©é˜µã€‚</li>
</ul>
<h1 id="why-decoupled-rope">Why Decoupled RoPE<a hidden class="anchor" aria-hidden="true" href="#why-decoupled-rope">#</a></h1>
<p>å‡è®¾ä¸åŠ  RoPE çš„æƒ…å†µä¸‹è¿›è¡Œ $q_{t,i}$, $k_{j,i}$ çš„å†…ç§¯åˆ™æœ‰</p>
$$
q_{t,i}^{T}\times k_{j,i}=(W_{(i)}^{UQ}c_{t}^{Q})^{T}\times W_{(i)}^{UK}c_{j}^{KV}=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}W_{(i)}^{UK}\times c_{j}^{KV} \tag{12}
$$<p>RoPE é€šè¿‡å¯¹å‘é‡åº”ç”¨ä¸€ä¸ª<strong>ä½ç½®ä¾èµ–çš„æ—‹è½¬å˜æ¢</strong>æ¥æ³¨å…¥ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚å¯¹äºä¸€ä¸ªå‘é‡ $X$ åœ¨ä½ç½® $t$ï¼ŒRoPE å¯ä»¥è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªæ—‹è½¬çŸ©é˜µ $R_t$ ä¹˜ä»¥ $X$ï¼š
</p>
$$\text{RoPE}(X, t) = R_t X$$<p>
è¿™é‡Œçš„ $R_t$ æ˜¯ä¸€ä¸ªæ­£äº¤æ—‹è½¬çŸ©é˜µï¼Œå®ƒå–å†³äºä½ç½® $t$.</p>
<p>å¦‚æœç›´æ¥å¯¹å‹ç¼©å $k_t^C$ çš„ ä½¿ç”¨ RoPE é‚£ä¹ˆæƒ…å†µä¼šå˜æˆ</p>
$$
\begin{aligned}
q_{t,i}^{T}\times k_{j,i}&=(\mathcal{R}_{t}W_{(i)}^{UQ}c_{t}^{Q})^{T}\times\mathcal{R}_{j}W_{(i)}^{UK}c_{j}^{KV} \\
&=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}\mathcal{R}_{t}^{T}\mathcal{R}_{j}W_{(i)}^{UK}\times c_{j}^{KV}\\
&=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}\mathcal{R}_{t-j}W_{(i)}^{UK}\times c_{j}^{KV}
\end{aligned} \tag{13}
$$<p>ä¸­é—´çš„çŸ©é˜µä¸ç›¸å¯¹ä½ç½®æœ‰å…³ï¼Œæ— æ³•æå‰è®¡ç®—å‡ºæ¥ã€‚å› æ­¤æ–‡ä¸­å°±æ˜¯å¯¹æ‰€æœ‰å¤´éƒ½ä½¿ç”¨åŒä¸€ä¸ª k å’Œè®¡ç®— RoPE. æ‹¼æ¥åçš„å‘é‡å†è®¡ç®—æ—¶</p>
$$
q_{t,i}^T\times k_{j,i}=[q_{t,i}^C;q_{t,i}^R]^T\times[k_{j,i}^C;k_t^R]=(q_{t,i}^C,k_{j,i}^C)+(q_{t,i}^R,k_t^R) \tag{14}
$$<p>å‰ä¸€éƒ¨åˆ†æŒ‰ç…§å…¬å¼ (12) è¿›è¡Œè®¡ç®—ï¼Œåä¸€éƒ¨åˆ†æŒ‰ç…§ MQA æ–¹å¼è®¡ç®—ã€‚å› æ­¤åªç”¨ç¼“å­˜ $c_t^{KV}$ å’Œ $k_t^R$.</p>
<h1 id="source-code">Source Code<a hidden class="anchor" aria-hidden="true" href="#source-code">#</a></h1>
<p><a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/f6e34dd26772dd4a216be94a8899276c5dca9e43/inference/model.py#L393-L494">DeepSeek-V3 MLA</a> å¯¹åº”çš„æºç ä½ç½®</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MLA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Multi-Head Latent Attention (MLA) Layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Attributes:
</span></span></span><span class="line"><span class="cl"><span class="s2">        dim (int): Dimensionality of the input features.
</span></span></span><span class="line"><span class="cl"><span class="s2">        n_heads (int): Number of attention heads.
</span></span></span><span class="line"><span class="cl"><span class="s2">        n_local_heads (int): Number of local attention heads for distributed systems.
</span></span></span><span class="line"><span class="cl"><span class="s2">        q_lora_rank (int): Rank for low-rank query projection.
</span></span></span><span class="line"><span class="cl"><span class="s2">        kv_lora_rank (int): Rank for low-rank key/value projection.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_head_dim (int): Total dimensionality of query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        v_head_dim (int): Dimensionality of value projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        softmax_scale (float): Scaling factor for softmax in attention computation.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">ModelArgs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># è®¡ç®—å½“å‰è¿›ç¨‹ï¼ˆå¡ï¼‰è´Ÿè´£çš„æ³¨æ„åŠ›å¤´æ•°é‡ï¼Œç”¨äºæ¨¡å‹å¹¶è¡Œ</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">//</span> <span class="n">world_size</span> 
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">q_lora_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">kv_lora_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_nope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># QK å¤´æ€»ç»´åº¦ = é RoPE éƒ¨åˆ† + RoPE éƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">v_head_dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># æŸ¥è¯¢æŠ•å½± (wq) çš„ LoRA å®ç°</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å¦‚æœ q_lora_rank ä¸º 0ï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ LoRAï¼Œç›´æ¥è¿›è¡Œå…¨ç§©æŠ•å½±</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å°† dim ç»´åº¦çš„è¾“å…¥æŠ•å½±åˆ° n_heads * qk_head_dim ç»´åº¦</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å¦‚æœ q_lora_rank &gt; 0ï¼Œä½¿ç”¨ LoRA ç»“æ„è¿›è¡Œä½ç§©æŠ•å½±</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wq_a: dim -&gt; q_lora_rank (ä½ç§©æŠ•å½±çš„ç¬¬ä¸€æ­¥)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># q_norm: RMSNorm åº”ç”¨äºä½ç§©ç»´åº¦</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wq_b: q_lora_rank -&gt; n_heads * qk_head_dim (ä½ç§©æŠ•å½±çš„ç¬¬äºŒæ­¥)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq_b</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># é”®å€¼æŠ•å½± (wkv) çš„ LoRA å®ç°</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># wkv_a: dim -&gt; kv_lora_rank + qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># å¯¹åº”å›¾ä¸­çš„ W^{DKV} æŠ•å½±åˆ°ä½ç§© KV æ½œåœ¨ç©ºé—´ (kv_lora_rank) å’Œè§£è€¦çš„ RoPE é”® (qk_rope_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># è¿™é‡Œçš„ kv_lora_rank å¯¹åº”å…¬å¼ä¸­çš„ d_c</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># è¿™é‡Œçš„ qk_rope_head_dim å¯¹åº”å…¬å¼ä¸­çš„ d_h</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wkv_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># kv_norm: RMSNorm åº”ç”¨äºä½ç§©ç»´åº¦</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># wkv_b: kv_lora_rank -&gt; n_heads * (qk_nope_head_dim + v_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># å¯¹åº”å›¾ä¸­çš„ W^{UK} å’Œ W^{UV} çš„ç»„åˆ</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># å®ƒå°†å‹ç¼©åçš„ KV æ½œåœ¨å‘é‡ (kv_lora_rank) æŠ•å½±å›é RoPE é”® (qk_nope_head_dim) å’Œå€¼ (v_head_dim) çš„é«˜ç»´åº¦ç©ºé—´</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># è¾“å‡ºæŠ•å½± (wo)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">RowParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Softmax ç¼©æ”¾å› å­ï¼Œç”¨äºæ³¨æ„åŠ›åˆ†æ•°çš„ç¼©æ”¾ï¼Œé˜²æ­¢å†…ç§¯è¿‡å¤§</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># å¦‚æœåºåˆ—é•¿åº¦è¶…è¿‡åŸå§‹è®­ç»ƒé•¿åº¦ï¼Œæ ¹æ® RopeFactor è¿›è¡Œé¢å¤–ç¼©æ”¾ï¼Œç”¨äºå¤„ç†é•¿åºåˆ—å¤–æ¨é—®é¢˜</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">original_seq_len</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">mscale</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">mscale</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">rope_factor</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">*</span> <span class="n">mscale</span> <span class="o">*</span> <span class="n">mscale</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># æ ¹æ®æ³¨æ„åŠ›å®ç°æ–¹å¼ï¼ˆnaive æˆ– optimizedï¼‰é€‰æ‹©ä¸åŒçš„ KV ç¼“å­˜ç»“æ„</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># naive å®ç°ç›´æ¥ç¼“å­˜å®Œæ•´é”® K å’Œå€¼ V</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_cache: (max_batch_size, max_seq_len, n_local_heads, qk_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;k_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># v_cache: (max_batch_size, max_seq_len, n_local_heads, v_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;v_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># optimized å®ç°ç¼“å­˜å‹ç¼©åçš„ KV æ½œåœ¨å‘é‡å’Œè§£è€¦çš„ RoPE é”®</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># kv_cache: (max_batch_size, max_seq_len, kv_lora_rank) - å¯¹åº”è®ºæ–‡ä¸­çš„ c_t^{KV}</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;kv_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># pe_cache: (max_batch_size, max_seq_len, qk_rope_head_dim) - å¯¹åº”è®ºæ–‡ä¸­çš„ k_t^R</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;pe_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Forward pass for the Multi-Head Latent Attention (MLA) Layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).
</span></span></span><span class="line"><span class="cl"><span class="s2">            start_pos (int): Starting position in the sequence for caching.
</span></span></span><span class="line"><span class="cl"><span class="s2">            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s2">            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">            torch.Tensor: Output tensor with the same shape as the input.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_pos</span> <span class="o">=</span> <span class="n">start_pos</span> <span class="o">+</span> <span class="n">seqlen</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. æŸ¥è¯¢ (Q) çš„ç”Ÿæˆ</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å…¨ç§©æŠ•å½±</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># LoRA æŠ•å½±ï¼šx -&gt; wq_a -&gt; q_norm -&gt; wq_b</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wq_a</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape Q</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># åˆ†ç¦» Q çš„é RoPE éƒ¨åˆ†å’Œ RoPE éƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># q_nope å¯¹åº”è®ºæ–‡ä¸­çš„ q_{t,i}^C (éä½ç½®ä¿¡æ¯æŸ¥è¯¢)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># q_pe å¯¹åº”è®ºæ–‡ä¸­çš„ q_{t,i}^R (æºå¸¦ RoPE çš„è§£è€¦æŸ¥è¯¢)</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_nope</span><span class="p">,</span> <span class="n">q_pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># å¯¹ Q çš„ RoPE éƒ¨åˆ†åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># å¯¹åº”è®ºæ–‡ä¸­çš„ q_t^R = RoPE(W^{QR}c_t^Q) çš„ RoPE éƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_pe</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">q_pe</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. é”®å€¼ (KV) çš„ç”Ÿæˆ</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># å°†è¾“å…¥ x æŠ•å½±åˆ°ä½ç§© KV æ½œåœ¨ç©ºé—´å’Œè§£è€¦çš„ RoPE é”®</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># å¯¹åº”è®ºæ–‡ä¸­çš„ c_t^{KV} å’Œ k_t^R</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># åˆ†ç¦»å‡º KV æ½œåœ¨å‘é‡å’Œè§£è€¦çš„ RoPE é”®</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># kv å¯¹åº”è®ºæ–‡ä¸­çš„ c_t^{KV}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># k_pe å¯¹åº”è®ºæ–‡ä¸­çš„ k_t^R (RoPE è§£è€¦é”®)</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span><span class="p">,</span> <span class="n">k_pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># å¯¹ K çš„ RoPE éƒ¨åˆ†åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç </span>
</span></span><span class="line"><span class="cl">        <span class="c1"># æ³¨æ„ k_pe.unsqueeze(2) æ˜¯å› ä¸º apply_rotary_emb æœŸæœ› (..., seq_len, head_dim) ç»“æ„</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># è¿™é‡Œçš„ k_pe å¯èƒ½æ˜¯ (bsz, seqlen, qk_rope_head_dim)ï¼Œéœ€è¦æ·»åŠ ä¸€ä¸ª head ç»´åº¦</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_pe</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">k_pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">freqs_cis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. æ³¨æ„åŠ›è®¡ç®—ï¼šæ ¹æ®å®ç°æ–¹å¼ (naive æˆ– optimized)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Naive å®ç°ç›´æ¥æ‹¼æ¥ Q çš„ RoPE å’Œé RoPE éƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">q_nope</span><span class="p">,</span> <span class="n">q_pe</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q æ¢å¤ä¸º (bsz, seqlen, n_local_heads, qk_head_dim)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># å¯¹ KV æ½œåœ¨å‘é‡åº”ç”¨å½’ä¸€åŒ–ï¼Œå¹¶è¿›è¡Œç¬¬äºŒé˜¶æ®µæŠ•å½±</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å¯¹åº”è®ºæ–‡ä¸­å°† c_t^{KV} æŠ•å½±åˆ°é RoPE é”®å’Œå€¼çš„éƒ¨åˆ† (k_t^C å’Œ v_t^C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span><span class="p">(</span><span class="n">kv</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># å°† KV ç»“æœé‡å¡‘ä¸º (batch_size, seq_len, n_local_heads, qk_nope_head_dim + v_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># åˆ†ç¦»å‡ºé RoPE é”®å’Œå€¼</span>
</span></span><span class="line"><span class="cl">            <span class="n">k_nope</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># æ‹¼æ¥é RoPE é”®å’Œ RoPE é”®ï¼Œç»„æˆå®Œæ•´çš„é”® K</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_pe ä¹‹å‰æ˜¯ (bsz, seqlen, 1, qk_rope_head_dim)ï¼Œéœ€è¦ expand åˆ° n_local_heads</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k_nope</span><span class="p">,</span> <span class="n">k_pe</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># æ›´æ–° K å’Œ V ç¼“å­˜ (åœ¨æ¨ç†æ—¶ç”¨äºè‡ªå›å½’ç”Ÿæˆ)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">k_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">v_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Q @ K^T)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># scores: (batch_size, q_seq_len, n_local_heads, k_seq_len)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ä½¿ç”¨æ•´ä¸ªç¼“å­˜ä¸­çš„é”®è¿›è¡Œè®¡ç®—</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshd,bthd-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># optimized å®ç°</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># è·å– wkv_b æƒé‡ï¼Œå¦‚æœä½¿ç”¨äº†é‡åŒ–åˆ™è¿›è¡Œåé‡åŒ–</span>
</span></span><span class="line"><span class="cl">            <span class="n">wkv_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">weight</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">weight_dequant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="c1"># å°† wkv_b é‡å¡‘ä¸º (n_local_heads, head_dim, kv_lora_rank) ä»¥ä¾¿è¿›è¡Œé€å¤´çš„æ“ä½œ</span>
</span></span><span class="line"><span class="cl">            <span class="n">wkv_b</span> <span class="o">=</span> <span class="n">wkv_b</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">)</span> <span class="c1"># (n_heads, (qk_nope+v), kv_rank)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># è®¡ç®— Q_nope ä¸ K_nope çš„ç‚¹ç§¯ (é€šè¿‡ kv ç¼“å­˜)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># q_nope: (bsz, seqlen, n_local_heads, qk_nope_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wkv_b[:, :self.qk_nope_head_dim] æ˜¯ W^{UK} çš„éƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># è¿™å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(q_{t,i}^C @ c_{j,i}^{KV}) çš„ç¬¬ä¸€é¡¹</span>
</span></span><span class="line"><span class="cl">            <span class="n">q_nope</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshd,hdc-&gt;bshc&#34;</span><span class="p">,</span> <span class="n">q_nope</span><span class="p">,</span> <span class="n">wkv_b</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># æ›´æ–° KV ç¼“å­˜ (kv_cache å¯¹åº”è®ºæ–‡ä¸­çš„ c_t^{KV})</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span><span class="p">(</span><span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># æ›´æ–° PE ç¼“å­˜ (pe_cache å¯¹åº”è®ºæ–‡ä¸­çš„ k_t^R)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_pe ä¹‹å‰æ˜¯ (bsz, seqlen, 1, qk_rope_head_dim)ï¼Œsqueeze æ‰é‚£ä¸ª 1 ç»´åº¦</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">pe_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_pe</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ç¬¬ä¸€é¡¹: é RoPE æŸ¥è¯¢ q_nope ä¸ç¼“å­˜çš„ kv_cache (å‹ç¼©é”®) çš„ç‚¹ç§¯</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(q_{t,i}^C @ c_{j,i}^{KV}) çš„ç¬¬ä¸€éƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshc,btc-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q_nope</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span> <span class="o">+</span> \
</span></span><span class="line"><span class="cl">                      <span class="c1"># ç¬¬äºŒé¡¹: RoPE æŸ¥è¯¢ q_pe ä¸ç¼“å­˜çš„ pe_cache (è§£è€¦ RoPE é”®) çš„ç‚¹ç§¯</span>
</span></span><span class="line"><span class="cl">                      <span class="c1"># å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(q_{t,i}^R @ k_{j,i}^R) çš„ç¬¬äºŒéƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">                      <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshr,btr-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q_pe</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="c1"># åº”ç”¨ç¼©æ”¾å› å­</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># åº”ç”¨æ³¨æ„åŠ›æ©ç  (å¦‚å› æœæ©ç ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">+=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># unsqueeze(1) å¹¿æ’­åˆ° heads ç»´åº¦</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># å¯¹åˆ†æ•°åº”ç”¨ Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. å€¼ (V) çš„åŠ æƒæ±‚å’Œ</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Naive å®ç°ç›´æ¥ä¸ V ç¼“å­˜è¿›è¡Œç‚¹ç§¯</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å¯¹åº”è®ºæ–‡ä¸­çš„ sum(Softmax(...) * v_{j,i}^C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bsht,bthd-&gt;bshd&#34;</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># optimized å®ç°</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># optimized å®ç°é€šè¿‡ wkv_b çš„å€¼éƒ¨åˆ†å°†åŠ æƒåçš„å‹ç¼© KV è¿˜åŸä¸º V</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ç¬¬ä¸€æ­¥: å°†æ³¨æ„åŠ›æƒé‡ä¸ç¼“å­˜çš„ kv_cache (å‹ç¼©å€¼) è¿›è¡Œç‚¹ç§¯</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(...) * c_{j,i}^{KV} çš„ç¬¬ä¸€éƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bsht,btc-&gt;bshc&#34;</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># ç¬¬äºŒæ­¥: å°†åŠ æƒåçš„å‹ç¼©å€¼é€šè¿‡ wkv_b çš„å€¼æŠ•å½±éƒ¨åˆ†è¿˜åŸä¸ºæœ€ç»ˆçš„å€¼å‘é‡</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wkv_b[:, -self.v_head_dim:] æ˜¯ W^{UV} çš„éƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># å¯¹åº”è®ºæ–‡ä¸­çš„ Softmax(...) * v_{j,i}^C çš„ç¬¬äºŒéƒ¨åˆ†</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshc,hdc-&gt;bshd&#34;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wkv_b</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># å°†æ‰€æœ‰å¤´çš„ç»“æœå±•å¹³å¹¶è¿›è¡Œæœ€ç»ˆçš„è¾“å‡ºæŠ•å½±</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># x.flatten(2) å°† (bsz, seqlen, n_local_heads, v_head_dim) å±•å¹³ä¸º (bsz, seqlen, n_local_heads * v_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/deepseek/">DeepSeek</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/deepseek/deepseekmoe/">
    <span class="title">Â« Prev</span>
    <br>
    <span>DeepSeekMoE</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/">
    <span class="title">Next Â»</span>
    <br>
    <span>ServingLLMsOnHuaweiCloudMatrix384</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>Â© 2024-2025 WITHER</span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
