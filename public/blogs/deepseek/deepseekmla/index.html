<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DeepSeekMLA | WITHER</title>
<meta name="keywords" content="MLA">
<meta name="description" content="Principle of DeepSeekV3 MLA">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/deepseek/deepseekmla/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/deepseek/deepseekmla/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/deepseek/deepseekmla/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="DeepSeekMLA">
  <meta property="og:description" content="Principle of DeepSeekV3 MLA">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-06-19T11:24:45+08:00">
    <meta property="article:modified_time" content="2025-06-22T17:53:30+08:00">
    <meta property="article:tag" content="DeepSeek">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeepSeekMLA">
<meta name="twitter:description" content="Principle of DeepSeekV3 MLA">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DeepSeek Related",
      "item": "http://localhost:1313/blogs/deepseek/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "DeepSeekMLA",
      "item": "http://localhost:1313/blogs/deepseek/deepseekmla/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DeepSeekMLA",
  "name": "DeepSeekMLA",
  "description": "Principle of DeepSeekV3 MLA",
  "keywords": [
    "MLA"
  ],
  "articleBody": "Preliminary: What is RoPE Introduction 旋转位置编码 (RoPE) 是一种新颖的、基于相对位置的编码方法，它被设计用于提高 Transformer 模型处理长序列的能力，同时保持计算效率。与传统的绝对位置编码 (如正弦/余弦位置编码) 或直接的相对位置编码 (如 T5 中使用的相对偏置) 不同，RoPE 将位置信息集成到自注意力机制的 Q 和 K 的表示中，使得 Q 和 K 的点积自然地编码了相对位置信息。\nRoPE 的核心思想是，通过对查询和键向量应用特定的旋转操作，使得两个向量的点积结果只依赖于它们之间的相对距离，而不是它们的绝对位置。这使得模型能够更好地泛化到更长的序列，并且在处理位置信息时更加高效。\nRoPE 的主要优点包括：\n编码相对位置信息： 自然地将相对位置信息融入到注意力分数中。 长序列外推能力： 提高了模型在训练时未见过的更长序列上的性能。 与自注意力机制的兼容性： 无缝集成到 QKV 点积注意力中。 简单且高效： 实现相对简单，且不会显著增加计算复杂度。 Formular RoPE 的主要思想是通过对查询 $q$ 和键 $k$ 应用一个旋转矩阵 $R_t$ (取决于其绝对位置 $t$) ，使得点积 $q_m^T k_n$ 能够通过某种方式转化为只依赖于相对位置 $m-n$ 的函数。\n对于一个向量 $x \\in \\mathbb{R}^d$ 在位置 $m$ 处，RoPE 的变换函数 $f(x, m)$ 可以定义如下：\n如果向量维度是偶数 $d$，我们可以将其分成 $d/2$ 对，每对执行一个二维旋转。 对于向量 $x = [x_0, x_1, \\ldots, x_{d-1}]^T$，RoPE 对其每个维度对 $(x_{2i}, x_{2i+1})$ 应用旋转：\n$$\rf(x, m)_{2i} = x_{2i} \\cos(m\\theta_i) - x_{2i+1} \\sin(m\\theta_i) \\\\\rf(x, m)_{2i+1} = x_{2i} \\sin(m\\theta_i) + x_{2i+1} \\cos(m\\theta_i)\r$$其中 $\\theta_i$ 是预设的频率，通常定义为 $\\theta_i = 10000^{-2i/d}$. $i=0, \\dots, d/2 - 1$ 是维度对的索引。\n用矩阵形式表示： 我们可以将这种旋转操作表示为一个稀疏的块对角矩阵 $R_m^d$，其形式为： $$R_m^d = \\begin{pmatrix}\r\\cos(m\\theta_0) \u0026 -\\sin(m\\theta_0) \u0026 0 \u0026 0 \u0026 \\cdots \\\\\r\\sin(m\\theta_0) \u0026 \\cos(m\\theta_0) \u0026 0 \u0026 0 \u0026 \\cdots \\\\\r0 \u0026 0 \u0026 \\cos(m\\theta_1) \u0026 -\\sin(m\\theta_1) \u0026 \\cdots \\\\\r0 \u0026 0 \u0026 \\sin(m\\theta_1) \u0026 \\cos(m\\theta_1) \u0026 \\cdots \\\\\r\\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots\r\\end{pmatrix}$$ 那么，经过 RoPE 编码的查询和键可以表示为： $$\\mathbf{q}_m = R_m^d \\mathbf{q}$$ $$\\mathbf{k}_n = R_n^d \\mathbf{k}$$ 其中 $\\mathbf{q}$ 和 $\\mathbf{k}$ 是原始的查询和键向量 (不含位置信息) ，$\\mathbf{q}_m$ 和 $\\mathbf{k}_n$ 是经过 RoPE 处理后的查询和键向量。\nRoPE 的关键特性：点积与相对位置 经过 RoPE 变换后，注意力机制中的点积可以分解为： $$\\mathbf{q}_m^T \\mathbf{k}_n = (R_m^d \\mathbf{q})^T (R_n^d \\mathbf{k})$$ 由于 $R_m^d$ 是正交矩阵，其逆矩阵等于其转置，即 $(R_m^d)^T = (R_m^d)^{-1} = R_{-m}^d$. 因此有 $$\\mathbf{q}_m^T \\mathbf{k}_n = \\mathbf{q}^T (R_m^d)^T R_n^d \\mathbf{k} = \\mathbf{q}^T R_{-m}^d R_n^d \\mathbf{k} = \\mathbf{q}^T R_{n-m}^d \\mathbf{k}$$ 这个最终结果 $\\mathbf{q}^T R_{n-m}^d \\mathbf{k}$ 表明，两个向量的点积只依赖于它们的相对位置差 $n-m$，而与它们的绝对位置 $n$ 和 $m$ 无关。这就是 RoPE 能够编码相对位置信息的数学基础。\nWorkflow Notation $d$: embedding 维度 $d_h$: 每个注意力头的维度 $\\mathbf{h}_t\\in\\mathbb{R}^d$: 某个 attention 层第 t 个 token 的输入。 KV Compression $$\r\\textcolor{red}{c_t^{KV}} = W^{DKV}h_t \\tag{1}\r$$ $$\r[k_{t,1}^{C}, k_{t,2}^{C}, \\ldots, k_{t,n_h}^{C}] = k_t^C = W^{UK}c_t^{KV} \\tag{2}\r$$ $$\r\\textcolor{red}{k_t^R} = \\text{RoPE}(W^{KR}h_t) \\tag{3}\r$$ $$\rk_{t,i} = [k_{t,i}^C, k_{t}^R] \\tag{4}\r$$ $$\r[v_{t,1}^C, v_{t,2}^C, \\ldots, v_{t,n_h}^C] = v_t^C = W^{UV}c_t^{KV} \\tag{5}\r$$ $c_t^{KV} \\in \\mathbb{R}^{d_c}$: 压缩后的 KV 潜在向量。 $d_c (\\ll d_h n_h)$: KV 压缩到的维度。 $W^{DKV} \\in \\mathbb{R}^{d_c \\times d}$: KV 降维投影矩阵。 $W^{UK}, W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}$ 分别是 K \u0026 V 的升维投影矩阵。 $W^{KR} \\in \\mathbb{R}^{d_h^R \\times d}$: 用于生成携带 RoPE 的解耦键的矩阵 (Su et al., 2024) 红色的是需要缓存的向量，后续说明原因。注意到对 K 进行 RoPE 之前是对输入向量乘以了个投影再进行的。而且 K 的每个注意力头被拼接的都是同一个 $k_{t}^R$，有点类似于 MQA.\nQ Compression $$c_t^Q = W^{DQ}h_t \\tag{6}$$ $$[q_{t,1}^C, q_{t,2}^C, \\ldots, q_{t,n_h}^C] = q_t^C = W^{UQ}c_t^Q \\tag{7}$$ $$[q_{t,1}^R, q_{t,2}^R, \\ldots, q_{t,n_h}^R] = q_t^R = \\text{RoPE}(W^{QR}q_t^C) \\tag{8}$$ $$q_{t,i} = [q_{t,i}^C, q_{t,i}^R] \\tag{9}$$ $c_t^Q \\in \\mathbb{R}^{d_c'}$: Q 压缩后的潜在向量。 $d_c'(\\ll d_h n_h)$ 表示 Q 压缩后的维度。 $W^{DQ} \\in \\mathbb{R}^{d_c' \\times d}, W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c'}$: 分别是 Q 的降维和升维矩阵。 $W^{QR} \\in \\mathbb{R}^{d_h^R n_h \\times d_c'}$ 是用于生成携带 RoPE 的解耦 Q 的矩阵。 注意到对 Q 的 RoPE 是在压缩后进行的，即为每个注意力头都生成了一个位置编码信息后进行拼接。\nAttention Computation 最终 $q_{t,i}$, $k_{j,i}$, $v_{j,i}^C$ 被组合起来以生成最终的注意力输出 $u_t$\n$$\\mathbf{o}_{t,i} = \\sum_{j=1}^{t} \\text{Softmax}\\left(\\frac{q_{t,i}^T \\mathbf{k}_{j,i}}{\\sqrt{d_h + d_R}}\\right)v_{j,i}^C \\tag{10}$$ $$\\mathbf{u}_t = W^O[\\mathbf{o}_{t,1}, \\mathbf{o}_{t,2}, \\ldots, \\mathbf{o}_{t,n_h}] \\tag{11}$$ $W^O \\in \\mathbb{R}^{d \\times d_h n_h}$: 输出投影矩阵。 Why Decoupled RoPE 假设不加 RoPE 的情况下进行 $q_{t,i}$, $k_{j,i}$ 的内积则有\n$$\rq_{t,i}^{T}\\times k_{j,i}=(W_{(i)}^{UQ}c_{t}^{Q})^{T}\\times W_{(i)}^{UK}c_{j}^{KV}=(c_{t}^{Q})^{T}\\times(W_{(i)}^{UQ})^{T}W_{(i)}^{UK}\\times c_{j}^{KV} \\tag{12}\r$$RoPE 通过对向量应用一个位置依赖的旋转变换来注入相对位置信息。对于一个向量 $X$ 在位置 $t$，RoPE 可以被表示为一个旋转矩阵 $R_t$ 乘以 $X$： $$\\text{RoPE}(X, t) = R_t X$$ 这里的 $R_t$ 是一个正交旋转矩阵，它取决于位置 $t$.\n如果直接对压缩后 $k_t^C$ 的 使用 RoPE 那么情况会变成\n$$\r\\begin{aligned}\rq_{t,i}^{T}\\times k_{j,i}\u0026=(\\mathcal{R}_{t}W_{(i)}^{UQ}c_{t}^{Q})^{T}\\times\\mathcal{R}_{j}W_{(i)}^{UK}c_{j}^{KV} \\\\\r\u0026=(c_{t}^{Q})^{T}\\times(W_{(i)}^{UQ})^{T}\\mathcal{R}_{t}^{T}\\mathcal{R}_{j}W_{(i)}^{UK}\\times c_{j}^{KV}\\\\\r\u0026=(c_{t}^{Q})^{T}\\times(W_{(i)}^{UQ})^{T}\\mathcal{R}_{t-j}W_{(i)}^{UK}\\times c_{j}^{KV}\r\\end{aligned} \\tag{13}\r$$中间的矩阵与相对位置有关，无法提前计算出来。因此文中就是对所有头都使用同一个 k 和计算 RoPE. 拼接后的向量再计算时\n$$\rq_{t,i}^T\\times k_{j,i}=[q_{t,i}^C;q_{t,i}^R]^T\\times[k_{j,i}^C;k_t^R]=(q_{t,i}^C,k_{j,i}^C)+(q_{t,i}^R,k_t^R) \\tag{14}\r$$前一部分按照公式 (12) 进行计算，后一部分按照 MQA 方式计算。因此只用缓存 $c_t^{KV}$ 和 $k_t^R$.\nSource Code DeepSeek-V3 MLA 对应的源码位置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 class MLA(nn.Module): \"\"\" Multi-Head Latent Attention (MLA) Layer. Attributes: dim (int): Dimensionality of the input features. n_heads (int): Number of attention heads. n_local_heads (int): Number of local attention heads for distributed systems. q_lora_rank (int): Rank for low-rank query projection. kv_lora_rank (int): Rank for low-rank key/value projection. qk_nope_head_dim (int): Dimensionality of non-positional query/key projections. qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections. qk_head_dim (int): Total dimensionality of query/key projections. v_head_dim (int): Dimensionality of value projections. softmax_scale (float): Scaling factor for softmax in attention computation. \"\"\" def __init__(self, args: ModelArgs): super().__init__() self.dim = args.dim self.n_heads = args.n_heads # 计算当前进程（卡）负责的注意力头数量，用于模型并行 self.n_local_heads = args.n_heads // world_size self.q_lora_rank = args.q_lora_rank self.kv_lora_rank = args.kv_lora_rank self.qk_nope_head_dim = args.qk_nope_head_dim self.qk_rope_head_dim = args.qk_rope_head_dim # QK 头总维度 = 非 RoPE 部分 + RoPE 部分 self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim self.v_head_dim = args.v_head_dim # 查询投影 (wq) 的 LoRA 实现 if self.q_lora_rank == 0: # 如果 q_lora_rank 为 0，表示不使用 LoRA，直接进行全秩投影 # 将 dim 维度的输入投影到 n_heads * qk_head_dim 维度 self.wq = ColumnParallelLinear(self.dim, self.n_heads * self.qk_head_dim) else: # 如果 q_lora_rank \u003e 0，使用 LoRA 结构进行低秩投影 # wq_a: dim -\u003e q_lora_rank (低秩投影的第一步) self.wq_a = Linear(self.dim, self.q_lora_rank) # q_norm: RMSNorm 应用于低秩维度 self.q_norm = RMSNorm(self.q_lora_rank) # wq_b: q_lora_rank -\u003e n_heads * qk_head_dim (低秩投影的第二步) self.wq_b = ColumnParallelLinear(self.q_lora_rank, self.n_heads * self.qk_head_dim) # 键值投影 (wkv) 的 LoRA 实现 # wkv_a: dim -\u003e kv_lora_rank + qk_rope_head_dim # 对应图中的 W^{DKV} 投影到低秩 KV 潜在空间 (kv_lora_rank) 和解耦的 RoPE 键 (qk_rope_head_dim) # 这里的 kv_lora_rank 对应公式中的 d_c # 这里的 qk_rope_head_dim 对应公式中的 d_h self.wkv_a = Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim) # kv_norm: RMSNorm 应用于低秩维度 self.kv_norm = RMSNorm(self.kv_lora_rank) # wkv_b: kv_lora_rank -\u003e n_heads * (qk_nope_head_dim + v_head_dim) # 对应图中的 W^{UK} 和 W^{UV} 的组合 # 它将压缩后的 KV 潜在向量 (kv_lora_rank) 投影回非 RoPE 键 (qk_nope_head_dim) 和值 (v_head_dim) 的高维度空间 self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim)) # 输出投影 (wo) self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim) # Softmax 缩放因子，用于注意力分数的缩放，防止内积过大 self.softmax_scale = self.qk_head_dim ** -0.5 # 如果序列长度超过原始训练长度，根据 RopeFactor 进行额外缩放，用于处理长序列外推问题 if args.max_seq_len \u003e args.original_seq_len: mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0 self.softmax_scale = self.softmax_scale * mscale * mscale # 根据注意力实现方式（naive 或 optimized）选择不同的 KV 缓存结构 if attn_impl == \"naive\": # naive 实现直接缓存完整键 K 和值 V # k_cache: (max_batch_size, max_seq_len, n_local_heads, qk_head_dim) self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim), persistent=False) # v_cache: (max_batch_size, max_seq_len, n_local_heads, v_head_dim) self.register_buffer(\"v_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim), persistent=False) else: # optimized 实现缓存压缩后的 KV 潜在向量和解耦的 RoPE 键 # kv_cache: (max_batch_size, max_seq_len, kv_lora_rank) - 对应论文中的 c_t^{KV} self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False) # pe_cache: (max_batch_size, max_seq_len, qk_rope_head_dim) - 对应论文中的 k_t^R self.register_buffer(\"pe_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False) def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]): \"\"\" Forward pass for the Multi-Head Latent Attention (MLA) Layer. Args: x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim). start_pos (int): Starting position in the sequence for caching. freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings. mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention. Returns: torch.Tensor: Output tensor with the same shape as the input. \"\"\" bsz, seqlen, _ = x.size() end_pos = start_pos + seqlen # 1. 查询 (Q) 的生成 if self.q_lora_rank == 0: # 全秩投影 q = self.wq(x) else: # LoRA 投影：x -\u003e wq_a -\u003e q_norm -\u003e wq_b q = self.wq_b(self.q_norm(self.wq_a(x))) # reshape Q q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim) # 分离 Q 的非 RoPE 部分和 RoPE 部分 # q_nope 对应论文中的 q_{t,i}^C (非位置信息查询) # q_pe 对应论文中的 q_{t,i}^R (携带 RoPE 的解耦查询) q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1) # 对 Q 的 RoPE 部分应用旋转位置编码 # 对应论文中的 q_t^R = RoPE(W^{QR}c_t^Q) 的 RoPE 部分 q_pe = apply_rotary_emb(q_pe, freqs_cis) # 2. 键值 (KV) 的生成 # 将输入 x 投影到低秩 KV 潜在空间和解耦的 RoPE 键 # 对应论文中的 c_t^{KV} 和 k_t^R kv = self.wkv_a(x) # 分离出 KV 潜在向量和解耦的 RoPE 键 # kv 对应论文中的 c_t^{KV} # k_pe 对应论文中的 k_t^R (RoPE 解耦键) kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1) # 对 K 的 RoPE 部分应用旋转位置编码 # 注意 k_pe.unsqueeze(2) 是因为 apply_rotary_emb 期望 (..., seq_len, head_dim) 结构 # 这里的 k_pe 可能是 (bsz, seqlen, qk_rope_head_dim)，需要添加一个 head 维度 k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis) # 3. 注意力计算：根据实现方式 (naive 或 optimized) if attn_impl == \"naive\": # Naive 实现直接拼接 Q 的 RoPE 和非 RoPE 部分 q = torch.cat([q_nope, q_pe], dim=-1) # Q 恢复为 (bsz, seqlen, n_local_heads, qk_head_dim) # 对 KV 潜在向量应用归一化，并进行第二阶段投影 # 对应论文中将 c_t^{KV} 投影到非 RoPE 键和值的部分 (k_t^C 和 v_t^C) kv = self.wkv_b(self.kv_norm(kv)) # 将 KV 结果重塑为 (batch_size, seq_len, n_local_heads, qk_nope_head_dim + v_head_dim) kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim) # 分离出非 RoPE 键和值 k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1) # 拼接非 RoPE 键和 RoPE 键，组成完整的键 K # k_pe 之前是 (bsz, seqlen, 1, qk_rope_head_dim)，需要 expand 到 n_local_heads k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1) # 更新 K 和 V 缓存 (在推理时用于自回归生成) self.k_cache[:bsz, start_pos:end_pos] = k self.v_cache[:bsz, start_pos:end_pos] = v # 计算注意力分数 (Q @ K^T) # scores: (batch_size, q_seq_len, n_local_heads, k_seq_len) # 使用整个缓存中的键进行计算 scores = torch.einsum(\"bshd,bthd-\u003ebsht\", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale else: # optimized 实现 # 获取 wkv_b 权重，如果使用了量化则进行反量化 wkv_b = self.wkv_b.weight if self.wkv_b.scale is None else weight_dequant(self.wkv_b.weight, self.wkv_b.scale, block_size) # 将 wkv_b 重塑为 (n_local_heads, head_dim, kv_lora_rank) 以便进行逐头的操作 wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank) # (n_heads, (qk_nope+v), kv_rank) # 计算 Q_nope 与 K_nope 的点积 (通过 kv 缓存) # q_nope: (bsz, seqlen, n_local_heads, qk_nope_head_dim) # wkv_b[:, :self.qk_nope_head_dim] 是 W^{UK} 的部分 # 这对应论文中的 Softmax(q_{t,i}^C @ c_{j,i}^{KV}) 的第一项 q_nope = torch.einsum(\"bshd,hdc-\u003ebshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim]) # 更新 KV 缓存 (kv_cache 对应论文中的 c_t^{KV}) self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv) # 更新 PE 缓存 (pe_cache 对应论文中的 k_t^R) # k_pe 之前是 (bsz, seqlen, 1, qk_rope_head_dim)，squeeze 掉那个 1 维度 self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2) # 计算注意力分数 # 第一项: 非 RoPE 查询 q_nope 与缓存的 kv_cache (压缩键) 的点积 # 对应论文中的 Softmax(q_{t,i}^C @ c_{j,i}^{KV}) 的第一部分 scores = torch.einsum(\"bshc,btc-\u003ebsht\", q_nope, self.kv_cache[:bsz, :end_pos]) + \\ # 第二项: RoPE 查询 q_pe 与缓存的 pe_cache (解耦 RoPE 键) 的点积 # 对应论文中的 Softmax(q_{t,i}^R @ k_{j,i}^R) 的第二部分 torch.einsum(\"bshr,btr-\u003ebsht\", q_pe, self.pe_cache[:bsz, :end_pos]) scores *= self.softmax_scale # 应用缩放因子 # 应用注意力掩码 (如因果掩码，防止看到未来信息) if mask is not None: scores += mask.unsqueeze(1) # unsqueeze(1) 广播到 heads 维度 # 对分数应用 Softmax 得到注意力权重 scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x) # 4. 值 (V) 的加权求和 if attn_impl == \"naive\": # Naive 实现直接与 V 缓存进行点积 # 对应论文中的 sum(Softmax(...) * v_{j,i}^C) x = torch.einsum(\"bsht,bthd-\u003ebshd\", scores, self.v_cache[:bsz, :end_pos]) else: # optimized 实现 # optimized 实现通过 wkv_b 的值部分将加权后的压缩 KV 还原为 V # 第一步: 将注意力权重与缓存的 kv_cache (压缩值) 进行点积 # 对应论文中的 Softmax(...) * c_{j,i}^{KV} 的第一部分 x = torch.einsum(\"bsht,btc-\u003ebshc\", scores, self.kv_cache[:bsz, :end_pos]) # 第二步: 将加权后的压缩值通过 wkv_b 的值投影部分还原为最终的值向量 # wkv_b[:, -self.v_head_dim:] 是 W^{UV} 的部分 # 对应论文中的 Softmax(...) * v_{j,i}^C 的第二部分 x = torch.einsum(\"bshc,hdc-\u003ebshd\", x, wkv_b[:, -self.v_head_dim:]) # 将所有头的结果展平并进行最终的输出投影 x = self.wo(x.flatten(2)) # x.flatten(2) 将 (bsz, seqlen, n_local_heads, v_head_dim) 展平为 (bsz, seqlen, n_local_heads * v_head_dim) return x ",
  "wordCount" : "3711",
  "inLanguage": "en",
  "datePublished": "2025-06-19T11:24:45+08:00",
  "dateModified": "2025-06-22T17:53:30+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/deepseek/deepseekmla/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/deepseek/">DeepSeek Related</a></div>
    <h1 class="post-title entry-hint-parent">
      DeepSeekMLA
    </h1>
    <div class="post-description">
      Principle of DeepSeekV3 MLA
    </div>
    <div class="post-meta"><span title='2025-06-19 11:24:45 +0800 CST'>Jun-19-2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;3711 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#preliminary-what-is-rope" aria-label="Preliminary: What is RoPE">Preliminary: What is RoPE</a><ul>
                            
                    <li>
                        <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                    <li>
                        <a href="#formular" aria-label="Formular">Formular</a></li></ul>
                    </li>
                    <li>
                        <a href="#workflow" aria-label="Workflow">Workflow</a><ul>
                            
                    <li>
                        <a href="#notation" aria-label="Notation">Notation</a></li>
                    <li>
                        <a href="#kv-compression" aria-label="KV Compression">KV Compression</a></li>
                    <li>
                        <a href="#q-compression" aria-label="Q Compression">Q Compression</a></li>
                    <li>
                        <a href="#attention-computation" aria-label="Attention Computation">Attention Computation</a></li></ul>
                    </li>
                    <li>
                        <a href="#why-decoupled-rope" aria-label="Why Decoupled RoPE">Why Decoupled RoPE</a></li>
                    <li>
                        <a href="#source-code" aria-label="Source Code">Source Code</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="preliminary-what-is-rope">Preliminary: What is RoPE<a hidden class="anchor" aria-hidden="true" href="#preliminary-what-is-rope">#</a></h1>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>旋转位置编码 (RoPE) 是一种新颖的、基于相对位置的编码方法，它被设计用于提高 Transformer 模型处理长序列的能力，同时保持计算效率。与传统的绝对位置编码 (如正弦/余弦位置编码) 或直接的相对位置编码 (如 T5 中使用的相对偏置) 不同，RoPE 将位置信息集成到自注意力机制的 Q 和 K 的表示中，使得 Q 和 K 的点积自然地编码了<strong>相对位置信息</strong>。</p>
<p>RoPE 的核心思想是，通过对查询和键向量应用特定的旋转操作，使得两个向量的点积结果只依赖于它们之间的相对距离，而不是它们的绝对位置。这使得模型能够更好地泛化到更长的序列，并且在处理位置信息时更加高效。</p>
<p><strong>RoPE 的主要优点包括：</strong></p>
<ul>
<li><strong>编码相对位置信息：</strong> 自然地将相对位置信息融入到注意力分数中。</li>
<li><strong>长序列外推能力：</strong> 提高了模型在训练时未见过的更长序列上的性能。</li>
<li><strong>与自注意力机制的兼容性：</strong> 无缝集成到 QKV 点积注意力中。</li>
<li><strong>简单且高效：</strong> 实现相对简单，且不会显著增加计算复杂度。</li>
</ul>
<h2 id="formular">Formular<a hidden class="anchor" aria-hidden="true" href="#formular">#</a></h2>
<p>RoPE 的主要思想是通过对查询 $q$ 和键 $k$ 应用一个旋转矩阵 $R_t$ (取决于其绝对位置 $t$) ，使得点积 $q_m^T k_n$ 能够通过某种方式转化为只依赖于相对位置 $m-n$ 的函数。</p>
<p>对于一个向量 $x \in \mathbb{R}^d$ 在位置 $m$ 处，RoPE 的变换函数 $f(x, m)$ 可以定义如下：</p>
<p>如果向量维度是偶数 $d$，我们可以将其分成 $d/2$ 对，每对执行一个二维旋转。
对于向量 $x = [x_0, x_1, \ldots, x_{d-1}]^T$，RoPE 对其每个维度对 $(x_{2i}, x_{2i+1})$ 应用旋转：</p>
$$
f(x, m)_{2i} = x_{2i} \cos(m\theta_i) - x_{2i+1} \sin(m\theta_i) \\
f(x, m)_{2i+1} = x_{2i} \sin(m\theta_i) + x_{2i+1} \cos(m\theta_i)
$$<p>其中 $\theta_i$ 是预设的频率，通常定义为 $\theta_i = 10000^{-2i/d}$. $i=0, \dots, d/2 - 1$ 是维度对的索引。</p>
<p><strong>用矩阵形式表示：</strong>
我们可以将这种旋转操作表示为一个稀疏的块对角矩阵 $R_m^d$，其形式为：
</p>
$$R_m^d = \begin{pmatrix}
\cos(m\theta_0) & -\sin(m\theta_0) & 0 & 0 & \cdots \\
\sin(m\theta_0) & \cos(m\theta_0) & 0 & 0 & \cdots \\
0 & 0 & \cos(m\theta_1) & -\sin(m\theta_1) & \cdots \\
0 & 0 & \sin(m\theta_1) & \cos(m\theta_1) & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}$$<p>
那么，经过 RoPE 编码的查询和键可以表示为：
</p>
$$\mathbf{q}_m = R_m^d \mathbf{q}$$<p>
</p>
$$\mathbf{k}_n = R_n^d \mathbf{k}$$<p>
其中 $\mathbf{q}$ 和 $\mathbf{k}$ 是原始的查询和键向量 (不含位置信息) ，$\mathbf{q}_m$ 和 $\mathbf{k}_n$ 是经过 RoPE 处理后的查询和键向量。</p>
<p><strong>RoPE 的关键特性：点积与相对位置</strong>
经过 RoPE 变换后，注意力机制中的点积可以分解为：
</p>
$$\mathbf{q}_m^T \mathbf{k}_n = (R_m^d \mathbf{q})^T (R_n^d \mathbf{k})$$<p>
由于 $R_m^d$ 是正交矩阵，其逆矩阵等于其转置，即 $(R_m^d)^T = (R_m^d)^{-1} = R_{-m}^d$. 因此有
</p>
$$\mathbf{q}_m^T \mathbf{k}_n = \mathbf{q}^T (R_m^d)^T R_n^d \mathbf{k} = \mathbf{q}^T R_{-m}^d R_n^d \mathbf{k} = \mathbf{q}^T R_{n-m}^d \mathbf{k}$$<p>
这个最终结果 $\mathbf{q}^T R_{n-m}^d \mathbf{k}$ 表明，两个向量的点积只依赖于它们的<strong>相对位置差 $n-m$</strong>，而与它们的绝对位置 $n$ 和 $m$ 无关。这就是 RoPE 能够编码相对位置信息的数学基础。</p>
<h1 id="workflow">Workflow<a hidden class="anchor" aria-hidden="true" href="#workflow">#</a></h1>
<h2 id="notation">Notation<a hidden class="anchor" aria-hidden="true" href="#notation">#</a></h2>
<ul>
<li>$d$: embedding 维度</li>
<li>$d_h$: 每个注意力头的维度</li>
<li>$\mathbf{h}_t\in\mathbb{R}^d$: 某个 attention 层第 t 个 token 的输入。</li>
</ul>
<h2 id="kv-compression">KV Compression<a hidden class="anchor" aria-hidden="true" href="#kv-compression">#</a></h2>
$$
\textcolor{red}{c_t^{KV}} = W^{DKV}h_t  \tag{1}
$$<p>
</p>
$$
[k_{t,1}^{C}, k_{t,2}^{C}, \ldots, k_{t,n_h}^{C}] = k_t^C = W^{UK}c_t^{KV}  \tag{2}
$$<p>
</p>
$$
\textcolor{red}{k_t^R} = \text{RoPE}(W^{KR}h_t)  \tag{3}
$$<p>
</p>
$$
k_{t,i} = [k_{t,i}^C, k_{t}^R] \tag{4}
$$<p>
</p>
$$
[v_{t,1}^C, v_{t,2}^C, \ldots, v_{t,n_h}^C] = v_t^C = W^{UV}c_t^{KV} \tag{5}
$$<ul>
<li>$c_t^{KV} \in \mathbb{R}^{d_c}$: 压缩后的 KV 潜在向量。</li>
<li>$d_c (\ll d_h n_h)$: KV 压缩到的维度。</li>
<li>$W^{DKV} \in \mathbb{R}^{d_c \times d}$: KV 降维投影矩阵。</li>
<li>$W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ 分别是 K &amp; V 的升维投影矩阵。</li>
<li>$W^{KR} \in \mathbb{R}^{d_h^R \times d}$: 用于生成携带 RoPE 的解耦键的矩阵 (Su et al., 2024)</li>
</ul>
<p>红色的是需要缓存的向量，后续说明原因。注意到对 K 进行 RoPE 之前是对输入向量乘以了个投影再进行的。而且 K 的每个注意力头被拼接的都是同一个 $k_{t}^R$，有点类似于 MQA.</p>
<h2 id="q-compression">Q Compression<a hidden class="anchor" aria-hidden="true" href="#q-compression">#</a></h2>
$$c_t^Q = W^{DQ}h_t \tag{6}$$<p>
</p>
$$[q_{t,1}^C, q_{t,2}^C, \ldots, q_{t,n_h}^C] = q_t^C = W^{UQ}c_t^Q \tag{7}$$<p>
</p>
$$[q_{t,1}^R, q_{t,2}^R, \ldots, q_{t,n_h}^R] = q_t^R = \text{RoPE}(W^{QR}q_t^C) \tag{8}$$<p>
</p>
$$q_{t,i} = [q_{t,i}^C, q_{t,i}^R] \tag{9}$$<ul>
<li>$c_t^Q \in \mathbb{R}^{d_c'}$: Q 压缩后的潜在向量。</li>
<li>$d_c'(\ll d_h n_h)$ 表示 Q 压缩后的维度。</li>
<li>$W^{DQ} \in \mathbb{R}^{d_c' \times d}, W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}$: 分别是 Q 的降维和升维矩阵。</li>
<li>$W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c'}$ 是用于生成携带 RoPE 的解耦 Q 的矩阵。</li>
</ul>
<p>注意到对 Q 的 RoPE 是在压缩后进行的，即为每个注意力头都生成了一个位置编码信息后进行拼接。</p>
<h2 id="attention-computation">Attention Computation<a hidden class="anchor" aria-hidden="true" href="#attention-computation">#</a></h2>
<p>最终 $q_{t,i}$, $k_{j,i}$, $v_{j,i}^C$ 被组合起来以生成最终的注意力输出 $u_t$</p>
$$\mathbf{o}_{t,i} = \sum_{j=1}^{t} \text{Softmax}\left(\frac{q_{t,i}^T \mathbf{k}_{j,i}}{\sqrt{d_h + d_R}}\right)v_{j,i}^C \tag{10}$$<p>
</p>
$$\mathbf{u}_t = W^O[\mathbf{o}_{t,1}, \mathbf{o}_{t,2}, \ldots, \mathbf{o}_{t,n_h}] \tag{11}$$<ul>
<li>$W^O \in \mathbb{R}^{d \times d_h n_h}$: 输出投影矩阵。</li>
</ul>
<h1 id="why-decoupled-rope">Why Decoupled RoPE<a hidden class="anchor" aria-hidden="true" href="#why-decoupled-rope">#</a></h1>
<p>假设不加 RoPE 的情况下进行 $q_{t,i}$, $k_{j,i}$ 的内积则有</p>
$$
q_{t,i}^{T}\times k_{j,i}=(W_{(i)}^{UQ}c_{t}^{Q})^{T}\times W_{(i)}^{UK}c_{j}^{KV}=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}W_{(i)}^{UK}\times c_{j}^{KV} \tag{12}
$$<p>RoPE 通过对向量应用一个<strong>位置依赖的旋转变换</strong>来注入相对位置信息。对于一个向量 $X$ 在位置 $t$，RoPE 可以被表示为一个旋转矩阵 $R_t$ 乘以 $X$：
</p>
$$\text{RoPE}(X, t) = R_t X$$<p>
这里的 $R_t$ 是一个正交旋转矩阵，它取决于位置 $t$.</p>
<p>如果直接对压缩后 $k_t^C$ 的 使用 RoPE 那么情况会变成</p>
$$
\begin{aligned}
q_{t,i}^{T}\times k_{j,i}&=(\mathcal{R}_{t}W_{(i)}^{UQ}c_{t}^{Q})^{T}\times\mathcal{R}_{j}W_{(i)}^{UK}c_{j}^{KV} \\
&=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}\mathcal{R}_{t}^{T}\mathcal{R}_{j}W_{(i)}^{UK}\times c_{j}^{KV}\\
&=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}\mathcal{R}_{t-j}W_{(i)}^{UK}\times c_{j}^{KV}
\end{aligned} \tag{13}
$$<p>中间的矩阵与相对位置有关，无法提前计算出来。因此文中就是对所有头都使用同一个 k 和计算 RoPE. 拼接后的向量再计算时</p>
$$
q_{t,i}^T\times k_{j,i}=[q_{t,i}^C;q_{t,i}^R]^T\times[k_{j,i}^C;k_t^R]=(q_{t,i}^C,k_{j,i}^C)+(q_{t,i}^R,k_t^R) \tag{14}
$$<p>前一部分按照公式 (12) 进行计算，后一部分按照 MQA 方式计算。因此只用缓存 $c_t^{KV}$ 和 $k_t^R$.</p>
<h1 id="source-code">Source Code<a hidden class="anchor" aria-hidden="true" href="#source-code">#</a></h1>
<p><a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/f6e34dd26772dd4a216be94a8899276c5dca9e43/inference/model.py#L393-L494">DeepSeek-V3 MLA</a> 对应的源码位置</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MLA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Multi-Head Latent Attention (MLA) Layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Attributes:
</span></span></span><span class="line"><span class="cl"><span class="s2">        dim (int): Dimensionality of the input features.
</span></span></span><span class="line"><span class="cl"><span class="s2">        n_heads (int): Number of attention heads.
</span></span></span><span class="line"><span class="cl"><span class="s2">        n_local_heads (int): Number of local attention heads for distributed systems.
</span></span></span><span class="line"><span class="cl"><span class="s2">        q_lora_rank (int): Rank for low-rank query projection.
</span></span></span><span class="line"><span class="cl"><span class="s2">        kv_lora_rank (int): Rank for low-rank key/value projection.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_head_dim (int): Total dimensionality of query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        v_head_dim (int): Dimensionality of value projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        softmax_scale (float): Scaling factor for softmax in attention computation.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">ModelArgs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算当前进程（卡）负责的注意力头数量，用于模型并行</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">//</span> <span class="n">world_size</span> 
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">q_lora_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">kv_lora_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_nope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># QK 头总维度 = 非 RoPE 部分 + RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">v_head_dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 查询投影 (wq) 的 LoRA 实现</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果 q_lora_rank 为 0，表示不使用 LoRA，直接进行全秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 dim 维度的输入投影到 n_heads * qk_head_dim 维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果 q_lora_rank &gt; 0，使用 LoRA 结构进行低秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wq_a: dim -&gt; q_lora_rank (低秩投影的第一步)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># q_norm: RMSNorm 应用于低秩维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wq_b: q_lora_rank -&gt; n_heads * qk_head_dim (低秩投影的第二步)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq_b</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 键值投影 (wkv) 的 LoRA 实现</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># wkv_a: dim -&gt; kv_lora_rank + qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应图中的 W^{DKV} 投影到低秩 KV 潜在空间 (kv_lora_rank) 和解耦的 RoPE 键 (qk_rope_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 kv_lora_rank 对应公式中的 d_c</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 qk_rope_head_dim 对应公式中的 d_h</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wkv_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># kv_norm: RMSNorm 应用于低秩维度</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># wkv_b: kv_lora_rank -&gt; n_heads * (qk_nope_head_dim + v_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应图中的 W^{UK} 和 W^{UV} 的组合</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 它将压缩后的 KV 潜在向量 (kv_lora_rank) 投影回非 RoPE 键 (qk_nope_head_dim) 和值 (v_head_dim) 的高维度空间</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 输出投影 (wo)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">RowParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Softmax 缩放因子，用于注意力分数的缩放，防止内积过大</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果序列长度超过原始训练长度，根据 RopeFactor 进行额外缩放，用于处理长序列外推问题</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">original_seq_len</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">mscale</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">mscale</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">rope_factor</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">*</span> <span class="n">mscale</span> <span class="o">*</span> <span class="n">mscale</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 根据注意力实现方式（naive 或 optimized）选择不同的 KV 缓存结构</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># naive 实现直接缓存完整键 K 和值 V</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_cache: (max_batch_size, max_seq_len, n_local_heads, qk_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;k_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># v_cache: (max_batch_size, max_seq_len, n_local_heads, v_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;v_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># optimized 实现缓存压缩后的 KV 潜在向量和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># kv_cache: (max_batch_size, max_seq_len, kv_lora_rank) - 对应论文中的 c_t^{KV}</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;kv_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># pe_cache: (max_batch_size, max_seq_len, qk_rope_head_dim) - 对应论文中的 k_t^R</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;pe_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Forward pass for the Multi-Head Latent Attention (MLA) Layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).
</span></span></span><span class="line"><span class="cl"><span class="s2">            start_pos (int): Starting position in the sequence for caching.
</span></span></span><span class="line"><span class="cl"><span class="s2">            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s2">            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">            torch.Tensor: Output tensor with the same shape as the input.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_pos</span> <span class="o">=</span> <span class="n">start_pos</span> <span class="o">+</span> <span class="n">seqlen</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. 查询 (Q) 的生成</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 全秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># LoRA 投影：x -&gt; wq_a -&gt; q_norm -&gt; wq_b</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wq_a</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape Q</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 分离 Q 的非 RoPE 部分和 RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># q_nope 对应论文中的 q_{t,i}^C (非位置信息查询)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># q_pe 对应论文中的 q_{t,i}^R (携带 RoPE 的解耦查询)</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_nope</span><span class="p">,</span> <span class="n">q_pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 对 Q 的 RoPE 部分应用旋转位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应论文中的 q_t^R = RoPE(W^{QR}c_t^Q) 的 RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_pe</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">q_pe</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. 键值 (KV) 的生成</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 将输入 x 投影到低秩 KV 潜在空间和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应论文中的 c_t^{KV} 和 k_t^R</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 分离出 KV 潜在向量和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># kv 对应论文中的 c_t^{KV}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># k_pe 对应论文中的 k_t^R (RoPE 解耦键)</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span><span class="p">,</span> <span class="n">k_pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 对 K 的 RoPE 部分应用旋转位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 注意 k_pe.unsqueeze(2) 是因为 apply_rotary_emb 期望 (..., seq_len, head_dim) 结构</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 k_pe 可能是 (bsz, seqlen, qk_rope_head_dim)，需要添加一个 head 维度</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_pe</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">k_pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">freqs_cis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. 注意力计算：根据实现方式 (naive 或 optimized)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Naive 实现直接拼接 Q 的 RoPE 和非 RoPE 部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">q_nope</span><span class="p">,</span> <span class="n">q_pe</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q 恢复为 (bsz, seqlen, n_local_heads, qk_head_dim)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 对 KV 潜在向量应用归一化，并进行第二阶段投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中将 c_t^{KV} 投影到非 RoPE 键和值的部分 (k_t^C 和 v_t^C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span><span class="p">(</span><span class="n">kv</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 KV 结果重塑为 (batch_size, seq_len, n_local_heads, qk_nope_head_dim + v_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 分离出非 RoPE 键和值</span>
</span></span><span class="line"><span class="cl">            <span class="n">k_nope</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 拼接非 RoPE 键和 RoPE 键，组成完整的键 K</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_pe 之前是 (bsz, seqlen, 1, qk_rope_head_dim)，需要 expand 到 n_local_heads</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k_nope</span><span class="p">,</span> <span class="n">k_pe</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 K 和 V 缓存 (在推理时用于自回归生成)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">k_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">v_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算注意力分数 (Q @ K^T)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># scores: (batch_size, q_seq_len, n_local_heads, k_seq_len)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 使用整个缓存中的键进行计算</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshd,bthd-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># optimized 实现</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 获取 wkv_b 权重，如果使用了量化则进行反量化</span>
</span></span><span class="line"><span class="cl">            <span class="n">wkv_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">weight</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">weight_dequant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 wkv_b 重塑为 (n_local_heads, head_dim, kv_lora_rank) 以便进行逐头的操作</span>
</span></span><span class="line"><span class="cl">            <span class="n">wkv_b</span> <span class="o">=</span> <span class="n">wkv_b</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">)</span> <span class="c1"># (n_heads, (qk_nope+v), kv_rank)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算 Q_nope 与 K_nope 的点积 (通过 kv 缓存)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># q_nope: (bsz, seqlen, n_local_heads, qk_nope_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wkv_b[:, :self.qk_nope_head_dim] 是 W^{UK} 的部分</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 这对应论文中的 Softmax(q_{t,i}^C @ c_{j,i}^{KV}) 的第一项</span>
</span></span><span class="line"><span class="cl">            <span class="n">q_nope</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshd,hdc-&gt;bshc&#34;</span><span class="p">,</span> <span class="n">q_nope</span><span class="p">,</span> <span class="n">wkv_b</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 KV 缓存 (kv_cache 对应论文中的 c_t^{KV})</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span><span class="p">(</span><span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 PE 缓存 (pe_cache 对应论文中的 k_t^R)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_pe 之前是 (bsz, seqlen, 1, qk_rope_head_dim)，squeeze 掉那个 1 维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">pe_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_pe</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算注意力分数</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第一项: 非 RoPE 查询 q_nope 与缓存的 kv_cache (压缩键) 的点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(q_{t,i}^C @ c_{j,i}^{KV}) 的第一部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshc,btc-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q_nope</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span> <span class="o">+</span> \
</span></span><span class="line"><span class="cl">                      <span class="c1"># 第二项: RoPE 查询 q_pe 与缓存的 pe_cache (解耦 RoPE 键) 的点积</span>
</span></span><span class="line"><span class="cl">                      <span class="c1"># 对应论文中的 Softmax(q_{t,i}^R @ k_{j,i}^R) 的第二部分</span>
</span></span><span class="line"><span class="cl">                      <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshr,btr-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q_pe</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="c1"># 应用缩放因子</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 应用注意力掩码 (如因果掩码，防止看到未来信息)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">+=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># unsqueeze(1) 广播到 heads 维度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 对分数应用 Softmax 得到注意力权重</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. 值 (V) 的加权求和</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Naive 实现直接与 V 缓存进行点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 sum(Softmax(...) * v_{j,i}^C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bsht,bthd-&gt;bshd&#34;</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># optimized 实现</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># optimized 实现通过 wkv_b 的值部分将加权后的压缩 KV 还原为 V</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第一步: 将注意力权重与缓存的 kv_cache (压缩值) 进行点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(...) * c_{j,i}^{KV} 的第一部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bsht,btc-&gt;bshc&#34;</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第二步: 将加权后的压缩值通过 wkv_b 的值投影部分还原为最终的值向量</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wkv_b[:, -self.v_head_dim:] 是 W^{UV} 的部分</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(...) * v_{j,i}^C 的第二部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshc,hdc-&gt;bshd&#34;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wkv_b</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 将所有头的结果展平并进行最终的输出投影</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># x.flatten(2) 将 (bsz, seqlen, n_local_heads, v_head_dim) 展平为 (bsz, seqlen, n_local_heads * v_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/deepseek/">DeepSeek</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/deepseek/deepseekmoe/">
    <span class="title">« Prev</span>
    <br>
    <span>DeepSeekMoE</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/">
    <span class="title">Next »</span>
    <br>
    <span>ServingLLMsOnHuaweiCloudMatrix384</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
