<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DualPipe | WITHER</title>
<meta name="keywords" content="DeepSeek, DualPipe">
<meta name="description" content="Source code reading of DualPipe">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/deepseek/dualpipe/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/deepseek/dualpipe/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/deepseek/dualpipe/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="DualPipe">
  <meta property="og:description" content="Source code reading of DualPipe">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-06-21T22:00:13+08:00">
    <meta property="article:modified_time" content="2025-06-21T22:00:13+08:00">
    <meta property="article:tag" content="DeepSeek">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DualPipe">
<meta name="twitter:description" content="Source code reading of DualPipe">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DeepSeek Related",
      "item": "http://localhost:1313/blogs/deepseek/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "DualPipe",
      "item": "http://localhost:1313/blogs/deepseek/dualpipe/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DualPipe",
  "name": "DualPipe",
  "description": "Source code reading of DualPipe",
  "keywords": [
    "DeepSeek", "DualPipe"
  ],
  "articleBody": "Preliminary 本节先回顾流水线并行以及 DeepSeek-V3 中作为 baseline 的 PipeDream 论文中的 1F1B 和 ZeroBubble 论文中的 ZB1P (ZB-H1 的自动搜索结果).\nPipeDream 1F1B 1F1B (One-Forward-One-Backward) 的工作流程如图所示，想象一条工厂流水线，用于组装一个复杂的设备。这个设备需要经过多个工位（GPU），每个工位负责一部分装配任务（模型的不同层）。当第一个产品的第一个部件在工位1上加工时，其他所有工位都在闲置等待。当它被传递到工位2时，工位1开始加工第二个产品，但工位3、4…依然在等待。这种在流水线启动和结束阶段产生的设备空闲时间，就是流水线气泡 (Pipeline Bubble). 在大模型训练中，这意味着 GPU 算力被浪费，直接导致训练时间延长和成本增加。\n1F1B pipeline Schedule\n后续批次的后向传播永远在前一批次的后向传播全部启动后才开始，为了防止激活占用内存过多，图中 1F1B 的 bs=8，流水线并行过程中最多保存 4 个 batch 的激活，当 batch1 反向传播结束后再进行 batch5 的正向传播。为了减少激活占用，1F1B 中进行反向传播的优先级高于正向传播。\nZeroBubble ZB1P ZeroBubble 减少气泡的关键是将反向传播中对于权重和输入的梯度计算分开进行。传统上，一个层的反向传播包含两个核心任务:\nB Pass: 计算关于输入梯度并将其传递给前一层，这是误差反向传播链的一部分。 W Pass: 计算该层自身权重的梯度，用于后续的参数更新。 如图所示第 i-1 层的 B Pass 依赖于第 i 层的 B Pass. 但第 i 层的 W Pass，只要在其 B Pass 完成之后，可以被灵活地安排在任何时间点执行。\nComputation Graph for MLP Handcrafted Pipeline Schedule\n基于这个思想，文中提出了两个手工设计的调度方案作为概念验证:\nZB-H1 (Memory Efficient Schedule): 在维持与 1F1B 相似峰值内存消耗的情况下，通过将 W Pass 推迟执行，填充了流水线末尾的 cooldown 气泡，成功将气泡大小减少到 1F1B 的三分之一。 ZB-H2 (Zero Bubble Schedule): 当内存预算更宽松时，在流水线 warm-up 安排更多的 F Pass，并巧妙地重排 W Pass，将整个流水线的执行过程从一个梯形变成了一个平行四边形，从而在理论上完全消除了气泡。 Handcrafted pipeline schedules ZB-H1 (top) \u0026 ZB-H2 (bottom)\n文中基于一个标准的 Transformer架构，其中 FFN 的中间层维度是模型隐藏维度 h 的4倍。给出了 F, B, W 各自的计算量和激活占用。其中计算量只统计占据主要部分的矩阵乘法的浮点运算次数。\nb: microbatch size s: sequence length h: hidden dimension size a: number of attention heads Transformer Architecture Table1: FLOPs and activations memory required per transformer layer for each pass\nPass FLOPs Activations Memory Required F $sbh(24h+4s)$ 0 B $sbh(24h+8s)$ $sb(34h+5as)$ W $sbh(24h)$ $32sbh$ 前向传播 $T_F \\approx (8bsh^2 + 4bs^2h) + 16bsh^2 = 24bsh^2 + 4bs^2h = sbh(24h + 4s)$. 反向传播关于权重的计算量等于 Linear 层的 GEMM.\nSelf-Attention: $6bsh^2 + 2bs^2h + 2bs^2h + 2bsh^2 = 8bsh^2 + 4bs^2h$\nQ, K, V Projection：输入 (b, s, h) 通过与权重矩阵 (h, h) 相乘，生成Q, K, V。这涉及到3次矩阵乘法。$\\text{FLOPs} \\approx 2 \\times b \\times s \\times h \\times 3h = 6bsh^2$ Attention Score:Q (b, a, s, h/a) 与 K^T (b, a, h/a, s) 相乘。$\\text{FLOPs} \\approx 2 \\times b \\times a \\times s \\times (h/a) \\times s = 2bshs$. Score@V：注意力分数 (b, a, s, s) 与 V (b, a, s, h/a) 相乘。$\\text{FLOPs} \\approx 2 \\times b \\times a \\times s \\times s \\times (h/a) = 2bshs$. O Projecyion：结果与输出权重矩阵 (h, h) 相乘。$\\text{FLOPs} \\approx 2 \\times b \\times s \\times h \\times h = 2bsh^2$. FFN FLOPs: $8bsh^2 + 8bsh^2 = 16bsh^2$\nUp Projection：输入 (b, s, h) 与权重矩阵 (h, 4h) 相乘。$\\text{FLOPs} \\approx 2 \\times b \\times s \\times h \\times 4h = 8bsh^2$. Down Projection：中间结果 (b, s, 4h) 与权重矩阵 (4h, h) 相乘。$\\text{FLOPs} \\approx 2 \\times b \\times s \\times 4h \\times h = 8bsh^2$. 激活占用方面除了 Dropout Mask 是 INT8 类型以外，假设 activations 均以 16-bit float 类型保存。表中的 activation memory 均以字节为单位进行统计。和权重梯度无关的部分只有 dropout 相关的以及 Softmax output.\nCategory Item Original TP Attention Total $11sbh + 5as^2b$ $3sbh + \\frac{8sbh}{t} + \\frac{5as^2b}{t}$ QKV input $2sbh$ $2sbh$ QK^T $4sbh$ $\\frac{4sbh}{t}$ Softmax output $2as^2b$ $\\frac{2as^2b}{t}$ Dropout mask $as^2b$ $\\frac{as^2b}{t}$ Dropout output $2as^2b$ $\\frac{2as^2b}{t}$ V $2sbh$ $\\frac{2sbh}{t}$ Linear projection input $2sbh$ $\\frac{2sbh}{t}$ Attention dropout mask $sbh$ $sbh$ MLP Total $19sbh$ $3sbh + \\frac{16sbh}{t}$ Linear1 input $2sbh$ $2sbh$ GeLU input $8sbh$ $\\frac{8sbh}{t}$ Linear2 input $8sbh$ $\\frac{8sbh}{t}$ Dropout mask $sbh$ $sbh$ LayerNorm Total $4sbh$ $4sbh$ LayerNorm1 input $2sbh$ $2sbh$ LayerNorm2 input $2sbh$ $2sbh$ 在没有 $T_F = T_B = T_W$ 假设的情况下，ZB-H1 和 ZB-H2 的峰值激活内存和气泡大小如 Table 2 所示。值得注意的是，对于设备 i，其在 ZB-H1 方案下的激活内存为 $(p-i+1)M_B + (i-1)M_W$，在 ZB-H2 方案下的激活内存为 $(2p - 2i + 1)M_B + (2i - 2)M_W$。如 Table 1 所示，W 所需的激活内存小于 B 所需的激活内存。因此，ZB-H1 和 ZB-H2 的峰值激活内存分别为 $pM_B$ 和 $(2p-1)M_B$。\nTable 2: Comparison between 1F1B and our handcrafted schedules.\nSchedule Bubble size Peak activations memory 1F1B $(p-1)(T_{F}+T_{B}+T_{W})$ $pM_{B}$ ZB-H1 $(p-1)(T_{F}+T_{B}-T_{W})$ $pM_{B}$ ZB-H2 $(p-1)(T_{F}+T_{B}-2T_{W})$ $(2p-1)M_{B}$ Automatic Pipeline Scheduling\n手工调度依赖于 F、B、W 的执行时间相等的理想情况。为了应对真实世界中复杂的执行时间和通信延迟，该文开发了一个自动化流水线调度算法。该算法通过一系列启发式策略，在一个给定的内存限制下，自动地为流水线生成一个高效的调度方案。核\nWarm-up：\n在内存限制的范围内，算法会尽可能多地调度 F pass ，以最小化在第一个 B pass 开始前产生的气泡。 此阶段使用一个超参数来控制是否要调度一个可能会延迟后续B Pass的额外F Pass。 Steady State：\n热身阶段结束后，调度进入一个迭代模式，轮流调度一个F Pass和一个B Pass。 为了填充气泡，算法会伺机插入 W pass. 插入策略是： 当出现一个大于 $T_W$ (W Pass 执行时间) 的气泡时，直接插入一个W Pass. 当出现一个小于 $T_W$ 的气泡时，如果这个气泡会导致当前阶段的累计气泡时间成为所有阶段中最长的，那么仍然会插入一个W Pass. 当内存达到上限时，也会插入 W Pass 以回收和释放部分内存。 通常这个启发式策略在稳态阶段会形成一个 1F-1B-1W 的调度模式。 Global Schedule：\n在整个调度过程中，算法始终保证在 F Pass 用完之前，第 i 阶段调度的 F Pass 数量至少比第 i+1 阶段多一个。 当这个数量差超过一时，会使用另一个超参数来决定在不产生额外气泡的前提下，是否要跳过第 i 阶段的一次F Pass调度。 算法会通过 grid search 来寻找这些超参数的最佳组合。 Final：当某个阶段的 F Pass 和 B Pass 都执行完毕后，算法会一次性逐个调度完所有剩余的 W Pass.\nBypassing Optimizer Synchronization\n要实现完美的平行四边形调度，还需要解决优化器同步（Optimizer Synchronization）. 在分布式训练中，通常需要在更新模型参数前，在所有 GPU 间进行一次 All-Reduce，以进行梯度裁剪（Gradient Clipping）或检查数值稳定性 (NaN/INF). 这个同步点会强制所有设备等待，从而破坏平行四边形，重新引入气泡。\n论文提出了 Bypassing Optimizer Synchronization，每个 GPU 在执行优化器更新步骤时，不再等待全局同步，而是基于从前一个 GPU 传来的部分 reduce 的信息进行推测性更新。该 micro-batch 完整的全局状态会在下一个迭代的 warp 阶段异步地传回。每个 GPU 在收到最终的全局状态后，会验证自己上一步的更新是否合法。如果发现不一致（例如，全局梯度范数超出了裁剪阈值），它会执行一次原地回滚（In-place Rollback），然后使用正确的全局状态重新执行优化器步骤。\nThe Post-validation Strategy to Replace Optimizer Synchronization\nDualPipe DualPipe 是一种创新的双向流水线并行算法。它的核心思想是在一组设备上同时处理两个方向的数据流：一个前向流水线和一个反向流水线。使得计算和通信能够更充分地重叠，从而减少流水线气泡（即 GPU 空闲时间）.\n与传统的 GPipe（1F1B）只有一个数据流方向不同，DualPipe 将设备对折，形成两条对称的流水线。例如，在一个有 8 个 PP ranks (GPU) 的设置中：\n前向流水线 (Forward Pipeline): 数据从 rank 0 -\u003e 1 -\u003e 2 -\u003e 3. 反向流水线 (Backward Pipeline): 同时有另一组数据从 rank 7 -\u003e 6 -\u003e 5 -\u003e 4. Rank 3 和 Rank 4 成为两条流水线的中间节点，它们之间会交换数据。每个设备实际上会处理两个流水线阶段的模型块，一个用于前向流水线，另一个用于反向流水线。 Initialization modules: 每个 DualPipe 实例接收一个元组，包含两个 nn.Module. modules[0] 用于处理前向-\u003e反向的计算，modules[1] 用于处理反向-\u003e前向的计算。 Rank 角色判断: 代码会根据当前 rank 的 ID 判断其在整个流水线中的位置（是否是第一个、最后一个、是否在后半部分、是否是中间节点）. 这个角色判断对于后续的通信和计算调度至关重要。例如 is_in_second_half 决定了该 rank 的 phase 0 和 phase 1 究竟对应前向流水线还是反向流水线。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class DualPipe(nn.Module): def __init__( self, modules: Tuple[nn.Module, nn.Module], # ... ) -\u003e None: super().__init__() # 每个 rank 持有两个模型模块 self.module = nn.ModuleList(modules) # ... self.group = process_group or dist.distributed_c10d._get_default_group() self.num_ranks = self.group.size() # ... # 计算当前 rank 在流水线中的角色 self.rank = rank_mapping[self.group.rank()] self.is_first_rank = self.rank == 0 self.is_last_rank = self.rank == self.num_ranks - 1 # 判断 rank 是否在对折后的后半部分 self.is_in_second_half = self.rank \u003e= self.num_ranks // 2 # 判断是否为中间的 rank self.is_middle_rank = (self.rank == self.num_ranks // 2 - 1) or (self.rank == self.num_ranks // 2) Core Function: step step 方法是 DualPipe 的核心，它协调了所有 micro-batches 的计算和通信。整个过程被划分为 8 个阶段，以实现最大程度的计算-通信重叠。\n输入处理: 只有 rank 0 和 rank N-1 会接收外部输入数据 inputs 和 labels. 这些数据被 scatter (dualpipe/utils.py) 切分成 half_num_chunk 个 micro-batch 。Rank 0 的输入用于前向流水线，Rank N-1 的输入用于反向流水线。\ndef step( self, *inputs: Optional[torch.Tensor], num_chunks: int = 0, # ... ) -\u003e Tuple[Optional[torch.Tensor], Optional[Union[torch.Tensor, Tuple[torch.Tensor]]]]: # ... # 重置状态 self._reset_states() # 将输入数据切分成 micro-batch inputs = scatter(inputs, half_num_chunks, self.batch_dim) labels = scatter(labels, half_num_chunks, self.batch_dim) if self.is_first_rank: self.input_chunks = (inputs, []) self.labels = ([], labels) elif self.is_last_rank: self.input_chunks = ([], inputs) self.labels = (labels, []) # ... 接下来是 8 个核心调度阶段的，在此之前会进行一些准备工作：\n状态重置: _reset_states() 清空上一轮迭代的缓存，如输入/输出块、梯度、损失等。 rank 确定: 计算 num_half_ranks（流水线对折后的一半设备数）和 half_rank（当前秩在对折流水线中的位置. 这些变量将决定每个阶段的循环次数。 数据分发: scatter 函数将输入数据 inputs 和 labels 切分成 half_num_chunks 个 micro-batch 。根据 is_first_rank 或 is_last_rank，将这些 micro-batch 存放到 self.input_chunks 和 self.labels 中。 调度示意图如下图所示，红色线分隔了每个步骤\nDualPipe Schedule\nStep 1: Warm-up Forward nF0\n这是一个纯前向计算阶段，用于填满流水线。距离流水线中点越远的 rank（half_rank 越小）执行的预热步骤越多。 _forward_chunk(0) 被调用，在此函数内部:\n_recv_forward(0): 尝试接收前一个 rank 的数据。对于 rank 0 来说，它直接使用 self.input_chunks 的数据，不接收。 _commit_and_wait_comm(): 等待数据接收完成。 _forward_compute_chunk(0): 执行 self.module[0] 的前向计算。 _send_forward(0): 将计算结果异步地发送给下一个 rank. 1 2 3 step_1 = (num_half_ranks - half_rank - 1) * 2 for i in range(step_1): self._forward_chunk(0) Step 2: Dual Forward nF0F1\n两条流水线都开始执行前向计算。两条流水线都开始工作。当前 rank 不仅继续处理 phase 0 的前向计算，也开始处理从另一端（phase 1）传来的数据的前向计算。\n_forward_chunk(0, recv=False, ...) 处理一个 phase 0 的 micro-batch ，但不立即接收下一个，因为前面已经调用了 _recv_forward(0). _forward_chunk(1, ...): 处理一个 phase 1 的 micro-batch 。 1 2 3 4 5 6 7 8 9 # Step 2: nF0F1 step_2 = half_rank + 1 self._recv_forward(0) for i in range(step_2): self._forward_chunk(0, recv=False, send=self.is_middle_rank) self._recv_forward(0) self._forward_chunk(1, send=(not self.is_middle_rank) or (i \u003c step_2 - 1)) if not self.is_middle_rank: self._send_forward(0) Step 3: 前向-后向-权重混合阶段 (Zero Bubble) nB1W1F1\n这是 DualPipe 提高效率的关键。当一条流水线开始进行反向计算时，另一条流水线仍在进行前向计算。\n_backward_chunk(1, enable_zb=True): 执行反向计算，并启用 Zero Bubble (ZB) 优化。ZB 通过 WeightGradStore 将权重梯度（weight gradients）的计算（通常在反向传播中阻塞）缓存起来，推迟执行，从而让路给其他计算或通信。 _weight_chunk(): 执行被推迟的权重梯度计算。 _forward_chunk(1): 同时执行另一个方向的前向计算。 # Step 3: nB1W1F1 (Use zero bubble) step_3 = num_half_ranks - half_rank - 1 for i in range(step_3): self._backward_chunk(1, enable_zb=True) self._recv_forward(1) self._weight_chunk() self._forward_chunk(1, recv=False) Step 4: Main Steady State nF0B1F1B0\n这是流水线完全填满后的主循环。在一个循环迭代中，一个 rank 会执行两次计算和通信的重叠操作：一次是（前向计算 + 反向计算），另一次也是（前向计算 + 反向计算）. 这里调用 _forward_backward_chunk(0, 1) 和 _forward_backward_chunk(1, 0). 这个函数尝试将一个方向的前向计算（F）与另一个方向的反向计算（B）打包在一起执行，实现 F\u0026B 重叠。\n# Step 4 (Main step): nF0B1F1B0 step_4 = half_num_chunks - num_ranks + half_rank + 1 for i in range(step_4): # ... self._forward_backward_chunk(0, 1) # i != 0 self._forward_backward_chunk(1, 0) Step 5 \u0026 6: 后向-后向混合阶段 (Cooldown Backward) nB1F1B0 和 nB1B0\n当前向数据流耗尽后，流水线进入收尾阶段。这个阶段主要执行剩余的反向计算。同样，ZB 优化在后半段被启用，以减少气泡。\n# Step 5: nB1F1B0 step_5 = num_half_ranks - half_rank - 1 for i in range(step_5): self._backward_chunk(1) self._forward_backward_chunk(1, 0) # Step 6: nB1B0 (The second half of the chunks use zero bubble) step_6 = half_rank + 1 enable_zb = False for i in range(step_6): if i == step_6 // 2 and half_rank % 2 == 1: enable_zb = True self._backward_chunk(1, enable_zb=enable_zb) if i == step_6 // 2 and half_rank % 2 == 0: enable_zb = True self._backward_chunk(0, enable_zb=enable_zb) Step 7 \u0026 8: 权重更新收尾阶段 nWB0 和 nW\nStep 7 将最后的后向计算与权重计算重叠。 Step 8 是纯粹的权重计算阶段，循环调用 _weight_chunk() 直到 WeightGradStore.funcs_queue 队列为空，确保所有梯度都已计算完毕。 # Step 7: nWB0 (Use zero bubble) step_7 = num_half_ranks - half_rank - 1 for i in range(step_7): self._weight_chunk() self._backward_chunk(0, enable_zb=True) # Step 8: nW step_8 = half_rank + 1 for i in range(step_8): self._weight_chunk() assert WeightGradStore.funcs_queue.empty() Computation-Communication Overlap _forward_backward_compute_chunk 函数是实现计算重叠的关键。在理想情况下（如果模型结构支持），它可以将一个 micro-batch 的前向计算和另一个 micro-batch 的反向计算在同一个函数调用中完成。该函数在 step4 使用的_forward_backward_chunk 函数中被调用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 def _forward_backward_compute_chunk(self, phase0: int, phase1: int) -\u003e None: # ... if not self.overlapped_forward_backward: self._forward_compute_chunk(phase0) self._backward_compute_chunk(phase1) return # ... # forward \u0026 backward outputs0, loss0 = type(module0).overlapped_forward_backward( module0, inputs0, criterion0, labels0, module1, loss1, outputs1, output_grads1, ) # ... 如果模型定义了一个 overlapped_forward_backward (@classmethod)，DualPipe 就会调用它。在这个方法里，开发者可以自定义前向和后向计算的交错执行顺序，以达到最佳的重叠效果。DeepSeek-v3 的重叠方法在技术报告里已经讲解。\nReal Case 通过 examples/example_dualpipe.py 中的 main 函数来详细讲解一个完整的 DualPipe 流程。\n环境初始化和配置 分布式设置: main 函数首先初始化 PyTorch 的分布式通信组（init_process_group），并为每个进程（rank）分配一个 GPU. 参数配置: 定义了 micro-batch 数量 (num_chunks)、每个 micro-batch 的大小 (micro_batch_size) 等超参数。 P2P通信设置: 在执行 DualPipe 的 step 方法前，必须调用 set_p2p_tensor_shapes 和 set_p2p_tensor_dtype 来告知 DualPipe 在流水线中传递的张量的形状和数据类型。这是因为 DualPipe 需要预先分配内存来接收来自其他 rank 的数据。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def main(rank, pp_size): # 判断当前进程的角色 is_first_rank = rank == 0 is_last_rank = rank == pp_size - 1 # 初始化分布式环境 dist.init_process_group(backend='nccl', init_method=\"env://\", world_size=pp_size, rank=rank) torch.cuda.set_device(rank) torch.set_default_device(f\"cuda:{rank}\") torch.manual_seed(233) os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" # 定义流水线参数 num_chunks = 20 micro_batch_size = 3 seq_len = 256 hidden_size = 512 if is_first_rank: print(f\"{pp_size=}, {num_chunks=}, {seq_len=}, {hidden_size=}\", flush=True) # 设置P2P通信的Tensor形状和类型 set_p2p_tensor_shapes([(micro_batch_size, seq_len, hidden_size)]) set_p2p_tensor_dtype(torch.float32) 模型和参考基准的创建 1 2 3 4 5 6 7 8 9 # 创建一个完整的、未分割的模型 full_modules = nn.Sequential(*[PipelineStage(hidden_size) for _ in range(pp_size)]) # 创建完整的输入数据 full_x = torch.randn(num_chunks * micro_batch_size, seq_len, hidden_size) full_l = torch.randn(num_chunks * micro_batch_size, seq_len, hidden_size) # 参考步骤：在一个GPU上，用标准的数据并行方式运行完整模型，得到基准结果 loss_ref, output_ref = ref_step(full_x, full_l, full_modules, num_chunks) 创建模型: 代码首先创建了一个完整的 nn.Sequential 模型 (full_modules)，它包含了流水线所有的阶段。 参考步骤 (ref_step): 为了验证 DualPipe 的正确性，ref_step 函数模拟了标准的、非流水线并行的训练过程。它将数据分块，依次通过完整模型计算损失和输出。loss_ref 和 output_ref 将作为后续比较的正确答案。 DualPipe模型的创建和输入准备 模型分割: 每个 rank r 会持有两个 PipelineStage: 一个是 full_modules[r]，另一个是 full_modules[pp_size - 1 - r]. 这就是 Dual (双向) 的体现。例如，在一个 4-GPU 的设置中： Rank 0 持有 stage 0 和 stage 3 的模型。 Rank 1 持有 stage 1 和 stage 2 的模型。 Rank 2 持有 stage 2 和 stage 1 的模型。 Rank 3 持有 stage 3 和 stage 0 的模型。 输入数据分割: DualPipe 有两个数据入口点：rank 0 和最后一个 rank. rank 0 接收前半部分的输入 (full_x.chunk(2)[0]) 和 后半部分 的标签 (full_l.chunk(2)[1]). last rank 接收后半部分的输入 (full_x.chunk(2)[1]) 和 前半部分 的标签 (full_l.chunk(2)[0]). 一共有两个数据流: 一个从 rank 0 开始，其对应的标签在最后一个 rank；另一个从最后一个 rank 开始，其对应的标签在 rank 0.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # DualPipe 模型创建 # 每个 rank 获取两个处于对称位置的模型块 local_full_modules = nn.Sequential(full_modules[rank], full_modules[pp_size - 1 - rank]) local_modules = nn.Sequential(PipelineStage(hidden_size), PipelineStage(hidden_size)) # ... 加载权重 ... dualpipe_model = DualPipe(local_modules) # DualPipe输入数据准备 if is_first_rank: x = full_x.chunk(2)[0] l = full_l.chunk(2)[1] elif is_last_rank: x = full_x.chunk(2)[1] l = full_l.chunk(2)[0] else: x = None l = None 执行训练步骤 调用 dualpipe_model.step，触发了前面讲解中提到的复杂的8阶段调度流程。\nloss, outputs = dualpipe_model.step(x, num_chunks=num_chunks, criterion=criterion, labels=(l,), return_outputs=False) 结果验证 检查损失\nif is_first_rank: assert torch.equal(loss, loss_ref.chunk(2)[1]) elif is_last_rank: assert torch.equal(loss, loss_ref.chunk(2)[0]) else: assert loss is None 训练步骤完成后，step 方法会返回计算出的损失。\nrank0 计算出的 loss 对应的是从 last rank 输入的数据流，等于参考损失的后半部分 (loss_ref.chunk(2)[1]). 同理，last rank 计算出的 loss 对应的是从 rank0 输入的数据流，等于参考损失的前半部分 (loss_ref.chunk(2)[0]). 中间的 ranks 不计算最终损失，返回 None. 检查梯度\n1 2 3 4 5 6 7 8 9 for (p0, p1) in zip(local_modules[0].parameters(), local_modules[1].parameters()): # ... dist.all_gather_into_tensor(p0all, p0.grad) dist.all_gather_into_tensor(p1all, p1.grad) # 手动聚合对称rank的梯度 p0.grad += p1all[pp_size - 1 - rank] p1.grad += p0all[pp_size - 1 - rank] for ((n, p), p_ref) in zip(local_modules.named_parameters(), local_full_modules.parameters()): assert cal_diff(p.grad, p_ref.grad) \u003c 1e-13 由于每个 rank r 持有 r 和 pp_size - 1 - r 两个阶段的模型，如果这两个阶段在逻辑上是同一个权重（例如，在Encoder-Decoder结构中共享权重），那么它们的梯度需要手动聚合。示例通过 dist.all_gather_into_tensor 收集所有 rank 上对称模块的梯度，然后手动将它们相加。最后，将聚合后的梯度与 ref_step 中计算出的参考梯度进行比较，验证反向传播的正确性。\n",
  "wordCount" : "6075",
  "inLanguage": "en",
  "datePublished": "2025-06-21T22:00:13+08:00",
  "dateModified": "2025-06-21T22:00:13+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/deepseek/dualpipe/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/deepseek/">DeepSeek Related</a></div>
    <h1 class="post-title entry-hint-parent">
      DualPipe
    </h1>
    <div class="post-description">
      Source code reading of DualPipe
    </div>
    <div class="post-meta"><span title='2025-06-21 22:00:13 +0800 CST'>Jun-21-2025</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;6075 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#preliminary" aria-label="Preliminary">Preliminary</a><ul>
                            
                    <li>
                        <a href="#pipedream-1f1b" aria-label="PipeDream 1F1B">PipeDream 1F1B</a></li>
                    <li>
                        <a href="#zerobubble-zb1p" aria-label="ZeroBubble ZB1P">ZeroBubble ZB1P</a></li></ul>
                    </li>
                    <li>
                        <a href="#dualpipe" aria-label="DualPipe">DualPipe</a><ul>
                            
                    <li>
                        <a href="#initialization" aria-label="Initialization">Initialization</a></li>
                    <li>
                        <a href="#core-function-step" aria-label="Core Function: step">Core Function: step</a></li>
                    <li>
                        <a href="#computation-communication-overlap" aria-label="Computation-Communication Overlap">Computation-Communication Overlap</a></li></ul>
                    </li>
                    <li>
                        <a href="#real-case" aria-label="Real Case">Real Case</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="preliminary">Preliminary<a hidden class="anchor" aria-hidden="true" href="#preliminary">#</a></h1>
<p>本节先回顾流水线并行以及 DeepSeek-V3 中作为 baseline 的 <a href="https://arxiv.org/pdf/1806.03377">PipeDream</a> 论文中的 1F1B 和 <a href="https://openreview.net/pdf?id=tuzTN0eIO5">ZeroBubble</a> 论文中的 ZB1P (ZB-H1 的自动搜索结果).</p>
<h2 id="pipedream-1f1b">PipeDream 1F1B<a hidden class="anchor" aria-hidden="true" href="#pipedream-1f1b">#</a></h2>
<p>1F1B (One-Forward-One-Backward) 的工作流程如图所示，想象一条工厂流水线，用于组装一个复杂的设备。这个设备需要经过多个工位（GPU），每个工位负责一部分装配任务（模型的不同层）。当第一个产品的第一个部件在工位1上加工时，其他所有工位都在闲置等待。当它被传递到工位2时，工位1开始加工第二个产品，但工位3、4…依然在等待。这种在流水线启动和结束阶段产生的设备空闲时间，就是流水线气泡 (Pipeline Bubble). 在大模型训练中，这意味着 GPU 算力被浪费，直接导致训练时间延长和成本增加。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB787301b994e9c90258f8ad84fd1f8b67?method=download&amp;shareKey=db772d656fe8be439988e887fd6910a3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB787301b994e9c90258f8ad84fd1f8b67?method=download&amp;shareKey=db772d656fe8be439988e887fd6910a3" alt="1F1B pipeline Schedule">
    </a><figcaption>1F1B pipeline Schedule</figcaption></figure></p>
<p>后续批次的后向传播永远在前一批次的后向传播全部启动后才开始，为了防止激活占用内存过多，图中 1F1B 的 bs=8，流水线并行过程中最多保存 4 个 batch 的激活，当 batch1 反向传播结束后再进行 batch5 的正向传播。为了减少激活占用，1F1B 中进行反向传播的优先级高于正向传播。</p>
<h2 id="zerobubble-zb1p">ZeroBubble ZB1P<a hidden class="anchor" aria-hidden="true" href="#zerobubble-zb1p">#</a></h2>
<p>ZeroBubble 减少气泡的关键是将反向传播中对于权重和输入的梯度计算分开进行。传统上，一个层的反向传播包含两个核心任务:</p>
<ul>
<li>B Pass: 计算关于输入梯度并将其传递给前一层，这是误差反向传播链的一部分。</li>
<li>W Pass: 计算该层自身权重的梯度，用于后续的参数更新。</li>
</ul>
<p>如图所示第 i-1 层的 B Pass 依赖于第 i 层的 B Pass. 但第 i 层的 W Pass，只要在其 B Pass 完成之后，可以被灵活地安排在任何时间点执行。</p>
<h2 id="computation-graph-for-mlp">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBeef090dd44e296a4a4e985200c62e4c7?method=download&amp;shareKey=2d9e68a50dd5b0b3c46f079499ed2bec" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBeef090dd44e296a4a4e985200c62e4c7?method=download&amp;shareKey=2d9e68a50dd5b0b3c46f079499ed2bec" alt="Computation Graph for MLP">
    </a><figcaption>Computation Graph for MLP</figcaption></figure></h2>
<p><strong>Handcrafted Pipeline Schedule</strong></p>
<p>基于这个思想，文中提出了两个手工设计的调度方案作为概念验证:</p>
<ul>
<li>ZB-H1 (Memory Efficient Schedule): 在维持与 1F1B 相似峰值内存消耗的情况下，通过将 W Pass 推迟执行，填充了流水线末尾的 cooldown 气泡，成功将气泡大小减少到 1F1B 的三分之一。</li>
<li>ZB-H2 (Zero Bubble Schedule): 当内存预算更宽松时，在流水线 warm-up 安排更多的 F Pass，并巧妙地重排 W Pass，将整个流水线的执行过程从一个梯形变成了一个平行四边形，从而在理论上完全消除了气泡。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9f702f88f4b48c048d602ddfe7b69ffb?method=download&amp;shareKey=010eea5b8230b6175d7777444e4dcc64" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9f702f88f4b48c048d602ddfe7b69ffb?method=download&amp;shareKey=010eea5b8230b6175d7777444e4dcc64" alt="Handcrafted pipeline schedules ZB-H1 (top)  &amp; ZB-H2 (bottom)">
    </a><figcaption>Handcrafted pipeline schedules ZB-H1 (top)  &amp; ZB-H2 (bottom)</figcaption></figure></p>
<p>文中基于一个标准的 Transformer架构，其中 FFN 的中间层维度是模型隐藏维度 <code>h</code> 的4倍。给出了 F, B, W 各自的计算量和激活占用。其中计算量只统计占据主要部分的矩阵乘法的浮点运算次数。</p>
<ul>
<li><code>b</code>: microbatch size</li>
<li><code>s</code>: sequence length</li>
<li><code>h</code>: hidden dimension size</li>
<li><code>a</code>: number of attention heads</li>
</ul>
<h2 id="transformer-architecture">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd9f810c1f506d383eb59a0c1186e602b?method=download&amp;shareKey=ee9e8a521ed880701b05e9b25b1ae001" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd9f810c1f506d383eb59a0c1186e602b?method=download&amp;shareKey=ee9e8a521ed880701b05e9b25b1ae001" alt="Transformer Architecture">
    </a><figcaption>Transformer Architecture</figcaption></figure></h2>
<p><em>Table1: FLOPs and activations memory required per transformer layer for each pass</em></p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Pass</th>
          <th style="text-align: center">FLOPs</th>
          <th style="text-align: center">Activations Memory Required</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">F</td>
          <td style="text-align: center">$sbh(24h+4s)$</td>
          <td style="text-align: center">0</td>
      </tr>
      <tr>
          <td style="text-align: center">B</td>
          <td style="text-align: center">$sbh(24h+8s)$</td>
          <td style="text-align: center">$sb(34h+5as)$</td>
      </tr>
      <tr>
          <td style="text-align: center">W</td>
          <td style="text-align: center">$sbh(24h)$</td>
          <td style="text-align: center">$32sbh$</td>
      </tr>
  </tbody>
</table>
<hr>
<p>前向传播 $T_F \approx (8bsh^2 + 4bs^2h) + 16bsh^2 = 24bsh^2 + 4bs^2h = sbh(24h + 4s)$. 反向传播关于权重的计算量等于 Linear 层的 GEMM.</p>
<ul>
<li>
<p><strong>Self-Attention</strong>: $6bsh^2 + 2bs^2h + 2bs^2h + 2bsh^2 = 8bsh^2 + 4bs^2h$</p>
<ul>
<li><strong>Q, K, V Projection</strong>：输入 <code>(b, s, h)</code> 通过与权重矩阵 <code>(h, h)</code> 相乘，生成Q, K, V。这涉及到3次矩阵乘法。$\text{FLOPs} \approx 2 \times b \times s \times h \times 3h = 6bsh^2$</li>
<li><strong>Attention Score</strong>:<code>Q</code> <code>(b, a, s, h/a)</code> 与 <code>K^T</code> <code>(b, a, h/a, s)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times a \times s \times (h/a) \times s = 2bshs$.</li>
<li><strong>Score@V</strong>：注意力分数 <code>(b, a, s, s)</code> 与 <code>V</code> <code>(b, a, s, h/a)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times a \times s \times s \times (h/a) = 2bshs$.</li>
<li><strong>O Projecyion</strong>：结果与输出权重矩阵 <code>(h, h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times h \times h = 2bsh^2$.</li>
</ul>
</li>
<li>
<p><strong>FFN FLOPs</strong>: $8bsh^2 + 8bsh^2 = 16bsh^2$</p>
<ul>
<li><strong>Up Projection</strong>：输入 <code>(b, s, h)</code> 与权重矩阵 <code>(h, 4h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times h \times 4h = 8bsh^2$.</li>
<li><strong>Down Projection</strong>：中间结果 <code>(b, s, 4h)</code> 与权重矩阵 <code>(4h, h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times 4h \times h = 8bsh^2$.</li>
</ul>
</li>
</ul>
<hr>
<p>激活占用方面除了 Dropout Mask 是 INT8 类型以外，假设 activations 均以 16-bit float 类型保存。表中的 activation memory 均以字节为单位进行统计。和权重梯度无关的部分只有 dropout 相关的以及 Softmax output.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Category</th>
          <th style="text-align: center">Item</th>
          <th style="text-align: center">Original</th>
          <th style="text-align: center">TP</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><strong>Attention</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$11sbh + 5as^2b$</strong></td>
          <td style="text-align: center"><strong>$3sbh + \frac{8sbh}{t} + \frac{5as^2b}{t}$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">QKV input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">QK^T</td>
          <td style="text-align: center">$4sbh$</td>
          <td style="text-align: center">$\frac{4sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Softmax output</td>
          <td style="text-align: center">$2as^2b$</td>
          <td style="text-align: center">$\frac{2as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout mask</td>
          <td style="text-align: center">$as^2b$</td>
          <td style="text-align: center">$\frac{as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout output</td>
          <td style="text-align: center">$2as^2b$</td>
          <td style="text-align: center">$\frac{2as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">V</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$\frac{2sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear projection input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$\frac{2sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Attention dropout mask</td>
          <td style="text-align: center">$sbh$</td>
          <td style="text-align: center">$sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>MLP</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$19sbh$</strong></td>
          <td style="text-align: center"><strong>$3sbh + \frac{16sbh}{t}$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear1 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">GeLU input</td>
          <td style="text-align: center">$8sbh$</td>
          <td style="text-align: center">$\frac{8sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear2 input</td>
          <td style="text-align: center">$8sbh$</td>
          <td style="text-align: center">$\frac{8sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout mask</td>
          <td style="text-align: center">$sbh$</td>
          <td style="text-align: center">$sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>LayerNorm</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$4sbh$</strong></td>
          <td style="text-align: center"><strong>$4sbh$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">LayerNorm1 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">LayerNorm2 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
  </tbody>
</table>
<hr>
<p>在没有 $T_F = T_B = T_W$ 假设的情况下，ZB-H1 和 ZB-H2 的峰值激活内存和气泡大小如 Table 2 所示。值得注意的是，对于设备 <em>i</em>，其在 ZB-H1 方案下的激活内存为 $(p-i+1)M_B + (i-1)M_W$，在 ZB-H2 方案下的激活内存为 $(2p - 2i + 1)M_B + (2i - 2)M_W$。如 Table 1 所示，<em>W</em> 所需的激活内存小于 <em>B</em> 所需的激活内存。因此，ZB-H1 和 ZB-H2 的峰值激活内存分别为 $pM_B$ 和 $(2p-1)M_B$。</p>
<p><em>Table 2: Comparison between 1F1B and our handcrafted schedules.</em></p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Schedule</th>
          <th style="text-align: center">Bubble size</th>
          <th style="text-align: center">Peak activations memory</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">1F1B</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}+T_{W})$</td>
          <td style="text-align: center">$pM_{B}$</td>
      </tr>
      <tr>
          <td style="text-align: center">ZB-H1</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}-T_{W})$</td>
          <td style="text-align: center">$pM_{B}$</td>
      </tr>
      <tr>
          <td style="text-align: center">ZB-H2</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}-2T_{W})$</td>
          <td style="text-align: center">$(2p-1)M_{B}$</td>
      </tr>
  </tbody>
</table>
<p><strong>Automatic Pipeline Scheduling</strong></p>
<p>手工调度依赖于 F、B、W 的执行时间相等的理想情况。为了应对真实世界中复杂的执行时间和通信延迟，该文开发了一个自动化流水线调度算法。该算法通过一系列启发式策略，在一个给定的内存限制下，自动地为流水线生成一个高效的调度方案。核</p>
<ol>
<li>
<p><strong>Warm-up</strong>：</p>
<ul>
<li>在内存限制的范围内，算法会尽可能多地调度 F pass ，以最小化在第一个 B pass 开始前产生的气泡。</li>
<li>此阶段使用一个超参数来控制是否要调度一个可能会延迟后续B Pass的额外F Pass。</li>
</ul>
</li>
<li>
<p><strong>Steady State</strong>：</p>
<ul>
<li>热身阶段结束后，调度进入一个迭代模式，轮流调度一个F Pass和一个B Pass。</li>
<li>为了填充气泡，算法会伺机插入 W pass. 插入策略是：
<ul>
<li>当出现一个大于 $T_W$ (W Pass 执行时间) 的气泡时，直接插入一个W Pass.</li>
<li>当出现一个小于 $T_W$ 的气泡时，如果这个气泡会导致当前阶段的累计气泡时间成为所有阶段中最长的，那么仍然会插入一个W Pass.</li>
<li>当内存达到上限时，也会插入 W Pass 以回收和释放部分内存。</li>
</ul>
</li>
<li>通常这个启发式策略在稳态阶段会形成一个 1F-1B-1W 的调度模式。</li>
</ul>
</li>
<li>
<p><strong>Global Schedule</strong>：</p>
<ul>
<li>在整个调度过程中，算法始终保证在 F Pass 用完之前，第 i 阶段调度的 F Pass 数量至少比第 i+1 阶段多一个。</li>
<li>当这个数量差超过一时，会使用另一个超参数来决定在不产生额外气泡的前提下，是否要跳过第 i 阶段的一次F Pass调度。</li>
<li>算法会通过 grid search 来寻找这些超参数的最佳组合。</li>
</ul>
</li>
<li>
<p><strong>Final</strong>：当某个阶段的 F Pass 和 B Pass 都执行完毕后，算法会一次性逐个调度完所有剩余的 W Pass.</p>
</li>
</ol>
<hr>
<p><strong>Bypassing Optimizer Synchronization</strong></p>
<p>要实现完美的平行四边形调度，还需要解决优化器同步（Optimizer Synchronization）. 在分布式训练中，通常需要在更新模型参数前，在所有 GPU 间进行一次 All-Reduce，以进行梯度裁剪（Gradient Clipping）或检查数值稳定性 (NaN/INF). 这个同步点会强制所有设备等待，从而破坏平行四边形，重新引入气泡。</p>
<p>论文提出了 Bypassing Optimizer Synchronization，每个 GPU 在执行优化器更新步骤时，不再等待全局同步，而是基于从前一个 GPU 传来的部分 reduce 的信息进行推测性更新。该 micro-batch 完整的全局状态会在下一个迭代的 warp 阶段异步地传回。每个 GPU 在收到最终的全局状态后，会验证自己上一步的更新是否合法。如果发现不一致（例如，全局梯度范数超出了裁剪阈值），它会执行一次原地回滚（In-place Rollback），然后使用正确的全局状态重新执行优化器步骤。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB69834a68b841e1af30873b5a95a2fc90?method=download&amp;shareKey=0544362bccaefd3cad59bb0be406a145" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB69834a68b841e1af30873b5a95a2fc90?method=download&amp;shareKey=0544362bccaefd3cad59bb0be406a145" alt="The Post-validation Strategy to Replace Optimizer Synchronization">
    </a><figcaption>The Post-validation Strategy to Replace Optimizer Synchronization</figcaption></figure></p>
<h1 id="dualpipe">DualPipe<a hidden class="anchor" aria-hidden="true" href="#dualpipe">#</a></h1>
<p>DualPipe 是一种创新的双向流水线并行算法。它的核心思想是在一组设备上同时处理两个方向的数据流：一个前向流水线和一个反向流水线。使得计算和通信能够更充分地重叠，从而减少流水线气泡（即 GPU 空闲时间）.</p>
<p>与传统的 GPipe（1F1B）只有一个数据流方向不同，DualPipe 将设备对折，形成两条对称的流水线。例如，在一个有 8 个 PP ranks (GPU) 的设置中：</p>
<ul>
<li>前向流水线 (Forward Pipeline): 数据从 rank 0 -&gt; 1 -&gt; 2 -&gt; 3.</li>
<li>反向流水线 (Backward Pipeline): 同时有另一组数据从 rank 7 -&gt; 6 -&gt; 5 -&gt; 4.
Rank 3 和 Rank 4 成为两条流水线的中间节点，它们之间会交换数据。每个设备实际上会处理两个流水线阶段的模型块，一个用于前向流水线，另一个用于反向流水线。</li>
</ul>
<h2 id="initialization">Initialization<a hidden class="anchor" aria-hidden="true" href="#initialization">#</a></h2>
<ul>
<li>modules: 每个 DualPipe 实例接收一个元组，包含两个 nn.Module. <code>modules[0]</code> 用于处理前向-&gt;反向的计算，<code>modules[1]</code> 用于处理反向-&gt;前向的计算。</li>
<li>Rank 角色判断: 代码会根据当前 rank 的 ID 判断其在整个流水线中的位置（是否是第一个、最后一个、是否在后半部分、是否是中间节点）. 这个角色判断对于后续的通信和计算调度至关重要。例如 <code>is_in_second_half</code> 决定了该 rank 的 phase 0 和 phase 1 究竟对应前向流水线还是反向流水线。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DualPipe</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">modules</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 每个 rank 持有两个模型模块</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">process_group</span> <span class="ow">or</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_get_default_group</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算当前 rank 在流水线中的角色</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank_mapping</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">rank</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_last_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 判断 rank 是否在对折后的后半部分</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_in_second_half</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 判断是否为中间的 rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="core-function-step">Core Function: step<a hidden class="anchor" aria-hidden="true" href="#core-function-step">#</a></h2>
<p><a href="https://github.com/deepseek-ai/DualPipe/blob/main/dualpipe/dualpipe.py#L294">step</a> 方法是 <code>DualPipe</code> 的核心，它协调了所有 micro-batches 的计算和通信。整个过程被划分为 8 个阶段，以实现最大程度的计算-通信重叠。</p>
<p>输入处理: 只有 rank 0 和 rank N-1 会接收外部输入数据 <code>inputs</code> 和 <code>labels</code>. 这些数据被 <code>scatter</code> (<code>dualpipe/utils.py</code>) 切分成 <code>half_num_chunk</code> 个 micro-batch 。Rank 0 的输入用于前向流水线，Rank N-1 的输入用于反向流水线。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 重置状态</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_states</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将输入数据切分成 micro-batch</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">half_num_chunks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">labels</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">half_num_chunks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">input_chunks</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[])</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">([],</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">input_chunks</span> <span class="o">=</span> <span class="p">([],</span> <span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span></code></pre></div><p>接下来是 8 个核心调度阶段的，在此之前会进行一些准备工作：</p>
<ul>
<li>状态重置: <code>_reset_states()</code> 清空上一轮迭代的缓存，如输入/输出块、梯度、损失等。</li>
<li>rank 确定: 计算 <code>num_half_ranks</code>（流水线对折后的一半设备数）和 <code>half_rank</code>（当前秩在对折流水线中的位置. 这些变量将决定每个阶段的循环次数。</li>
<li>数据分发: <code>scatter</code> 函数将输入数据 inputs 和 labels 切分成 half_num_chunks 个 micro-batch 。根据 is_first_rank 或 is_last_rank，将这些 micro-batch 存放到 self.input_chunks 和 self.labels 中。</li>
</ul>
<p>调度示意图如下图所示，红色线分隔了每个步骤</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" alt="DualPipe Schedule">
    </a><figcaption>DualPipe Schedule</figcaption></figure></p>
<p><strong>Step 1: Warm-up Forward nF0</strong></p>
<p>这是一个纯前向计算阶段，用于填满流水线。距离流水线中点越远的 rank（half_rank 越小）执行的预热步骤越多。 <code>_forward_chunk(0)</code> 被调用，在此函数内部:</p>
<ol>
<li><code>_recv_forward(0)</code>: 尝试接收前一个 rank 的数据。对于 rank 0 来说，它直接使用 self.input_chunks 的数据，不接收。</li>
<li><code>_commit_and_wait_comm()</code>: 等待数据接收完成。</li>
<li><code>_forward_compute_chunk(0)</code>: 执行 <code>self.module[0]</code> 的前向计算。</li>
<li><code>_send_forward(0)</code>: 将计算结果异步地发送给下一个 rank.</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">step_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<p><strong>Step 2: Dual Forward nF0F1</strong></p>
<p>两条流水线都开始执行前向计算。两条流水线都开始工作。当前 rank 不仅继续处理 phase 0 的前向计算，也开始处理从另一端（phase 1）传来的数据的前向计算。</p>
<ul>
<li><code>_forward_chunk(0, recv=False, ...)</code> 处理一个 phase 0 的 micro-batch ，但不立即接收下一个，因为前面已经调用了 <code>_recv_forward(0).</code></li>
<li><code>_forward_chunk(1, ...)</code>: 处理一个 phase 1 的 micro-batch 。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 2: nF0F1</span>
</span></span><span class="line"><span class="cl"><span class="n">step_2</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">recv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">send</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">send</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">step_2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_send_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<p><strong>Step 3: 前向-后向-权重混合阶段 (Zero Bubble) nB1W1F1</strong></p>
<p>这是 DualPipe 提高效率的关键。当一条流水线开始进行反向计算时，另一条流水线仍在进行前向计算。</p>
<ul>
<li><code>_backward_chunk(1, enable_zb=True)</code>: 执行反向计算，并启用 Zero Bubble (ZB) 优化。ZB 通过 <code>WeightGradStore</code> 将权重梯度（weight gradients）的计算（通常在反向传播中阻塞）缓存起来，推迟执行，从而让路给其他计算或通信。</li>
<li><code>_weight_chunk()</code>: 执行被推迟的权重梯度计算。</li>
<li><code>_forward_chunk(1)</code>: 同时执行另一个方向的前向计算。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 3: nB1W1F1 (Use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_3</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">recv</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 4: Main Steady State nF0B1F1B0</strong></p>
<p>这是流水线完全填满后的主循环。在一个循环迭代中，一个 rank 会执行两次计算和通信的重叠操作：一次是（前向计算 + 反向计算），另一次也是（前向计算 + 反向计算）. 这里调用 <code>_forward_backward_chunk(0, 1)</code> 和 <code>_forward_backward_chunk(1, 0)</code>. 这个函数尝试将一个方向的前向计算（F）与另一个方向的反向计算（B）打包在一起执行，实现 F&amp;B 重叠。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 4 (Main step): nF0B1F1B0</span>
</span></span><span class="line"><span class="cl"><span class="n">step_4</span> <span class="o">=</span> <span class="n">half_num_chunks</span> <span class="o">-</span> <span class="n">num_ranks</span> <span class="o">+</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># i != 0</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 5 &amp; 6: 后向-后向混合阶段 (Cooldown Backward) nB1F1B0 和 nB1B0</strong></p>
<p>当前向数据流耗尽后，流水线进入收尾阶段。这个阶段主要执行剩余的反向计算。同样，ZB 优化在后半段被启用，以减少气泡。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 5: nB1F1B0</span>
</span></span><span class="line"><span class="cl"><span class="n">step_5</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 6: nB1B0 (The second half of the chunks use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_6</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_6</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">step_6</span> <span class="o">//</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">half_rank</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="n">enable_zb</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">step_6</span> <span class="o">//</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">half_rank</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="n">enable_zb</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 7 &amp; 8: 权重更新收尾阶段 nWB0 和 nW</strong></p>
<ul>
<li>Step 7 将最后的后向计算与权重计算重叠。</li>
<li>Step 8 是纯粹的权重计算阶段，循环调用 _weight_chunk() 直到 WeightGradStore.funcs_queue 队列为空，确保所有梯度都已计算完毕。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 7: nWB0 (Use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_7</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_7</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 8: nW</span>
</span></span><span class="line"><span class="cl"><span class="n">step_8</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="n">WeightGradStore</span><span class="o">.</span><span class="n">funcs_queue</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>
</span></span></code></pre></div><h2 id="computation-communication-overlap">Computation-Communication Overlap<a hidden class="anchor" aria-hidden="true" href="#computation-communication-overlap">#</a></h2>
<p><code>_forward_backward_compute_chunk</code> 函数是实现计算重叠的关键。在理想情况下（如果模型结构支持），它可以将一个 micro-batch 的前向计算和另一个 micro-batch 的反向计算在同一个函数调用中完成。该函数在 step4 使用的<code>_forward_backward_chunk</code> 函数中被调用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_forward_backward_compute_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase0</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">phase1</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">overlapped_forward_backward</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_forward_compute_chunk</span><span class="p">(</span><span class="n">phase0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_compute_chunk</span><span class="p">(</span><span class="n">phase1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># forward &amp; backward</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs0</span><span class="p">,</span> <span class="n">loss0</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module0</span><span class="p">)</span><span class="o">.</span><span class="n">overlapped_forward_backward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">module0</span><span class="p">,</span> <span class="n">inputs0</span><span class="p">,</span> <span class="n">criterion0</span><span class="p">,</span> <span class="n">labels0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">module1</span><span class="p">,</span> <span class="n">loss1</span><span class="p">,</span> <span class="n">outputs1</span><span class="p">,</span> <span class="n">output_grads1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果模型定义了一个 <code>overlapped_forward_backward</code> (@classmethod)，DualPipe 就会调用它。在这个方法里，开发者可以自定义前向和后向计算的交错执行顺序，以达到最佳的重叠效果。DeepSeek-v3 的重叠方法在技术报告里已经讲解。</p>
<h1 id="real-case">Real Case<a hidden class="anchor" aria-hidden="true" href="#real-case">#</a></h1>
<p>通过 <code>examples/example_dualpipe.py </code>中的 main 函数来详细讲解一个完整的 DualPipe 流程。</p>
<ol>
<li>环境初始化和配置</li>
</ol>
<ul>
<li>分布式设置: main 函数首先初始化 PyTorch 的分布式通信组（init_process_group），并为每个进程（rank）分配一个 GPU.</li>
<li>参数配置: 定义了 micro-batch 数量 (num_chunks)、每个 micro-batch 的大小 (micro_batch_size) 等超参数。</li>
<li>P2P通信设置: 在执行 DualPipe 的 step 方法前，必须调用 <code>set_p2p_tensor_shapes</code> 和 <code>set_p2p_tensor_dtype</code> 来告知 DualPipe 在流水线中传递的张量的形状和数据类型。这是因为 DualPipe 需要预先分配内存来接收来自其他 rank 的数据。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">pp_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 判断当前进程的角色</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_first_rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_last_rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 初始化分布式环境</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s2">&#34;env://&#34;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">pp_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">set_default_device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">233</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;CUBLAS_WORKSPACE_CONFIG&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;:4096:8&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 定义流水线参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks</span> <span class="o">=</span> <span class="mi">20</span>
</span></span><span class="line"><span class="cl">    <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">256</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">512</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">pp_size</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">num_chunks</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">hidden_size</span><span class="si">=}</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 设置P2P通信的Tensor形状和类型</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_p2p_tensor_shapes</span><span class="p">([(</span><span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_p2p_tensor_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>模型和参考基准的创建</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 创建一个完整的、未分割的模型</span>
</span></span><span class="line"><span class="cl"><span class="n">full_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pp_size</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建完整的输入数据</span>
</span></span><span class="line"><span class="cl"><span class="n">full_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_chunks</span> <span class="o">*</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">full_l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_chunks</span> <span class="o">*</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 参考步骤：在一个GPU上，用标准的数据并行方式运行完整模型，得到基准结果</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_ref</span><span class="p">,</span> <span class="n">output_ref</span> <span class="o">=</span> <span class="n">ref_step</span><span class="p">(</span><span class="n">full_x</span><span class="p">,</span> <span class="n">full_l</span><span class="p">,</span> <span class="n">full_modules</span><span class="p">,</span> <span class="n">num_chunks</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>创建模型: 代码首先创建了一个完整的 <code>nn.Sequential</code> 模型 (full_modules)，它包含了流水线所有的阶段。</li>
<li>参考步骤 (ref_step): 为了验证 DualPipe 的正确性，<code>ref_step</code> 函数模拟了标准的、非流水线并行的训练过程。它将数据分块，依次通过完整模型计算损失和输出。<code>loss_ref</code> 和 <code>output_ref</code> 将作为后续比较的正确答案。</li>
</ul>
<ol start="3">
<li>DualPipe模型的创建和输入准备</li>
</ol>
<ul>
<li>模型分割: 每个 rank r 会持有两个 PipelineStage: 一个是 <code>full_modules[r]</code>，另一个是 <code>full_modules[pp_size - 1 - r]</code>. 这就是 Dual (双向) 的体现。例如，在一个 4-GPU 的设置中：
<ul>
<li>Rank 0 持有 stage 0 和 stage 3 的模型。</li>
<li>Rank 1 持有 stage 1 和 stage 2 的模型。</li>
<li>Rank 2 持有 stage 2 和 stage 1 的模型。</li>
<li>Rank 3 持有 stage 3 和 stage 0 的模型。</li>
</ul>
</li>
<li>输入数据分割: DualPipe 有两个数据入口点：rank 0 和最后一个 rank.
<ul>
<li>rank 0 接收前半部分的输入 (<code>full_x.chunk(2)[0]</code>) 和 后半部分 的标签 (<code>full_l.chunk(2)[1]</code>).</li>
<li>last rank 接收后半部分的输入 (<code>full_x.chunk(2)[1]</code>) 和 前半部分 的标签 (<code>full_l.chunk(2)[0]</code>).</li>
</ul>
</li>
</ul>
<p>一共有两个数据流: 一个从 rank 0 开始，其对应的标签在最后一个 rank；另一个从最后一个 rank 开始，其对应的标签在 rank 0.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># DualPipe 模型创建</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 每个 rank 获取两个处于对称位置的模型块</span>
</span></span><span class="line"><span class="cl"><span class="n">local_full_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">full_modules</span><span class="p">[</span><span class="n">rank</span><span class="p">],</span> <span class="n">full_modules</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">local_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">),</span> <span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ... 加载权重 ...</span>
</span></span><span class="line"><span class="cl"><span class="n">dualpipe_model</span> <span class="o">=</span> <span class="n">DualPipe</span><span class="p">(</span><span class="n">local_modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># DualPipe输入数据准备</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">full_x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="n">full_l</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">elif</span> <span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">full_x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="n">full_l</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="kc">None</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>执行训练步骤</li>
</ol>
<p>调用 <code>dualpipe_model.step</code>，触发了前面讲解中提到的复杂的8阶段调度流程。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">dualpipe_model</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_chunks</span><span class="o">=</span><span class="n">num_chunks</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">(</span><span class="n">l</span><span class="p">,),</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><ol start="5">
<li>结果验证</li>
</ol>
<p>检查损失</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_ref</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">elif</span> <span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_ref</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span>
</span></span></code></pre></div><p>训练步骤完成后，step 方法会返回计算出的损失。</p>
<ul>
<li>rank0 计算出的 loss 对应的是从 last rank 输入的数据流，等于参考损失的后半部分 (<code>loss_ref.chunk(2)[1]</code>).</li>
<li>同理，last rank 计算出的 loss 对应的是从 rank0 输入的数据流，等于参考损失的前半部分 (<code>loss_ref.chunk(2)[0]</code>).</li>
<li>中间的 ranks 不计算最终损失，返回 None.</li>
</ul>
<p>检查梯度</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">local_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">local_modules</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">p0all</span><span class="p">,</span> <span class="n">p0</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">p1all</span><span class="p">,</span> <span class="n">p1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 手动聚合对称rank的梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">p0</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">p1all</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">p1</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">p0all</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">p_ref</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">local_modules</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(),</span> <span class="n">local_full_modules</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">cal_diff</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">p_ref</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-13</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>由于每个 rank r 持有 r 和 <code>pp_size - 1 - r</code> 两个阶段的模型，如果这两个阶段在逻辑上是同一个权重（例如，在Encoder-Decoder结构中共享权重），那么它们的梯度需要手动聚合。示例通过 <code>dist.all_gather_into_tensor</code> 收集所有 rank 上对称模块的梯度，然后手动将它们相加。最后，将聚合后的梯度与 ref_step 中计算出的参考梯度进行比较，验证反向传播的正确性。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/deepseek/">DeepSeek</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/">
    <span class="title">Next »</span>
    <br>
    <span>DeepSeek-V3 Technical Report</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
