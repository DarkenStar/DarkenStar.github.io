<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>TX8 Backend | WITHER</title>
<meta name="keywords" content="tx8">
<meta name="description" content="TX8 backend description.">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/tx8_backend/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/tx8_backend/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/tx8_backend/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="TX8 Backend">
  <meta property="og:description" content="TX8 backend description.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-07-23T11:49:02+08:00">
    <meta property="article:modified_time" content="2025-09-19T09:20:48+08:00">
    <meta property="article:tag" content="Tx8">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TX8 Backend">
<meta name="twitter:description" content="TX8 backend description.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "TX8 Backend",
      "item": "http://localhost:1313/blogs/tx8_backend/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "TX8 Backend",
  "name": "TX8 Backend",
  "description": "TX8 backend description.",
  "keywords": [
    "tx8"
  ],
  "articleBody": "TX8 Hardware Overview TX8 采用的是空间计算型结构 (Special Computing Architecture)，市面上普遍采用的共享内存结构 (Shared Memory Architecture)，它的数据通信交互主要是依赖于 DDR，一个 thread 把 DDR 的数据改变之后，另外一个 thread 再从 DDR 中才能得知到这个数据已经被改变。这么做有一个很明显的缺陷，就是它瓶颈在于内存容量以及访问内存的带宽延迟。空间计算型的结构它是由中间的NOC (Network On Chip) 来构成模块之间的互联。这样很好的避免了这个 DDR 的瓶颈，同时也有了更好的 scale out 能力。\n(a) Shared Memory Architecture (b) Spatial Computing Architecture\n下图为 TX8 两个芯片互连的逻辑结构。每个芯片由 4x4 总计 16 Tile 以 mesh 拓扑结构进行互连。每一个 Tile 是一个计算核心，是一个图灵完备 (Turing Complete) 的系统，既具有调度控制以及计算通信以及存储的能力。片上 NoC 采用的是 stream (一种轻量级 DMA 技术). 片上 DDR 大小为 64GB，芯片之间是通过 high speed IO 进行互连的。\nTile\n单芯片与单卡 A100 性能对比如下表所示\nTX8 单卡性能 最大组网性能 A100 单卡性能 INT8 256T 1E 624T BF16 128T 0.5E 312T TFP32 128T 0.5E 156T FP32 21T 40P 19.5T 内存带宽 200GB/s - 显存带宽 1935GB/s PCIe 64GB/s - 64GB/s 内存容量 64GB 128TB 显存容量 80GB TsingMicro-Link 1600Gbps - NV-Link 600GB/s Single Tile 下图是单 Tile 的硬件结构，实际上每个 Tile 上会有两个 kernel core 和 special core，图中只画了一个。还有个 neural core，主要是负责计算以及数据搬运等等。\nTile Microarchitecture\nkernel core 主要用于下发指令。它会从 DDR 中取址，然后送到这个 neural core 的 NCC controller 里面。NCC controller 又会把根据这个指令的类型下发到 CT/NE/LSU. 他们三个是执行不同种类指令的三个小模块，后面会讲到。这三个小模块会从 SPM (Scratched Pad Memory) 上读取数据，然后再计算，或者再存回 SPM上。值得注意的是，LSU 是用来负责这个数据搬运的，所以它可以把这个 SPM 上的数据直接搬到DDR，或者是从 DDR 搬到 SPM 上。CT 和 NE 都是负责计算的模块，其中 scalar unit 位于 NCC controller，是一个负责标量计算的模块。\nspecial core 用来和 NOC 进行连接，它可以从 DDR 中读取数据，然后通过配置 DTE 模块和这个远程的 Tile 进行通信。DTE 模块也可以通过 special core 将本 Tile 上的 SPM 与远程 Tile 上的 SPM 进行通信。\nCGRA Tensor CGRA Tensor 模块支持算术运算，逻辑运算，位操作，激活函数，超越函数，规约，池化，数据搬移，格式转换，辅助计算。\nCGRA\nNeural Core Controller 下发指令到 CTRL_UNIT，然后 CTRL_UNIT 下发指令到 RAM_ACC_UNIT. RAM_ACC_UNIT 读入 SPM 的数据，然后送入 Pipe Unit 进行运算之后把结果存回 SPM.\nCGRA 指令格式如下。例如 CGRATensor_ArithOp_V_V_abs，指令操作指的是对向量元素求绝对值。\n指令格式 CGRATensor_function_format_name.type Function 描述该单元的主要功能，如算数运算、关系运算、逻辑运算等； Format 描述数据的存储方式，如VV、VS、Tensor、VuV 分别表示向量与向量计算、向量与标量计算、Tensor计算、向量与单元向量计算； Name 描述具体的操作，如加、减、乘、除等； Type 表示数据类型，如 bf16/fp32 等； 下面具体讲一下在 BN 算子开发中用到的 CGRATensor_ArithOp_V_VuV_mul_loop (bf16 *src, bf16 *dst, bf16 *unit, int rnd, int src_elem_num, int unit_elem_num, int full_src_elem_num, int full_unit_elem_num).\nsrc/dst/unit 分别表示 也是原数据/存数/单元向量的地址。 src_elem_num 是做一次这个 VuV 中原数据的个数。 unit_elem_num 是做一次这个 VuV 中单元向量数据的个数。 在讲 VuV_mul_loop 之前，先来看一下这个 VuV_mul 也就是没有循环的单次版本。分为两次进行，第一次是前四个蓝色的方块与橙色方块相乘，第二次为后四个蓝色方块与橙色方块相乘。VuV_mul_loop 即把这个过程重复很多次，所以要求 full_src_elem_num/full_unit_elem_num == src_elem_num/unit_elem_num，并且unit_elem_num=64.\nVuV_mul_loop\nTensor Layout layout 可以分为以下几种\nlayout_str: 中端使用 CNN Op: 1. Feature (NCHW/NHWC) etc. 2. Weight (OIHW/HWOI) etc. Non-CNN Op: 大模型中常见，Tensor/NTensor，它们的区别是第 0 维是否为 1. mem_layout: 后端使用，代表了在芯片上的实际排布 Tensor/NTensor: 数据的紧密排布 Cx/NCx: 对 Tensor/NTensor 格式化后的结果，方便易硬件读取。 dtype channel description bf16/fp16 /fp32/tf32 c \u003c= 32 NHWC, C向4/8/16/32对齐，N 的起始地址向 2048bit 对齐 c \u003e 32 N[CxHW64, HWC0], C0 向 4/8/16/32 对齐，N 的起始地址向2048bit 对齐\n在一个 batch 内将 tensor 按 C 分成 Cx*64 和 C0两部分 int8 c \u003c= 64 NHWC, C 向 4/8/16/32/64对齐，N的起始地址向2048bit对齐 c \u003e 64 N[CxHW128, HWC0], C0 向 4/8/16/32/64 对齐，N的起始地址向 2048bit 对齐 在一个 batch 内将 tensor 按 C 分成 Cx*128 和C0 两部分 对于 fp16 的 2x1x2x131 的数据，NTensor 格式存储起始地址为 0x0000 按各存储格式排列如下\nNTensor Layout\nNCx: 131 = 64 x 2 + 3, 将 C 分成 2(Cx) 个 64 和 4(C0). batch0 的结束地址是 0x1080 (4224), batch1 起始地址需对齐到 2048bit，即 4224–\u003e2048*3=6144 (0x1800).\nNCx Layout\nNeural Engine Neural engine 类似于 GPU Tensor Core，主要是完成各种矩阵 (op_Gemm) 和卷积 (op_Conv) 类型的高效并行 Tensor 计算。PE Array 它的进行矩阵运算的部分，一次完成 8x16x8 大小的矩阵乘法。然后它的输入有激活 input，还有 psum，还有 weight，也就是权重。\n计算之后，还饿可以进行后处理，对这个结果进行 BN/量化/激活等等，然后再到输出，然后我们要用到neural engine 的算子其实并不多，只有 op_Gemm 和 op_Conv.\nNeural Engine\nLSU LSU 是负责数据搬运的 DMA 控制器。具体它有三部分:\nRDMA: Read DDR –\u003e SPM，对应指令有 op_loadVar，op_loadConst，op_rdmaGather. WDMA: Write SPM –\u003e DDR，对应指令有 op_dma_store，op_wdmaScatter. TDMA: 对所属 Tile SPM 上的数据进行操作，对应指令有 op_reshape，op_gatherScatter. LSU\n一种经常使用 TDMA 的情况是进行低精度到高精度的转换。以 fp16 -\u003e fp32 为例，首先会调用 op_gatherScatter 指令把紧密排布的低精度数据读进来然后 scatter 到 SPM 上的对应位置以保留空间存储转换后的数据；然后再调用 CGAR convert_fp16_fp32 指令进行精度转换。\nfp16 to fp32 Conversion\nTX8 Compiler 和一般编译器差不多，先获取前端的 Tensorflow/Pytorch 等等生成的 mhlo 计算图，经过中端的处理，然后转到后端。变成后端 IR. 同时又会调用 OPLIB 算子库中的算子来生成 main.c，就是可以直接放在不同平台上运行的主程序。平台可以选择 RISCV 即真实的硬件，或者是 Cmodel 进行模拟。\nBEIR 主要是接过中端传进来的 IR，然后进行各类的图优化的 Pass，包括一些算子切分，还有内存调度等等。最终 codegen 这个可编译执行的 main.c 的文件。然后再放在平台上去编译完再运行。\nTX8 Compiler Workflow\nTX8 BE 后端 IR 使用的是 MLIR，继承 Dialect，定义了许多 Operations, Attributes, Types.\ndef Tx8be_Dialect : Dialect { let name = \"tx8be\"; let summary = \"A low-level dialect for tx8 backend specification\"; let cppNamespace = \"::tx8be_mir::tx8be\"; let useDefaultAttributePrinterParser = 1; } Attribute 下面介绍一些常用的 Attribute.\nparallel_attr 主要是表示 tensor 每个维度上数据并行和张量并行的切分策略。\ndef Tx8be_ParallelAttr : Tx8be_Attr\u003c\"Parallel\", \"parallel_attr\"\u003e { let summary = \"Structure of parallel information.\"; let parameters = (ins \"ParallelModeAttr\" : $parallel, \"bool\" : $is_dp_inner, // dp dimension is in the inner, otherwise tp \"i32\" : $dp_dim_x, // data parallel dimension at x axis \"i32\" : $dp_dim_y, // data parallel dimension at y axis \"i32\" : $dp_dim_z, // data parallel dimension at z axis \"i32\" : $tp_dim_x, // tensor parallel dimension at x axis \"i32\" : $tp_dim_y, // tensor parallel dimension at y axis \"i32\" : $tp_dim_z, // tensor parallel dimension at z axis \"bool\" : $sharding_is_given, // true: is given, false: is not \"::mlir::DenseI32ArrayAttr\" : $shape_spatial_sharding // Shape split info ); let cppNamespace = \"::tx8be_mir::tx8be\"; let assemblyFormat = \"`\u003c` struct($params) 1\"; } dev_attr 属性包含\nimm_size，也就是用到的这个辅助空间的大小。 mem_layout 也就是数据的存储数据的排布。 multi_buf_en 指是否使用 double buffer. out_shape_buf_idx 指的是输出使用第几个缓冲区。 temporal_mem_slice 是单个 Tile 每次处理的数据大小。 def Tx8be_DevAttr : Tx8be_Attr\u003c\"Dev\", \"dev_attr\"\u003e { let summary = \"Structure of op parameters on device.\"; let parameters = (ins \"uint64_t\" : $imm_size, // Output memory addr offset \"LayoutModeAttr\" : $mem_layout, // Layout \"bool\" : $multi_buf_en, // for double buffering \"int32_t\" : $multi_buf_num, // for double buffering \"mlir::DenseI64ArrayAttr\" : $out_shape_buf_idx, // index for dynamic shape buffer on runtime \"mlir::DenseI64ArrayAttr\" : $temporal_mem_slice, // for compute local buffer size \"int32_t\" : $source_type, // Software pipeline stage \"int64_t\" : $imm_addr, \"mlir::DenseI64ArrayAttr\" : $mem_addr // use array for multibuffer ); let cppNamespace = \"::tx8be_mir::tx8be\"; let assemblyFormat = \"`\u003c` struct($params) `\u003e`\"; } MemScopeMode 用于描述数据存储在哪里。\ndef Tx8be_MemScopeMode : I32EnumAttr\u003c\"MemScopeMode\", \"Specify the memory scope\", [ I32EnumAttrCase\u003c\"DDR\", 0\u003e, I32EnumAttrCase\u003c\"SPM\", 1\u003e, I32EnumAttrCase\u003c\"3DDRAM\", 2\u003e ]\u003e { let genSpecializedAttr = 0; let cppNamespace = \"::tx8be_mir::tx8be\"; } Types 定义了很多类型，实际上常用的就是 AnyTensorOrNone.\ndef AnyTensorOrNone: AnyTypeOf\u003c[AnyRankedTensor, NoneType]\u003e; def Tx8be_Tuple : NestedTupleOf\u003c[AnyRankedTensor]\u003e; def AnyTensorOrTuple : AnyTypeOf\u003c[AnyRankedTensor, Tx8be_Tuple]\u003e; def Tx8be_Pred : TypeAlias\u003cI1, \"pred (AKA boolean or 1-bit integer)\"\u003e; def Tx8be_PredTensor : TensorOf\u003c[Tx8be_Pred]\u003e; def Tx8be_Token : Type\u003cCPred \"{$_self-\u003eisa()}\", \"token\"\u003e; def Tx8be_TensorOrTokenOrTuple : AnyTypeOf\u003c[AnyTensor, Tx8be_Token, Tx8be_Tuple]\u003e; def Tx8be_SInt : SignlessIntOfWidths\u003c[4, 8, 16, 32, 64]\u003e; def Tx8be_UInt : UnsignedIntOfWidths\u003c[4, 8, 16, 32, 64]\u003e; def Tx8be_Int : AnyTypeOf\u003c[Tx8be_SInt, Tx8be_UInt]\u003e; Operations 以开发的 BatchNorm_InferenceOp 为例讲解一下 Tx8be 中关于算子的定义。首先 batchnorm 是将通道维度视作样本，计算其他维度的平均值和方差后进行归一化的操作。\n$$ \\begin{aligned} BatchNorm\\colon y\u0026=\\gamma\\:\\frac{x-Mean(x)}{\\sqrt{Var(x)+\\varepsilon}}+\\beta\\\\ Mean(x)\u0026=\\frac{1}{N}\\sum_{i=1}^{N}x_{i}\\\\ Var(x)\u0026=\\frac{1}{N}\\sum_{i=1}^{N}(x_{i}-Mean(x))^{2}\\end{aligned}$$中括号内是一些需要继承的 Interface. 其允许 attributes, operations 和 types 公开方法调用接口，而不需要调用者知道特定的派生类型。\narguments 指定了算子需要的输入，包括参数以及之前介绍到的一些属性。\ndef Tx8be_BatchNorm_InferenceOp : Tx8be_Op\u003c\"BatchNorm_Inference\", [DeclareOpInterfaceMethods\u003coplibinterface\u003e, DeclareOpInterfaceMethods\u003cShardingInterface\u003e, DeclareOpInterfaceMethods\u003cComputeInterface\u003e] { let summary = \"BatchNorm inference\"; let description = [{ Normalizes the operand tensor across all dimensions except for the c dimension and produce a result tensor. }]; let arguments = (ins AnyTensor:$input, AnyTensor:$scale, AnyTensor:$offset, AnyTensor:$mean, AnyTensor:$variance, DefaultValueOptionalStrAttr\u003cStrAttr, \"Unknown\"\u003e:$layout_str, // The following are backend parameters OptionalAttr\u003cTx8be_ParallelAttr\u003e:$chip_parallel, OptionalAttr\u003cTx8be_ParallelAttr\u003e:$tile_parallel, OptionalAttr\u003cTx8be_DevAttr\u003e:$dev_info ); let results = (outs AnyTensor:$output); } Interface Interface 定义一些通用的方法或行为，这些方法没有具体实现。要通过继承某个 Interface 来具体实现该接口的方法和行为。tx8中定义了 5 个 Interface: OpLibInterface, ComputeInterface, ShapeInferenceOpInterface, ShardingInterface, StreamConfigInterface.\nBatchNorm 算子开发中只用到了前四个，下面依次介绍一下。\nShapeInferenceOpInterface 定义了两个方法 inferShapes 和 inferLayout. 继承这个接口的话就需要实现这两种方法。根主要是根据输入来推断输出的形状和布局。\ndef ShapeInferenceOpInterface : OpInterface\u003c\"ShapeInferenceOpInterface\"\u003e { let description = [{ }]; let cppNamespace = \"::tx8be_mlir\"; let methods = [ InterfaceMethod\u003c [{ }], /*retType=*/\"mlir::LogicalResult\", /*methodName=*/\"inferShapes\", // method name /*args=*/(ins \"DynamicShapeParam\" : $shapeParam) \u003e, InterfaceMethod\u003c [{ }], /*retType=*/\"mlir::LogicalResult\", /*methodName=*/\"inferLayout\", // method name /*args=*/(ins) \u003e ]; } 由于 batchnorm 不对这两者进行改变，因此输出和输入相同。如果是需要改变的算子比如 transpose 就需要进行改变。\ninput_data ",
  "wordCount" : "21991",
  "inLanguage": "en",
  "datePublished": "2025-07-23T11:49:02+08:00",
  "dateModified": "2025-09-19T09:20:48+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/tx8_backend/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      TX8 Backend
    </h1>
    <div class="post-description">
      TX8 backend description.
    </div>
    <div class="post-meta"><span title='2025-07-23 11:49:02 +0800 CST'>Jul-23-2025</span>&nbsp;·&nbsp;44 min&nbsp;·&nbsp;21991 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#tx8-hardware-overview" aria-label="TX8 Hardware Overview">TX8 Hardware Overview</a></li>
                    <li>
                        <a href="#single-tile" aria-label="Single Tile">Single Tile</a></li>
                    <li>
                        <a href="#cgra-tensor" aria-label="CGRA Tensor">CGRA Tensor</a></li>
                    <li>
                        <a href="#tensor-layout" aria-label="Tensor Layout">Tensor Layout</a></li>
                    <li>
                        <a href="#neural-engine" aria-label="Neural Engine">Neural Engine</a></li>
                    <li>
                        <a href="#lsu" aria-label="LSU">LSU</a></li>
                    <li>
                        <a href="#tx8-compiler" aria-label="TX8 Compiler">TX8 Compiler</a></li>
                    <li>
                        <a href="#tx8-be" aria-label="TX8 BE">TX8 BE</a><ul>
                            
                    <li>
                        <a href="#attribute" aria-label="Attribute">Attribute</a></li>
                    <li>
                        <a href="#types" aria-label="Types">Types</a></li>
                    <li>
                        <a href="#operations" aria-label="Operations">Operations</a></li>
                    <li>
                        <a href="#interface" aria-label="Interface">Interface</a></li>
                    <li>
                        <a href="#shardinginterface" aria-label="ShardingInterface">ShardingInterface</a></li>
                    <li>
                        <a href="#oplibinterface" aria-label="OpLibInterface">OpLibInterface</a></li>
                    <li>
                        <a href="#computeinteface" aria-label="ComputeInteface">ComputeInteface</a></li></ul>
                    </li>
                    <li>
                        <a href="#test-case" aria-label="Test Case">Test Case</a></li>
                    <li>
                        <a href="#overview-of-workflow" aria-label="Overview of Workflow">Overview of Workflow</a></li>
                    <li>
                        <a href="#layout-initialization-and-pass" aria-label="Layout Initialization and Pass.">Layout Initialization and Pass.</a><ul>
                            
                    <li>
                        <a href="#layoutinitpass" aria-label="layoutInitPass">layoutInitPass</a></li>
                    <li>
                        <a href="#layouttransmitpass" aria-label="layoutTransmitPass">layoutTransmitPass</a></li>
                    <li>
                        <a href="#layoutaligntonpupass" aria-label="layoutAlignToNpuPass">layoutAlignToNpuPass</a></li></ul>
                    </li>
                    <li>
                        <a href="#const-management" aria-label="Const Management">Const Management</a><ul>
                            
                    <li>
                        <a href="#moveconstantpass" aria-label="MoveConstantPass">MoveConstantPass</a></li>
                    <li>
                        <a href="#constnormpass" aria-label="constNormPass">constNormPass</a></li></ul>
                    </li>
                    <li>
                        <a href="#sharding-search-and-spm-management" aria-label="Sharding Search and SPM Management">Sharding Search and SPM Management</a><ul>
                            
                    <li>
                        <a href="#grouppatternpass" aria-label="GroupPatternPass">GroupPatternPass</a></li>
                    <li>
                        <a href="#groupoptimizationpass" aria-label="GroupOptimizationPass">GroupOptimizationPass</a></li>
                    <li>
                        <a href="#groupldstpass" aria-label="GroupLdStPass">GroupLdStPass</a></li>
                    <li>
                        <a href="#groupmappingpass" aria-label="GroupMappingPass">GroupMappingPass</a></li>
                    <li>
                        <a href="#groupcostpass" aria-label="GroupCostPass">GroupCostPass</a><ul>
                            
                    <li>
                        <a href="#datasplitnewpass" aria-label="DataSplitNewPass">DataSplitNewPass</a></li>
                    <li>
                        <a href="#ts_swpipelinepass" aria-label="TS_SwPipelinePass">TS_SwPipelinePass</a></li>
                    <li>
                        <a href="#spmallocpass" aria-label="SPMAllocPass">SPMAllocPass</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#compile-option-1-opt_barrier" aria-label="Compile Option 1: opt_barrier">Compile Option 1: opt_barrier</a></li>
                    <li>
                        <a href="#compile-option-1-opt_ddr" aria-label="Compile Option 1: opt_ddr">Compile Option 1: opt_ddr</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="tx8-hardware-overview">TX8 Hardware Overview<a hidden class="anchor" aria-hidden="true" href="#tx8-hardware-overview">#</a></h1>
<p>TX8 采用的是空间计算型结构 (Special Computing Architecture)，市面上普遍采用的共享内存结构 (Shared Memory Architecture)，它的数据通信交互主要是依赖于 DDR，一个 thread 把 DDR 的数据改变之后，另外一个 thread 再从 DDR 中才能得知到这个数据已经被改变。这么做有一个很明显的缺陷，就是它瓶颈在于内存容量以及访问内存的带宽延迟。空间计算型的结构它是由中间的NOC (Network On Chip) 来构成模块之间的互联。这样很好的避免了这个 DDR 的瓶颈，同时也有了更好的 scale out 能力。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB04f094c9d80d3990221020a51ce93433?method=download&amp;shareKey=f6149480beae12e20d31f124e425ef84" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB04f094c9d80d3990221020a51ce93433?method=download&amp;shareKey=f6149480beae12e20d31f124e425ef84" alt="(a) Shared Memory Architecture (b) Spatial Computing Architecture">
    </a><figcaption>(a) Shared Memory Architecture (b) Spatial Computing Architecture</figcaption></figure></p>
<p>下图为 TX8 两个芯片互连的逻辑结构。每个芯片由 4x4 总计 16 Tile 以 mesh 拓扑结构进行互连。每一个 Tile 是一个计算核心，是一个图灵完备 (Turing Complete) 的系统，既具有调度控制以及计算通信以及存储的能力。片上 NoC 采用的是 stream (一种轻量级 DMA 技术). 片上 DDR 大小为 64GB，芯片之间是通过 high speed IO 进行互连的。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB98d1d8b5916da28e40cf3c77b63dcbe6?method=download&amp;shareKey=1ca36bb999bfe3f524fc2525e1cb0ab7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB98d1d8b5916da28e40cf3c77b63dcbe6?method=download&amp;shareKey=1ca36bb999bfe3f524fc2525e1cb0ab7" alt="Tile">
    </a><figcaption>Tile</figcaption></figure></p>
<p>单芯片与单卡 A100 性能对比如下表所示</p>
<table>
  <thead>
      <tr>
          <th>TX8</th>
          <th>单卡性能</th>
          <th>最大组网性能</th>
          <th>A100</th>
          <th>单卡性能</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>INT8</strong></td>
          <td>256T</td>
          <td>1E</td>
          <td></td>
          <td>624T</td>
      </tr>
      <tr>
          <td><strong>BF16</strong></td>
          <td>128T</td>
          <td>0.5E</td>
          <td></td>
          <td>312T</td>
      </tr>
      <tr>
          <td><strong>TFP32</strong></td>
          <td>128T</td>
          <td>0.5E</td>
          <td></td>
          <td>156T</td>
      </tr>
      <tr>
          <td><strong>FP32</strong></td>
          <td>21T</td>
          <td>40P</td>
          <td></td>
          <td>19.5T</td>
      </tr>
      <tr>
          <td><strong>内存带宽</strong></td>
          <td>200GB/s</td>
          <td>-</td>
          <td><strong>显存带宽</strong></td>
          <td>1935GB/s</td>
      </tr>
      <tr>
          <td><strong>PCIe</strong></td>
          <td>64GB/s</td>
          <td>-</td>
          <td></td>
          <td>64GB/s</td>
      </tr>
      <tr>
          <td><strong>内存容量</strong></td>
          <td>64GB</td>
          <td>128TB</td>
          <td><strong>显存容量</strong></td>
          <td>80GB</td>
      </tr>
      <tr>
          <td><strong>TsingMicro-Link</strong></td>
          <td>1600Gbps</td>
          <td>-</td>
          <td><strong>NV-Link</strong></td>
          <td>600GB/s</td>
      </tr>
  </tbody>
</table>
<h1 id="single-tile">Single Tile<a hidden class="anchor" aria-hidden="true" href="#single-tile">#</a></h1>
<p>下图是单 Tile 的硬件结构，实际上每个 Tile 上会有两个 kernel core 和 special core，图中只画了一个。还有个 neural core，主要是负责计算以及数据搬运等等。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8e08cce47b2de4d373c8dd667a988463?method=download&amp;shareKey=ba34989999d6febf20e7abeda09a81b2" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8e08cce47b2de4d373c8dd667a988463?method=download&amp;shareKey=ba34989999d6febf20e7abeda09a81b2" alt="Tile Microarchitecture">
    </a><figcaption>Tile Microarchitecture</figcaption></figure></p>
<ul>
<li>
<p>kernel core 主要用于下发指令。它会从 DDR 中取址，然后送到这个 neural core 的 NCC controller 里面。NCC controller 又会把根据这个指令的类型下发到 CT/NE/LSU. 他们三个是执行不同种类指令的三个小模块，后面会讲到。这三个小模块会从 SPM (Scratched Pad Memory) 上读取数据，然后再计算，或者再存回 SPM上。值得注意的是，LSU 是用来负责这个数据搬运的，所以它可以把这个 SPM 上的数据直接搬到DDR，或者是从 DDR 搬到 SPM 上。CT 和 NE 都是负责计算的模块，其中 scalar unit 位于 NCC controller，是一个负责标量计算的模块。</p>
</li>
<li>
<p>special core 用来和 NOC 进行连接，它可以从 DDR 中读取数据，然后通过配置 DTE 模块和这个远程的 Tile 进行通信。DTE 模块也可以通过 special core 将本 Tile 上的 SPM 与远程 Tile 上的 SPM 进行通信。</p>
</li>
</ul>
<h1 id="cgra-tensor">CGRA Tensor<a hidden class="anchor" aria-hidden="true" href="#cgra-tensor">#</a></h1>
<p>CGRA Tensor 模块支持算术运算，逻辑运算，位操作，激活函数，超越函数，规约，池化，数据搬移，格式转换，辅助计算。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9086e7454a92ac6077331ed5c7f4fc56?method=download&amp;shareKey=834faad50224b52bd4c82ccf738f5293" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9086e7454a92ac6077331ed5c7f4fc56?method=download&amp;shareKey=834faad50224b52bd4c82ccf738f5293" alt="CGRA">
    </a><figcaption>CGRA</figcaption></figure></p>
<p>Neural Core Controller 下发指令到 CTRL_UNIT，然后 CTRL_UNIT 下发指令到 RAM_ACC_UNIT. RAM_ACC_UNIT 读入 SPM 的数据，然后送入 Pipe Unit 进行运算之后把结果存回 SPM.</p>
<p>CGRA 指令格式如下。例如 CGRATensor_ArithOp_V_V_abs，指令操作指的是对向量元素求绝对值。</p>
<table>
  <thead>
      <tr>
          <th>指令格式</th>
          <th>CGRATensor_function_format_name.type</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Function</strong></td>
          <td>描述该单元的主要功能，如算数运算、关系运算、逻辑运算等；</td>
      </tr>
      <tr>
          <td><strong>Format</strong></td>
          <td>描述数据的存储方式，如VV、VS、Tensor、VuV 分别表示</br>向量与向量计算、向量与标量计算、Tensor计算、向量与单元向量计算；</td>
      </tr>
      <tr>
          <td><strong>Name</strong></td>
          <td>描述具体的操作，如加、减、乘、除等；</td>
      </tr>
      <tr>
          <td><strong>Type</strong></td>
          <td>表示数据类型，如 bf16/fp32 等；</td>
      </tr>
  </tbody>
</table>
<p>下面具体讲一下在 BN 算子开发中用到的 <code>CGRATensor_ArithOp_V_VuV_mul_loop (bf16 *src, bf16 *dst, bf16 *unit, int rnd, int src_elem_num, int unit_elem_num, int full_src_elem_num, int  full_unit_elem_num)</code>.</p>
<ul>
<li>src/dst/unit 分别表示 也是原数据/存数/单元向量的地址。</li>
<li>src_elem_num 是做一次这个 VuV 中原数据的个数。</li>
<li>unit_elem_num 是做一次这个 VuV 中单元向量数据的个数。</li>
</ul>
<p>在讲 VuV_mul_loop 之前，先来看一下这个 VuV_mul 也就是没有循环的单次版本。分为两次进行，第一次是前四个蓝色的方块与橙色方块相乘，第二次为后四个蓝色方块与橙色方块相乘。VuV_mul_loop 即把这个过程重复很多次，所以要求 <code>full_src_elem_num/full_unit_elem_num == src_elem_num/unit_elem_num</code>，并且<code>unit_elem_num=64</code>.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB367ed5ab2178c80abdeb160cb55b409d?method=download&amp;shareKey=05e534d23de8ab1be6cef33b8c8e0e4e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB367ed5ab2178c80abdeb160cb55b409d?method=download&amp;shareKey=05e534d23de8ab1be6cef33b8c8e0e4e" alt="VuV_mul_loop">
    </a><figcaption>VuV_mul_loop</figcaption></figure></p>
<h1 id="tensor-layout">Tensor Layout<a hidden class="anchor" aria-hidden="true" href="#tensor-layout">#</a></h1>
<p>layout 可以分为以下几种</p>
<ul>
<li>layout_str: 中端使用
<ul>
<li>CNN Op: 1. Feature (NCHW/NHWC) etc. 2. Weight (OIHW/HWOI) etc.</li>
<li>Non-CNN Op: 大模型中常见，Tensor/NTensor，它们的区别是第 0 维是否为 1.</li>
</ul>
</li>
<li>mem_layout: 后端使用，代表了在芯片上的实际排布
<ul>
<li>Tensor/NTensor: 数据的紧密排布</li>
<li>Cx/NCx: 对 Tensor/NTensor 格式化后的结果，方便易硬件读取。</li>
</ul>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>dtype</th>
          <th>channel</th>
          <th>description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>bf16/fp16 <br>/fp32/tf32</td>
          <td>c &lt;= 32</td>
          <td>NHWC, C向4/8/16/32对齐，N 的起始地址向 2048bit 对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 32</td>
          <td>N[CxHW64, HWC0], C0 向 4/8/16/32 对齐，N 的起始地址向2048bit 对齐<br>在一个 batch 内将 tensor 按 C 分成 Cx*64 和 C0两部分</td>
      </tr>
      <tr>
          <td>int8</td>
          <td>c &lt;= 64</td>
          <td>NHWC, C 向 4/8/16/32/64对齐，N的起始地址向2048bit对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 64</td>
          <td>N[CxHW128, HWC0], C0 向 4/8/16/32/64 对齐，N的起始地址向 2048bit 对齐 <br> 在一个 batch 内将 tensor 按 C 分成 Cx*128 和C0 两部分</td>
      </tr>
  </tbody>
</table>
<p>对于 fp16 的 2x1x2x131 的数据，NTensor 格式存储起始地址为 0x0000 按各存储格式排列如下</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB47732820f52aa7a0bfd0d22bb2e61e00?method=download&amp;shareKey=03c9e8f103abf576755a3e33e5d5cbc5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB47732820f52aa7a0bfd0d22bb2e61e00?method=download&amp;shareKey=03c9e8f103abf576755a3e33e5d5cbc5" alt="NTensor Layout">
    </a><figcaption>NTensor Layout</figcaption></figure></p>
<p>NCx: 131 = 64 x 2 + 3, 将 C 分成 2(Cx) 个 64 和 4(C0). batch0 的结束地址是 0x1080 (4224), batch1 起始地址需对齐到 2048bit，即 4224&ndash;&gt;2048*3=6144 (0x1800).</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB53406947a6dafa2bbe98df922f46a954?method=download&amp;shareKey=3e75616e0238e7360bd6930b98941909" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB53406947a6dafa2bbe98df922f46a954?method=download&amp;shareKey=3e75616e0238e7360bd6930b98941909" alt="NCx Layout">
    </a><figcaption>NCx Layout</figcaption></figure></p>
<h1 id="neural-engine">Neural Engine<a hidden class="anchor" aria-hidden="true" href="#neural-engine">#</a></h1>
<p>Neural engine 类似于 GPU Tensor Core，主要是完成各种矩阵 (op_Gemm) 和卷积 (op_Conv) 类型的高效并行 Tensor 计算。PE Array 它的进行矩阵运算的部分，一次完成 8x16x8 大小的矩阵乘法。然后它的输入有激活 input，还有 psum，还有 weight，也就是权重。</p>
<p>计算之后，还饿可以进行后处理，对这个结果进行 BN/量化/激活等等，然后再到输出，然后我们要用到neural engine 的算子其实并不多，只有 op_Gemm 和 op_Conv.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBf07a7034750a8723bf35f5cb311251e2?method=download&amp;shareKey=d0c242fdf504441bb4284c72b48908c4" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBf07a7034750a8723bf35f5cb311251e2?method=download&amp;shareKey=d0c242fdf504441bb4284c72b48908c4" alt="Neural Engine">
    </a><figcaption>Neural Engine</figcaption></figure></p>
<h1 id="lsu">LSU<a hidden class="anchor" aria-hidden="true" href="#lsu">#</a></h1>
<p>LSU 是负责数据搬运的 DMA 控制器。具体它有三部分:</p>
<ul>
<li>RDMA: Read DDR &ndash;&gt; SPM，对应指令有 op_loadVar，op_loadConst，op_rdmaGather.</li>
<li>WDMA: Write SPM &ndash;&gt; DDR，对应指令有 op_dma_store，op_wdmaScatter.</li>
<li>TDMA: 对所属 Tile SPM 上的数据进行操作，对应指令有 op_reshape，op_gatherScatter.</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd7f94b1349b939c2c20a37abf5e57bbc?method=download&amp;shareKey=fb7dd9facaf7ca29424c72eaa4991000" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd7f94b1349b939c2c20a37abf5e57bbc?method=download&amp;shareKey=fb7dd9facaf7ca29424c72eaa4991000" alt="LSU">
    </a><figcaption>LSU</figcaption></figure></p>
<p>一种经常使用 TDMA 的情况是进行低精度到高精度的转换。以 fp16 -&gt; fp32 为例，首先会调用 op_gatherScatter 指令把紧密排布的低精度数据读进来然后 scatter 到 SPM 上的对应位置以保留空间存储转换后的数据；然后再调用 CGAR convert_fp16_fp32 指令进行精度转换。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB1079912e78f03abc7a30d0db12ffb046?method=download&amp;shareKey=20f9d8abd3fb47b3194540d639a2f9ee" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB1079912e78f03abc7a30d0db12ffb046?method=download&amp;shareKey=20f9d8abd3fb47b3194540d639a2f9ee" alt="fp16 to fp32 Conversion">
    </a><figcaption>fp16 to fp32 Conversion</figcaption></figure></p>
<h1 id="tx8-compiler">TX8 Compiler<a hidden class="anchor" aria-hidden="true" href="#tx8-compiler">#</a></h1>
<p>和一般编译器差不多，先获取前端的 Tensorflow/Pytorch 等等生成的 mhlo 计算图，经过中端的处理，然后转到后端。变成后端 IR. 同时又会调用 OPLIB 算子库中的算子来生成 main.c，就是可以直接放在不同平台上运行的主程序。平台可以选择 RISCV 即真实的硬件，或者是 Cmodel 进行模拟。</p>
<p>BEIR 主要是接过中端传进来的 IR，然后进行各类的图优化的 Pass，包括一些算子切分，还有内存调度等等。最终 codegen 这个可编译执行的 main.c 的文件。然后再放在平台上去编译完再运行。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB95d5755abd98d22b6bcdaca440c0e8c4?method=download&amp;shareKey=e72432cabeea4cbab293db667ecb0648" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB95d5755abd98d22b6bcdaca440c0e8c4?method=download&amp;shareKey=e72432cabeea4cbab293db667ecb0648" alt="TX8 Compiler Workflow">
    </a><figcaption>TX8 Compiler Workflow</figcaption></figure></p>
<h1 id="tx8-be">TX8 BE<a hidden class="anchor" aria-hidden="true" href="#tx8-be">#</a></h1>
<p>后端 IR 使用的是 MLIR，继承 Dialect，定义了许多 Operations, Attributes, Types.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_Dialect <span class="p">:</span> Dialect <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">name =</span> <span class="s">&#34;tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;A low-level dialect for tx8 backend specification&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">useDefaultAttributePrinterParser =</span> <span class="m">1</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="attribute">Attribute<a hidden class="anchor" aria-hidden="true" href="#attribute">#</a></h2>
<p>下面介绍一些常用的 Attribute.</p>
<p><code>parallel_attr</code> 主要是表示 tensor 每个维度上数据并行和张量并行的切分策略。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_ParallelAttr <span class="p">:</span> Tx8be_Attr<span class="p">&lt;</span><span class="s">&#34;Parallel&#34;</span><span class="p">,</span> <span class="s">&#34;parallel_attr&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;Structure of parallel information.&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">parameters =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        <span class="s">&#34;ParallelModeAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>is_dp_inner<span class="p">,</span>    <span class="c">// dp dimension is in the inner, otherwise tp
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_x<span class="p">,</span>    <span class="c">// data parallel dimension at x axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_y<span class="p">,</span>    <span class="c">// data parallel dimension at y axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_z<span class="p">,</span>    <span class="c">// data parallel dimension at z axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_x<span class="p">,</span>    <span class="c">// tensor parallel dimension at x axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_y<span class="p">,</span>    <span class="c">// tensor parallel dimension at y axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_z<span class="p">,</span>    <span class="c">// tensor parallel dimension at z axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>sharding_is_given<span class="p">,</span>    <span class="c">// true: is given, false: is not
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;::mlir::DenseI32ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>shape_spatial_sharding    <span class="c">// Shape split info
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">assemblyFormat =</span> <span class="s">&#34;`&lt;` struct($params) 1&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>dev_attr</code> 属性包含</p>
<ul>
<li>imm_size，也就是用到的这个辅助空间的大小。</li>
<li>mem_layout 也就是数据的存储数据的排布。</li>
<li>multi_buf_en 指是否使用 double buffer.</li>
<li>out_shape_buf_idx 指的是输出使用第几个缓冲区。</li>
<li>temporal_mem_slice 是单个 Tile 每次处理的数据大小。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_DevAttr <span class="p">:</span> Tx8be_Attr<span class="p">&lt;</span><span class="s">&#34;Dev&#34;</span><span class="p">,</span> <span class="s">&#34;dev_attr&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;Structure of op parameters on device.&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">parameters =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        <span class="s">&#34;uint64_t&#34;</span> <span class="p">:</span> <span class="err">$</span>imm_size<span class="p">,</span>    <span class="c">// Output memory addr offset
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;LayoutModeAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>mem_layout<span class="p">,</span>    <span class="c">// Layout
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>multi_buf_en<span class="p">,</span>    <span class="c">// for double buffering
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int32_t&#34;</span> <span class="p">:</span> <span class="err">$</span>multi_buf_num<span class="p">,</span>    <span class="c">// for double buffering
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>out_shape_buf_idx<span class="p">,</span>    <span class="c">// index for dynamic shape buffer on runtime
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>temporal_mem_slice<span class="p">,</span>    <span class="c">// for compute local buffer size
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int32_t&#34;</span> <span class="p">:</span> <span class="err">$</span>source_type<span class="p">,</span>    <span class="c">// Software pipeline stage
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int64_t&#34;</span> <span class="p">:</span> <span class="err">$</span>imm_addr<span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>mem_addr    <span class="c">// use array for multibuffer
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">assemblyFormat =</span> <span class="s">&#34;`&lt;` struct($params) `&gt;`&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>MemScopeMode</code> 用于描述数据存储在哪里。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_MemScopeMode <span class="p">:</span> I32EnumAttr<span class="p">&lt;</span><span class="s">&#34;MemScopeMode&#34;</span><span class="p">,</span> <span class="s">&#34;Specify the memory scope&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="p">[</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;DDR&#34;</span><span class="p">,</span> <span class="m">0</span><span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;SPM&#34;</span><span class="p">,</span> <span class="m">1</span><span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;3DDRAM&#34;</span><span class="p">,</span> <span class="m">2</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        let <span class="nl">genSpecializedAttr =</span> <span class="m">0</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">        let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></div><h2 id="types">Types<a hidden class="anchor" aria-hidden="true" href="#types">#</a></h2>
<p>定义了很多类型，实际上常用的就是 AnyTensorOrNone.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def AnyTensorOrNone<span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">,</span> NoneType<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Tuple <span class="p">:</span> NestedTupleOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def AnyTensorOrTuple <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">,</span> Tx8be_Tuple<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Pred <span class="p">:</span> TypeAlias<span class="p">&lt;</span>I1<span class="p">,</span> <span class="s">&#34;pred (AKA boolean or 1-bit integer)&#34;</span><span class="p">&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_PredTensor <span class="p">:</span> TensorOf<span class="p">&lt;[</span>Tx8be_Pred<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Token <span class="p">:</span> Type<span class="p">&lt;</span>CPred <span class="s">&#34;{$_self-&gt;isa&lt;TokenType&gt;()}&#34;</span><span class="p">,</span> <span class="s">&#34;token&#34;</span><span class="p">&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_TensorOrTokenOrTuple <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyTensor<span class="p">,</span> Tx8be_Token<span class="p">,</span> Tx8be_Tuple<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_SInt <span class="p">:</span> SignlessIntOfWidths<span class="p">&lt;[</span><span class="m">4</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="m">16</span><span class="p">,</span> <span class="m">32</span><span class="p">,</span> <span class="m">64</span><span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_UInt <span class="p">:</span> UnsignedIntOfWidths<span class="p">&lt;[</span><span class="m">4</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="m">16</span><span class="p">,</span> <span class="m">32</span><span class="p">,</span> <span class="m">64</span><span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Int <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>Tx8be_SInt<span class="p">,</span> Tx8be_UInt<span class="p">]&gt;</span><span class="err">;</span>
</span></span></code></pre></div><h2 id="operations">Operations<a hidden class="anchor" aria-hidden="true" href="#operations">#</a></h2>
<p>以开发的 BatchNorm_InferenceOp 为例讲解一下 Tx8be 中关于算子的定义。首先 batchnorm 是将通道维度视作样本，计算其他维度的平均值和方差后进行归一化的操作。</p>
$$
\begin{aligned}
BatchNorm\colon y&=\gamma\:\frac{x-Mean(x)}{\sqrt{Var(x)+\varepsilon}}+\beta\\
Mean(x)&=\frac{1}{N}\sum_{i=1}^{N}x_{i}\\
Var(x)&=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-Mean(x))^{2}\end{aligned}$$<p>中括号内是一些需要继承的 <a href="https://mlir.llvm.org/docs/Interfaces/">Interface</a>. 其允许 attributes, operations 和 types 公开方法调用接口，而不需要调用者知道特定的派生类型。</p>
<p>arguments 指定了算子需要的输入，包括参数以及之前介绍到的一些属性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_BatchNorm_InferenceOp <span class="p">:</span> Tx8be_Op<span class="p">&lt;</span><span class="s">&#34;BatchNorm_Inference&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span>DeclareOpInterfaceMethods<span class="p">&lt;</span>oplibinterface<span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">    DeclareOpInterfaceMethods<span class="p">&lt;</span>ShardingInterface<span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">    DeclareOpInterfaceMethods<span class="p">&lt;</span>ComputeInterface<span class="p">&gt;]</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;BatchNorm inference&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">        Normalizes the operand <span class="kt">tensor</span> across all dimensions except for the c dimension
</span></span><span class="line"><span class="cl">        and produce a result <span class="kt">tensor</span><span class="p">.</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">arguments =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>input<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>scale<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>offset<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>mean<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>variance<span class="p">,</span>
</span></span><span class="line"><span class="cl">        DefaultValueOptionalStrAttr<span class="p">&lt;</span>StrAttr<span class="p">,</span> <span class="s">&#34;Unknown&#34;</span><span class="p">&gt;:</span><span class="err">$</span>layout_str<span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="c">// The following are backend parameters
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        OptionalAttr<span class="p">&lt;</span>Tx8be_ParallelAttr<span class="p">&gt;:</span><span class="err">$</span>chip_parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        OptionalAttr<span class="p">&lt;</span>Tx8be_ParallelAttr<span class="p">&gt;:</span><span class="err">$</span>tile_parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        OptionalAttr<span class="p">&lt;</span>Tx8be_DevAttr<span class="p">&gt;:</span><span class="err">$</span>dev_info
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">results =</span> <span class="p">(</span>outs AnyTensor<span class="p">:</span><span class="err">$</span>output<span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="interface">Interface<a hidden class="anchor" aria-hidden="true" href="#interface">#</a></h2>
<p>Interface 定义一些通用的方法或行为，这些方法没有具体实现。要通过继承某个 Interface 来具体实现该接口的方法和行为。tx8中定义了 5 个 Interface: OpLibInterface, ComputeInterface, ShapeInferenceOpInterface, ShardingInterface, StreamConfigInterface.</p>
<p>BatchNorm 算子开发中只用到了前四个，下面依次介绍一下。</p>
<p><code>ShapeInferenceOpInterface</code> 定义了两个方法 <code>inferShapes</code> 和 <code>inferLayout</code>. 继承这个接口的话就需要实现这两种方法。根主要是根据输入来推断输出的形状和布局。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ShapeInferenceOpInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ShapeInferenceOpInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="p">[{</span> <span class="p">}],</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;inferShapes&#34;</span><span class="p">,</span>  <span class="c">// method name
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;DynamicShapeParam&#34;</span> <span class="p">:</span> <span class="err">$</span>shapeParam<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="p">[{</span> <span class="p">}],</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;inferLayout&#34;</span><span class="p">,</span>  <span class="c">// method name
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>由于 batchnorm 不对这两者进行改变，因此输出和输入相同。如果是需要改变的算子比如 transpose 就需要进行改变。</p>
<p><code>input_data &lt;shape=3x4x5x6, layout=NCHW&gt; --&gt; transpose&lt;permutation=(0,2,3,1)&gt; --&gt; output_data&lt;shape=3x5x6x4, layout=NHWC&gt;</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// BatchNorm_Interface.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">inferLayout</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">in_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">getInput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">cur_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">in_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">i_layout</span> <span class="o">=</span> <span class="n">in_op</span><span class="o">-&gt;</span><span class="n">getAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">).</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">StringAttr</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getValue</span><span class="p">().</span><span class="n">str</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">StringAttr</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">i_layout</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">in_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;dev_info&#34;</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">i_dev_layout</span> <span class="o">=</span> <span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">in_op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">cur_op</span><span class="p">,</span> <span class="n">i_dev_layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">inferShapes</span><span class="p">(</span><span class="n">DynamicShapeParam</span> <span class="o">&amp;</span><span class="n">shapeParam</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">getOutput</span><span class="p">().</span><span class="n">setType</span><span class="p">(</span><span class="n">getInput</span><span class="p">().</span><span class="n">getType</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="shardinginterface">ShardingInterface<a hidden class="anchor" aria-hidden="true" href="#shardinginterface">#</a></h2>
<p><code>tileShardingSplit</code> 和前面的 <code>inferShapes</code> 以及 <code>inferLayout</code> 不一样。后两者是从输入信息推出输出的信息。而 <code>tileShardingSplit</code> 是由输出的的切分的因子来推断出各个输入的切分因子。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8d4fed659394922243574186cf74ef3a?method=download&amp;shareKey=d1461507273efadf9613b1496fd1501c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8d4fed659394922243574186cf74ef3a?method=download&amp;shareKey=d1461507273efadf9613b1496fd1501c" alt="BatchNorm ShardingInterface">
    </a><figcaption>BatchNorm ShardingInterface</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ShardingInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ShardingInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*</span><span class="err">/</span><span class="p">[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="c">// vector for diff operand&#39;s info
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::ShardingSplitParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;tileShardingSplit&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;ShardingSplitParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::SliceParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;temporalSliceShape&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;SliceParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::WindowParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;backWindow&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;const WindowParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ul>
<li>Sharding 是空间上的切分，意思是将数据分散到不同的 Tile 上。</li>
<li>Split 是时间上的切分，意思是切分到 Tile 上的将数据按流水线方式轮流进行 load.</li>
</ul>
<p><code>temporalSliceShape</code> 返回的是 sharding + split 后一个 Tile 上单次处理的数据的实际 shape.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB4495c3579079b37d0c2288cc51408601?method=download&amp;shareKey=0513d93d3b51c4782d56c38acb83d0d5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB4495c3579079b37d0c2288cc51408601?method=download&amp;shareKey=0513d93d3b51c4782d56c38acb83d0d5" alt="BatchNorm Sharding Split">
    </a><figcaption>BatchNorm Sharding Split</figcaption></figure>
根据 batchnorm 算子定义 input 只能在通道维度上 sharding.
split 有两种选择</p>
<ol>
<li>对于 input 和 mean，var，scale，shift 都在 C 维度上做相同的切分。</li>
<li>不再 split mean，var，scale，shift，只对 input 的 NHW 进行 split.</li>
</ol>
<p>这里采用的是后者。由于 mean, variance, scale, shift 都是 1x1x1xC 的张量，因此 split 为 (1, 1, 1, 1). 切分搜索得到的符合要求的 ShardingSplitParam (下图中为 cn3) 会继续向上传递。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBc472f9ed0d922130ec05d93efed54186?method=download&amp;shareKey=8bf08e0631cd4c7880b9756969fd4bef" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBc472f9ed0d922130ec05d93efed54186?method=download&amp;shareKey=8bf08e0631cd4c7880b9756969fd4bef" alt="Sharding Split Search">
    </a><figcaption>Sharding Split Search</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ShardingSplitParam</span><span class="o">&gt;</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">tileShardingSplit</span><span class="p">(</span><span class="n">ShardingSplitParam</span> <span class="o">&amp;</span><span class="n">param</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">getOutput</span><span class="p">().</span><span class="n">getType</span><span class="p">().</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">param</span><span class="p">.</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">param</span><span class="p">.</span><span class="n">outSplit</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape_size</span> <span class="o">=</span> <span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ShardingSplitParam</span><span class="o">&gt;</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">param</span><span class="p">);</span> <span class="c1">// input
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">shape_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// can only shard in dim C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">[</span><span class="n">shape_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// can only split except dim C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramMean</span><span class="p">;</span> <span class="c1">// scale/shift/mean/variance
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">paramMean</span><span class="p">.</span><span class="n">outSharding</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">paramMean</span><span class="p">.</span><span class="n">outSplit</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">shape_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// shape is 1x1x1xC，split must be (1, 1, 1, 1)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramVar</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramScale</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramShift</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramScale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramShift</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramMean</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramVar</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="oplibinterface">OpLibInterface<a hidden class="anchor" aria-hidden="true" href="#oplibinterface">#</a></h2>
<p><code>OpLibInterface</code> 有四个方法，</p>
<ul>
<li><code>genOpCode</code>: 生成 main.c 文件的时候所调用的一个接口。</li>
<li><code>getOpClockCycle</code>: 获取 OP 的执行时间。</li>
<li><code>getImmSpSize</code>: 获取 SPM 上临时空间所需要的大小。</li>
<li><code>queryOpAttr</code>: 查询这个 OP 的一些属性。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">def</span> <span class="nl">OpLibInterface</span> <span class="p">:</span> <span class="n">OpInterface</span><span class="o">&lt;</span><span class="s">&#34;OpLibInterface&#34;</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">description</span> <span class="o">=</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">        <span class="n">These</span> <span class="n">are</span> <span class="n">the</span> <span class="n">interfaces</span> <span class="k">for</span> <span class="n">connecting</span> <span class="n">tx8be</span><span class="o">-</span><span class="n">oplib</span>
</span></span><span class="line"><span class="cl">        <span class="n">and</span> <span class="n">codegen</span><span class="p">.</span>
</span></span><span class="line"><span class="cl">    <span class="p">}];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">cppNamespace</span> <span class="o">=</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">methods</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">generate</span> <span class="n">the</span> <span class="n">code</span> <span class="n">of</span> <span class="n">op</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;std::string&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;genOpCode&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span> <span class="s">&#34;OpCodeParam&#34;</span> <span class="o">:</span> <span class="err">$</span><span class="n">param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">clock</span> <span class="n">cycle</span> <span class="n">of</span> <span class="n">the</span> <span class="n">op</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;uint64_t&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;getOpClockCycle&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">the</span> <span class="n">immediate</span> <span class="n">SPM</span> <span class="n">buffer</span> <span class="n">size</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;uint32_t&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s">&#34;getImmSpSize&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">the</span> <span class="n">opAttr</span> <span class="n">info</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;tx8be_mlr::opAttr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;queryOpAttr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>其中 <code>queryOpAttr</code> 接口只需要在对应的接口里给 OpAttr 里的参数赋值。</p>
<ul>
<li><code>alignMode</code>: 算子的对齐要求，有Cx对齐要求，NCx 对齐要求，或者不在意存储格式的。</li>
<li><code>defaultLayout</code>: 算子默认的排布。</li>
<li><code>needPresetToNPU</code>: OP 是否需要进行预设到和硬件匹配的 layout. 当算子用到的指令是带有 NHWC 的配置时候的需要。</li>
<li><code>memInplace</code>: 输入和输出能否使用同一片内存。</li>
<li><code>needLoad</code>: 算子是否需要 load 操作，比如 mask, embedding 就不需要，会跳过loadvar op 生成。bit0 表示 arg idx0，bit1 表示 arg idx1，一共能表示 64 个输入情况。如果是const输入，loadconst 也会跳过codegen 不生成 code.
<blockquote>
<p>一个op可能有多个 input 都没有 load，shape 更新只用最后一个没有 load 的 operand (为 0 的最高位). 如 embedding 的 shape使用最后一个 operand，第一个是 weight 不用管 gshape. scatter有的有load，有的没有，shape 更新只看没有 load 的那个。</p></blockquote>
</li>
<li><code>needStore</code>:  数据是否需要进行 store 操作，会跳过store op 生成。</li>
<li><code>parallel</code>: 是否允许并行模式。</li>
<li><code>alignCx</code>: 最低维度切分是否到 64/128 (i8).</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">OpAttr</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ALIGN_MODE</span> <span class="n">alignMode</span><span class="p">{</span><span class="n">ALIGN_MODE</span><span class="o">::</span><span class="n">NPU_UNKNOWN</span><span class="p">};</span>  <span class="c1">// 算子的对齐要求，有Cx对齐要求，NCx对齐要求，或者不在意存储格式的
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">defaultLayout</span><span class="p">{</span><span class="s">&#34;Tensor&#34;</span><span class="p">};</span>           <span class="c1">// 算子默认的layout
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">needPresetToNPU</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>                   <span class="c1">// op是否需要进行预设到和硬件匹配的layout. 当算子用到的指令是带有 nhwc 的配置时需要
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">ENGINE_TYPE</span> <span class="n">engine</span><span class="p">{</span><span class="n">NPU_ENGINE_CT</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">memInplace</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>                        <span class="c1">// op的输入和输出能否使用同一片memory，比如add的out使用in0的
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint64_t</span> <span class="n">needLoad</span><span class="p">{</span><span class="mh">0xFFFFFFFFFFFFFFFF</span><span class="p">};</span>         <span class="c1">// 算子是否需要load操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint64_t</span> <span class="n">needStore</span><span class="p">{</span><span class="mh">0xFFFFFFFFFFFFFFFF</span><span class="p">};</span>        <span class="c1">// 数据是否需要进行store操作，会跳过store op生成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">parallel</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>                              <span class="c1">// 一般要使能并行模式，不过有的memory可能有问题，就不使能
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">alignCx</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>                               <span class="c1">// 最低维度切分是否到64/128(i8)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><p>batchnorm 允许输入 in 的 layout 为 Cx/NCx，要在 mlir 层的 <code>queryOpAttr()</code> 里将 alignMode 设置为NPU_ALIGN, 维度为 2/3/4，数据类型为 bf16/fp16/fp32/tf32. 其他输入的格式为 fp32. 输出的维度和类型与 in 保持一致。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">OpAttr</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">queryOpAttr</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OpAttr</span> <span class="n">attr</span><span class="p">;</span>  <span class="c1">// 创建一个 OpAttr 对象
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">attr</span><span class="p">.</span><span class="n">alignMode</span> <span class="o">=</span> <span class="n">ALIGN_MODE</span><span class="o">::</span><span class="n">NPU_ALIGN</span><span class="p">;</span>  <span class="c1">// 设置对齐模式为 NPU_ALIGN
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">attr</span><span class="p">.</span><span class="n">needPresetToNPU</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>  <span class="c1">// 设置需要预设到 NPU
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 获取 in 的形状，并判断其第一个维度是否为 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getShape</span><span class="p">()[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">attr</span><span class="p">.</span><span class="n">defaultLayout</span> <span class="o">=</span> <span class="n">batch</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">?</span> <span class="s">&#34;Tensor&#34;</span> <span class="o">:</span> <span class="s">&#34;NTensor&#34;</span><span class="p">;</span>  <span class="c1">// 根据 batch 的值设置默认布局
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">attr</span><span class="p">;</span>  
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>如下图所示，后端编译器会调用 genOpCode 生成相对应的 main.c. 然后 host.cpp 再把 main.c 放到不同的平台上面去编译完再去执行。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBdb6a91e8bb45cbce292f6fdf1fafd0f4?method=download&amp;shareKey=91e74089c887f70b2b508d9a31b877fd" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBdb6a91e8bb45cbce292f6fdf1fafd0f4?method=download&amp;shareKey=91e74089c887f70b2b508d9a31b877fd" alt="OpLibInterface">
    </a><figcaption>OpLibInterface</figcaption></figure></p>
<p>main.c 主要做的就是 load &ndash;&gt; compute &ndash;&gt; store 这三步。伪代码如下，由于进行了时间上的 split，需要循环多次才能读取完整的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">input_done</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// load
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_dma_load</span> <span class="n">Input</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">input_done</span> <span class="o">=</span> <span class="n">Input</span><span class="p">.</span><span class="n">load_finish</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">scale</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">shift</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">mean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">varience</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// compute
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_batchnorm_inference</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">varience</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// store
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_store_var_ncx</span> <span class="n">out</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>op_batchnorm_inference</code> 的定义如下，其中 imm 是辅助空间，此处申请了 2xsizeof(input) Bytes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">uint64_t</span> <span class="nf">op_batchnorm_inference</span><span class="p">(</span><span class="n">BATCHNORM_INFER_PARAM</span> <span class="o">*</span><span class="n">param</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">TSR</span> <span class="o">*</span><span class="n">in</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">shift</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">mean</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">var</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">TSR</span> <span class="o">*</span><span class="n">imm</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">out</span><span class="p">);</span>
</span></span></code></pre></div><p>其中 TSR 是一个自定义的结构体，包括数据格式，地址以及一个 L_shape (load shape). 里面记录了张量完整的大小 shape_whole，以及本 Tile 上每个维度起始下标 shape_start，每个维度加载的大小 shape_slice 和 shape 的维度大小 dim.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">L_SHAPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape_whole</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// the whole shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_start</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// start idx of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_slice</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// length of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_real</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>   <span class="c1">// real length of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">dim</span><span class="p">;</span>                         <span class="c1">// dimension of the shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="n">L_SHAPE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">G_SHAPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">spatial_start</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// [start, end]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">spatial_end</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">dynamic_offset</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">dim</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">done</span><span class="p">;</span>                         <span class="c1">// done for dma load finish
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">batch_offset</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">G_SHAPE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TSR</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Data_Format</span> <span class="n">format</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint64_t</span> <span class="n">addr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">L_SHAPE</span><span class="o">*</span> <span class="n">shape</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TSR</span><span class="p">;</span>
</span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB863424dc56bf5d86b25f817e06a1c716?method=download&amp;shareKey=de577bac9b2b5b108c4a3e8a275d07ea" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB863424dc56bf5d86b25f817e06a1c716?method=download&amp;shareKey=de577bac9b2b5b108c4a3e8a275d07ea" alt="BatchNorm Design">
    </a><figcaption>BatchNorm Design</figcaption></figure></p>
<p>对于非 fp32 类型数据 (以 fp16 为例) 计算过程与空间分配如下图所示。</p>
<ol>
<li>类型转换成 fp32: gatherScatter.</li>
<li>调用 fp16-&gt;fp32 函数进行转换。</li>
<li>循环计算 x-Mean (因为对 in 的 NHW 维度进行了 split)，结果存入 imm_a.</li>
<li>Varience 自加 epsilon(1e-6).</li>
<li>Varience 进行 rsqrt 操作。</li>
<li>Varience 与 x-Mean 进行循环乘。</li>
<li>循环乘 scale.</li>
<li>循环加 shift.</li>
<li>fp32 转回 f16.</li>
<li>gatherScatter 到 out 处。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB427f45ff2571bf94eea6a0b81f897ba1?method=download&amp;shareKey=57a2c93fd8b252f86c47c8e71e325f2c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB427f45ff2571bf94eea6a0b81f897ba1?method=download&amp;shareKey=57a2c93fd8b252f86c47c8e71e325f2c" alt="Batchnorm Computation Flow">
    </a><figcaption>Batchnorm Computation Flow</figcaption></figure></p>
<p>这里需要注意的是 shift(1, 1, 1, C) 和归一化后的 x(N, H, W, C) 相乘的时候，这时候就用到了之前所说的 VuV_mul 和 VuV_mul_loop 指令。</p>
<p>当 C &lt;= 32 时，一个 batch 内的数据排布如下 (以 (4x112x2x30) x (1x1x1x30) 为例)，此时我们在 batch 维度上循环调用 VuV_mul 指令就可以。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB7e47cbd16c9c6777c91a22a0c2685f91?method=download&amp;shareKey=c305ac14d29df963a8884def061ef96f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB7e47cbd16c9c6777c91a22a0c2685f91?method=download&amp;shareKey=c305ac14d29df963a8884def061ef96f" alt="Channel &lt;= 32">
    </a><figcaption>Channel &lt;= 32</figcaption></figure></p>
<p>当 C &gt; 32 时，需要向 64 对齐，一个 batch 内的数据排布如下 (以 (4x112x2x129) x (1x1x1x129) 为例)，每一个 Cx/C0 对应着一次 VuV_mul. 此时我们在 batch 维度上循环调用 VuV_mul_loop 指令就可以。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBfacfc3b8b6722744fa958775ab8a88f4?method=download&amp;shareKey=968a25dfbb972d2e97c7b09f284733be" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBfacfc3b8b6722744fa958775ab8a88f4?method=download&amp;shareKey=968a25dfbb972d2e97c7b09f284733be" alt="Channel &gt; 32">
    </a><figcaption>Channel &gt; 32</figcaption></figure></p>
<p>下面来说明如何调用指令，首先要明确调用的指令是属于哪一个模块的。例如第四步加 epsilon 我们需要调用 addVs 指令，其属于 CGRA 模块。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">enum</span> <span class="nc">OP_INSTR_TYPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_CGRA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_NEUR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_RDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_WDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_TDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_SCALAR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_DTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_CSR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">OP_INSTR_TYPE</span><span class="p">;</span>
</span></span></code></pre></div><p>每个模块下的指令有自己的参数形式，下面列举一些。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// I_CGRA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">CT_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_CT_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_CT_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">CT_Param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_NEUR
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TsmNeInstr</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_NE_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_NE_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TsmNeInstr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_(R/W)DMA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">DMA_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_DMA_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_DMA_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">DMA_Param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_TDMA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TD_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_TDMA_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_TDMA_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TD_Param</span><span class="p">;</span>
</span></span></code></pre></div><p>还是以 AddVS 指令为例，流程如下</p>
<ol>
<li>声明模块的指令参数。</li>
<li>声明对应的指令类型指针，AddVS 属于 arith 类型的指令。getTsmOpPointer()-&gt;arith_pointer;`.</li>
<li>根据调用指令传入参数，指令会根据传入参数配置好 ct_param 上寄存器的值。然后再进行 TsmExecute. 最后再把单词指令的执行时间进行累加。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">CT_Param</span> <span class="n">ct_param</span> <span class="o">=</span> <span class="p">{</span><span class="n">I_CGRA</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">},</span> <span class="p">{</span><span class="mi">0</span><span class="p">}};</span>  <span class="c1">// step 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">TsmArith</span> <span class="o">*</span><span class="n">arith</span> <span class="o">=</span> <span class="p">(</span><span class="n">TsmArith</span> <span class="o">*</span><span class="p">)</span><span class="n">getTsmOpPointer</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">arith_pointer</span><span class="p">;</span>  <span class="c1">// step 2
</span></span></span><span class="line"><span class="cl"><span class="c1">// variance add epsilon
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">arith</span><span class="o">-&gt;</span><span class="n">addVS</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ct_param</span><span class="p">,</span>  <span class="c1">// engine params
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">varAddr</span><span class="p">,</span>  <span class="c1">// vector address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="o">*</span><span class="p">(</span><span class="kt">uint32_t</span> <span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">epsilon</span><span class="p">),</span>  <span class="c1">// scalar address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">varAddr</span><span class="p">,</span>  <span class="c1">// result address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">mid_tensor_info</span><span class="p">.</span><span class="n">total_num</span><span class="p">,</span>  <span class="c1">// vector elements num
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">RND_NEAREST_EVEN</span><span class="p">,</span>  <span class="c1">// round method
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">Fmt_FP32</span><span class="p">);</span>  <span class="c1">// data format
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">cycle_single</span> <span class="o">=</span> <span class="n">TsmExecute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ct_param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">cycle_total</span> <span class="o">=</span> <span class="n">ADD_VALID_CYCLE</span><span class="p">(</span><span class="n">cycle_total</span><span class="p">,</span> <span class="n">cycle_single</span><span class="p">);</span>
</span></span></code></pre></div><h2 id="computeinteface">ComputeInteface<a hidden class="anchor" aria-hidden="true" href="#computeinteface">#</a></h2>
<p><code>ComputeInterface</code> 这个接口主要是每个 OP 通过 onednn 得到 CPU 代码。或者计算比较简单的 OP 如果在 onednn 的接口中没有找到对应的计算，也可以在 compute 接口中手写当前 OP 的 CPU 实现的 C++代码。最终生成结果会用来检验算子正确性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ComputeInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ComputeInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">description =</span> <span class="p">[]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*</span><span class="err">/</span><span class="p">[],</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;::mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;compute&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;ComputeParam&amp;&#34;</span><span class="p">:</span><span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">  <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h1 id="test-case">Test Case<a hidden class="anchor" aria-hidden="true" href="#test-case">#</a></h1>
<p>TestCase 主要作用是写单算子或者多个 (单算子的上下文算子) 的测试，包括固定配置测试和随机配置测试,随机配置时主要对于算子支持的不同 dim, layout, dtype, shape 这四项做随机。流程主要做以下几件事。</p>
<p><strong>init_param</strong></p>
<p>通过数组来配置固定测试 case 或者随机测试范围，然后通过指定或随机的方式生成对应的输入，输出的 shape， dim 信息，除此之外参与随机的一般还包括数据对齐方式随机，数据类型随机，即在算子可支持的范围内产生随机的 FP16/FP32 不同的数据类型来保证测试的充分和全面。</p>
<p>除此之外还会生成 MLIR Module. 这个 module 是原来就给定的，在这里做的事情是首先新建一个空的 func. 然后在这个 func 中构造一个 block，里面去填入需要测试的这些 OP 的结构。</p>
<ul>
<li>Module：一个程序的容器，包含多个函数。</li>
<li>Func：定义一个函数，包含多个 Block.</li>
<li>Block：定义函数的基本执行单元，包含多个 Operation.</li>
<li>Operation：表示具体的计算或操作，是程序中的基本指令。</li>
</ul>
<h2 id="mlir-structure">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBfdb91efc229999a9b488d5131959f4b0?method=download&amp;shareKey=a3935de13b565d4c37e06857c6c43f90" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBfdb91efc229999a9b488d5131959f4b0?method=download&amp;shareKey=a3935de13b565d4c37e06857c6c43f90" alt="MLIR Structure">
    </a><figcaption>MLIR Structure</figcaption></figure></h2>
<p>init_data</p>
<p>这个方法主要用来通过上面 Param 生成的 dim、输入或者输出 shape、数据类型来生成随机的数据，数据范围一定要根据算子情况配置，不然无效数值可能会在结果中出现 Nan. 还要考虑一些算子的特点，保证测试的充分性，例如创建 relu 的数据时，最好正负值都有覆盖。</p>
<hr>
<p>compile</p>
<p>compile 方法有两个功能</p>
<ol>
<li>调用 Computelnterface 生成 onednn 或者手写 CPU 算子实现的结果。</li>
<li>添加一些配置参数，跑出 tx8be mlir codegen 的结果。这其中会经历一些非常复杂的 pass，稍后再介绍。</li>
</ol>
<hr>
<p>saveInfoFile</p>
<p>saveInfoFile 方法主要是把创建出的 Data 数据写成.bin 文件保存。并把创建出的 module 的信息保存在 json 文件。</p>
<h1 id="overview-of-workflow">Overview of Workflow<a hidden class="anchor" aria-hidden="true" href="#overview-of-workflow">#</a></h1>
<p>后端接收的是 MLIR 的计算图，然后经过编译器后端的处理，然后生成最后的 BE IR，其中中包含了一些 Oplib 的算子。最终这个 BEIR 会调用 OP 的算子，然后去跑在 C model 或者是实际的硬件芯片上面。后端编译器主要负责四个方面
layout 初始化和传递、const 管理、切分策略及其 SPM 分配和 DDR 分配。</p>
<h1 id="layout-initialization-and-pass">Layout Initialization and Pass.<a hidden class="anchor" aria-hidden="true" href="#layout-initialization-and-pass">#</a></h1>
<p>layout 可以分为以下几种</p>
<ul>
<li>layout_str: 中端使用
<ul>
<li>CNN Op: 1. Feature (NCHW/NHWC) etc. 2. Weight (OIHW/HWOI) etc.</li>
<li>Non-CNN Op: 大模型中常见，Tensor/NTensor，它们的区别是第 0 维是否为 1.</li>
</ul>
</li>
<li>mem_layout: 后端使用，代表了在芯片上的实际排布
<ul>
<li>Tensor/NTensor: 数据的紧密排布</li>
<li>Cx/NCx: 对 Tensor/NTensor 格式化后的结果，方便易硬件读取。</li>
</ul>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>dtype</th>
          <th>channel</th>
          <th>description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>bf16/fp16 <br>/fp32/tf32</td>
          <td>c &lt;= 32</td>
          <td>NHWC, C向4/8/16/32对齐，N 的起始地址向 2048bit 对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 32</td>
          <td>N[CxHW64, HWC0], C0 向 4/8/16/32 对齐，N 的起始地址向2048bit 对齐<br>在一个 batch 内将 tensor 按 C 分成 Cx*64 和 C0两部分</td>
      </tr>
      <tr>
          <td>int8</td>
          <td>c &lt;= 64</td>
          <td>NHWC, C 向 4/8/16/32/64对齐，N的起始地址向2048bit对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 64</td>
          <td>N[CxHW128, HWC0], C0 向 4/8/16/32/64 对齐，N的起始地址向 2048bit 对齐 <br> 在一个 batch 内将 tensor 按 C 分成 Cx*128 和C0 两部分</td>
      </tr>
  </tbody>
</table>
<h2 id="layoutinitpass">layoutInitPass<a hidden class="anchor" aria-hidden="true" href="#layoutinitpass">#</a></h2>
<p>layoutInitPass 用于初始化计算图中 GemmOP 和 ConvOP 的 layout_str，其他的所有算子 layout_str 都设置为 UNKNOWN. 下图中的 <code>GemmOP layout_str = &quot;Tensor-Tensor-Tensor&quot;</code> 分别表示两个输入和输出的数据排布。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB95a6e97d5ae8432b8367189a36987f31?method=download&amp;shareKey=8df50a04c4e2ed29be9e93d3da958e35" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB95a6e97d5ae8432b8367189a36987f31?method=download&amp;shareKey=8df50a04c4e2ed29be9e93d3da958e35" alt="LayoutStr">
    </a><figcaption>LayoutStr</figcaption></figure></p>
<h2 id="layouttransmitpass">layoutTransmitPass<a hidden class="anchor" aria-hidden="true" href="#layouttransmitpass">#</a></h2>
<p>layoutTransmitPass 会用已知的 GemmOP 和 ConvOP layout 信息进行扩散，得到全图的 layout_str.</p>
<ol>
<li>每个算子初始化为一个节点，有inputNodes容器和outputNodes容器分别存放自己的输入和输出节点。</li>
<li>GemmOp 和 ConvOp 作为起始节点，向前和向后推导 layout (算子的 <code>inferlayout()</code> 接口)，新推出layout 的节点作为下一批起始节点递归推导。</li>
<li>遇到无法推导的节点 (如 Reshape，BroadCast) 则终止推导。将其余无法推导的节点 layout 直接初始化为 Tensor.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9d187e7b17a4b2ee65f01169f8c6a141?method=download&amp;shareKey=c405975eba9c1da95539f89ac8b3de8a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9d187e7b17a4b2ee65f01169f8c6a141?method=download&amp;shareKey=c405975eba9c1da95539f89ac8b3de8a" alt="layoutTransmitPass">
    </a><figcaption>layoutTransmitPass</figcaption></figure></p>
<h2 id="layoutaligntonpupass">layoutAlignToNpuPass<a hidden class="anchor" aria-hidden="true" href="#layoutaligntonpupass">#</a></h2>
<p>layoutAlignToNpuPass 用于在数据对齐冲突的地方插入 channelNorm，并将 layout_str 映射到 mem_layout. 在 NPU 上某些算子只支持 <code>COMPACT</code> layout，有些只支持 <code>ALIGN</code> layout，有些则都可以 <code>BOTH</code>.</p>
<ol>
<li>输入默认非对齐排布，从输入出发遍历整图，检查当前算子与其所有 user 之间的对齐要求，若冲突，记录插入点 (算子的对齐要求可以在 <code>OpLibInterface</code> 接口中的 <code>queryOpAttr()</code> 方法中查询到).</li>
<li>根据记录的插入点，再次分析插入点前后的算子对齐要求，以确定channelnorm的方向，插入 channelnorm.</li>
<li>赋值 <code>dev_info</code>，将 <code>layout_str</code> 映射到 <code>mem_layout</code>.</li>
</ol>
<blockquote>
<p>dev_info用来描述数据在设备上的一些属性，有成员：imm_size (辅助空间大小), mem_layout, temporal_mem_slice, imm_addr, mem_addr.</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB41f3c705e0f54e9291a5a2a7916f6045?method=download&amp;shareKey=e9af8cbf6c2e348e4584018bcb4d4782" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB41f3c705e0f54e9291a5a2a7916f6045?method=download&amp;shareKey=e9af8cbf6c2e348e4584018bcb4d4782" alt="layoutAlignToNpuPass">
    </a><figcaption>layoutAlignToNpuPass</figcaption></figure></p>
<p>LayoutAlignOptPass 应用几个 RewritePattern 用于删除冗余的 channelnorm.</p>
<ol>
<li><strong>ConstChannelNormErase</strong>: ConstantOp 维度为 1 并且只有 1 个 user 的时候可以删去并且将 devInfolayout 设置为 Cx.</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">ConstChannelNormErase Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// const can be directly considered to be aligned
</span></span></span><span class="line"><span class="cl"><span class="c1">// constop(dim &lt; 2) -&gt; channelNorm -&gt; constop
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">ConstChannelNormErase</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ConstChannelNormErase</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="mi">1</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">txbe</span><span class="o">::</span><span class="n">ConstantOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// If const has multi user, can not erase
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">hasOneUse</span><span class="p">())</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">user</span> <span class="o">=</span> <span class="o">*</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">userVec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">userVec</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">userVec</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">user</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">user</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">channelNormUser</span> <span class="p">:</span> <span class="n">userVec</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">channelNormUser</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">user</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// set align=true
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getLayoutStr</span><span class="p">().</span><span class="n">str</span><span class="p">(),</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">user</span><span class="o">-&gt;</span><span class="n">use_empty</span><span class="p">())</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<details class="custom-details">
    <summary class="custom-summary">RedudantChannelnormErase Implementation</summary>
    <div><ol start="2">
<li><strong>RedudantChannelnormErase</strong>: 如果该 channelnormOp 的输入是来自一个 constOp 并且只有一个输出，则检查是否还有其他的 channelnormOp 也使用。如果是，则让它们直接使用该 channelnormOp 的结果，以消除多余的 channelnormOp.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// A pass to erase redundant channel normalization operations
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">RedundantChannelNormErase</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">RedundantChannelNormErase</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">)</span> <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="mi">1</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Define the input operation and its defining operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// def represents the operation that generates the op input data
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">def</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getInput</span><span class="p">().</span><span class="n">getDefiningOp</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Check if the defining operation is a ConstantOp and has more than one result
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">def</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">def</span><span class="o">-&gt;</span><span class="n">getNumResults</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span> <span class="c1">// Fail if conditions are not met
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Get the size in bits of the input shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">size</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getInput</span><span class="p">().</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getSizeInBits</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Operation</span> <span class="o">*</span><span class="n">sameOp</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span> <span class="c1">// Pointer to a potentially redundant operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Iterate over all users of the defining operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">def</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">user</span> <span class="o">==</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Skip if the user is the current operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span> <span class="c1">// Check if the user is another ChannelNormOp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">sameOp</span> <span class="o">=</span> <span class="n">user</span><span class="p">;</span> <span class="c1">// Store the redundant operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">sameOp</span><span class="p">)</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span> <span class="c1">// Fail if no redundant operation is found
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Replace all uses of the redundant operation with the current operation&#39;s results
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">op</span><span class="o">-&gt;</span><span class="n">replaceAllUsesWith</span><span class="p">(</span><span class="n">sameOp</span><span class="o">-&gt;</span><span class="n">getOpResults</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">use_empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Erase the current operation if it has no more uses
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span> <span class="c1">// Return success if the rewrite is completed
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div></div>
</details><br>
<h1 id="const-management">Const Management<a hidden class="anchor" aria-hidden="true" href="#const-management">#</a></h1>
<p>常量统一使用 <code>ConstContainer</code> 类来进行管理。通过 map 来记录每个常量对应的 ParamInfo. 一个常量可能被分配到多个芯片上，每个芯片上数据可能相同，也可能不同。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ParamInfo</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;*</span> <span class="n">data_ptr</span><span class="p">;</span>  <span class="c1">// const value
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">set</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">chip_id</span><span class="p">;</span>  <span class="c1">// which chips has this const, -1 indicates all chip has the same param.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint32_t</span> <span class="n">label</span><span class="p">;</span>  <span class="c1">// Indicates whether the data is assigned to a certain chip_id. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// class ConstContainer {
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">ConstContainer</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">ConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">virtual</span> <span class="o">~</span><span class="n">ConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// some public functions
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ParamInfo</span><span class="o">&gt;&gt;</span> <span class="n">_data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="p">,</span> <span class="kt">uint64_t</span><span class="o">&gt;&gt;</span> <span class="n">oidToSize</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="kt">uint32_t</span><span class="o">&gt;</span> <span class="n">oidToNid</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="moveconstantpass">MoveConstantPass<a hidden class="anchor" aria-hidden="true" href="#moveconstantpass">#</a></h2>
<p>MoveConstantPass: 创建图的 <code>ConstContainer</code>，然后应用 <code>ConstantToLoadConst</code> Rewrite Pattern. 转换完成后会调用 <code>updateConstContainer</code> 更新 <code>ConstContainer</code> 各个 const 的 ID. 用一个大小为 <code>4*1024*tile_num</code> (DDR_BANK_SIZE) <code>thresholdSize</code> 将大于这个值的 const 全部放在前面，小的放在后面。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MoveConstantPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// create constant container
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">createConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// get module op
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ModuleOp</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="n">RewritePatternSet</span> <span class="nf">patterns</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">patterns</span><span class="p">.</span><span class="n">insert</span><span class="o">&lt;</span><span class="n">ConstantToLoadConst</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="n">FrozenRewritePatternSet</span> <span class="n">frozen_patterns</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">FrozenRewritePatternSet</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">patterns</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set config
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">GreedyRewriteConfig</span> <span class="n">config</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">config</span><span class="p">.</span><span class="n">useTopDownTraversal</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">func</span> <span class="p">:</span> <span class="n">module</span><span class="p">.</span><span class="n">getOps</span><span class="o">&lt;</span><span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span><span class="o">&gt;</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Region</span> <span class="o">&amp;</span><span class="n">body</span> <span class="o">=</span> <span class="n">func</span><span class="p">.</span><span class="n">getBody</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">failed</span><span class="p">(</span><span class="n">applyPatternsAndFoldGreedily</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">frozen_patterns</span><span class="p">,</span> <span class="n">config</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">llvm</span><span class="o">::</span><span class="n">errs</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed when move const in main graph.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">subgraph</span> <span class="p">:</span> <span class="n">module</span><span class="p">.</span><span class="n">getOps</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="o">&gt;</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Region</span> <span class="o">&amp;</span><span class="n">body</span> <span class="o">=</span> <span class="n">subgraph</span><span class="p">.</span><span class="n">getBody</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">failed</span><span class="p">(</span><span class="n">applyPatternsAndFoldGreedily</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">frozen_patterns</span><span class="p">,</span> <span class="n">config</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">llvm</span><span class="o">::</span><span class="n">errs</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed when move const in subgraph.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">TileInfo</span> <span class="n">tinfo</span> <span class="o">=</span> <span class="n">get_tileinfo</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">updateConstContainer</span><span class="p">(</span><span class="n">tinfo</span><span class="p">.</span><span class="n">tile_num</span><span class="p">);</span>  <span class="c1">// update id by thresholdSize
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">updateLdConstop</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>ConstantToLoadConst</code> 首先通过分析该常量的所有 users，来判断这个常量是否需要 LoadConstOp. 如果需要加载，它会将原始常量的数据注册到一个全局容器中并获得一个 ID，然后创建一个新的 LoadConstOp ，并将此 ID 及其他硬件属性赋予它。接着，它会更新所有使用者，将它们的输入从旧的 ConstantOp 重定向到这个新的 LoadConstOp，最后再删除无用的原始常量。最后再更新所有 const 的 ID.</p>
<details class="custom-details">
    <summary class="custom-summary">ConstantToLoadConst Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ConstantToLoadConst</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ConstantToLoadConst</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span>
</span></span><span class="line"><span class="cl">  <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span> <span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Store constant data to constant container 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Determine if this constant operation needs an explicit load instruction.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">needLoad</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">v</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Iterate over all operations that use this output value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user_op</span> <span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Get the argument index of the user op that corresponds to our output value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">int32_t</span> <span class="n">arg_idx</span> <span class="o">=</span> <span class="n">getArgumentIdx</span><span class="p">(</span><span class="n">user_op</span><span class="p">,</span> <span class="n">v</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Assert that the user operation implements our custom OpLibInterface.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">ASSERT</span><span class="p">(</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Get the library attributes for this user operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">opAttr</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">).</span><span class="n">queryOpAttr</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Skip if the user is a TupleOp, which might have special handling.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">TupleOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">opAttr</span><span class="p">.</span><span class="n">needLoad</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">arg_idx</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// Check if the &#39;needLoad&#39; attribute
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">needLoad</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">ASSERT</span><span class="p">(</span><span class="n">needLoad</span> <span class="o">==</span> <span class="nb">false</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Set attributes
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Safely iterate over the users. This is important because we are modifying the use-list inside the loop.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">use</span> <span class="p">:</span> <span class="n">llvm</span><span class="o">::</span><span class="n">make_early_inc_range</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">().</span><span class="n">getUses</span><span class="p">()))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">Operation</span> <span class="o">*</span><span class="n">userOp</span> <span class="o">=</span> <span class="n">use</span><span class="p">.</span><span class="n">getOwner</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Create the new, hardware-specific LoadConst operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">txbe</span><span class="o">::</span><span class="n">LoadConstOp</span> <span class="n">newLoadConst</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">            <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">().</span><span class="n">getType</span><span class="p">(),</span> <span class="n">ValueRange</span><span class="p">{},</span> <span class="n">attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">needLoad</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// this constant does not need an explicit load... 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Get a builder to set attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">OpBuilder</span> <span class="nf">builder</span><span class="p">(</span><span class="n">newLoadConst</span><span class="p">.</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Set a &#39;bypasscodegen&#39; attribute, signaling special handling for this op in later stages.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">newLoadConst</span><span class="p">.</span><span class="n">getOperation</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;bypasscodegen&#34;</span><span class="p">,</span> <span class="n">builder</span><span class="p">.</span><span class="n">getBoolAttr</span><span class="p">(</span><span class="nb">true</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Set the layout string attribute on the new LoadConst op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">newLoadConst</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">,</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// CRITICAL STEP: Rewire the user&#39;s operand to point to the result of the new LoadConst op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">userOp</span><span class="o">-&gt;</span><span class="n">setOperand</span><span class="p">(</span><span class="n">use</span><span class="p">.</span><span class="n">getOperandNumber</span><span class="p">(),</span> <span class="n">newLoadConst</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// After all uses have been replaced, erase the original, now-dead ConstantOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<h2 id="constnormpass">constNormPass<a hidden class="anchor" aria-hidden="true" href="#constnormpass">#</a></h2>
<p>constNormPass: 遍历图中的 LoadConstOp. 它会寻找一个特定的模式：如果一个 LoadConstOp 的唯一 user 是一个 ChannelNormOp，那么会通过 <code>constChannelNormErase</code> 函数进行消除和将对其信息同步到 LoadConstOp. 最后通过 <code>processMultiUse</code> 确保所有加载同一个底层常量数据的 LoadConstOp 实例，都具有完全相同的内存布局。</p>
<details class="custom-details">
    <summary class="custom-summary">ConstNormPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ConstNormPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ModuleOp</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">mainGraphFunc</span> <span class="o">=</span> <span class="n">getMainFuncOp</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span> <span class="o">*&gt;</span> <span class="n">deletedChannelnorm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Walk the main function to find a specific pattern: LoadConst -&gt; ChannelNorm.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">users</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">            <span class="kt">bool</span> <span class="n">flag</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Check if any user is a ChannelNormOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="n">flag</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1">// If the LoadConst has exactly one user, and that user is a ChannelNormOp,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// mark the ChannelNormOp for deletion.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">(</span><span class="n">flag</span> <span class="o">&amp;&amp;</span> <span class="n">users</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="c1">// The erase logic is commented out, maybe handled by constChannelNormErase or done later.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                    <span class="n">deletedChannelnorm</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">it</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Erase all the marked ChannelNormOps. This is done in a separate loop
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// to avoid iterator invalidation issues.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">deletedChannelnorm</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">op</span><span class="o">-&gt;</span><span class="n">erase</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Set up and run a nested pass pipeline.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">OpPassManager</span> <span class="nf">thisPM</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">getOpName</span><span class="p">().</span><span class="n">value</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// This pipeline will only apply to LoadConstOp operations inside functions.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">OpPassManager</span> <span class="o">&amp;</span><span class="n">loadConstOpPM</span> <span class="o">=</span> <span class="n">thisPM</span><span class="p">.</span><span class="n">nest</span><span class="o">&lt;</span><span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span><span class="o">&gt;</span><span class="p">().</span><span class="n">nest</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Add the ConstNormDoPass to the pipeline.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">loadConstOpPM</span><span class="p">.</span><span class="n">addPass</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">ConstNormDoPass</span><span class="o">&gt;</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Run the newly constructed pipeline on the module.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">result</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">runPipeline</span><span class="p">(</span><span class="n">thisPM</span><span class="p">,</span> <span class="n">getOperation</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// After the pipeline, run a final cleanup/consistency check function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">processMultiUse</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// change unpack input0 qweight shape after ConstNormDoPass. (Original comment)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// This logic is likely inside the runOnOperation() method of ConstNormDoPass.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="c1">// Collect all users of this LoadConstOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">users</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">          
</span></span><span class="line"><span class="cl">          <span class="c1">// Check if any user is an UnpackOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="kt">bool</span> <span class="n">flag</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">UnpackOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="n">flag</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                  <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// If there is exactly one user, and it&#39;s an UnpackOp...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="n">flag</span> <span class="o">&amp;&amp;</span> <span class="n">users</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// This check seems to ensure we are modifying the correct operand.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="k">if</span> <span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">it</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// Get the original shape and type.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="p">,</span> <span class="mi">6</span><span class="o">&gt;</span> <span class="n">oShape</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                  <span class="k">auto</span> <span class="n">type</span> <span class="o">=</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">                  <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">type</span><span class="p">.</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                  <span class="c1">// Apply the shape transformation: e.g., for unpacking packed data.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">oShape</span><span class="p">.</span><span class="n">push_back</span><span class="p">((</span><span class="kt">int32_t</span><span class="p">)</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">4</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                  <span class="n">oShape</span><span class="p">.</span><span class="n">push_back</span><span class="p">((</span><span class="kt">int32_t</span><span class="p">)</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                  <span class="c1">// Create a new tensor type with the new shape.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="k">auto</span> <span class="n">oType</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">::</span><span class="n">RankedTensorType</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="n">oShape</span><span class="p">,</span> <span class="n">type</span><span class="p">.</span><span class="n">getElementType</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// Update the type of the LoadConstOp&#39;s result in-place.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">setType</span><span class="p">(</span><span class="n">oType</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                  <span class="p">}</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>constChannelNormErase</code> 处理 LoadConstOp -&gt; ChannelNormOp 这种模式。让所有原本使用 ChannelNormOp 计算结果的操作，现在改为直接使用 ChannelNormOp 的输入数据。获取 LoadConstOp 当前的设备信息和 layout，计算出一个新的经过对齐的布局 <code>align_dev_layout</code>，然后用这个新布局去更新 LoadConstOp.</p>
<details class="custom-details">
    <summary class="custom-summary">constChannelNormErase Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// This function erases a ChannelNormOp by bypassing it and updating the source constant&#39;s layout.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">constChannelNormErase</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Find the defining operation of the ChannelNorm&#39;s operand, which should be a LoadConstOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">defOp</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast_or_null</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getDefiningOp</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// If the source is not a LoadConstOp, do nothing.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">defOp</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Collect all users of the ChannelNormOp&#39;s result.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">userVec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">userVec</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">userVec</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">userVec</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Replace all uses of the ChannelNormOp&#39;s result with the result of the LoadConstOp..
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">user</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// After bypassing, the layout of the source constant might need to be adjusted
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// to reflect the transformation that the ChannelNormOp was supposed to perform.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// set const layout to cx mode 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">dev_layout</span> <span class="o">=</span> <span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">defOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">align_dev_layout</span> <span class="o">=</span> <span class="n">get_aligned_layout</span><span class="p">((</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">dev_layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">defOp</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">defOp</span><span class="p">,</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LayoutMode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">align_dev_layout</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<p><code>processMultiUse</code> 保证所有对同一份常量数据的引用，其 mem_layout 都是完全一致的。流程如下</p>
<ol>
<li><code>processMultiUse</code> 遍历计算图中的所有 LoadConstOp，以 <code>const_map_id</code> 为 key，将所有指向同一个物理常量的 LoadConstOp 实例分组存放在一起。</li>
<li>遍历这个 map，只处理那些包含多个 LoadConstOp 实例的组 (<code>kv.second.size() &gt; 1</code>).</li>
<li>在每个组内，确定一个正确的布局。代码逻辑是以组内的第一个 LoadConstOp 的布局为基准，但如果发现组内有 <code>is_cx_layout</code>，则会采用这个优先的布局作为标准。</li>
<li>一旦确定了标准布局，会再次遍历该组内的所有 LoadConstOp 实例。调用 <code>setDevInfoWithLayout</code> 函数，强制将每一个实例的布局属性修改为刚才确定的那个标准布局。</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">processMultiUse Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// This function processes multi-use constants to ensure their layouts are consistent.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="n">ConstNormPass</span><span class="o">::</span><span class="n">processMultiUse</span><span class="p">(</span><span class="n">ModuleOp</span> <span class="n">module</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">mainGraphFunc</span> <span class="o">=</span> <span class="n">getMainFuncOp</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// When a const is used by multiple users, multiple loadconsts will be generated,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// but only one loadconst will have its layout set. The others will be skipped.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// We need to go over them uniformly. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// First, find all previous useless constant ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Group all LoadConstOp instances by their underlying constant data ID (const_map_id).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span> <span class="o">*&gt;&gt;</span> <span class="n">allconst</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">auto</span> <span class="n">cOp</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="kt">uint32_t</span> <span class="n">t_map_id</span> <span class="o">=</span> <span class="n">cOp</span><span class="p">.</span><span class="n">getConstMapId</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">      <span class="n">allconst</span><span class="p">[</span><span class="n">t_map_id</span><span class="p">].</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">constOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Based on duplication, find if the layout needs to be changed to cx. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// Check if there is also a Cx with the same layout. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Iterate over each group of LoadConstOps that share the same data.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">kv</span> <span class="p">:</span> <span class="n">allconst</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Process only if there are multiple users.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Assume the layout of the first user is the correct one.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">auto</span> <span class="n">layout</span> <span class="o">=</span> <span class="p">(</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">front</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">      <span class="c1">// This loop is for validation, checking if layouts are inconsistent.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">layout2</span> <span class="o">=</span> <span class="p">(</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">is_cx_layout</span><span class="p">(</span><span class="n">layout2</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ALIGN_NOT</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">layout</span> <span class="o">=</span> <span class="n">layout2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// Force all LoadConstOps in this group to have the same, correct layout.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="n">ASSERT</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;dev_info&#34;</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="s">&#34;Must have dev_info!&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LayoutMode</span><span class="p">)</span><span class="n">layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<h1 id="sharding-search-and-spm-management">Sharding Search and SPM Management<a hidden class="anchor" aria-hidden="true" href="#sharding-search-and-spm-management">#</a></h1>
<p>第一步是对算子进行 Group 划分，插入 load &amp; store. 对每一个 subGraph 会应用如下的 3 个 Pass:</p>
<ul>
<li><strong>GroupPatternPass</strong>：应用配置好的 group config (opt_group).</li>
<li><strong>GroupOptimizationPass</strong>: 如果没有配置，则会为每个 compute op 创建一个 group.</li>
<li><strong>GroupLdStPass</strong>: 为每个需要的 groupOp 插 入loadOp 和 storeOp，并添加 group_tag.
<ul>
<li>group_tag = 0: 需要 load 或 store，意味着该 group 需要后续的切分搜索。</li>
<li>group_tag = 2: 不需要 load 或 store，意味着该 group 的op 都在 DDR 上操作，无需参与后续的切分搜索。</li>
</ul>
</li>
</ul>
<p>SPM 上一定要能放下切分后的结果。Group 是切分搜索和 SPM 分配的基本的单位。思想就是尽量把连续执行的算子组合在一起，一直在 SPM 上运行而不是存回 DDR 再读入，以此来减少访存时间。GroupOp 在 td 文件中定义所包含的输入如下:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-tablegen" data-lang="tablegen"><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">regions</span> <span class="p">=</span> <span class="p">(</span><span class="nv">region</span> <span class="nv">SizedRegion</span><span class="p">&lt;</span><span class="m">1</span><span class="p">&gt;:</span><span class="nv">$body</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">arguments</span> <span class="p">=</span> <span class="p">(</span><span class="nv">ins</span>
</span></span><span class="line"><span class="cl">    <span class="nv">Variadic</span><span class="p">&lt;</span><span class="nv">AnyTensorOrNone</span><span class="p">&gt;:</span><span class="nv">$operands</span><span class="p">,</span>      <span class="c">// 输入参数为 操作数的数量可变的的张量
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">BoolAttr</span><span class="p">,</span> <span class="s">&#34;false&#34;</span><span class="p">&gt;:</span><span class="nv">$pipeline_parallel</span><span class="p">,</span> <span class="c">// 是否用流水线并行
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">I32Attr</span><span class="p">,</span> <span class="s">&#34;1&#34;</span><span class="p">&gt;:</span><span class="nv">$sp_stage_num</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">Tx8e_RegionAttr</span><span class="p">&gt;:</span><span class="nv">$dev_region</span><span class="p">,</span> <span class="c">// 设备的空间属性
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">UI32Attr</span><span class="p">&gt;:</span><span class="nv">$spm_alloc_size</span><span class="p">,</span>   <span class="c">// group占用的spm大小
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">I32Attr</span><span class="p">&gt;:</span><span class="nv">$group_tag</span><span class="p">,</span>         <span class="c">// 0: 正常切分, 1: split nht, 2: 不切分 (reshape)
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">DenseI32ArrayAttr</span><span class="p">&gt;:</span><span class="nv">$stream_online_check</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">DenseI32ArrayAttr</span><span class="p">&gt;:</span><span class="nv">$stream_offline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">BoolAttr</span><span class="p">,</span> <span class="s">&#34;true&#34;</span><span class="p">&gt;:</span><span class="nv">$need_barrier</span><span class="p">,</span>   <span class="c">// 是否需要tile同步
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">SI32Attr</span><span class="p">,</span> <span class="s">&#34;-1&#34;</span><span class="p">&gt;:</span><span class="nv">$group_id</span><span class="p">,</span>         <span class="c">// group id序号
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">SI32Attr</span><span class="p">,</span> <span class="s">&#34;-1&#34;</span><span class="p">&gt;:</span><span class="nv">$template_id</span>      <span class="c">// 复用其他group的id, 小于0为不复用
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">results</span> <span class="p">=</span> <span class="p">(</span><span class="nv">outs</span> <span class="nv">Variadic</span><span class="p">&lt;</span><span class="nv">AnyTensorOrNone</span><span class="p">&gt;:</span><span class="nv">$results</span><span class="p">);</span>
</span></span></code></pre></div><p>还有一些常用到的结构体
<code>SecsInfo</code> 记录了单个 Op在分布式策略搜索过程中的所有状态和信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">SecsInfo</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">sharding</span><span class="p">;</span>  <span class="c1">// space 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">split</span><span class="p">;</span>  <span class="c1">// time
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">splitry</span><span class="p">;</span>  <span class="c1">// 当前搜索的 sharding 的 split
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">reduceSplit</span><span class="p">;</span>  <span class="c1">// 针对需要进行规约 (Reduction) 的维度的切分策略。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int32_t</span> <span class="n">reducesplit</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 一个标记位，用于指示reduceSplit是否被使用
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ******************** 以下变量为factorSpace使用部分 ********************
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="n">sfinish</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>  <span class="c1">// 标记 split/reduceSplit 相关的策略是否已确定。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 枚举类型，定义了当前算子所处的切分模式，特别关注需要通信的Reduce维度。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="cm">/* SHARDING_MODE 的可能值解释：
</span></span></span><span class="line"><span class="cl"><span class="cm">   * SHARDING_INIT: 初始状态，尚未确定模式。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 0: 不切分规约 (reduce) 维度。意味着数据在每个设备上是完整的，无需通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 1: 单边切分规约维度。例如，只切分权重，不切分输入，数据在不同tile上需要通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 2: 两边都切分规约维度。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 3: 对权重(weight)的输出通道(output channel)维度进行切分，但不属于张量并行(TP)，可能需要fn/oc通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">  */</span>
</span></span><span class="line"><span class="cl">  <span class="n">SHARDING_MODE</span> <span class="n">shardingMode</span><span class="p">{</span><span class="n">SHARDING_INIT</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="n">rfinish</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>   <span class="c1">// 标记 reduceSplit 相关的策略是否已完成处理。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">nfirst</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 标记搜索方向。1: search from dim0 -&gt; dim n-1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">finish</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 表示该算子的策略搜索是否已全部完成。整个搜索流程: sharding -&gt; shardingmode -&gt; split -&gt; reduceSplit
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span> <span class="n">sliceShapeMin</span><span class="p">;</span> <span class="c1">// 标记切分后的张量 (slice) 在每个维度上是否已达到某个最小尺寸限制。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ******************** 以下变量为sliceInfo使用部分 ********************
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span> <span class="n">TemporalShape</span><span class="p">;</span>  <span class="c1">// 切分后，临时的张量形状
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">reduce_sharding_space</span><span class="p">;</span>  <span class="c1">// 规约维度切分的搜索空间
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">reduce_sharding</span><span class="p">;</span>  <span class="c1">// // 最终选定的规约维度切分策略
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">sharding2_finish</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 标记第二阶段切分 (可能与规约相关) 是否完成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><h2 id="grouppatternpass">GroupPatternPass<a hidden class="anchor" aria-hidden="true" href="#grouppatternpass">#</a></h2>
<p><code>GroupPatternPass</code> 其核心功能是在给定的计算图 (subgraphOp) 中，通过一种高效的模式匹配算法，识别出预定义的、可优化的子图模式 (Operator Patterns)，并将匹配到的算子 (Operations) 进行分组。这种分组通常是图优化 (如算子融合、算子调度) 的第一步。</p>
<p>该 Pass 首先获取配置，决定从哪里加载模式 (一个 map，其键是模式，即一个算子序列 <code>std::vector&lt;TX8BE_OPS&gt;</code>，值是一个整数 <code>int</code>，代表模式优先级，越大优先级越高) 。然后，它调用 <code>aca.insertPatterns</code> 将这些模式&quot;编译&quot;到 Automation 引擎中。接着，调用 <code>aca.search</code> 执行匹配。最后，从 manager 中获取匹配结果 (groups) ，并对这些 groups 进行后续处理，例如创建新的逻辑分组和进行拓扑排序。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"> <span class="kt">void</span> <span class="n">GroupPatternPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">     <span class="n">TFUNC_SCOPE</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">subgraphOp</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span> <span class="c1">// Get the current operation (e.g., a function) the pass is running on.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> 
</span></span><span class="line"><span class="cl">     <span class="n">PatternManager</span> <span class="n">manager</span><span class="p">;</span> <span class="c1">// A manager to hold graph rewriting information.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">Automation</span> <span class="nf">aca</span><span class="p">(</span><span class="o">&amp;</span><span class="n">manager</span><span class="p">);</span> <span class="c1">// Custom &#39;Automation&#39; class for pattern matching logic.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> 
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">minfo</span> <span class="o">=</span> <span class="n">getModuleConfig</span><span class="p">(</span><span class="n">getModuleByOp</span><span class="p">(</span><span class="n">getOperation</span><span class="p">()));</span> 
</span></span><span class="line"><span class="cl">     <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">path</span> <span class="o">=</span> <span class="s">&#34;&#34;</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">path</span> <span class="o">!=</span> <span class="s">&#34;&#34;</span> <span class="o">?</span> <span class="n">getPatternsFromFile</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>  <span class="c1">// Load patterns from a file if path is specified.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                           <span class="o">:</span> <span class="p">(</span><span class="n">patternConfigMap</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">GroupPatternMode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">minfo</span><span class="p">.</span><span class="n">opt_group</span><span class="p">)));</span> <span class="c1">// Otherwise, load from a pre-defined map using a config key.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">TLOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;[GroupPatternPass] config id: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">minfo</span><span class="p">.</span><span class="n">opt_group</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">     <span class="n">aca</span><span class="p">.</span><span class="n">insertPatterns</span><span class="p">(</span><span class="n">temp</span><span class="p">);</span> <span class="c1">// Insert the loaded patterns into the Automation engine. This is the starting point for building the matching structure.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">TLOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;[Automation]: </span><span class="se">\n</span><span class="s">&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">printTree</span><span class="p">(</span><span class="n">aca</span><span class="p">.</span><span class="n">root</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">     <span class="n">aca</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="n">subgraphOp</span><span class="p">);</span> <span class="c1">// Execute the search for all patterns on the given subgraph. (search function code is not provided but its role is clear).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">manager</span><span class="p">.</span><span class="n">applyAll</span><span class="p">();</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">manager</span><span class="p">.</span><span class="n">getGroups</span><span class="p">();</span> <span class="c1">// Retrieve the groups of operations that were matched.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">manager</span><span class="p">.</span><span class="n">show</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">newGroups</span> <span class="o">=</span> <span class="n">createGroups</span><span class="p">(</span><span class="n">subgraphOp</span><span class="p">,</span> <span class="n">groups</span><span class="p">);</span> <span class="c1">// Create new group structures from the matched results.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">group</span> <span class="p">:</span> <span class="n">newGroups</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">         <span class="n">sortTopologically</span><span class="p">(</span><span class="n">group</span><span class="o">-&gt;</span><span class="n">getBlock</span><span class="p">());</span> <span class="c1">// Topologically sort the operations within each new group to maintain data dependencies.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span></code></pre></div><p><code>insertPatterns</code> 对于每一个模式，它首先调用 processPattern 来处理其中的 OR, WILDCARD 算子。</p>
<ul>
<li>当遇到 OR 时，它会将模式拆分。例如，A B OR C D 这样的模式会被拆解成两个独立的模式 A B 和 C D 进行处理。</li>
<li>当遇到 WILDCARD 时，它会生成多个模式。根据代码 <code>for (int i = 0; i &lt; 5; i++)</code> 和 <code>temp.push_back(*(it - 1))</code>，OP * 可能会被扩展成 OP, OP OP, OP OP OP, OP OP OP OP 等一系列重复模式。</li>
<li>它通过递归调用自身，以处理一个模式中包含多个特殊算子的情况。
最终，它返回一个由多个具体、无特殊算子的模式组成的列表。然后，它将这些扩展后的具体模式逐一传递给 <code>insertPattern</code> 函数，以构建 Trie 树。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">insertPatterns</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">patterns</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;&gt;</span> <span class="n">tempPatterns</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">patterns</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate through each pattern from the input map.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">processPattern</span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">first</span><span class="p">);</span> <span class="c1">// Pre-process the pattern. This can expand one pattern into many.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">p</span> <span class="p">:</span> <span class="n">temp</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// For each of the generated concrete patterns...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">insertPattern</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">);</span> <span class="c1">// ...insert it into the main data structure (the Trie).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>insertPattern</code> 接收一个具体的模式，并将其插入到 Trie 树中。Trie 树是实现高效前缀匹配的关键。从root节点开始 遍历模式中的每个 op. 如果当前节点没有指向op的子节点，就创建一个然后移动到该子节点。当模式遍历完成后，在最终的节点上存储完整模式本身 (<code>node-&gt;pattern</code>) 和它的 ID (<code>node-&gt;output</code>) 。这表明一个有效的模式在此结束。</p>
<details class="custom-details">
    <summary class="custom-summary">insertPattern Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">TrieNode</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">TrieNode</span><span class="p">(</span><span class="n">TX8BE_OPS</span> <span class="n">id</span><span class="p">)</span> <span class="o">:</span> <span class="n">id</span><span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="p">{}</span> <span class="c1">// Constructor to initialize the node with an operation ID.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">TX8BE_OPS</span> <span class="n">id</span><span class="p">;</span> <span class="c1">// The operation (Op) type this node represents. This is the &#39;character&#39; in our sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">output</span><span class="p">;</span> <span class="c1">// Stores the integer IDs of the patterns that end at this node. A non-empty vector indicates a valid pattern match.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// Stores the complete operator sequence for the pattern that ends here.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="p">,</span> <span class="n">NodePtr</span><span class="o">&gt;</span> <span class="n">children</span><span class="p">;</span> <span class="c1">// A map from an operation type to the next node in the trie. `NodePtr` is likely a shared_ptr or unique_ptr to another TrieNode.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">insertPattern</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span> <span class="n">pattern</span><span class="p">,</span> <span class="kt">int</span> <span class="n">index</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">patterns_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">pattern</span><span class="p">);</span> <span class="c1">// Store the raw pattern vector.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">node</span> <span class="o">=</span> <span class="n">root</span><span class="p">;</span> <span class="c1">// Start from the root of the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">pattern</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate through each operation in the pattern sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">op</span><span class="p">)</span> <span class="o">==</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If a path for this operation does not exist...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">TrieNode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// ...create a new node in the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">node</span> <span class="o">=</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">op</span><span class="p">];</span> <span class="c1">// Move to the next node in the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">node</span><span class="o">-&gt;</span><span class="n">pattern</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// At the end of the pattern, mark this node as a terminal node by storing the full pattern.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">node</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">index</span><span class="p">);</span> <span class="c1">// Store the original pattern index/ID at this terminal node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>searchOp</code> 函数的功能是：给定一个起始 Trie 节点 (parentNode) 和一个MLIR算子 (op)，它会尝试将 op 与parentNode 的子节点进行匹配，并在匹配成功后，递归地对其所有后继算子 (users) 进行 DFS 模式匹配，最终返回这条路径上所能找到的“最佳”匹配模式的末端Trie节点。</p>
<p>这里的“最佳”通常指最长的匹配模式，或者在有多个同样长度的模式时，选择优先级最高的那个 (根据节点中的 <code>output.front()</code>) 大小比较来判断。</p>
<details class="custom-details">
    <summary class="custom-summary">searchOp Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">NodePtr</span> <span class="n">Automation</span><span class="o">::</span><span class="n">searchOp</span><span class="p">(</span><span class="n">NodePtr</span> <span class="n">parentNode</span><span class="p">,</span> <span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">opId</span> <span class="o">=</span> <span class="n">getOpId</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Get the enumerated ID (e.g., TX8BE_OPS::CONV) for the current MLIR operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isRealOp</span><span class="p">(</span><span class="n">op</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">opId</span><span class="p">)</span> <span class="o">==</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// If the current op is a &#34;real&#34; operation (not a terminator, etc.) but cannot be found in the children of the parent Trie node, it&#39;s a mismatch.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// This &#39;if&#39; block seems to be an early exit for a specific case, possibly redundant with the final return.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">opId</span><span class="p">)</span> <span class="o">!=</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If a path exists in the Trie for the current operation `opId`. This is a potential match.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// If the current op matches, continue downwards
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">currentNode</span> <span class="o">=</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">opId</span><span class="p">];</span> <span class="c1">// Move to the matched Trie node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">tempNode</span> <span class="o">=</span> <span class="n">currentNode</span><span class="p">;</span> <span class="c1">// `tempNode` will store the longest match found so far starting from this path.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// --- Query Operation Attributes and Users ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">queryInterface</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e_mlir</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Get a specific interface from the operation for querying attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">needStore</span> <span class="o">=</span> <span class="n">queryInterface</span><span class="p">.</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needStore</span><span class="p">;</span> <span class="c1">// Check an attribute, e.g., if the op&#39;s result needs to be stored.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">llvm</span><span class="o">::</span><span class="n">SmallSet</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="mi">1</span><span class="o">&gt;</span> <span class="n">users</span><span class="p">;</span> <span class="c1">// Find all direct users of the current operation&#39;s result.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">sortedUsers</span> <span class="o">=</span> <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">sortOps</span><span class="p">(</span><span class="n">users</span><span class="p">);</span> <span class="c1">// Sort the users, likely topologically or based on some priority.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// --- Recursively Search Through Users ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">sortedUsers</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">isRealOp</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip non-essential ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">            <span class="k">auto</span> <span class="n">interface</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e_mlir</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="k">auto</span> <span class="n">needLoad</span> <span class="o">=</span> <span class="n">interface</span><span class="p">.</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needLoad</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">needStore</span> <span class="o">&amp;&amp;</span> <span class="n">needLoad</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip paths with certain attribute mismatches (e.g., store-load dependency).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Recursively call searchOp for the user operation, starting from the current Trie node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">auto</span> <span class="n">terminalNode</span> <span class="o">=</span> <span class="n">searchOp</span><span class="p">(</span><span class="n">currentNode</span><span class="p">,</span> <span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">// --- Update Best Match ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">tempNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If both the previous best match (`tempNode`) and the new match (`terminalNode`) are valid patterns...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="c1">// Compare priority, take the one with the highest priority as the current node pattern)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="k">if</span> <span class="p">(</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">tempNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">())</span> <span class="p">{</span> <span class="o">/</span> <span class="p">...</span><span class="n">update</span> <span class="err">`</span><span class="n">tempNode</span><span class="err">`</span> <span class="n">to</span> <span class="n">the</span> <span class="k">new</span> <span class="n">one</span> <span class="k">if</span> <span class="n">it</span> <span class="n">has</span> <span class="n">a</span> <span class="n">higher</span> <span class="n">priority</span> <span class="p">(</span><span class="n">assuming</span> <span class="n">the</span> <span class="kt">int</span> <span class="n">ID</span> <span class="n">represents</span> <span class="n">priority</span><span class="p">).</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tempNode</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If `tempNode` was not a valid pattern end, but `terminalNode` is, update it.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="n">tempNode</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// TFOOTER(TRACE)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">return</span> <span class="n">tempNode</span><span class="p">;</span> <span class="c1">// Return the node corresponding to the longest/best pattern found from this point.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Indicates parent node cannot match current op, return parent node)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">parentNode</span><span class="p">;</span> <span class="c1">// If no match was found for `opId` in the Trie, return the original `parentNode`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>search</code>遍历计算子图 (subgraph) 中的每一个算子，并以该算子为起点，尝试进行模式匹配。</p>
<ol>
<li>预处理阶段 (第一个 walk)
在正式开始匹配之前，函数会先遍历一次整个子图，目的是收集和注册一些元数据：</li>
</ol>
<ul>
<li><code>manager_-&gt;opOrder_</code>: 一个 vector 记录图中所有算子的出现顺序。</li>
<li><code>manager_-&gt;opIndexMap_</code>: 为每个算子分配一个唯一的整数索引。
这些信息对于后续的管理和可能的图变换 (如拓扑排序) 非常重要。</li>
</ul>
<ol start="2">
<li>逐点匹配阶段 (第二个 walk)它再次遍历子图中的每一个算子 op 每次都是从 Trie 树的根节点 root 开始 <code>searchOp(root, op)</code> 函数。意味着尝试从零开始匹配所有已知的模式。 searchOp 会返回从 op 开始能找到的最长/最优的匹配模式的末端节点 (terminalNode).</li>
</ol>
<ul>
<li>如果其 output 列表不为空，说明 searchOp 成功地找到了一条完整的匹配路径。函数就会将这个匹配结果记录下来：在 manager 中更新 Pattern 对象，并在本地的 result map 中建立从起始算子 op到模式ID的映射。</li>
<li>反之说明从 op 开始无法匹配任何完整的模式，于是就什么也不做，继续检查下一个算子。</li>
</ul>
<details class="custom-details">
    <summary class="custom-summary">search Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">search</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span> <span class="n">subgraph</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// k: the starting operation of a matched pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// v: the type/ID of the matched pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">initDefsMap</span><span class="p">(</span><span class="n">subgraph</span><span class="p">);</span> <span class="c1">// Initialize manager with definition information from the subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">subgraph</span><span class="o">-&gt;</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// First pass: walk through the subgraph to gather metadata.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">opOrder_</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Record the sequential order of all operations.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">opIndexMap_</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span><span class="o">++</span><span class="p">;</span> <span class="c1">// Assign a unique index to each operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Second pass: walk through the subgraph again to perform the actual pattern matching.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">subgraph</span><span class="o">-&gt;</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Skip the return operation of the subgraph as it&#39;s not part of a computational pattern.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphReturnOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">WalkResult</span><span class="o">::</span><span class="n">skip</span><span class="p">();</span> <span class="c1">// In newer MLIR, this might be `return;`. Skips processing this op&#39;s children.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">pattern</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Pattern</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Create a Pattern object, representing a potential match starting at `op`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">patterns_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">pattern</span><span class="p">);</span> <span class="c1">// Add this potential pattern to the manager&#39;s list.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">patternMap_</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// Map the operation `op` to its corresponding Pattern object.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// terminalNode 就是最后匹配到的一个Node (terminalNode is the final matched Node)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// This is the main call to the recursive search function, starting from the Trie root for each `op`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">terminalNode</span> <span class="o">=</span> <span class="n">searchOp</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// If the Node has an output, it means a match was found. If multiple matches exist, they are replaced based on priority during the search phase
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// The final result is a match for the highest-priority pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Check if the search returned a valid pattern-terminating node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// If a match was found, update the Pattern object with the results from the terminal node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">pattern</span><span class="o">-&gt;</span><span class="n">setPattern</span><span class="p">(</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">(),</span> <span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">pattern</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Record the result: map the starting operation `op` to the matched pattern&#39;s ID.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">WalkResult</span><span class="o">::</span><span class="n">advance</span><span class="p">();</span> <span class="c1">// Proceed to the next operation in the walk.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupoptimizationpass">GroupOptimizationPass<a hidden class="anchor" aria-hidden="true" href="#groupoptimizationpass">#</a></h2>
<p>会遍历一个计算 subGraph 中的所有 OP. 对于每一个通过筛选的普通计算操作，会调用 <code>createSingleGroup</code> 函数来为其创建一个专属的 GroupOp.
<code>createSingleGroup</code> 会检查原始 OP 的所有输入。如果输入来自另一个计算操作，那么这个输入就会成为新 GroupOp 的输入。如果输入是 LoadConstOp，则被视为这个分组的内部依赖，而不是外部输入。原始 op 的所有输出会直接成为新 GroupOp 的输出。</p>
<p>新的 GroupOp 拥有上一步定义的输入和输出。原始的操作 op 和它的常量依赖 (dependencies) 被移动到这个新创建的 GroupOp 内部。最后，修改原始操作 OP 的连接关系，使其在分组内部能够正确地接收输入并产生输出。伪代码如下</p>
<pre tabindex="0"><code>for op in subGraph.ops:

  // 检查操作的类型
  if op == (GroupOp || ReturnOp || LoadConstOp || NoneOp):
    continue

  createSingleGroup(op)

------------------------------------
createSingleGroup(op):
  for pre_op in op.inputsOp:
    // 判断前置操作是否为“加载常量”或“空操作”
    if pre_op == (LoadConstOp || NoneOp):
      // 如果是，则将其添加到依赖项 (dependencies) 集合中
      dependencies.add(pre_op)
    else:
      // 如果是其他普通操作，则将其结果添加到新分组的输入 (groupInput) 中
      groupInput.add(pre_op.result)

  for result in op.results:  // 遍历当前操作的所有输出结果
    // 将这些结果添加到新分组的输出 (groupOutput) 中
    groupOutput.add(result)

  // 使用收集好的输入和输出创建一个新的 GroupOp (分组操作) 
  create GroupOp(groupInput, groupOutput)

  // 将依赖项 (如常量) 移动到新分组的末尾 (或内部) 
  move dependencies to group end

  // 将原始操作 op 本身也移动到新分组的末尾 (或内部) 
  move op to group end

  // 修改原始操作 op 的输入和输出，使其在新分组内部正确连接
  change op input and output
</code></pre><p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBca62e625dd418d0b51deb2e46c83f873?method=download&amp;shareKey=3b9dddfeca5108a0665fce242dd1019d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBca62e625dd418d0b51deb2e46c83f873?method=download&amp;shareKey=3b9dddfeca5108a0665fce242dd1019d" alt="GroupOptimizationPass">
    </a><figcaption>GroupOptimizationPass</figcaption></figure></p>
<h2 id="groupldstpass">GroupLdStPass<a hidden class="anchor" aria-hidden="true" href="#groupldstpass">#</a></h2>
<p><code>GroupLdStPass</code> 作用用是处理 GroupOp 的输入和输出，通过显式插入 Load 和 Store 操作，来“固化”和“隔离”GroupOp 的边界。</p>
<p>Load 插入流程</p>
<ol>
<li>识别 Load 需求: 函数遍历 GroupOp 的每一个输入参数v。然后，它查找所有在 GroupOp 外部使用 v 的算子 (userOp) 。通过检查这些userOp的属性 (needLoad) ，它判断哪些 userOp 需要一个显式的 Load 操作来获取 v 的值。</li>
<li>处理特殊布局: 代码中有一段特殊的逻辑 (<code>if(isa&lt;...&gt;)</code>) ，用于处理 Add、Sub 等二元算子。它检查输入的layout 如果存在不匹配的情况 (例如一个NCx布局和一个Tensor布局) ，它可能会强制layout统一，以确保硬件能够正确计算。</li>
<li>插入 LoadVarOp: 在确定了所有需要 Load 的外部用户后，如果这样的用户存在 (<code>usersLoad.size() != 0</code>)，它会在GroupOp的入口处创建一个tx8e::LoadVarOp操作。</li>
<li>重定向数据流: 将所有外部用户对原始输入 v 的连接 (SSA use-def chain) ，全部断开，并重新连接到新创建的LoadVarOp的输出上 (replaceUsesOfWith).</li>
</ol>
<p>Store 插入流程</p>
<ol>
<li>识别存储需求: 函数找到 GroupOp 内部的 return 操作，并遍历它的每一个操作数 (即 GroupOp 的输出值). 通过检查产生这些输出值的内部算子 (pre_op) 的needStore属性，来判断哪些输出需要被显式地Store，以便外部世界能够访问。</li>
<li>插入 StoreVarOp: 如果一个输出值需要被存储，函数会在 GroupOp 的末尾、return 操作之前，创建一个tx8e::StoreVarOp 接收 GroupOp 的内部计算结果。</li>
<li>更新返回结果: StoreVarOp本身也有一个输出。函数会更新 GroupOp 的 return 操作，使其返回 StoreVarOp 的输出，而不是原始的内部计算结果。</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">GroupLdStPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">GroupLdStPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">subgraph</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span> <span class="n">g_op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//  For each group&#39;s input, insert a load. If used by multiple ops, multiple loads are inserted
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">v</span> <span class="p">:</span> <span class="n">g_op</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">().</span><span class="n">getArguments</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Iterate over each input argument of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">Operation</span><span class="o">*</span> <span class="n">pre_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">v</span><span class="p">);</span> <span class="c1">// Find the operation that produces this input.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">usersLoad</span><span class="p">;</span> <span class="c1">// A map to store users that need to load this input.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">userOp</span> <span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Find all users of this input argument.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Check if the user needs a &#39;load&#39; based on its attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">((</span><span class="o">!</span><span class="n">opAttr</span><span class="p">.</span><span class="n">needLoad</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">arg_idx</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// If a load is needed, record the user and its argument index.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">usersLoad</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_pair</span><span class="p">(</span><span class="n">userOp</span><span class="p">,</span> <span class="n">arg_idx</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// This block handles complex layout logic for Add/Sub/Mul/Div ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// It seems to ensure that if one input to &#39;add&#39; is rank1 tensor, the other is also handled correctly,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// potentially by forcing a specific layout (`LayoutMode::Cx`).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">AddOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">SubOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">DivOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">MulOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">userOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [复杂布局逻辑]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="p">}</span>
</span></span><span class="line"><span class="cl">          
</span></span><span class="line"><span class="cl">          <span class="k">if</span> <span class="p">(</span><span class="n">usersLoad</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// there are users that require a load operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">NamedAttribute</span><span class="o">&gt;</span> <span class="n">tmp_attrs</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [构建LoadVarOp的属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="c1">// Create the Load operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="k">auto</span> <span class="n">ld</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">LoadVarOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">g_op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">v</span><span class="p">.</span><span class="n">getType</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">tmp_attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [设置动态shape属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              
</span></span><span class="line"><span class="cl">              <span class="c1">// For each user that needs the load...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">userOp</span> <span class="p">:</span> <span class="n">usersLoad</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// ...replace its use of the original input `v` with the result of the new `Load` operation `ld`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">userOp</span><span class="p">.</span><span class="n">first</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ld</span><span class="p">.</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// For each group&#39;s output, insert a store
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">builder</span><span class="p">.</span><span class="n">setInsertionPointToEnd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">block</span><span class="p">);</span> <span class="c1">// Set the insertion point to the end of the group&#39;s body.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">Operation</span> <span class="o">*</span><span class="n">g_return</span> <span class="o">=</span> <span class="n">g_op</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">().</span><span class="n">getTerminator</span><span class="p">();</span> <span class="c1">// Get the return operation of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">getNumOperands</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate over each output of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">value</span> <span class="o">=</span> <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">          <span class="k">auto</span> <span class="n">pre_op</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="n">getDefiningOp</span><span class="p">();</span> <span class="c1">// Find the operation inside the group that produces this output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// Check if this output value needs to be stored for external users.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">pre_op</span><span class="p">)).</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needStore</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">i</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// ... [构建StoreVarOp的属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// Create the Store operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">st</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">StoreVarOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">g_op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">value</span><span class="p">.</span><span class="n">getType</span><span class="p">(),</span> <span class="n">value</span><span class="p">,</span> <span class="n">st_attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">          <span class="c1">// ... [设置动态shape属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">          <span class="c1">// Update the group&#39;s return instruction to return the result of the store op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">setOperand</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">st</span><span class="p">.</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">moveBefore</span><span class="p">(</span><span class="n">gBlock</span><span class="p">,</span> <span class="n">block</span><span class="p">.</span><span class="n">end</span><span class="p">());</span> <span class="c1">// Move the return instruction (not standard MLIR, might be custom logic).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">updateIR</span><span class="p">(</span><span class="n">g_op</span><span class="p">);</span> <span class="c1">// Update the IR of the group op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupmappingpass">GroupMappingPass<a hidden class="anchor" aria-hidden="true" href="#groupmappingpass">#</a></h2>
<p><code>GroupMappingPass</code> 作用是将顶层模块 (Module) 中定义的全局维度信息 (x_dim 和 y_dim) 设置到每一个 GroupOp 或 GroupOp 的调用点上。</p>
<details class="custom-details">
    <summary class="custom-summary">GroupMappingPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Defines a function to perform a simple mapping of groups.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">simpleGroupMapping</span><span class="p">(</span><span class="n">ModuleOp</span> <span class="n">module</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get x and y dimension from the module&#39;s attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// These attributes are likely defined globally for the entire compilation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">uint32_t</span> <span class="n">x_dim</span> <span class="o">=</span> <span class="n">module</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">TileDx</span><span class="p">).</span><span class="n">getInt</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="kt">uint32_t</span> <span class="n">y_dim</span> <span class="o">=</span> <span class="n">module</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">TileDy</span><span class="p">).</span><span class="n">getInt</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Create an OpBuilder instance, which is a helper to create/modify MLIR operations.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">OpBuilder</span> <span class="n">builder</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get the &#39;main&#39; function from the module.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">main</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">getMainFuncOp</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get the first block (entry block) of the main function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">main_block</span> <span class="o">=</span> <span class="n">main</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="nl">inner</span> <span class="p">:</span> <span class="n">main_block</span><span class="p">.</span><span class="n">getOperations</span><span class="p">())</span> <span class="p">{</span>  <span class="c1">// Iterate over all operations within the main function&#39;s body
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">CallOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// The module&#39;s main function contains CallOps. This implies an indirect call to a subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Find the subgraph definition (&#39;SubraphOp&#39;) using the symbol name from the CallOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span> <span class="n">sg</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">lookupSymbol</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">CallOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">).</span><span class="n">getCallee</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">      <span class="c1">// Walk through the operations inside the called subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// We are looking for the &#39;GroupOp&#39; which is the actual unit of computation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">sg</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span> <span class="n">gop</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Set a &#39;dev_region&#39; attribute on the located GroupOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">setDevRegionAttr</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">gop</span><span class="p">.</span><span class="n">getOperation</span><span class="p">(),</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">});</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// The module&#39;s main function directly contains GroupOps.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Directly set the &#39;dev_region&#39; attribute on the GroupOp found in the main function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">setDevRegionAttr</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                       <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">).</span><span class="n">getOperation</span><span class="p">(),</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">GroupMappingPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>  <span class="c1">// It will operate on the entire ModuleOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="n">simpleGroupMapping</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupcostpass">GroupCostPass<a hidden class="anchor" aria-hidden="true" href="#groupcostpass">#</a></h2>
<p><code>GroupCostPass</code> 作用是为一个 GroupOp 在所有可能的切分策略中，通过 Cost Model 搜索并应用最优的一个。算法流程如下。</p>
<p>准备阶段 (Preparation):</p>
<ol>
<li>Bailout Condition: <code>if (gop-&gt;hasAttr(&quot;group_tag&quot;) &amp;&amp; ... == 2) return;</code> 如果 GroupOp的 <code>group_tag==2</code>，那么这个 Pass 就无需为它搜索切分策略了，直接返回。</li>
<li>拷贝编译选项: <code>costoption_lg.dynCompile = compileOption_-&gt;dynCompile;</code> 从一个全局的<code>compileOption_</code> 中拷贝了一系列编译参数到局部的 costoption_lg 中. 表明 Pass 的行为可以被外部配置所影响。</li>
<li>创建搜索空间: <code>auto space = std::make_shared&lt;SliceSpace&gt;();</code> 创建了一个名为 space 的对象，这个 SliceSpace 类封装了该 GroupOp 的完整搜索空间。它包含了所有可能的张量切分方式。</li>
<li>模板机制: <code>if (useTemplate) { ... }</code> 检查 <code>compileOption_-&gt;sliceHelpMap</code> 的全局映射。如果之前已经为相似的 GroupOp (由 GroupKey 标识) 计算过最优策略，它就会直接从缓存中读取结果 (sliceHelp) ，从而避免昂贵的重复搜索。如果找到了模板，它会直接应用并提前返回。</li>
</ol>
<p>搜迭代搜索循环 (The Core: Iterative Search Loop)</p>
<ol>
<li><code>while (1)</code> 循环: 这个无限循环是搜索算法的主体。</li>
<li>探索策略: 在循环内部，space对象会生成一个候选的切分策略。这通过 <code>space-&gt;shardingLevel</code> 和<code>space-&gt;factorSpace_</code> 来控制，它们共同定义了当前正在尝试的切分维度和方式。</li>
<li>判断搜索是否完成: <code>if (space-&gt;shardingLevel.isSpaceFinish() &amp;&amp; ...)</code>. 在每次迭代开始时，它会检查是否已经遍历了所有的切分可能性。如果搜索空间已耗尽，循环就会终止。</li>
<li>成本估算: 如果找到一个有效的候选策略，接下来就是估算这个策略的成本。动态构建Pass流水线:</li>
</ol>
<ul>
<li><code>auto pm = std::make_unique&lt;LgPassManager&gt;(...);</code> 添加一系列估算Pass:
<ul>
<li><code>pm-&gt;add_pass(createDataSplitNewPass(space));</code> // 根据策略进行数据切分</li>
<li><code>pm-&gt;add_pass(createTimeStepNewPass(space));</code> // 划分时间步</li>
<li><code>pm-&gt;add_pass(createSPMAllocPass(space));</code>    // 模拟SPM (片上内存) 分配</li>
<li><code>pm-&gt;add_pass(createEstimatePass(space));</code>    // 估算性能/成本</li>
</ul>
</li>
<li>运行估算流程: <code>pm-&gt;run(gop);</code></li>
</ul>
<ol start="5">
<li>比较和选择最优解: 估算完成后，<code>space-&gt;status</code> 会被更新 (SSTATUS_OK 表示估算成功，SSTATUS_SA_MemAlloc 表示内存分配失败) . 如果估算成功，它会获取成本 t，并与已知的 bestCost 进行比较。如果当前策略更优，就更新 bestCost 和 bestStrategy。</li>
</ol>
<p>应用最优策略 (Applying the Best Strategy)</p>
<ol>
<li>应用策略: <code>sliceHelp.strategy = space-&gt;strategy;</code> 和后续的 <code>compileOption_-&gt;IRHelp.ops[gop] = space-&gt;stageOps;</code> 等赋值操作，就是将搜索到的最优策略结果 (包括每个操作的切分方式、循环信息等) 保存到 compileOption_中，供后续的 Pass 使用。</li>
<li>具体计算: <code>gop-&gt;walk(...)</code> 它遍历 GroupOp 内部的操作 (如GemmOp) ，并根据策略 (lSharding, rSharding) 计算出具体的循环边界 (ls, rs) 和分片长度 (pLen) ，这些信息会被存入 gls (<code>GroupLoopSpace</code>) 对象中。</li>
</ol>
<h3 id="datasplitnewpass">DataSplitNewPass<a hidden class="anchor" aria-hidden="true" href="#datasplitnewpass">#</a></h3>
<p>其中也包括好几个 pass
<code>DS_SpaceInitPass</code> 作用是初始化分布式策略的搜索空间。对 groupOp 中的每一个算子，它会调用 <code>space_-&gt;shardinglevel.init</code> 这个函数会根据算子自身的特性、全局约束 (如 max_sharding) 以及用户配置 (如 opt_search) ，生成该算子所有可能的切分方式。</p>
<p><code>init</code> 函数首先获取了算子的维度 dim 和目标切分路数 maxSharding，然后调用 getShardings 找出一个张量在所有维度上进行整数倍切分、且总切分路数恰好等于 maxSharding 的所有组合来填充 shardings 列表。随后，将这些组合 (并额外加上了不切分的方案) 包装成带有性能评估因子的 ShardingSpace 对象，并存入一个有序集合 <code>std::set&lt;ShardingSpace&gt; spaces</code> 中。ShardingSpace 重载了小于操作符用于对切分策略排序。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ShardingSpace</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">list</span><span class="o">&lt;</span><span class="n">ShardingInfo</span><span class="o">&gt;</span> <span class="n">shardings</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 预估的性能参数，即空间上能用到pow(2,x)个tile
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">uint8_t</span> <span class="n">factor</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 关键点：重载小于操作符，定义排序规则
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="k">operator</span><span class="o">&lt;</span><span class="p">(</span><span class="k">const</span> <span class="n">ShardingSpace</span> <span class="o">&amp;</span><span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 性能高的在前面
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="n">factor</span> <span class="o">&gt;</span> <span class="n">other</span><span class="p">.</span><span class="n">factor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="k">operator</span><span class="o">==</span><span class="p">(</span><span class="k">const</span> <span class="n">ShardingSpace</span> <span class="o">&amp;</span><span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">factor</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">factor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ShardingLevel</span><span class="o">::</span><span class="n">init</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">nFirst</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">opt_search</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ... 清理和准备工作 ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  
</span></span><span class="line"><span class="cl">  <span class="c1">// 1. 获取算子输出Tensor的维度数量 (Rank) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int32_t</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getRank</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 2. 准备容器
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&gt;</span> <span class="n">shardings</span><span class="p">;</span> <span class="c1">// 用于接收所有合法的sharding方案
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">tempSharding</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>   <span class="c1">// 一个临时的、大小为dim的向量，用于递归
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 3. 调用核心递归函数，启动搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - curDim=0: 从第0维开始搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - allDim=dim: 总共有dim个维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - curSharded=1: 当前已累乘的切分系数为1 (乘法单位元) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - maxSharding: 最大切分数目，即为每个 chip 的 tile 数目 (16)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">getShardings</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">tempSharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 4. 手动添加“不切分”的方案
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    递归函数只会寻找乘积等于maxSharding的组合，但[1, 1, ..., 1] (不切分)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    是一个非常重要的基础方案，这里手动添加进去。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">shardings</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// ... 后续处理 ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">sharding</span> <span class="p">:</span> <span class="n">shardings</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSpace</span> <span class="n">newShardingSpace</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isValid</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 1. 为每个sharding方案计算性能因子
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">newShardingSpace</span><span class="p">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">getFactor</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">sharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// ... (省略部分逻辑) ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    
</span></span><span class="line"><span class="cl">    <span class="c1">// 2. 将包含factor的ShardingSpace对象插入set中
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">spaces</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">newShardingSpace</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>getShardings</code> 函数采用的是递归算法，目标是找到所有整数向量 <code>s = {s_0, s_1, ..., s_{dim-1}}</code>，使得 <code>s_0 * s_1 * ... * s_{dim-1} == maxSharding</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ShardingLevel</span><span class="o">::</span><span class="n">getShardings</span><span class="p">(</span><span class="kt">int32_t</span> <span class="n">curDim</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">allDim</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">curSharded</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">maxSharding</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&amp;</span> <span class="n">sharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 1. 递归终止条件 (Base Case) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">curDim</span> <span class="o">==</span> <span class="n">allDim</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 已经处理完所有维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">curSharded</span> <span class="o">==</span> <span class="n">maxSharding</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 并且累乘结果正好等于目标
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// // succeeded
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">shardings</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">sharding</span><span class="p">);</span> <span class="c1">// 找到了一个合法解，存入结果列表
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span><span class="p">;</span> <span class="c1">// 回溯
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 2. 递归主体：遍历当前维度的所有可能切分系数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">maxSharding</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 尝试将当前维度(curDim)的切分系数设为 i
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">sharding</span><span class="p">[</span><span class="n">curDim</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 更新已累乘的切分系数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">curSharded</span> <span class="o">*=</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 3. 剪枝优化 (Pruning) ：这是算法效率的关键！
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 如果当前累乘的结果已经超过了目标，那么无论后续维度如何取值，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 最终结果必然大于 maxSharding，所以没有必要继续递归下去了。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">curSharded</span> <span class="o">&lt;=</span> <span class="n">maxSharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 如果还有希望，则对下一个维度进行递归搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">getShardings</span><span class="p">(</span><span class="n">curDim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">allDim</span><span class="p">,</span> <span class="n">curSharded</span><span class="p">,</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">sharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 4. 回溯 (Backtracking) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 无论上面的递归是否成功，当它返回后，我们需要“撤销”当前的选择，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 以便在 for 循环的下一次迭代中尝试新的值。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">curSharded</span> <span class="o">/=</span> <span class="n">i</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>getFactor</code> 遍历每个维度，基于内存对齐等硬件限制，计算出该维度上最大合理的切分数量 maxShardingDim.
将用户提议的切分数量 <code>sharding[i]</code> 与 maxShardingDim 取最小值，得到该维度上的有效切分数量。将所有维度上的有效切分数量相乘，得到总的有效并行度 tileNum. 对 tileNum 取以2为底的对数并向上取整后返回。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">uint8_t</span> <span class="n">ShadingLevel</span><span class="o">::</span><span class="n">getFactor</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span> <span class="n">sharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">tileNum</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">rank</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// a. 判断是否需要对齐：这里只对最后一个维度特殊处理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">align</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// b. 获取对齐基数 (alignBase)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    如果需要对齐，则调用 GetAlignBase 获取一个对齐值，否则为1 (相当于不对齐) 。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这个 alignBase 很可能代表硬件一次最优处理的最小数据块大小。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint32_t</span> <span class="n">alignBase</span> <span class="o">=</span> <span class="n">align</span> <span class="o">?</span> <span class="n">GetAlignBase</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span> <span class="o">:</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// c. 计算当前维度的最大合理切分路数 (maxShardingDim)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    一个维度能被切成多少份，不仅取决于它的总大小，还取决于对齐要求。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    例如，一个维度大小为100，但硬件要求必须按16对齐处理，那么最多只能切成 ceil(100/16) = 7 份。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">maxShardingDim</span> <span class="o">=</span> <span class="n">CEIL</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alignBase</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// d. 计算“有效”的切分路数并累乘
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这是关键！它在“提议的切分路数(sharding[i])”和“最大合理切分路数(maxShardingDim)”之间取最小值。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这意味着，即使你提议将一个维度切16份，但如果硬件限制最多只能切7份，那也只能算7份的贡献。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这可以防止对一个维度进行“无效的过度切分”。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">tileNum</span> <span class="o">*=</span> <span class="n">MIN</span><span class="p">(</span><span class="n">maxShardingDim</span><span class="p">,</span> <span class="n">sharding</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">ceil</span><span class="p">(</span><span class="n">log2</span><span class="p">(</span><span class="n">tileNum</span><span class="p">)));</span> <span class="c1">// 向上取整
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>一个例子如下</p>
<pre tabindex="0"><code>storeOp outShape[3, 4, 128, 4096]
level0: [1, 1, 1, 16], [1, 1, 2, 8], [1, 1, 4, 4]...   factor=16
level1:[1, 8, 1, 2], [1, 8, 2, 1]....                  factor=8
level2:[1, 16, 1, 1]                                   factor=4 
level3:[16, 1, 1, 1]                                   factor=2
</code></pre><p><code>DS_TileShardingPass</code> 按顺序遍历 groupOp 中的算子，并像一个状态机一样检查和更新各算子的分布式策略状态。其在每次执行时，仅为当前的待定算子，从其预先生成并排好序的候选策略列表中，选出下一个最优的切分方案并进行更新，然后立即终止当次运行。整个图的最终切分方案是通过反复执行此 Pass，将决策从图的入口逐步传播到出口而最终确定的。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBda0206ff3ade37b3cb94b73f3a564489?method=download&amp;shareKey=bd6f962093568ee599a982e7b7b1a300" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBda0206ff3ade37b3cb94b73f3a564489?method=download&amp;shareKey=bd6f962093568ee599a982e7b7b1a300" alt="An Example of Sharding">
    </a><figcaption>An Example of Sharding</figcaption></figure></p>
<p><code>DS_TileSplitPass</code> 首先检查算子是否需要 <code>reduceSplit</code> (例如 GeMM 切分 k 维度). 如果 reduce 维度切分状态为 <code>s.srfinish = true</code> 才会进行后续的 split 方案。</p>
<ol>
<li>从后向前 (或根据 nFirst 标志决定方向) 检查算子的各个维度，找到第一个“还可以再切分”的维度。判断依据是该维度切分后的大小是否已达到系统设定的最小值 (s.sliceShapeMin) .</li>
<li>一旦找到目标维度 updateDim，它会调用一个名为 <code>getNextSplit</code> 的函数。它会根据当前维度的切分值 <code>s.splitTry[updateDim]</code> 计算出下一个可能的切分值。例如，如果当前是 2，getNextSplit 可能会返回 4.</li>
<li>更新与记录：它将这个新的切分值更新到尝试性方案 <code>s.splitTry</code> 中，并记录下这次更新<code>space_-&gt;splitRecord.update(...)</code>.</li>
<li>在对当前算子的循环结束时，它会将探索出的 <code>s.splitTry</code> 赋值给最终方案 <code>s.split</code>.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBab1aedb258273670b60e2b54295f1f6c?method=download&amp;shareKey=afd6b38561485627a11fd118670b8431" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBab1aedb258273670b60e2b54295f1f6c?method=download&amp;shareKey=afd6b38561485627a11fd118670b8431" alt="An Example of Split try of above Sharding">
    </a><figcaption>An Example of Split try of above Sharding</figcaption></figure></p>
<p><code>DS_SlicePropagatePass</code> 后序遍历 (即从 groupOp 的输出到输入) 的方式反向传播切分决策，其逻辑是：对于每一个算子 (消费者)，它会调用该算子实现的 <code>ShardingInterface</code> 接口中的 <code>tileShardingSplit</code> 方法，来精确计算出其上游算子 (生产者) 应该如何切分数据以满足消费者的需求。这如果自动接口推导失败，它会回退去读取算子上预设的 <code>tile_parallel</code> 属性作为人工指令。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBb14ede2d32d997490222f36c1f0acc21?method=download&amp;shareKey=a244cf2ec9b5f31b5f3ef0ff2aa370a0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBb14ede2d32d997490222f36c1f0acc21?method=download&amp;shareKey=a244cf2ec9b5f31b5f3ef0ff2aa370a0" alt="An Example of Propagation">
    </a><figcaption>An Example of Propagation</figcaption></figure></p>
<p><code>DS_UpdateSliceIRPass</code> 核心策略是通过分析图中 reduceOp 来反向推断和划分流水线阶段。通过检查每个 reduceOp 自身的并行复杂度 (例如，tpSplit &gt; 1) 来判断其上游的计算类型，从而为不同的流水线打上诸如 STAGEIC2OC (模型并行规约段) 或 STAGEOC2NH (模式切换段) 的标签。在完成对所有算子的阶段划分后，它会最终计算每个阶段的流水线深度，并整理输出一份包含并行循环类型、算子分组和流水线阶段信息的完整执行。</p>
<ol>
<li>
<p>首先从 reduceOps 栈中取出一个关卡算子。然后，它利用 <code>getNEOPTPSlice</code> 等辅助函数，分析这个算子自身的切分策略，判断它具体采用了哪种张量并行方式。</p>
</li>
<li>
<p>确定连接到当前这个 reduceOp 的上一段流水线是什么类型</p>
</li>
</ol>
<ul>
<li><code>if (tpSplit &gt; 1)</code>: 如果这个关卡算子本身是一个张量并行度大于 1 的算子，代码就判断出：通往这个算子的路径，是一段需要最终进行集合通信 (C) 的路径。因此，它将这段路径的类型标记为 <code>STAGEIC2OC</code>.</li>
<li><code>else if (s.reduceSplit &gt; 1)</code>：如果不是上面那种情况，代码会检查另一种模型并行模式。如果一个算子的规约维度被切分了，同样意味着后续需要一个 AllReduce 集合通信。因此，它把这段路径标记为 <code>STAGEIC2IC</code>.</li>
<li>如果两个条件都不满足，意味着这可能是一个不同并行模式之间的切换，例如从模型并行切换回数据并行，此时会使用默认的 <code>STAGEOC2NH</code> 标记.</li>
</ul>
<ol start="3">
<li>通过 <code>updateLoopStage</code> 函数，将两个 reduceOp 算子之间的所有普通算子，都归类到刚刚在第 2 步中决策出的 lastRuduceLoopStage.</li>
<li>处理完所有的 reduceOp 后遍历所有算子，根据 LoopStageMap_ 中的记录，将算子放入对应的“篮子”里。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBa90c52245265edade98cd9f5b15d59bb?method=download&amp;shareKey=ccf9d74acc3642b1eac881133e98c6b9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBa90c52245265edade98cd9f5b15d59bb?method=download&amp;shareKey=ccf9d74acc3642b1eac881133e98c6b9" alt="DS_UpdateSliceIRPass">
    </a><figcaption>DS_UpdateSliceIRPass</figcaption></figure></p>
<h3 id="ts_swpipelinepass">TS_SwPipelinePass<a hidden class="anchor" aria-hidden="true" href="#ts_swpipelinepass">#</a></h3>
<p>TS_SwPipelinePass 核心是调用 getPipeline 函数。其内部通过顺序执行以下三个关键步骤，。</p>
<p><code>getInitPipelineOps</code></p>
<ol>
<li>为每个流水线阶段 (如 STAGENH2OC, STAGEOC2IC等) 创建一个独立的 pipeline 列表。</li>
<li>按 IC -&gt; OC -&gt; NH 顺序来拼接这些列表。在拼接时，它会检查每个阶段的循环次数。如果循环次数大于1：它并不会简单地将操作列表复制多次，而是创建一个特殊的、类型为 PipelineOpsBase 的 <strong>Repeat 节点</strong>。这个节点内部包含需要重复的子流水线 (<code>repeatBase.repeat</code>) 和重复次数 (<code>repeatBase.repeatTimes</code>) . 然后，它将这个Repeat 节点作为一个单一的、原子性的元素，插入到下一个阶段的流水线中。这是一种高效表示嵌套循环的方法。
如果循环次数不为 1：它就直接使用 splice 操作，将当前阶段的算子列表完整地移动并拼接到下一个阶段的尾部。</li>
</ol>
<p>经过层层拼接和嵌套，该函数最终返回一个名为 groupPipeline 的 std::list。这个列表就是一份完整的、线性的逻辑执行剧本，其中所有的嵌套循环都被抽象成了 Repeat 节点。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB58fc5fb9b7b1fb4880eb020bce68afd5?method=download&amp;shareKey=f4ae610fb730689a22fe38a7226ee6e5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB58fc5fb9b7b1fb4880eb020bce68afd5?method=download&amp;shareKey=f4ae610fb730689a22fe38a7226ee6e5" alt="getInitPipelineOps">
    </a><figcaption>getInitPipelineOps</figcaption></figure></p>
<p><code>pipeline </code></p>
<p>主要工作是处理上一阶段生成的 Repeat 节点，并对流水线的衔接处进行深度优化，以减少气泡 (硬件空闲周期) 。</p>
<ol>
<li>
<p>当它在流水线中遇到一个 Repeat 节点时，它会对该节点内部的子流水线再次调用pipeline函数 (<code>auto inner = pipeline((it).repeat, ...)</code>). 通过这种方式展开任意层级的嵌套循环。</p>
</li>
<li>
<p>在处理循环的边界时，它调用 getRetract 和 doRetract 这对复杂的优化工具。</p>
</li>
</ol>
<ul>
<li><code>getRetract</code>: 在连接两个循环迭代 (或不同的流水线段) 时，通过 canParallel 函数检查后一个迭代的“头部指令”是否可以和前一个迭代的“尾部指令”并行执行，从而计算出最大可以“回缩” (即提前执行) 的指令数量。</li>
<li><code>doRetract</code>: 在 getRetract 探明了可回缩的数量后，doRetract 负责物理地修改流水线。它通过 splice 操作，将后一个迭代头部的指令，合并到前一个迭代尾部的指令列表中，从而填补了潜在的执行空隙。</li>
</ul>
<p><code>getEnginsPipeline</code> 将优化后的操作序列，翻译成具体的、分配到不同硬件引擎的指令。</p>
<ol>
<li>
<p>函数遍历输入的 pipelineOps 列表。列表中的每个元素 opsBase 代表一个流水线周期 (一“帧”) 内需要共同执行的一组MLIR操作。</p>
</li>
<li>
<p>对于每个周期，它创建一个 enginsBase 对象。这个对象是一个结构体，包含了分别对应不同硬件引擎 (如 <code>ld</code> for Load, <code>st</code> for Store, <code>ne</code> for Neural Engine, <code>tdma</code> for DMA) 的成员变量。</p>
</li>
<li>
<p>遍历当前周期的所有 op，通过查询每个 op 的 engine 属性 <code>queryOpAttr().engine</code>，得知这个操作预定由哪个硬件引擎来执行。接着，它将这个 op 的指针存放到 enginsBase 对象中对应的引擎 slot 里。例如，一个 <code>NPU_ENGINE_LOAD</code> 类型的操作会被放入 <code>enginsBase.ld</code> 列表。</p>
</li>
</ol>
<p>函数最终返回一个 <code>std::list&lt;PipelineBase&gt;</code> 描述了在同一个时钟周期内，加载、存储、计算等多个硬件单元应该同时执行**哪些不同的操作。</p>
<h3 id="spmallocpass">SPMAllocPass<a hidden class="anchor" aria-hidden="true" href="#spmallocpass">#</a></h3>
<p>SPMAllocPass 包括三个 pass，下面依次介绍，首先介绍用到的数据结构</p>
<p><code>BufferLabel</code> 作为缓冲区的唯一标识符，将其链接到程序中的特定 <code>mlir::Value</code> ，并注意它是否为 Imm.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @struct BufferLabel
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @brief A unique identifier for a memory buffer.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * This struct links a buffer to a specific MLIR Value and tracks whether it&#39;s
</span></span></span><span class="line"><span class="cl"><span class="cm"> * a special &#34;immediate&#34; buffer. It&#39;s used as a key in maps to associate
</span></span></span><span class="line"><span class="cl"><span class="cm"> * MLIR Values with their buffer metadata.
</span></span></span><span class="line"><span class="cl"><span class="cm"> */</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">BufferLabel</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// The MLIR Value that this buffer represents, typically a tensor produced
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// by an operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mlir</span><span class="o">::</span><span class="n">Value</span> <span class="n">v</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// A flag indicating if this buffer holds a special &#34;immediate&#34; value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Immediate values might be treated differently during allocation (e.g.,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// small constants or internal scratchpads for an op).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">isImm</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @brief Equality operator to compare two labels.
</span></span></span><span class="line"><span class="cl"><span class="cm">     *
</span></span></span><span class="line"><span class="cl"><span class="cm">     * Two labels are considered equal if they refer to the same MLIR Value
</span></span></span><span class="line"><span class="cl"><span class="cm">     * and have the same &#39;isImm&#39; status. This is necessary for using
</span></span></span><span class="line"><span class="cl"><span class="cm">     * BufferLabel as a key in std::map or std::unordered_map.
</span></span></span><span class="line"><span class="cl"><span class="cm">     */</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="k">operator</span><span class="o">==</span><span class="p">(</span><span class="k">const</span> <span class="n">BufferLabel</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">v</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">isImm</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">isImm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p><code>ValueBuffer</code> 包含单个缓冲区所需的所有元数据，包括其标识、生存期、大小和最终内存位置。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @struct ValueBuffer
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @brief Represents the metadata for a single memory buffer, including its
</span></span></span><span class="line"><span class="cl"><span class="cm"> * lifetime, size, and allocation information.
</span></span></span><span class="line"><span class="cl"><span class="cm"> */</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ValueBuffer</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// The unique identifier for this buffer.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">BufferLabel</span> <span class="n">label</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Represents the starting point of the buffer&#39;s lifetime (inclusive),
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// measured in pipeline cycles. After memory allocation, this field may be
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// repurposed to store the starting memory address.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">start</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Represents the ending point of the buffer&#39;s lifetime (inclusive),
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// measured in pipeline cycles. After memory allocation, this field may be
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// repurposed to store the ending memory address.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">end</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// The total size of this buffer in bytes, as required by its tensor shape.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">allSize</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Size of an intermediate/temporary buffer that an operator might need
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// internally. This is often allocated contiguously with the main output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// buffer. For example, the final output address would be &#39;offset + immSize&#39;.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">immSize</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// The final memory offset assigned to this buffer in the scratchpad memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// This value is determined by the final memory allocation pass.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">offset</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @brief Less-than operator, used for sorting ValueBuffer objects.
</span></span></span><span class="line"><span class="cl"><span class="cm">     *
</span></span></span><span class="line"><span class="cl"><span class="cm">     * The active implementation sorts buffers primarily by their lifetime start
</span></span></span><span class="line"><span class="cl"><span class="cm">     * time. This is a common strategy for greedy &#34;first fit&#34; style memory
</span></span></span><span class="line"><span class="cl"><span class="cm">     * allocation algorithms. The commented-out code shows an alternative
</span></span></span><span class="line"><span class="cl"><span class="cm">     * strategy of sorting by buffer size.
</span></span></span><span class="line"><span class="cl"><span class="cm">     */</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="k">operator</span><span class="o">&lt;</span><span class="p">(</span><span class="k">const</span> <span class="n">ValueBuffer</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// return this-&gt;allSize &lt; other.allSize; // Alternative sorting by size
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">return</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">start</span> <span class="o">&lt;=</span> <span class="n">other</span><span class="p">.</span><span class="n">start</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p><code>SA_BufferLifePass</code>的核心功能是分析并确定每一个需要存放在 ScratchPad Memory 中的数据块 (Buffer，即mlir::Value对应的张量) 的生命周期。</p>
<ol>
<li>构建“定义-使用”时间表。Pass 的输入是 <code>TS_SwPipelinePass</code> 生成的最终流水线执行序列 pipelineReal. 这个序列的每一项都代表一个流水线周期，以及该周期内各个硬件引擎执行的操作。代码遍历这个流水线序列，逐个周期 (由timeStepNum计数) 地分析。它会构建两个核心的映射表：</li>
</ol>
<ul>
<li><code>opIsTemp</code>: 记录在哪一个时间步 (timeStepNum) ，有哪些值 (mlir::Value) 被定义或产出。例如，ld (加载) 和 ne (计算) 操作的输出都会被记录。</li>
<li><code>consumerOps</code>: 记录在哪一个时间步，有哪些值被作为输入消费掉了。</li>
</ul>
<p>产出：这个步骤完成后，Pass就拥有了一份完整的、按时间步索引的“谁在何时被创建”和“谁在何时被使用”的清单。</p>
<ol start="2">
<li>确定每个Buffer的生命周期。Pass会遍历所有算子和它们的输入 (operands) ，为每一个作为输入的 Value (即inValue) 计算其生命周期。</li>
</ol>
<ul>
<li>确定生命周期终点 (end)：一个 Value 的生命周期，在其被作为输入 (被消费) 时达到一个终点。因此，当代码在时间步 curTs 处理一个消费者算子时，其输入 inValue 的 <code>buf.end</code> 就被设置为 curTs.</li>
<li>确定生命周期起点 (start)：为了找到inValue何时被创建，代码会调用一个 <code>getNearestProducer</code> 的函数。这个函数会拿着当前的消费时间 curTs 和 inValue，去第一步生成的 opIsTemp (定义时间表) 中反向查找，找到离 curTs 最近的、inValue 被定义的那个时间步 <code>buf.start</code>.</li>
</ul>
<p>计算出的 start 和 end，连同 Value 的标识 (BufferLabel) ，被封装在 ValueBuffer 结构体中，并存入一个全局的数据结构 <code>space_-&gt;vBuffer</code> 里。</p>
<ol start="3">
<li>特殊情况处理</li>
</ol>
<ul>
<li><code>In-place</code>: 对于输入和输出复用同一块内存的 in-place 操作，其生命周期计算必须追溯到最初提供这块内存的那个非in-place算子。代码通过 <code>getInplaceIndex</code> 递归地回溯in-place链，以确保生命周期的 start 时间是正确的、最开始的那个定义时间。</li>
<li>中间值 (imm) 与累加值 (Psum): 代码会识别一些特殊的、可能在多个时间步中存在的中间值或累加值 (由getImmSize 或 isPsumValue 识别) . 对于这些值，它们可能会有多个离散的生存区间。Pass 中可能包含一些后处理逻辑，将这些离散的区间合并成一个从“最早的start”到“最晚的end”的连续大区间，以简化后续的内存分配。</li>
</ul>
<p><code>SA_BufferMergePass</code> 的任务就是清理这些冗余或复杂的生命周期记录，具体来说，就是合并那些存在时间上重叠或包含关系的生命周期区间，为后续的内存分配器提供一个最簡洁、无冗余的区间列表。</p>
<p>遍历由上一个 Pass 生成的 space_-&gt;vBuffer 这个map. 其中的每一项，key 是缓冲区的唯一标识 BufferLabel，value是该缓冲区所有生命周期区间的列表 <code>std::vector&lt;ValueBuffer&gt;</code>. 对于每一个value的生命周期列表，它都调用 <code>mergeOverlap</code> 来进行处理。最后，它用函数返回的、经过清理和合并的新的列表，来替换掉 map 中旧的列表。该函数流程如下</p>
<ol>
<li>根据 ValueBuffer 重载的 <code>operator&lt;</code> (即按 start 时间升序) ，将所有生命周期区间进行排序。</li>
<li>遍历已排序的列表，将 start 时间相同的连续区间收集到一个临时的 buf 向量中。遇到一个不同 start 时间的区间时，它会按照结束时间 end 排序之前收集的 buf，然后将处理后的结果 (除了最后一个元素) 重新放回 valueBuf.</li>
<li>合并被完全包含的子区间。它维护着当前最大的生命周期区间 (<code>[usedTSStart, usedTSEnd]</code>). 遍历列表中的每一个区间 <code>*it</code>. 根据 <code>bool isSub = ((*it).start &gt;= usedTSStart) &amp;&amp; ((*it).end &lt;= usedTSEnd);</code> 判断区间是否在时间上被上一个“激活”的区间完全覆盖。</li>
</ol>
<ul>
<li>如果 isSub 为 true，意味着 *it 是一个冗余的子区间。因为只要为那个更大的激活区间分配了内存，这个子区间的内存需求自然也就满足了。因此，代码通过 valueBuf.erase(it); 将这个冗余的子区间直接删除。</li>
<li>如果 isSub 为 false，说明遇到了一个新的、没有被覆盖的生命周期，于是它将成为新的“激活”区间，用于和后续的区间进行比较。</li>
</ul>
<h1 id="compile-option-1-opt_barrier">Compile Option 1: opt_barrier<a hidden class="anchor" aria-hidden="true" href="#compile-option-1-opt_barrier">#</a></h1>
<p>由 <code>groupDAGPass</code> 实现。通过 group 间的依赖关系来给 group 定层级，同一层级的 group 只有最后一个 group 需要 barrier.</p>
<ol>
<li>
<p>初始化所有 group 的 need_barrier 属性为 false。</p>
</li>
<li>
<p>从后往前遍历 group，若 group 的结果无 user 或要 return，设置 layerNum 为 0，否则设置为 userOp 的 layerNum + 1. 同时维护两个 vector: firstOpInLayers 和lastOpInLayers 来记录每一层级的第一个 op 和最后一个 op. 遍历结束把 lastOpInLayers中的 group 的 need_barrier 属性设为 true.</p>
</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8e3541f411fb9039713f3992b450f4b9?method=download&amp;shareKey=4f5dfad0d8b5a4bd8cf49ce94d5ad7c9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8e3541f411fb9039713f3992b450f4b9?method=download&amp;shareKey=4f5dfad0d8b5a4bd8cf49ce94d5ad7c9" alt="opt_barrier">
    </a><figcaption>opt_barrier</figcaption></figure></p>
<h1 id="compile-option-1-opt_ddr">Compile Option 1: opt_ddr<a hidden class="anchor" aria-hidden="true" href="#compile-option-1-opt_ddr">#</a></h1>
<p>由 <code>ddrConstReorderPass</code> 和 <code>ddrVarReorderPass</code> 实现。通过改变 const 和 var 在 ddr 中的排布，使其对齐 DDR_BANK(4096Bytes)，实现加速读取。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd3b8e4e14ab8e2c23f22a625b1d03ddd?method=download&amp;shareKey=948a94db71e0adcddb5f6107ad94a6d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd3b8e4e14ab8e2c23f22a625b1d03ddd?method=download&amp;shareKey=948a94db71e0adcddb5f6107ad94a6d0" alt="opt_ddr">
    </a><figcaption>opt_ddr</figcaption></figure></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/tx8/">Tx8</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/leetcode/03_binarysearch/">
    <span class="title">« Prev</span>
    <br>
    <span>03 BinarySearch</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/leetcode/02_slidingwindow/">
    <span class="title">Next »</span>
    <br>
    <span>02 Sliding Window</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
