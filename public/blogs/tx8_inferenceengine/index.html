<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>TX8 Inference Engine | WITHER</title>
<meta name="keywords" content="tx8">
<meta name="description" content="TX8 inference engine description.">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/tx8_inferenceengine/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/tx8_inferenceengine/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/tx8_inferenceengine/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="TX8 Inference Engine">
  <meta property="og:description" content="TX8 inference engine description.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-08-07T21:47:33+08:00">
    <meta property="article:modified_time" content="2025-09-19T09:20:48+08:00">
    <meta property="article:tag" content="Tx8">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TX8 Inference Engine">
<meta name="twitter:description" content="TX8 inference engine description.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "TX8 Inference Engine",
      "item": "http://localhost:1313/blogs/tx8_inferenceengine/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "TX8 Inference Engine",
  "name": "TX8 Inference Engine",
  "description": "TX8 inference engine description.",
  "keywords": [
    "tx8"
  ],
  "articleBody": "Overview vLLM 调用 txda 初始化 context. 程序结束后释放 context. vLLM 根据 hf_dir 中的配置文件创建模型, 其中 config.json 决定使用 TxNN 中定义的哪个 model class; 也决定了这个 class init 时的参数是什么. 调用 TxNN 的 model class 的 load weight 函数和 forward 函数, 具体实现都依赖于 TxDA. Argument def parse_args(): parser = argparse.ArgumentParser() parser.add_argument('--hf_dir', type=str, help='Huggingface model path') parser.add_argument('--max_tokens', type=int, default=32, help='Max generated tokens') parser.add_argument('--tp_size', type=int, default=1, help='Model TP size') # tensor parallel degree, must Keep consistent with model.json parser.add_argument( '--device', type=lambda e: [int(e) for e in str(e).split(',')], default=[0, 1, 2, 3], help=\"Tx device idxs; E.g., --device 0,1,2,3\" ) parser.add_argument(\"--log_file\", type=str, default=\"\", help=\"Log file path\") args = parser.parse_args() # Get case_dir from huggingface model config with open(f\"{args.hf_dir}/config.json\", \"r\", encoding=\"utf-8\") as file: hf_config = json.load(file) args.case_dir = hf_config[\"param_dir\"] # kcorebin path, with model_info.json and chip0/, chip1/ ... in the directory print( \"Inference info:\\n\" f\"| - hf_dir: {args.hf_dir}\\n\" f\"| - case_dir: {args.case_dir}\\n\" f\"| - max_tokens: {args.max_tokens}\\n\" f\"| - tp_size: {args.tp_size}\\n\" f\"| - ENV:TXDA_VISIBLE_DEVICES: \" f\"{os.environ.get('TXDA_VISIBLE_DEVICES', default='Not specified')}\\n\" f\"| - device: {args.device}\\n\" f\"| - log_file: {args.log_file}\" ) return args hf_dir: 指的是模型的 hugging_face 路径。 case_dir: 是解析 hf_dir 下的 config.json 文件的字段，指的是后端生成的 kcore 文件夹路径。除此之外还需要在 json 文件中额外配置并行度相关信息。 { // ... \"param_dir\": \"...\", \"tensor_parallel\": { \"use_tp\": false, \"parallel_size\": 1 } } 会根据我们传入的 --device 和 tp_size 参数创建逻辑到实际物理芯片的映射。这里 InternLM3 使用的是 4 卡机器，张量并行度为 2. 计算图中 batchsize=1.\n// setDeviceIdx std::unordered_map\u003cuint32_t, std::vector\u003cuint32_t\u003e\u003e c4_TP2x2 = { {0, {0, 1}}, {1, {2, 3}} }; std::unordered_map\u003cuint32_t, std::vector\u003cuint32_t\u003e\u003e c4_TP2x2_hw = { {0, {0, 1}}, {1, {2, 3}} }; auto set_TP2_C4_Map = [\u0026](std::unordered_map\u003cuint32_t, std::vector\u003cuint32_t\u003e\u003e\u0026 user_map, std::unordered_map\u003cuint32_t, std::vector\u003cuint32_t\u003e\u003e\u0026 hw_map, std::vector\u003cuint32_t\u003e\u0026 user_settings) { for (auto it_TP2x2 : user_map) { if (user_settings == it_TP2x2.second) { device_setting_ = hw_map[it_TP2x2.first]; } } }; if (TxdATPMode::C4TP2B1 == tp_mode_) { set_TP2_C4_Map(c4_TP2x2, c4_TP2x2_hw, user_settings); } TxdaContext::initDevice 的主要功能是初始化设备 (TsmDevice)，并将其封装为 TxdaDevice 对象，最终存储到设备组 (device_group_) 中。\nTxdaContext::initChipProperty 的主要功能是初始化芯片相关的属性和配置，包括设备组、模型数量、环境变量解析、内存地址初始化以及设备启动参数的设置。\nTxdaContext::initModel：负责初始化模型，解析案例目录，设置 TP 大小和 TP 模式，并调用 TxdaModel::init 初始化每个模型 的基本属性，包括案例目录、JSON 配置和编译选项。\nTxNN configuration_internlm3.py 中主要负责模型结构参数的初始化 (根 hugging face 中的 config.json 保持一致) 以及张量并行的设置。\ninput_output_internlm3.py 负责在 python 端为要从后端加载进来的输入和输出注册张量和开辟空间。-\nInputBuffer 会创建一个 IntermediateKVCache 实例。这是 KV Cache 的实际存储空间，大小根据序列长度、头的数量预先分配好。还会注册各种输入相关的缓冲区 input_ids: 当前需要处理的输入 token ID。 start_index, seq_len: 用于管理当前生成序列的位置信息。 k_gather_end_indices: 为每一层配置 对应的 KV cache 的起始索引。 OutputBuffer 和 InputBuffer 类似，它也创建了一个 IntermediateKVCache 实例。这个 Cache 用于存放计算完成后更新的 Key 和 Value，这些更新后的值将在下一步生成token时作为输入使用。\n注册所有KV Cache层: 它立即将所有层的 KV Cache 缓冲区注册到自己的 _buffer 中。 注册输出缓冲区: 预先分配并注册了用于存放最终结果的内存空间： lm_head_reshape_out: 用于存放语言模型最后的输出（通常是 logits），其维度为 (序列长度, 词汇表大小)。 lm_head_argmax_out: 用于存放最终预测出的 token ID（对 logits 取 argmax 后的结果）。 里面负责定义模型，在 python 端为数据开辟好内存，GraphBasedInternlm3ForCausalLM 在初始化时，会创建 InputBuffer 和 OutputBuffer 的实例。它为 tensor_parallel 的每一路都创建了缓冲区。\n当 forward 方法接收 input_ids. 不在 Python 中执行复杂的数学运算。而是通过 load_input 把后端编译生成的 bin 文件数据填充到 InputBuffer 中.\n通过 TxDA context 把加载好的数据放到硬件上然后 launch kernel将 InputBuffer 和 OutputBuffer 的内存地址传递给底层引擎。底层引擎直接从 InputBuffer 指向的内存中读取输入，执行高效的模型计算，然后将结果直接写入 OutputBuffer 指向的内存中。\nvllm 主要利用的是其推理框架，在 LLMEngine 里添加了自己的 TxNPUExecutor 里面包含自己的 TxNPUWorker，其中包含 TxNPUCacheEngine 和 TxNPUModelRunner.\nTxNPUMModelRunner 定义了专门从后端编译出的 chip_out 文件夹中的 param.bin 文件里加载模型参数到 TxNN 的定义模型的 ModelLoader. 然后推理的时候调用的 forward 流程如上。\n",
  "wordCount" : "1292",
  "inLanguage": "en",
  "datePublished": "2025-08-07T21:47:33+08:00",
  "dateModified": "2025-09-19T09:20:48+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/tx8_inferenceengine/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      TX8 Inference Engine
    </h1>
    <div class="post-description">
      TX8 inference engine description.
    </div>
    <div class="post-meta"><span title='2025-08-07 21:47:33 +0800 CST'>Aug-07-2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;1292 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#overview" aria-label="Overview">Overview</a></li>
                    <li>
                        <a href="#argument" aria-label="Argument">Argument</a></li>
                    <li>
                        <a href="#txnn" aria-label="TxNN">TxNN</a></li>
                    <li>
                        <a href="#vllm" aria-label="vllm">vllm</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h1>
<ol>
<li>vLLM 调用 txda 初始化 context. 程序结束后释放 context.</li>
<li>vLLM 根据 hf_dir 中的配置文件创建模型, 其中 config.json 决定使用 TxNN 中定义的哪个 model class; 也决定了这个 class init 时的参数是什么.</li>
<li>调用 TxNN 的 model class 的 load weight 函数和 forward 函数, 具体实现都依赖于 TxDA.</li>
</ol>
<h1 id="argument">Argument<a hidden class="anchor" aria-hidden="true" href="#argument">#</a></h1>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">parse_args</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--hf_dir&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Huggingface model path&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_tokens&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Max generated tokens&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--tp_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Model TP size&#39;</span><span class="p">)</span>  <span class="c1"># tensor parallel degree, must Keep consistent with model.json</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;--device&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nb">type</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">        <span class="n">default</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">help</span><span class="o">=</span><span class="s2">&#34;Tx device idxs; E.g., --device 0,1,2,3&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&#34;--log_file&#34;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&#34;Log file path&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Get case_dir from huggingface model config</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">hf_dir</span><span class="si">}</span><span class="s2">/config.json&#34;</span><span class="p">,</span> <span class="s2">&#34;r&#34;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&#34;utf-8&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hf_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">args</span><span class="o">.</span><span class="n">case_dir</span> <span class="o">=</span> <span class="n">hf_config</span><span class="p">[</span><span class="s2">&#34;param_dir&#34;</span><span class="p">]</span>  <span class="c1"># kcorebin path, with model_info.json and chip0/, chip1/ ... in the directory</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;Inference info:</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - hf_dir: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">hf_dir</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - case_dir: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">case_dir</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - max_tokens: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">max_tokens</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - tp_size: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">tp_size</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - ENV:TXDA_VISIBLE_DEVICES: &#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;TXDA_VISIBLE_DEVICES&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;Not specified&#39;</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - device: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - log_file: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">log_file</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">args</span>
</span></span></code></pre></div><ul>
<li><code>hf_dir</code>: 指的是模型的 hugging_face 路径。</li>
<li><code>case_dir</code>: 是解析 <code>hf_dir</code> 下的 config.json 文件的字段，指的是后端生成的 kcore 文件夹路径。除此之外还需要在 json 文件中额外配置并行度相关信息。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="nt">&#34;param_dir&#34;</span><span class="p">:</span> <span class="s2">&#34;...&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;tensor_parallel&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;use_tp&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;parallel_size&#34;</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>会根据我们传入的 <code>--device</code> 和 <code>tp_size</code> 参数创建逻辑到实际物理芯片的映射。这里 InternLM3 使用的是 4 卡机器，张量并行度为 2. 计算图中 batchsize=1.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// setDeviceIdx
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;</span> <span class="n">c4_TP2x2</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">}},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;</span> <span class="n">c4_TP2x2_hw</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">}},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">auto</span> <span class="n">set_TP2_C4_Map</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">user_map</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">hw_map</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&amp;</span> <span class="n">user_settings</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it_TP2x2</span> <span class="p">:</span> <span class="n">user_map</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">user_settings</span> <span class="o">==</span> <span class="n">it_TP2x2</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">device_setting_</span> <span class="o">=</span> <span class="n">hw_map</span><span class="p">[</span><span class="n">it_TP2x2</span><span class="p">.</span><span class="n">first</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">TxdATPMode</span><span class="o">::</span><span class="n">C4TP2B1</span> <span class="o">==</span> <span class="n">tp_mode_</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_TP2_C4_Map</span><span class="p">(</span><span class="n">c4_TP2x2</span><span class="p">,</span> <span class="n">c4_TP2x2_hw</span><span class="p">,</span> <span class="n">user_settings</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>TxdaContext::initDevice 的主要功能是初始化设备 (TsmDevice)，并将其封装为 TxdaDevice 对象，最终存储到设备组 (device_group_) 中。</p>
<p>TxdaContext::initChipProperty 的主要功能是初始化芯片相关的属性和配置，包括设备组、模型数量、环境变量解析、内存地址初始化以及设备启动参数的设置。</p>
<p>TxdaContext::initModel：负责初始化模型，解析案例目录，设置 TP 大小和 TP 模式，并调用 TxdaModel::init 初始化每个模型 的基本属性，包括案例目录、JSON 配置和编译选项。</p>
<h1 id="txnn">TxNN<a hidden class="anchor" aria-hidden="true" href="#txnn">#</a></h1>
<p>configuration_internlm3.py 中主要负责模型结构参数的初始化 (根 hugging face 中的 config.json 保持一致) 以及张量并行的设置。</p>
<p>input_output_internlm3.py 负责在 python 端为要从后端加载进来的输入和输出注册张量和开辟空间。-</p>
<ul>
<li>InputBuffer 会创建一个 IntermediateKVCache 实例。这是 KV Cache 的实际存储空间，大小根据序列长度、头的数量预先分配好。还会注册各种输入相关的缓冲区
<ul>
<li>input_ids: 当前需要处理的输入 token ID。</li>
<li>start_index, seq_len: 用于管理当前生成序列的位置信息。</li>
<li>k_gather_end_indices: 为每一层配置 对应的 KV cache 的起始索引。</li>
</ul>
</li>
</ul>
<p>OutputBuffer 和 InputBuffer 类似，它也创建了一个 IntermediateKVCache 实例。这个 Cache 用于存放计算完成后更新的 Key 和 Value，这些更新后的值将在下一步生成token时作为输入使用。</p>
<ul>
<li>注册所有KV Cache层: 它立即将所有层的 KV Cache 缓冲区注册到自己的 _buffer 中。</li>
<li>注册输出缓冲区: 预先分配并注册了用于存放最终结果的内存空间：
<ul>
<li>lm_head_reshape_out: 用于存放语言模型最后的输出（通常是 logits），其维度为 (序列长度, 词汇表大小)。</li>
<li>lm_head_argmax_out: 用于存放最终预测出的 token ID（对 logits 取 argmax 后的结果）。</li>
</ul>
</li>
</ul>
<p>里面负责定义模型，在 python 端为数据开辟好内存，GraphBasedInternlm3ForCausalLM 在初始化时，会创建 InputBuffer 和 OutputBuffer 的实例。它为 tensor_parallel 的每一路都创建了缓冲区。</p>
<p>当 forward 方法接收 input_ids. 不在 Python 中执行复杂的数学运算。而是通过 load_input 把后端编译生成的 bin 文件数据填充到 InputBuffer 中.</p>
<p>通过 TxDA context 把加载好的数据放到硬件上然后 launch kernel将 InputBuffer 和 OutputBuffer 的内存地址传递给底层引擎。底层引擎直接从 InputBuffer 指向的内存中读取输入，执行高效的模型计算，然后将结果直接写入 OutputBuffer 指向的内存中。</p>
<h1 id="vllm">vllm<a hidden class="anchor" aria-hidden="true" href="#vllm">#</a></h1>
<p>主要利用的是其推理框架，在 LLMEngine 里添加了自己的 TxNPUExecutor 里面包含自己的 TxNPUWorker，其中包含 TxNPUCacheEngine 和 TxNPUModelRunner.</p>
<p>TxNPUMModelRunner 定义了专门从后端编译出的 chip_out 文件夹中的 param.bin 文件里加载模型参数到 TxNN 的定义模型的 ModelLoader. 然后推理的时候调用的 forward 流程如上。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/tx8/">Tx8</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/productivity/cpplambdaexpression/">
    <span class="title">« Prev</span>
    <br>
    <span>Cpp Lambda Expression</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/leetcode/07_dynamicprogramming/">
    <span class="title">Next »</span>
    <br>
    <span>07 DynamicProgramming</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
