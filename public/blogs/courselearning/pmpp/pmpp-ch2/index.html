<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>PMPP Learning-Chapter 2 Heterogeneous Data Parallel | WITHER</title>
<meta name="keywords" content="CUDA">
<meta name="description" content="Personal notebook 2 of Programming Massively Parallel">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch2/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="PMPP Learning-Chapter 2 Heterogeneous Data Parallel">
  <meta property="og:description" content="Personal notebook 2 of Programming Massively Parallel">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2024-09-03T22:48:12+08:00">
    <meta property="article:modified_time" content="2025-06-07T23:40:58+08:00">
    <meta property="article:tag" content="PMPP Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PMPP Learning-Chapter 2 Heterogeneous Data Parallel">
<meta name="twitter:description" content="Personal notebook 2 of Programming Massively Parallel">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Course Learning",
      "item": "http://localhost:1313/blogs/courselearning/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Programming Massive Parallel",
      "item": "http://localhost:1313/blogs/courselearning/pmpp/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "PMPP Learning-Chapter 2 Heterogeneous Data Parallel",
      "item": "http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "PMPP Learning-Chapter 2 Heterogeneous Data Parallel",
  "name": "PMPP Learning-Chapter 2 Heterogeneous Data Parallel",
  "description": "Personal notebook 2 of Programming Massively Parallel",
  "keywords": [
    "CUDA"
  ],
  "articleBody": "2 Heterogeneous Data Parallel Computing 数据并行 (Data Parallel) 是指在数据集的不同部分上执行的计算工作可以彼此独立地完成，从而可以并行执行的现象。\n2.1 Data Parallel 在图像处理中，将彩色像素转换为灰度只需要该像素的数据。模糊图像将每个像素的颜色与附近像素的颜色平均，只需要像素的小邻域的数据。即使是一个看似全局的操作，比如找到图像中所有像素的平均亮度，也可以分解成许多可以独立执行的较小的计算。这种对不同数据块的独立计算是数据并行性的基础。 为了将彩色图像转换为灰度图像，我们通过以下加权和公式计算每个像素的亮度值L. 这些逐像素计算都不依赖于彼此，都可以独立执行。显然，彩色图到灰度图的转换具有大量的数据并行性。 $L=0.21r+0.72g+0.07b$\nTask Parallelism vs. Data Parallelism 数据并行并不是并行编程中使用的唯一类型的并行。任务并行 (Task Parallelism) 在并行编程中也得到了广泛的应用。任务并行性通常通过应用程序的任务分解来暴露。例如，一个简单的应用程序可能需要做一个向量加法和一个矩阵-向量乘法。每个都是一个任务。如果两个任务可以独立完成，则存在任务并行性。I/O和数据传输也是常见的任务。 Data Parallelsim in Image2Grayscale Conversion\n2.2 CUDA C Program Structure CUDA C 用最少的新语法和库函数扩展了流行的 ANSI C 语言。CUDA C 程序的结构反映了计算机中主机 (CPU) 和一个或多个设备 (GPU) 的共存。每个 CUDA C 源文件可以同时包含主机 (host) 代码和设备 (device) 代码。 CUDA程序的执行流程如下图所示。执行从主机代码 (CPU 串行代码) 开始，当调用内核函数 (kernel function) 时，会在设备上启动大量线程1来执行内核。由内核调用启动的所有线程统称为网格 (grid)。这些线程是 CUDA 并行执行的主要载体。\nExecution of a CUDA Program\n2.3 A vector addition kernel 使用向量加法来展示 CUDA C 程序结构。下面展示了一个简单的传统 C 程序，它由一个主函数和一个向量加法函数组成。\n当需要区分主机和设备数据时，我们都会在主机使用的变量名后面加上 “_h”，而在设备使用的变量名后面加上 “_d”.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // Compute vector sum h_C = h_A+h_B void vecAdd(float* h_A, float* h_B, float* h_C, int n) { for (int i = 0; i \u003c n; i++) h_C[i] = h_A[i] + h_B[i]; } int main() { // Memory allocation for h_A, h_B, and h_C // I/O to read h_A and h_B, N elements each // … vecAdd(h_A, h_B, h_C, N); } 并行执行向量加法的一种直接方法是修改 vecAdd 函数并将其计算移到设备上。修改后的结构如下所示。\nStructure of the Modified VecAdd\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include // … void vecAdd(float* A, float* B, float* C, int n) { int size = n* sizeof(float); float *d_A *d_B, *d_C; /* … 1. // Allocate device memory for A, B, and C // copy A and B to device memory 2. // Kernel launch code – to have the device // to perform the actual vector addition 3. // copy C from the device memory // Free device vectors */ } 2.4 Device Global Memory and Data Transfer 在当前的CUDA系统中，设备通常是带有自己的 DRAM 的硬件卡，称为 (设备)全局内存 (device global memory). 对于向量加法内核，在调用内核之前，程序员需要在设备全局内存中分配空间，并将数据从主机内存传输到设备全局内存中分配的空间。这对应于 1. 部分。类似地，在设备执行之后，程序员需要将结果数据从设备全局内存传输回主机内存，并释放设备全局内存中不再需要的已分配空间。这对应于 3. 部分。 cudaMalloc 函数可以从主机代码中调用，为对象分配一块设备全局内存。第一个参数是指针变量的地址，该变量将被设置为指向分配的对象。指针变量的地址应强制转换为 void**，这样可以允许 cudaMalloc 函数将分配内存的地址写入所提供的指针变量中，而不考虑其类型2。\n1 cudaError_t cudaMalloc(void** devPtr, size_t size); devPtr：指向指向设备内存的指针的指针。 size：要分配的内存大小（以字节为单位）。 cudaFree 函数通过释放设备内存并将其返回到可用内存池来管理设备内存资源。它只需要 A_d 的值来识别要释放的内存区域，而不需要改变 A_d 指针本身的地址。\n在主机代码中对设备全局内存指针进行解引用引用可能导致异常或其他类型的运行错误。\ncudaMemcpy 函数是 CUDA 中用于在主机内存和设备内存之间传输数据的核心函数。它允许将数据从主机内存复制到设备内存，或从设备内存复制到主机内存。\n1 cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind); dst：目标内存地址，可以是主机内存地址或设备内存地址。 src： 源内存地址，可以是主机内存地址或设备内存地址。 count： 要复制的数据大小（以字节为单位）。 kind： 复制方向，可以使用以下枚举值： cudaMemcpyHostToDevice：主机内存-\u003e设备内存。 cudaMemcpyDeviceToHost：设备内存-\u003e主机内存。 cudaMemcpyDeviceToDevice：设备内存-\u003e设备内存。 cudaMemcpyHostToHost：主机内存-\u003e主机内存 了解完这些后，可以更新代码的框架如下\nChecking and Handling in CUDA CUDA API 函数返回一个 cudaError_t 类型的标志，指示当它们处理请求时是否发生错误。 在 CUDA 运行时库的头文件 cuda_runtime.h 中，cudaError_t 被定义为一个 int 类型的别名\n1 typedef int cudaError_t; 一个例子如下\n1 2 3 4 5 6 7 8 // ... float *d_a; cudaError_t err = cudaMalloc(\u0026d_a, 1024 * sizeof(float)); if (err != cudaSuccess) { printf(\"cudaMalloc failed: %s\\n\", cudaGetErrorString(err)); return 1; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 void vecAdd(float* A, float* B, float* C, int n) { int size = n* sizeof(float); float *d_A *d_B, *d_C; cudaMalloc((void **) %d_A, size); cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice); cudaMalloc((void **) %d_B, size); cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice); cudaMalloc((void **) %d_C, size); // Kernel invocation code - to be shown later // ... cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost); // Free device memory for A, B, C cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); } 2.5 Kernel functions and threading 内核函数指所有线程在并行阶段执行的代码，网格中的所有线程执行相同的内核代码。。当程序的主机代码调用内核时，CUDA runtime 系统启动一个线程网格，这些线程被组织成一个两级层次结构。每个网格都被组织为线程块 (thread block, 简称为块) 数组。网格的所有块都是相同的大小。在调用内核时，每个线程块中的线程总数由主机代码指定。 同一个内核可以在主机代码的不同部分用不同数量的线程调用。对于给定的线程网格，一个块中的线程数可以在名为 blockDim 的内置变量中获得，它是一个具有三个无符号整数字段 (x, y, z) 的结构体。 下图给出了一个示例，其中每个块由256个线程组成。每个线程都用一个箭头表示，标有线程在块中的索引号的方框。由于数据是一维向量，因此每个线程块被组织为一维线程数组。blockDim.x 的值表示每个块中的线程总数。threadaIdx 变量表示每个线程在块中的坐标。全局索引 i 的计算公式为 i = blockIdx.x * blockDim.x + threadIdx.x\n许多编程语言都有内置变量。这些变量具有特殊的含义和目的。这些变量的值通常由运行时系统预先初始化，并且在程序中通常是只读的。\nHierarchical Organization in CUDA\n向量加法的核函数定义如下。网格中的每个线程对应于原始循环的一次迭代，这被称为循环并行 (loop parallel)，意为原始顺序代码的迭代由线程并行执行。addVecKernel 中有一个 if (i \u003c n) 语句，因为并非所有的向量长度都可以表示为块大小的倍数。\n1 2 3 4 5 6 __global__ void vecAddKernel(float* A, float* B, float* C, int n) { int i = blockDim.x * blockIdx.x + threadIdx.x; if(i \u003c n) C[i] = A[i] + B[i]; } CUDA C 使用了三个可以在函数声明中使用的限定字。下表展示了这些关键词的意义。\n__host__ 就是在主机上执行的传统 C 函数，只能从另一个主机函数调用。 __global__ 表示被声明的函数是 CUDA C 内核函数。内核函数在设备上执行，并且可以从主机上调用。 __device__ 函数在 CUDA 设备上执行，只能从内核函数或其他设备函数调用。 可以在函数声明中同时使用 __host__ 和 __device__. 编译系统会为同一个函数生成两个版本的目标代码。\nQualifier Keyword Callable From Executed on Executed by __host__ (default) Host Host Caller host thread __global__ Host/Device Device New grid of device thread __device__ Device Device Caller device thread 2.6 Calling kernel functions 实现内核函数之后，剩下的步骤是从主机代码调用该函数来启动网格。当主机代码调用内核时，它通过执行配置参数 (execution configuration parameters) 设置网格和线程块大小配置参数在在传统的C函数参数之前由 \u003c\u003c\u003c...\u003e\u003e\u003e 之间给出。第一个配置参数给出网格中的块数量。第二个参数指定每个块中的线程数。\n1 2 3 4 5 6 7 int vectAdd(float* A, float* B, float* C, int n) { // d_A, d_B, d_C allocations and copies omitted // ... // Run ceil(n/256) (or by (n + 256 - 1) / 256) blocks of 256 threads each vecAddKernel\u003c\u003c\u003cceil(n/256.0), 256\u003e\u003e\u003e(d_A, d_B, d_C, n); } 下面展示了 vecAdd 函数中的最终主机代码。所有的线程块操作向量的不同部分。它们可以按任意顺序执行。\n实际上，分配设备内存、从主机到设备的输入数据传输、从设备到主机的输出数据传输以及释放设备内存的开销可能会使生成的代码比原始顺序代码慢，这是因为内核完成的计算量相对于处理或传输的数据量来说很小。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 void vecAdd(float* A, float* B, float* C, int n) { int size = n * sizeof(float); float *d_A *d_B, *d_C; cudaMalloc(\u0026d_A, size); cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice); cudaMalloc(\u0026d_B, size); cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice); cudaMalloc(\u0026d_C, size); vecAddKernel\u003c\u003c\u003cceil(n/256.0), 256\u003e\u003e\u003e(d_A, d_B, d_C, n); cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost); // Free device memory for A, B, C cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); } 2.7 Compilation NVCC (NVIDIA C Compiler) 处理一个C处理一个CUDA C程序，使用 CUDA 关键字来分离主机代码和设备代码。\n主机代码是就是普通的ANSI C代码，使用 C/C++ 编译器进行编译，并作为传统的 CPU 进程运行。 设备代码及其相关辅助函数和数据结构的CUDA关键字，由NVCC编译成称为 PTX (Parallel Thread Execution) 文件的虚拟二进制文件, 由 NVCC runtime 组件进一步编译成目标文件，并在支持 cuda 的 GPU 设备上执行。 Overview of the Compilation Process of a CUDA C Program\n线程由程序的代码、正在执行的代码中的位置以及它的变量和数据结构的值组成。 ↩︎\ncudaMalloc 与 C 语言 malloc 函数的格式不同。前者接受两个参数，指针变量其地址作为第一个参数给出。后者只接受一个参数来指定分配对象的大小，返回一个指向分配对象的指针。 ↩︎\n",
  "wordCount" : "3466",
  "inLanguage": "en",
  "datePublished": "2024-09-03T22:48:12+08:00",
  "dateModified": "2025-06-07T23:40:58+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/courselearning/">Course Learning</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/courselearning/pmpp/">Programming Massive Parallel</a></div>
    <h1 class="post-title entry-hint-parent">
      PMPP Learning-Chapter 2 Heterogeneous Data Parallel
    </h1>
    <div class="post-description">
      Personal notebook 2 of Programming Massively Parallel
    </div>
    <div class="post-meta"><span title='2024-09-03 22:48:12 +0800 CST'>Sep-03-2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;3466 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#2-heterogeneous-data-parallel-computing" aria-label="2 Heterogeneous Data Parallel Computing">2 Heterogeneous Data Parallel Computing</a><ul>
                            
                    <li>
                        <a href="#21-data-parallel" aria-label="2.1 Data Parallel">2.1 Data Parallel</a></li>
                    <li>
                        <a href="#22-cuda-c-program-structure" aria-label="2.2 CUDA C Program Structure">2.2 CUDA C Program Structure</a></li>
                    <li>
                        <a href="#23-a-vector-addition-kernel" aria-label="2.3 A vector addition kernel">2.3 A vector addition kernel</a></li>
                    <li>
                        <a href="#24-device-global-memory-and-data-transfer" aria-label="2.4 Device Global Memory and Data Transfer">2.4 Device Global Memory and Data Transfer</a></li>
                    <li>
                        <a href="#25-kernel-functions-and-threading" aria-label="2.5 Kernel functions and threading">2.5 Kernel functions and threading</a></li>
                    <li>
                        <a href="#26-calling-kernel-functions" aria-label="2.6 Calling kernel functions">2.6 Calling kernel functions</a></li>
                    <li>
                        <a href="#27-compilation" aria-label="2.7 Compilation">2.7 Compilation</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="2-heterogeneous-data-parallel-computing">2 Heterogeneous Data Parallel Computing<a hidden class="anchor" aria-hidden="true" href="#2-heterogeneous-data-parallel-computing">#</a></h1>
<p>数据并行 (<em>Data Parallel</em>) 是指在数据集的不同部分上执行的计算工作可以彼此独立地完成，从而可以并行执行的现象。</p>
<h2 id="21-data-parallel">2.1 Data Parallel<a hidden class="anchor" aria-hidden="true" href="#21-data-parallel">#</a></h2>
<p>在图像处理中，将彩色像素转换为灰度只需要该像素的数据。模糊图像将每个像素的颜色与附近像素的颜色平均，只需要像素的小邻域的数据。即使是一个看似全局的操作，比如找到图像中所有像素的平均亮度，也可以分解成许多可以独立执行的较小的计算。这种对不同数据块的独立计算是数据并行性的基础。
为了将彩色图像转换为灰度图像，我们通过以下加权和公式计算每个像素的亮度值L. 这些逐像素计算都不依赖于彼此，都可以独立执行。显然，彩色图到灰度图的转换具有大量的数据并行性。
$L=0.21r+0.72g+0.07b$</p>
<details class="custom-details">
    <summary class="custom-summary">Task Parallelism vs. Data Parallelism</summary>
    <div>数据并行并不是并行编程中使用的唯一类型的并行。任务并行 (<em>Task Parallelism</em>) 在并行编程中也得到了广泛的应用。任务并行性通常通过应用程序的任务分解来暴露。例如，一个简单的应用程序可能需要做一个向量加法和一个矩阵-向量乘法。每个都是一个任务。如果两个任务可以独立完成，则存在任务并行性。I/O和数据传输也是常见的任务。</div>
</details><br>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB56d71b169d207ac51adc718f79fb006c?method=download&amp;shareKey=d97c1f60eb44182ac8bb99f8a81035fe" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB56d71b169d207ac51adc718f79fb006c?method=download&amp;shareKey=d97c1f60eb44182ac8bb99f8a81035fe" alt="Data Parallelsim in Image2Grayscale Conversion">
    </a><figcaption>Data Parallelsim in Image2Grayscale Conversion</figcaption></figure></p>
<h2 id="22-cuda-c-program-structure">2.2 CUDA C Program Structure<a hidden class="anchor" aria-hidden="true" href="#22-cuda-c-program-structure">#</a></h2>
<p>CUDA C 用最少的新语法和库函数扩展了流行的 ANSI C 语言。CUDA C 程序的结构反映了计算机中主机 (CPU) 和一个或多个设备 (GPU) 的共存。每个 CUDA C 源文件可以同时包含主机 (<em>host</em>) 代码和设备 (<em>device</em>) 代码。
CUDA程序的执行流程如下图所示。执行从主机代码 (CPU 串行代码) 开始，当调用内核函数 (<em>kernel function</em>) 时，会在设备上启动大量线程<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>来执行内核。由内核调用启动的所有线程统称为网格 (grid)。这些线程是 CUDA 并行执行的主要载体。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBcfe42671ed5897d29371195eb557fa00?method=download&amp;shareKey=2d501a2775f21e11292189f68d89e39a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBcfe42671ed5897d29371195eb557fa00?method=download&amp;shareKey=2d501a2775f21e11292189f68d89e39a" alt="Execution of a CUDA Program">
    </a><figcaption>Execution of a CUDA Program</figcaption></figure></p>
<h2 id="23-a-vector-addition-kernel">2.3 A vector addition kernel<a hidden class="anchor" aria-hidden="true" href="#23-a-vector-addition-kernel">#</a></h2>
<p>使用向量加法来展示 CUDA C 程序结构。下面展示了一个简单的传统 C 程序，它由一个主函数和一个向量加法函数组成。</p>
<p>当需要区分主机和设备数据时，我们都会在主机使用的变量名后面加上 “<code>_h</code>”，而在设备使用的变量名后面加上 “<code>_d</code>”.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Compute vector sum h_C = h_A+h_B
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">h_A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">h_B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">h_C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">h_C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">h_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Memory allocation for h_A, h_B, and h_C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// I/O to read h_A and h_B, N elements each
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// …
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">vecAdd</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">h_C</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>并行执行向量加法的一种直接方法是修改 <code>vecAdd</code> 函数并将其计算移到设备上。修改后的结构如下所示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1aadc269025d2f33b2bd42b7838c7cc3?method=download&amp;shareKey=04dc8aeb81948f9dd66f6dccb54e8bd5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1aadc269025d2f33b2bd42b7838c7cc3?method=download&amp;shareKey=04dc8aeb81948f9dd66f6dccb54e8bd5" alt="Structure of the Modified VecAdd">
    </a><figcaption>Structure of the Modified VecAdd</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// …
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span><span class="n">d_A</span> <span class="o">*</span><span class="n">d_B</span><span class="p">,</span> <span class="o">*</span><span class="n">d_C</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="cm">/*
</span></span></span><span class="line"><span class="cl"><span class="cm">    …
</span></span></span><span class="line"><span class="cl"><span class="cm">    1. // Allocate device memory for A, B, and C
</span></span></span><span class="line"><span class="cl"><span class="cm">       // copy A and B to device memory
</span></span></span><span class="line"><span class="cl"><span class="cm">    2. // Kernel launch code – to have the device
</span></span></span><span class="line"><span class="cl"><span class="cm">       // to perform the actual vector addition
</span></span></span><span class="line"><span class="cl"><span class="cm">    3. // copy C from the device memory
</span></span></span><span class="line"><span class="cl"><span class="cm">       // Free device vectors
</span></span></span><span class="line"><span class="cl"><span class="cm">    */</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="24-device-global-memory-and-data-transfer">2.4 Device Global Memory and Data Transfer<a hidden class="anchor" aria-hidden="true" href="#24-device-global-memory-and-data-transfer">#</a></h2>
<p>在当前的CUDA系统中，设备通常是带有自己的 DRAM 的硬件卡，称为 (设备)全局内存 (<em>device global memory</em>). 对于向量加法内核，在调用内核之前，程序员需要在设备全局内存中分配空间，并将数据从主机内存传输到设备全局内存中分配的空间。这对应于 1. 部分。类似地，在设备执行之后，程序员需要将结果数据从设备全局内存传输回主机内存，并释放设备全局内存中不再需要的已分配空间。这对应于 3. 部分。
<code>cudaMalloc</code> 函数可以从主机代码中调用，为对象分配一块设备全局内存。第一个参数是指针变量的地址，该变量将被设置为指向分配的对象。指针变量的地址应强制转换为 <code>void**</code>，这样可以允许 <code>cudaMalloc</code> 函数将分配内存的地址写入所提供的指针变量中，而不考虑其类型<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">cudaError_t</span> <span class="nf">cudaMalloc</span><span class="p">(</span><span class="kt">void</span><span class="o">**</span> <span class="n">devPtr</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">size</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>devPtr</code>：指向指向设备内存的指针的指针。</li>
<li><code>size</code>：要分配的内存大小（以字节为单位）。</li>
</ul>
<hr>
<p>cudaFree 函数通过释放设备内存并将其返回到可用内存池来管理设备内存资源。它只需要 A_d 的值来识别要释放的内存区域，而不需要改变 A_d 指针本身的地址。</p>
<p>在主机代码中对设备全局内存指针进行解引用引用可能导致异常或其他类型的运行错误。</p>
<p>cudaMemcpy 函数是 CUDA 中用于在主机内存和设备内存之间传输数据的核心函数。它允许将数据从主机内存复制到设备内存，或从设备内存复制到主机内存。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">cudaError_t</span> <span class="nf">cudaMemcpy</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span> <span class="n">dst</span><span class="p">,</span> <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">src</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">count</span><span class="p">,</span> <span class="n">cudaMemcpyKind</span> <span class="n">kind</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>dst</code>：目标内存地址，可以是主机内存地址或设备内存地址。</li>
<li><code>src</code>： 源内存地址，可以是主机内存地址或设备内存地址。</li>
<li><code>count</code>： 要复制的数据大小（以字节为单位）。</li>
<li><code>kind</code>： 复制方向，可以使用<a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g18fa99055ee694244a270e4d5101e95b">以下枚举值</a>：
<ul>
<li><code>cudaMemcpyHostToDevice</code>：主机内存-&gt;设备内存。</li>
<li><code>cudaMemcpyDeviceToHost</code>：设备内存-&gt;主机内存。</li>
<li><code>cudaMemcpyDeviceToDevice</code>：设备内存-&gt;设备内存。</li>
<li><code>cudaMemcpyHostToHost</code>：主机内存-&gt;主机内存</li>
</ul>
</li>
</ul>
<p>了解完这些后，可以更新代码的框架如下</p>
<details class="custom-details">
    <summary class="custom-summary">Checking and Handling in CUDA</summary>
    <div><p>CUDA API 函数返回一个 <code>cudaError_t</code> 类型的标志，指示当它们处理请求时是否发生错误。
在 CUDA 运行时库的头文件 cuda_runtime.h 中，cudaError_t 被定义为一个 int 类型的别名</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="kt">int</span> <span class="n">cudaError_t</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>一个例子如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="o">*</span><span class="n">d_a</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span> <span class="mi">1024</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">printf</span><span class="p">(</span><span class="s">&#34;cudaMalloc failed: %s</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span><span class="n">d_A</span> <span class="o">*</span><span class="n">d_B</span><span class="p">,</span> <span class="o">*</span><span class="n">d_C</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span> <span class="o">%</span><span class="n">d_A</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">h_A</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span> <span class="o">%</span><span class="n">d_B</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_B</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span> <span class="o">%</span><span class="n">d_C</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Kernel invocation code - to be shown later
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_C</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Free device memory for A, B, C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_A</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_B</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_C</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="25-kernel-functions-and-threading">2.5 Kernel functions and threading<a hidden class="anchor" aria-hidden="true" href="#25-kernel-functions-and-threading">#</a></h2>
<p>内核函数指所有线程在并行阶段执行的代码，<strong>网格中的所有线程执行相同的内核代码</strong>。。当程序的主机代码调用内核时，CUDA runtime 系统启动一个线程网格，这些线程被组织成一个两级层次结构。每个网格都被组织为线程块 (<em>thread block</em>, 简称为块) 数组。网格的所有块都是相同的大小。在调用内核时，每个线程块中的线程总数由主机代码指定。
同一个内核可以在主机代码的不同部分用不同数量的线程调用。对于给定的线程网格，一个块中的线程数可以在名为 <code>blockDim</code> 的内置变量中获得，它是一个具有三个无符号整数字段 <code>(x, y, z)</code> 的结构体。
下图给出了一个示例，其中每个块由256个线程组成。每个线程都用一个箭头表示，标有线程在块中的索引号的方框。由于数据是一维向量，因此每个线程块被组织为一维线程数组。<code>blockDim.x</code> 的值表示每个块中的线程总数。<code>threadaIdx</code> 变量表示每个线程在块中的坐标。全局索引 i 的计算公式为 <code>i = blockIdx.x * blockDim.x + threadIdx.x</code></p>
<p>许多编程语言都有内置变量。这些变量具有特殊的含义和目的。这些变量的值通常由运行时系统预先初始化，并且在程序中通常是只读的。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB67f8186e554926a97be5d005a8c86056?method=download&amp;shareKey=55b9fec27854c9abf7e06eaff5c5a612" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB67f8186e554926a97be5d005a8c86056?method=download&amp;shareKey=55b9fec27854c9abf7e06eaff5c5a612" alt="Hierarchical Organization in CUDA">
    </a><figcaption>Hierarchical Organization in CUDA</figcaption></figure></p>
<p>向量加法的核函数定义如下。网格中的每个线程对应于原始循环的一次迭代，这被称为循环并行 (<em>loop parallel</em>)，意为原始顺序代码的迭代由线程并行执行。<code>addVecKernel</code> 中有一个 <code>if (i &lt; n)</code> 语句，因为并非所有的向量长度都可以表示为块大小的倍数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">vecAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>CUDA C 使用了三个可以在函数声明中使用的限定字。下表展示了这些关键词的意义。</p>
<ul>
<li><code>__host__ </code> 就是在主机上执行的传统 C 函数，只能从另一个主机函数调用。</li>
<li><code>__global__</code> 表示被声明的函数是 CUDA C 内核函数。内核函数在设备上执行，并且可以从主机上调用。</li>
<li><code>__device__</code> 函数在 CUDA 设备上执行，只能从内核函数或其他设备函数调用。</li>
</ul>
<p>可以在函数声明中同时使用 <code>__host__</code>  和 <code>__device__</code>. 编译系统会为同一个函数生成两个版本的目标代码。</p>
<table>
  <thead>
      <tr>
          <th>Qualifier Keyword</th>
          <th>Callable From</th>
          <th>Executed on</th>
          <th>Executed by</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>__host__ </code>(default)</td>
          <td>Host</td>
          <td>Host</td>
          <td>Caller host thread</td>
      </tr>
      <tr>
          <td><code>__global__</code></td>
          <td>Host/Device</td>
          <td>Device</td>
          <td>New grid of device thread</td>
      </tr>
      <tr>
          <td><code>__device__</code></td>
          <td>Device</td>
          <td>Device</td>
          <td>Caller device thread</td>
      </tr>
  </tbody>
</table>
<h2 id="26-calling-kernel-functions">2.6 Calling kernel functions<a hidden class="anchor" aria-hidden="true" href="#26-calling-kernel-functions">#</a></h2>
<p>实现内核函数之后，剩下的步骤是从主机代码调用该函数来启动网格。当主机代码调用内核时，它通过执行配置参数 (<em>execution configuration parameters</em>) 设置网格和线程块大小配置参数在在传统的C函数参数之前由 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> 之间给出。第一个配置参数给出网格中的块数量。第二个参数指定每个块中的线程数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">vectAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// d_A, d_B, d_C allocations and copies omitted
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Run ceil(n/256) (or by (n + 256 - 1) / 256) blocks of 256 threads each 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">vecAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">ceil</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">256.0</span><span class="p">),</span> <span class="mi">256</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面展示了 <code>vecAdd</code> 函数中的最终主机代码。所有的线程块操作向量的不同部分。它们可以按任意顺序执行。</p>
<blockquote>
<p>实际上，分配设备内存、从主机到设备的输入数据传输、从设备到主机的输出数据传输以及释放设备内存的开销可能会使生成的代码比原始顺序代码慢，这是因为内核完成的计算量相对于处理或传输的数据量来说很小。</p></blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span><span class="n">d_A</span> <span class="o">*</span><span class="n">d_B</span><span class="p">,</span> <span class="o">*</span><span class="n">d_C</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_A</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">h_A</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_B</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_B</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_C</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">vecAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">ceil</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">256.0</span><span class="p">),</span> <span class="mi">256</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_C</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Free device memory for A, B, C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_A</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_B</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_C</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="27-compilation">2.7 Compilation<a hidden class="anchor" aria-hidden="true" href="#27-compilation">#</a></h2>
<p>NVCC (NVIDIA C Compiler) 处理一个C处理一个CUDA C程序，使用 CUDA 关键字来分离主机代码和设备代码。</p>
<ul>
<li>主机代码是就是普通的ANSI C代码，使用 C/C++ 编译器进行编译，并作为传统的 CPU 进程运行。</li>
<li>设备代码及其相关辅助函数和数据结构的CUDA关键字，由NVCC编译成称为 PTX (Parallel Thread Execution) 文件的虚拟二进制文件, 由 NVCC runtime 组件进一步编译成目标文件，并在支持 cuda 的 GPU 设备上执行。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB0a32f3aa7a8ffb0fbf51b81c298fcc26?method=download&amp;shareKey=9eb69ac9f65a39b57002dcb02da3a39c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB0a32f3aa7a8ffb0fbf51b81c298fcc26?method=download&amp;shareKey=9eb69ac9f65a39b57002dcb02da3a39c" alt="Overview of the Compilation Process of a CUDA C Program">
    </a><figcaption>Overview of the Compilation Process of a CUDA C Program</figcaption></figure></p>
<hr>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>线程由程序的代码、正在执行的代码中的位置以及它的变量和数据结构的值组成。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><code>cudaMalloc</code> 与 C 语言 <code>malloc</code> 函数的格式不同。前者接受两个参数，指针变量其地址作为第一个参数给出。后者只接受一个参数来指定分配对象的大小，返回一个指向分配对象的指针。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/pmpp-learning/">PMPP Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch3/">
    <span class="title">« Prev</span>
    <br>
    <span>PMPP Learning-Chapter 3 Multidimensional Grids and Data</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch1/">
    <span class="title">Next »</span>
    <br>
    <span>PMPP Learning-Chapter 1 Introduction</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PMPP Learning-Chapter 2 Heterogeneous Data Parallel on x"
            href="https://x.com/intent/tweet/?text=PMPP%20Learning-Chapter%202%20Heterogeneous%20Data%20Parallel&amp;url=http%3a%2f%2flocalhost%3a1313%2fblogs%2fcourselearning%2fpmpp%2fpmpp-ch2%2f&amp;hashtags=PMPPlearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PMPP Learning-Chapter 2 Heterogeneous Data Parallel on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fblogs%2fcourselearning%2fpmpp%2fpmpp-ch2%2f&amp;title=PMPP%20Learning-Chapter%202%20Heterogeneous%20Data%20Parallel&amp;summary=PMPP%20Learning-Chapter%202%20Heterogeneous%20Data%20Parallel&amp;source=http%3a%2f%2flocalhost%3a1313%2fblogs%2fcourselearning%2fpmpp%2fpmpp-ch2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PMPP Learning-Chapter 2 Heterogeneous Data Parallel on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fblogs%2fcourselearning%2fpmpp%2fpmpp-ch2%2f&title=PMPP%20Learning-Chapter%202%20Heterogeneous%20Data%20Parallel">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PMPP Learning-Chapter 2 Heterogeneous Data Parallel on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fblogs%2fcourselearning%2fpmpp%2fpmpp-ch2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share PMPP Learning-Chapter 2 Heterogeneous Data Parallel on telegram"
            href="https://telegram.me/share/url?text=PMPP%20Learning-Chapter%202%20Heterogeneous%20Data%20Parallel&amp;url=http%3a%2f%2flocalhost%3a1313%2fblogs%2fcourselearning%2fpmpp%2fpmpp-ch2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
