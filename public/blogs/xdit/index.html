<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>xDiT Principle | WITHER</title>
<meta name="keywords" content="xDiT">
<meta name="description" content="This is a brief introduction to the xDiT Principle.">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/xdit/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/xdit/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/xdit/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="xDiT Principle">
  <meta property="og:description" content="This is a brief introduction to the xDiT Principle.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-06-07T20:44:50+08:00">
    <meta property="article:modified_time" content="2025-06-07T23:40:58+08:00">
    <meta property="article:tag" content="XDiT">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="xDiT Principle">
<meta name="twitter:description" content="This is a brief introduction to the xDiT Principle.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "xDiT Principle",
      "item": "http://localhost:1313/blogs/xdit/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "xDiT Principle",
  "name": "xDiT Principle",
  "description": "This is a brief introduction to the xDiT Principle.",
  "keywords": [
    "xDiT"
  ],
  "articleBody": "Parse Config Arguments ä¼šä»å‘½ä»¤è¡Œå‚æ•°ä¸­è·å–æœ‰å…³ Model, Runtime, Parallel Processing \u0026 Input æœ‰å…³çš„ä¿¡æ¯ã€‚å‰ä¸‰è€…è¢«åŒ…å«åœ¨ engine_config ä¸­ï¼Œè€Œæœ€åè€…åˆ™è¢«åŒ…å«åœ¨ input_config ä¸­ã€‚åœ¨ create_config() å‡½æ•°ä¸­ï¼Œä¼šåˆå§‹åŒ– _WORLD å…¨å±€å˜é‡ï¼Œå®ƒæ˜¯ä¸€ä¸ª GroupCoordinator å®ä¾‹ã€‚å¾ˆæ˜æ˜¾å®ƒåªæœ‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰çš„è®¾å¤‡è¿›ç¨‹ç»„ã€‚ GroupCoordinator GroupCoordinator ç±»æ˜¯ä¸€ä¸ª PyTorch çš„è¿›ç¨‹ç»„å°è£…å™¨ï¼Œä¸»è¦ç”¨äºç®¡ç†ä¸€ç»„è¿›ç¨‹ä¹‹é—´çš„é€šä¿¡ã€‚å®ƒå¯ä»¥æ ¹æ®ä¸åŒçš„é€šä¿¡åç«¯ï¼ˆå¦‚ NCCLã€Glooã€MPI ç­‰ï¼‰æ¥åè°ƒè¿›ç¨‹ä¹‹é—´çš„æ“ä½œã€‚åŒ…å«ä»¥ä¸‹ä¿¡æ¯\nrank: å½“å‰è¿›ç¨‹çš„å…¨å±€ç´¢å¼•ï¼ˆå…¨å±€å”¯ä¸€ï¼‰ã€‚ ranks: ç»„å†…æ‰€æœ‰è¿›ç¨‹çš„å…¨å±€ç´¢å¼•åˆ—è¡¨ã€‚ world_size: ç»„çš„å¤§å°ï¼Œå³è¿›ç¨‹çš„æ•°é‡ len(ranks) local_rank: å½“å‰è¿›ç¨‹åœ¨æœ¬åœ°èŠ‚ç‚¹ä¸­çš„ç´¢å¼•ã€‚ rank_in_group: å½“å‰è¿›ç¨‹åœ¨ç»„å†…çš„ç´¢å¼•ã€‚ cpu_group: ç”¨äº CPU é€šä¿¡çš„è¿›ç¨‹ç»„ã€‚ device_group: ç”¨äºè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰é€šä¿¡çš„è¿›ç¨‹ç»„ã€‚ 1 2 3 4 5 6 if we have a group of size 4 across two nodes: Process | Node | Rank | Local Rank | Rank in Group 0 | 0 | 0 | 0 | 0 1 | 0 | 1 | 1 | 1 2 | 1 | 2 | 0 | 2 3 | 1 | 3 | 1 | 3 __init__ æ–¹æ³•æ¥æ”¶ä»¥ä¸‹å‚æ•°ï¼š\ngroup_ranks: ä¸€ä¸ªåŒ…å«å¤šä¸ªè¿›ç¨‹ç´¢å¼•åˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨è¡¨ç¤ºä¸€ä¸ªè¿›ç¨‹ç»„ã€‚ local_rank: å½“å‰è¿›ç¨‹çš„æœ¬åœ°ç´¢å¼•ã€‚ torch_distributed_backend: æŒ‡å®šç”¨äºé€šä¿¡çš„åç«¯ç±»å‹ (å¦‚ â€œglooâ€ æˆ– â€œncclâ€). åˆå§‹åŒ–è¿‡ç¨‹ï¼š\nä½¿ç”¨ torch.distributed.get_rank() è·å–å½“å‰è¿›ç¨‹çš„å…¨å±€ç´¢å¼•ã€‚ éå†ä¼ å…¥çš„ group_ranks åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªå­åˆ—è¡¨åˆ›å»ºä¸€ä¸ªæ–°çš„è®¾å¤‡ç»„å’Œ CPU ç»„ã€‚ å¦‚æœå½“å‰è¿›ç¨‹çš„ç´¢å¼•åœ¨å½“å‰å­åˆ—è¡¨ä¸­ï¼Œåˆ™è®¾ç½®è¯¥è¿›ç¨‹çš„ç»„å†…ä¿¡æ¯ (åŒ…æ‹¬ ranksã€world_size å’Œ rank_in_group). ç¡®ä¿ CPU ç»„å’Œè®¾å¤‡ç»„éƒ½å·²æˆåŠŸåˆ›å»ºã€‚ æ ¹æ®æ˜¯å¦å¯ç”¨ CUDA è®¾ç½®å½“å‰è®¾å¤‡ä¸º GPU æˆ– CPU. 1 2 3 4 5 6 def main(): parser = FlexibleArgumentParser(description=\"xFuser Arguments\") args = xFuserArgs.add_cli_args(parser).parse_args() # Add Command Line Interface (CLI) arguments engine_args = xFuserArgs.from_cli_args(args) # Extract CLI args and pass them to xFuserArgs Constructor engine_config, input_config = engine_args.create_config() # Init _WORLD. engine_config: model, run_time \u0026 parallel infos, input_config: input shape, prompt \u0026 sampler infos local_rank = get_world_group().local_rank å…³äºå¯ä»¥æ”¯æŒçš„å¹¶è¡Œç­–ç•¥å¦‚ä¸‹ï¼ŒåŒ…æ‹¬ Data Parallel, Sequence Parallel, Pipefusion Parallel \u0026 Tensor Parallel.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Parallel Processing Options: --use_cfg_parallel Use split batch in classifier_free_guidance. cfg_degree will be 2 if set --data_parallel_degree DATA_PARALLEL_DEGREE Data parallel degree. --ulysses_degree ULYSSES_DEGREE Ulysses sequence parallel degree. Used in attention layer. --ring_degree RING_DEGREE Ring sequence parallel degree. Used in attention layer. --pipefusion_parallel_degree PIPEFUSION_PARALLEL_DEGREE Pipefusion parallel degree. Indicates the number of pipeline stages. --num_pipeline_patch NUM_PIPELINE_PATCH Number of patches the feature map should be segmented in pipefusion parallel. --attn_layer_num_for_pp [ATTN_LAYER_NUM_FOR_PP ...] List representing the number of layers per stage of the pipeline in pipefusion parallel --tensor_parallel_degree TENSOR_PARALLEL_DEGREE Tensor parallel degree. --split_scheme SPLIT_SCHEME Split scheme for tensor parallel. ä» CLI è§£æçš„å‚æ•°åä¼šåœ¨ create_config() ä¸­ç»„æˆå¦‚ä¸‹çš„ ParallelConfig.\nDataParallelConfig: æ€»çš„å¹¶è¡Œåº¦ä¸º dp_degree * cfg_degree. dp_degree: ç›¸å½“äºå¯¹ batch ç»´åº¦è¿›è¡Œåˆ‡åˆ†ï¼Œ cfg_degree: Class-free Guidance(cfg) ç”¨äºæ§åˆ¶æ— æ¡ä»¶çš„å›¾ç‰‡ç”Ÿæˆ (è‹¥ä½¿ç”¨ç›¸å½“äº batchsize *= 2). SequenceParallelConfig: æ€»çš„å¹¶è¡Œåº¦ä¸º sp_degree = ulysses_degree * ring_degree ulysses_degree: ç”¨äºæ§åˆ¶ DeepSeed-Ulesses çš„åºåˆ—å¹¶è¡Œåº¦ã€‚ ring_degree: ç”¨äºæ§åˆ¶è®¡ç®— Ring Attention æ—¶å¯¹ Q K V æ²¿ç€ Sequence ç»´åº¦çš„åˆ‡åˆ†å—æ•°ã€‚ TensorParallelConfig: æ€»çš„å¹¶è¡Œåº¦ä¸º tp_degree. tp_degree: ç”¨äºæ§åˆ¶ 2D Tensor Parallel çš„å¹¶è¡Œåº¦ã€‚ split_scheme: ç”¨äºæ§åˆ¶å¼ é‡åˆ‡åˆ†æ–¹å¼. PipeFusionParallelConfig: æ€»çš„å¹¶è¡Œåº¦ä¸º pp_degree=num_pipeline_patch. pp_degree: ç”¨äºæ§åˆ¶ PipeFusion ä¸­æ¨¡å‹ Transoformer Blocks çš„åˆ‡åˆ†ä¸ªæ•°ã€‚ num_pipeline_patch: ç”¨äºæ§åˆ¶å¯¹ latent feature map çš„åˆ‡åˆ†å—æ•°. attn_layer_num_for_pp: æ˜¯ä¸€ä¸ª listï¼Œè¡¨ç¤º pp_degree é‡Œæ¯ä¸ª stage çš„ Transformer å±‚æ•°ã€‚ Warning\nå…³äº PipeFusionï¼ŒåŸæ–‡è¯´åˆ‡åˆ†çš„ patch æ•°å’Œ pipeline å¤§å°å¯ä»¥ä¸åŒï¼Œä½†è¿™é‡Œè¦æ±‚ len(attn_layer_num_for_pp)=pp_degree\nInfo\nè®¾å¤‡æ•°å¿…é¡»ç­‰äº dp_degree * cfg_degree * sp_degree * tp_degree * num_pipeline_patchï¼Œå¹¶ä¸” pp_degree å¿…é¡»å°äºç­‰äºè®¾å¤‡æ•°ã€‚ ulysses_degree å¿…é¡»è¦å¤§äºä¸”èƒ½è¢« attention çš„å¤´æ•°æ•´é™¤ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 parallel_config = ParallelConfig( dp_config=DataParallelConfig( dp_degree=self.data_parallel_degree, use_cfg_parallel=self.use_cfg_parallel, ), sp_config=SequenceParallelConfig( ulysses_degree=self.ulysses_degree, ring_degree=self.ring_degree, ), tp_config=TensorParallelConfig( tp_degree=self.tensor_parallel_degree, split_scheme=self.split_scheme, ), pp_config=PipeFusionParallelConfig( pp_degree=self.pipefusion_parallel_degree, num_pipeline_patch=self.num_pipeline_patch, attn_layer_num_for_pp=self.attn_layer_num_for_pp, ), ) Construct Pipeline è§£æå®Œé…ç½®å‚æ•°å¹¶æ„å»ºäº† engine_config åï¼Œä¸‹ä¸€æ­¥æ˜¯æ„å»ºæ¨¡å‹çš„ pipeline.\n1 2 3 4 5 6 pipe = xFuserPixArtAlphaPipeline.from_pretrained( # First construct a PixArtAlphaPipeline, then pass it and engine_config to xFuserPipelineBaseWrapper pretrained_model_name_or_path=engine_config.model_config.model, engine_config=engine_config, torch_dtype=torch.float16, ).to(f\"cuda:{local_rank}\") pipe.prepare_run(input_config) xFuserPixArtAlphaPipeline ç»§æ‰¿è‡ª xFuserPipelineBaseWrapperï¼Œ_init_runtime_state å‡½æ•°ç»è¿‡ä¸€ç•ªè°ƒç”¨åä¼šä½¿ç”¨ initialize_model_parallel åˆå§‹åŒ– _RUNTIME æœ‰å…³æ¨¡å‹å‚æ•°çš„éƒ¨åˆ†å’Œæ¨¡å‹å¹¶è¡Œçš„å…¨å±€å˜é‡ _DP, _CFG, _PP, _SP, _TPï¼Œå®ƒæ˜¯ä¸€ä¸ª DiTRuntimeState (ç»§æ‰¿ RuntimeState) å®ä¾‹ï¼Œè®°å½•äº†æ¯ä¸ª Group åŒ…å«çš„è®¾å¤‡ç´¢å¼•ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜åŒ…æ‹¬ PipeFusionParallel ä¸­æœ‰å…³ patch ç´¢å¼•çš„å‚æ•° (åœ¨ç¨å pipeline æ‰§è¡Œçš„æ—¶å€™è®¡ç®—).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class xFuserPipelineBaseWrapper(xFuserBaseWrapper, metaclass=ABCMeta): def __init__( self, pipeline: DiffusionPipeline, engine_config: EngineConfig, ): self.module: DiffusionPipeline self._init_runtime_state(pipeline=pipeline, engine_config=engine_config) # backbone transformer = getattr(pipeline, \"transformer\", None) unet = getattr(pipeline, \"unet\", None) # vae vae = getattr(pipeline, \"vae\", None) # scheduler scheduler = getattr(pipeline, \"scheduler\", None) if transformer is not None: pipeline.transformer = self._convert_transformer_backbone(transformer) elif unet is not None: pipeline.unet = self._convert_unet_backbone(unet) if scheduler is not None: pipeline.scheduler = self._convert_scheduler(scheduler) super().__init__(module=pipeline) def _convert_transformer_backbone( self, transformer: nn.Module, ): #... logger.info(\"Transformer backbone found, paralleling transformer...\") wrapper = **xFuserTransformerWrappersRegister.get_wrapper(transformer)** transformer = wrapper(transformer=transformer) return transformer initialize_model_parallel è¯¥å‡½æ•°ä¸­ä¼šåˆå§‹åŒ–ä¸€ä¸ª RankGeneratorï¼Œå®ƒæ¥æ”¶æ¯ä¸ªå¹¶è¡Œæ–¹æ³•çš„è®¾å¤‡ç»„å¤§å°å’Œå¹¶è¡Œåº¦å¤§å°é¡ºåºã€‚å…¶ä¸»è¦çš„æ–¹æ³•æ˜¯é€šè¿‡ generate_masked_orthogonal_rank_groups å‡½æ•°ç¡®å®šæ¯ä¸ªå¹¶è¡Œç»„ç”±åŒ…å«å“ªäº›è®¾å¤‡ï¼Œå…ˆæŠŠå¹¶è¡Œæ–¹æ³•æŒ‰ç…§å¹¶è¡Œåº¦ä»å°åˆ°å¤§æ’åˆ—æˆ tp-sp-pp-cfg-dp. å†æ ¹æ®è¦ç”Ÿæˆçš„å¹¶è¡Œç»„äº§ç”Ÿå¯¹åº”çš„ mask. å³å¦‚æœè¦ç”Ÿæˆ pp ç»„å¯¹åº”çš„ rankï¼Œé‚£ä¹ˆ mask = [0, 0, 1, 0, 0]\nè¯¥å‡½æ•°é¦–å…ˆä¼šç”Ÿæˆéœ€è¦ç”Ÿæˆçš„å¹¶è¡Œç»„çš„å¤§å°ç»„æˆçš„ masked_shape å’Œä¸éœ€è¦ç”Ÿæˆçš„ unmasked_shape. é¦–å…ˆè¦ç”¨ prefix_product è®¡ç®— global_strideï¼Œå³æ¯ä¸ªå¹¶è¡Œåº¦çš„è®¾å¤‡ç»„åŒ…å«å‡ ä¸ªè®¾å¤‡ã€‚å†æ ¹æ® mask å–å‡ºå¯¹åº”çš„ mask_stride å’Œ unmaskd_stride. group_size = mask_stride[-1] å³ä¸ºæœ€å¤§å¹¶è¡Œåº¦çš„ç»„åŒ…å«çš„è®¾å¤‡æ•°ã€‚num_of_group = num_of_device / mask_stride[-1] å³ä¸ºè¦ç”Ÿæˆå‡ ä¸ªå¹¶è¡Œåº¦æœ€å¤§çš„ç»„ã€‚å…ˆéå†è¦ç”Ÿæˆçš„æ¯ä¸ªè®¾å¤‡ç»„ï¼Œå¹¶ç”¨ decompose å‡½æ•°ç¡®å®šè¯¥è®¾å¤‡ç»„åœ¨ä¸éœ€è¦å¹¶è¡Œç»´åº¦ä¸Šçš„ç´¢å¼•ï¼›å†éå†è¯¥ç»„ä¸­çš„æ¯ä¸ªè®¾å¤‡çš„ lock rankï¼Œç¡®å®šè¯¥è®¾å¤‡åœ¨éœ€è¦å¹¶è¡Œç»´åº¦ä¸Šçš„ç´¢å¼•ï¼Œæœ€åç”¨ inner_product ç¡®å®šè¯¥è®¾å¤‡çš„ global rank.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def generate_masked_orthogonal_rank_groups( world_size: int, parallel_size: List[int], mask: List[bool] ) -\u003e List[List[int]]: def prefix_product(a: List[int], init=1) -\u003e List[int]: # Exclusive r = [init] for v in a: init = init * v r.append(init) return r def inner_product(a: List[int], b: List[int]) -\u003e int: return sum([x * y for x, y in zip(a, b)]) def decompose(index, shape, stride=None): # index: ç¬¬å‡ ä¸ªå¹¶è¡Œç»„ # shape: å¹¶è¡Œç»„å¤§å°çš„ list \"\"\" This function solve the math problem below: There is an equation: index = sum(idx[i] * stride[i]) And given the value of index, stride. Return the idx. This function will used to get the pp/dp/pp_rank from group_index and rank_in_group. \"\"\" if stride is None: stride = prefix_product(shape) idx = [(index // d) % s for s, d in zip(shape, stride)] # è®¡ç®—åœ¨æ¯ä¸ªå¹¶è¡Œç»´åº¦ä¸Šçš„ç´¢å¼• # stride is a prefix_product result. And the value of stride[-1] # is not used. assert ( sum([x * y for x, y in zip(idx, stride[:-1])]) == index ), \"idx {} with shape {} mismatch the return idx {}\".format(index, shape, idx) return idx masked_shape = [s for s, m in zip(parallel_size, mask) if m] # éœ€è¦é‡‡å–å¹¶è¡Œçš„ç»´åº¦ unmasked_shape = [s for s, m in zip(parallel_size, mask) if not m] # ä¸éœ€è¦çš„ global_stride = prefix_product(parallel_size) # exclusive å‰ç¼€ç§¯ è¡¨ç¤ºå¤§çš„å¹¶è¡Œç»´åº¦åŒ…æ‹¬å‡ ä¸ªè®¾å¤‡ masked_stride = [d for d, m in zip(global_stride, mask) if m] unmasked_stride = [d for d, m in zip(global_stride, mask) if not m] group_size = prefix_product(masked_shape)[-1] # æœ€å¤§çš„ä¸€ä¸ªå¹¶è¡Œç»´åº¦åŒ…æ‹¬å‡ ä¸ªè®¾å¤‡ num_of_group = world_size // group_size # åˆ†æˆå‡ ä¸ªå¤§ç»„ ranks = [] for group_index in range(num_of_group): # éå†æ¯ä¸ªè®¾å¤‡ç»„ # get indices from unmaksed for group_index. decomposed_group_idx = decompose(group_index, unmasked_shape) # å¾—åˆ°åœ¨ä¸éœ€è¦é‡‡å–å¹¶è¡Œçš„ç»´åº¦ä¸Šçš„ç´¢å¼• rank = [] for rank_in_group in range(group_size): # éå†è¯¥ç»„ä¸­çš„æ¯ä¸ªè®¾å¤‡ local rank # get indices from masked for rank_in_group. decomposed_rank_idx = decompose(rank_in_group, masked_shape) # å¾—åˆ°æœ€å¤§å¹¶è¡Œç»„çš„æ¯ä¸ªè®¾å¤‡åœ¨é‡‡å–å¹¶è¡Œçš„ç»´åº¦ä¸Šçš„ç´¢å¼• rank.append( // ç›¸åŠ å¾—åˆ°å…¨å±€rank inner_product(decomposed_rank_idx, masked_stride) + inner_product(decomposed_group_idx, unmasked_stride) ) ranks.append(rank) return ranks Hybrid Parallelsim Design xDiTæ”¯æŒå››ç§å¹¶è¡Œæ–¹å¼ï¼šPipeFusionã€Sequenceã€Data å’Œ CFG Parallelã€‚å…¶ä¸­ï¼ŒData å’Œ CFG Parallelåœ¨å›¾åƒé—´å¹¶è¡Œç›¸å¯¹ç®€å•ï¼Œè€Œ PipeFusionå’Œ Sequence åœ¨å›¾åƒå†…éƒ¨çš„ä¸åŒ Patch é—´å¹¶è¡Œåˆ™è¾ƒä¸ºå¤æ‚ã€‚èƒ½\nPipeFusion åˆ©ç”¨ Input Tempor Redundancyç‰¹ç‚¹ï¼Œä½¿ç”¨è¿‡æ—¶çš„ KVï¼ˆStale KVï¼‰è¿›è¡Œ Attention è®¡ç®—ï¼Œè¿™ä½¿å¾— PipeFusion æ— æ³•åƒå¤§å‹è¯­è¨€æ¨¡å‹é‚£æ ·è½»æ¾åœ°å®ç°å¹¶è¡Œç­–ç•¥çš„æ··åˆã€‚ä½¿ç”¨æ ‡å‡†çš„åºåˆ—å¹¶è¡Œæ¥å£ï¼Œå¦‚RingAttentionã€Ulyssesæˆ– USPï¼Œæ— æ³•æ»¡è¶³ SP ä¸PipeFusionæ··åˆå¹¶è¡Œçš„éœ€æ±‚ã€‚\næˆ‘ä»¬å¯¹è¿™ä¸ªé—®é¢˜å…·ä½“è¯´æ˜ï¼Œä¸‹å›¾å±•ç¤ºäº†pipe_degree=4ï¼Œsp_degree=2çš„æ··åˆå¹¶è¡Œæ–¹æ³•ã€‚è®¾ç½® num_pipeline_patch=4ï¼Œå›¾ç‰‡åˆ‡åˆ†ä¸º M=num_pipeline_patch*sp_degree=8 ä¸ª Patchï¼Œåˆ†åˆ«æ˜¯ P0~P7.\nStandard SP Attention çš„è¾“å…¥Qï¼ŒKï¼ŒV å’Œè¾“å‡º O éƒ½æ˜¯æ²¿ç€åºåˆ—ç»´åº¦åˆ‡åˆ†ï¼Œä¸”åˆ‡åˆ†æ–¹å¼ä¸€è‡´ã€‚å¦‚æœä¸åŒ rank çš„è¾“å…¥ patch æ²¡æœ‰é‡å ï¼Œæ¯ä¸ª micro step è®¡ç®—å‡º fresh KV æ›´æ–°çš„ä½ç½®åœ¨ä¸åŒ rank é—´ä¹Ÿæ²¡æœ‰é‡å ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œstandard SP çš„ KV Buffer ä¸­é»„è‰²éƒ¨åˆ†æ˜¯ SP0 rank=0 æ‹¥æœ‰çš„ fresh KVï¼Œç»¿è‰²éƒ¨åˆ†æ˜¯ SP1 rank=1 æ‹¥æœ‰çš„fresh KVï¼ŒäºŒè€…å¹¶ä¸ç›¸åŒã€‚åœ¨è¿™ä¸ª diffusion step å†…ï¼Œdevice=0 æ— æ³•æ‹¿åˆ° P1,3,5,7 çš„ fresh KV è¿›è¡Œè®¡ç®—ï¼Œä½†æ˜¯ PipeFusion åˆ™éœ€è¦åœ¨ä¸‹ä¸€ä¸ª diffusion step ä¸­ï¼Œæ‹¥æœ‰ä¸Šä¸€ä¸ªdiffusion step å…¨éƒ¨çš„ KV. standard SP åªæ‹¥æœ‰ 1/sp_degree çš„ fresh kv bufferï¼Œå› æ­¤æ— æ³•è·å¾—æ··åˆå¹¶è¡Œæ¨ç†æ­£ç¡®çš„ç»“æœã€‚\nxDiTä¸“é—¨å®šåˆ¶äº†åºåˆ—å¹¶è¡Œçš„å®ç°æ–¹å¼ï¼Œä»¥é€‚åº”è¿™ç§æ··åˆå¹¶è¡Œçš„éœ€æ±‚ã€‚xDiTä½¿ç”¨ xFuserLongContextAttention æŠŠSPçš„ä¸­é—´ç»“æœå­˜åœ¨ KV Buffer å†…ã€‚æ•ˆæœå¦‚ä¸‹å›¾ï¼Œæ¯ä¸ª micro-step SP æ‰§è¡Œå®Œæ¯•åï¼ŒSP Group å†…ä¸åŒ rank è®¾å¤‡çš„ fresh KVæ˜¯ replicate çš„ã€‚è¿™æ ·ä¸€ä¸ª diffusion step åï¼ŒSP Group æ‰€æœ‰è®¾å¤‡çš„ KV Buffer éƒ½æ›´æ–°æˆæœ€æ–°ï¼Œä¾›ä¸‹ä¸€ä¸ª Diffusion Step ä½¿ç”¨ã€‚\nNote\nå‡è®¾ä¸€å…±æœ‰ 16 ä¸ª GPUï¼Œç´¢å¼•è¡¨ç¤ºä¸º g0 â€¦ g15ï¼Œå¹¶è¡Œæ–¹æ³•å’Œå¹¶è¡Œåº¦è®¾ç½®å¦‚ä¸‹\ndp_degree (2) * cfg_degree (2) * pp_degree (2) * sp_degree (2) = 16.\né‚£ä¹ˆä¸€å…±ä¼šåˆ›å»º 2 data parallel-groups, 8 CFG groups, 8 pipeline-parallel groups \u0026 8 sequence-parallel groups:\n2 data-parallel groups: [g0, g1, g2, g3, g4, g5, g6, g7], [g8, g9, g10, g11, g12, g13, g14, g15] 8 CFG-parallel groups: [g0, g4], [g1, g5], [g2, g6], [g3, g7], [g8, g12], [g9, g13], [g10, g14], [g11, g15] 8 pipeline-parallel groups: [g0, g2], [g4, g6], [g8, g10], [g12, g14], [g1, g3], [g5, g7], [g9, g11], [g13, g15] 8 sequence-parallel groups: [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15] Convert Model _split_transformer_blocks ä¼šå¯¹ transformer block è¿›è¡Œåˆ†é…ï¼Œå¦‚æœ parallel_config æŒ‡å®šäº† attn_layer_num_for_ppï¼Œå³å­˜æœ‰æ¯ä¸ª pipeFusion çš„è®¾å¤‡è¢«åˆ†é…çš„ transformer block æ•°é‡çš„åˆ—è¡¨ï¼ŒæŒ‰å…¶è¿›è¡Œåˆ†é…ï¼›å¦åˆ™å¹³å‡åˆ†ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def _split_transformer_blocks(self, transformer: nn.Module,): # omit # transformer layer split attn_layer_num_for_pp = ( # è·å–æ¯ä¸ª pipeFusion çš„è®¾å¤‡è¢«åˆ†é…çš„ transformer block æ•°é‡ get_runtime_state().parallel_config.pp_config.attn_layer_num_for_pp ) pp_rank = get_pipeline_parallel_rank() pp_world_size = get_pipeline_parallel_world_size() if attn_layer_num_for_pp is not None: if is_pipeline_first_stage(): transformer.transformer_blocks = transformer.transformer_blocks[ : attn_layer_num_for_pp[0]] else: transformer.transformer_blocks = transformer.transformer_blocks[sum(attn_layer_num_for_pp[: pp_rank - 1]) : sum(attn_layer_num_for_pp[:pp_rank])] else: # æ²¡æœ‰æŒ‡å®šåˆ™å¹³å‡åˆ† num_blocks_per_stage = (len(transformer.transformer_blocks) + pp_world_size - 1) // pp_world_size start_idx = pp_rank * num_blocks_per_stage end_idx = min((pp_rank + 1) * num_blocks_per_stage, len(transformer.transformer_blocks),) transformer.transformer_blocks = transformer.transformer_blocks[start_idx:end_idx] # position embedding if not is_pipeline_first_stage(): transformer.pos_embed = None if not is_pipeline_last_stage(): transformer.norm_out = None transformer.proj_out = None return transformer åŒæ—¶ä¹Ÿä¼š convert åŸå…ˆçš„ transformer backbone ä¸º xFuserPixArtTransformer2DWrapperï¼Œå…·ä½“è¡¨ç°ä¸ºåªæœ‰ pipeline çš„ç¬¬ä¸€é˜¶æ®µè¿›è¡Œ position embeddingï¼Œæœ€åä¸€é˜¶æ®µè¿›è¡Œ unpatchify å˜ä¸ºåŸæ¥çš„å›¾åƒå½¢çŠ¶ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @xFuserTransformerWrappersRegister.register(PixArtTransformer2DModel) class xFuserPixArtTransformer2DWrapper(xFuserTransformerBaseWrapper): def __init__( self, transformer: PixArtTransformer2DModel, ): super().__init__( transformer=transformer, submodule_classes_to_wrap=[nn.Conv2d, PatchEmbed], submodule_name_to_wrap=[\"attn1\"], ) @xFuserBaseWrapper.forward_check_condition def forward( self, hidden_states: torch.Tensor, encoder_hidden_states: Optional[torch.Tensor] = None, timestep: Optional[torch.LongTensor] = None, added_cond_kwargs: Dict[str, torch.Tensor] = None, cross_attention_kwargs: Dict[str, Any] = None, attention_mask: Optional[torch.Tensor] = None, encoder_attention_mask: Optional[torch.Tensor] = None, return_dict: bool = True, ): ''' ...... ''' height, width = self._get_patch_height_width() # * only pp rank 0 needs pos_embed (patchify) if is_pipeline_first_stage(): hidden_states = self.pos_embed(hidden_states) ''' ...... ''' if is_pipeline_last_stage(): ''' ...... ''' else: output = hidden_states if not return_dict: return (output,) return Transformer2DModelOutput(sample=output) Pipeline Execution åœ¨è¿›è¡Œ warm up åä¾¿ä¼šè¿›è¡Œæ¨¡å‹æ¨ç†å’Œé‡‡æ ·å™¨çš„å»å™ªè¿‡ç¨‹ã€‚æ¨¡å‹æ¨ç†é€šè¿‡è°ƒç”¨ pipeline çš„ __call__ æ–¹æ³•å®ç°ã€‚åœ¨åŸå…ˆ diffusers åŒ…ä¸­çš„ PixaeArtAlphaPipeline åŸºç¡€ä¸Šåšäº†ä¸€äº›ä¿®æ”¹ã€‚æˆ‘ä»¬ç›´æ¥çœ‹ä¿®æ”¹çš„éƒ¨åˆ†ã€‚\nget_runtime_state() è¿”å› _RUNTIME ï¼Œå†è°ƒç”¨ set_input_parameters æ–¹æ³•ï¼Œè®¾ç½®è¾“å…¥å‚æ•°å’Œè®¡ç®— PipeFusionParallel ä¸­æœ‰å…³ patch ç´¢å¼•çš„å‚æ•°ã€‚\n1 2 3 4 5 6 get_runtime_state().set_input_parameters( height=height, width=width, batch_size=batch_size, num_inference_steps=num_inference_steps, ) è¯¥å‡½æ•°ä¼šè®¡ç®—\npipeline parallel ä¸­æ¯ä¸ª patch çš„é«˜åº¦ï¼Œå¿…é¡»æ˜¯ patch_size * num_sp_patches çš„æ•´æ•°å€ã€‚ å°†æ¯ä¸ªæµæ°´çº¿é˜¶æ®µçš„ patch é«˜åº¦å‡åŒ€åœ°åˆ†é…ç»™ num_sp_patches ä¸ªåºåˆ—å¹¶è¡Œè®¾å¤‡ï¼Œè®¡ç®—æ¯ä¸ªè®¾å¤‡çš„ patch é«˜åº¦å’Œèµ·å§‹ç´¢å¼•ã€‚ ç„¶åä¼šå¯¹ prompt åµŒå…¥åçš„æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬åœ¨ cfg parallel ç»„ä¸­çš„è®¾å¤‡è¿›è¡Œåˆ†å‰², rank 0 è´Ÿæ ·æœ¬ï¼Œrank 1 æ­£æ ·æœ¬ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 if do_classifier_free_guidance: (prompt_embeds, prompt_attention_mask,) = self._process_cfg_split_batch(negative_prompt_embeds, prompt_embeds, negative_prompt_attention_mask, prompt_attention_mask,) def _process_cfg_split_batch(self, concat_group_0_negative: torch.Tensor, concat_group_0: torch.Tensor, concat_group_1_negative: torch.Tensor, concat_group_1: torch.Tensor,): if get_classifier_free_guidance_world_size() == 1: concat_group_0 = torch.cat([concat_group_0_negative, concat_group_0], dim=0) concat_group_1 = torch.cat([concat_group_1_negative, concat_group_1], dim=0) elif get_classifier_free_guidance_rank() == 0: concat_group_0 = concat_group_0_negative concat_group_1 = concat_group_1_negative elif get_classifier_free_guidance_rank() == 1: concat_group_0 = concat_group_0 concat_group_1 = concat_group_1 else: raise ValueError(\"Invalid classifier free guidance rank\") return concat_group_0, concat_group_1 Async Pipeline Initialize Pipeline é¦–å…ˆä¼šåˆå§‹åŒ– pipelineï¼Œrank 0 ä¼šæ¥æ”¶ warmup é˜¶æ®µçš„ latents ç„¶åæ²¿ç€ H ç»´åº¦è¿›è¡Œåˆ†å—ï¼Œrank -1 ä¹Ÿä¼šæ²¿ç€ H ç»´åº¦è¿›è¡Œåˆ†å—ã€‚ç„¶åä¸ºæ¯ä¸ª patch åˆ›å»ºæ¥æ”¶çš„ä»»åŠ¡ï¼Œæ³¨æ„ rank 0 ç¬¬ä¸€æ¬¡æ˜¯ä» warmup é˜¶æ®µæ¥æ”¶ latentsï¼Œæ‰€ä»¥ä»–çš„éœ€è¦æ¥æ”¶çš„ timestep å°‘ä¸€ä¸ªã€‚ patch_latents è¡¨ç¤ºå½“å‰è®¾å¤‡æ­£åœ¨å¤„ç†çš„ patch æ•°æ®ï¼Œå®ƒä¼šåœ¨æµæ°´çº¿çš„æ¯ä¸€é˜¶æ®µè¿›è¡Œå¤„ç†å’Œä¼ é€’ã€‚last_patch_latents åªåœ¨æµæ°´çº¿çš„æœ€åé˜¶æ®µè®¾å¤‡ä¸­ä½¿ç”¨ï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ª patch çš„æœ€ç»ˆè®¡ç®—ç»“æœã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 if len(timesteps) == 0: return latents num_pipeline_patch = get_runtime_state().num_pipeline_patch num_pipeline_warmup_steps = get_runtime_state().runtime_config.warmup_steps patch_latents = self._init_async_pipeline( num_timesteps=len(timesteps), latents=latents, num_pipeline_warmup_steps=num_pipeline_warmup_steps, ) last_patch_latents = ( # æ¯ä¸ª pipeline group æœ€åçš„è®¾å¤‡æ¥æ”¶æ‰€æœ‰çš„ patch [None for _ in range(num_pipeline_patch)] if (is_pipeline_last_stage()) else None ) def _init_async_pipeline( self, num_timesteps: int, latents: torch.Tensor, num_pipeline_warmup_steps: int, ): get_runtime_state().set_patched_mode(patch_mode=True) if is_pipeline_first_stage(): # get latents computed in warmup stage # ignore latents after the last timestep latents = (get_pp_group().pipeline_recv() if num_pipeline_warmup_steps \u003e 0 else latents) patch_latents = list(latents.split(get_runtime_state().pp_patches_height, dim=2)) elif is_pipeline_last_stage(): patch_latents = list(latents.split(get_runtime_state().pp_patches_height, dim=2)) else: patch_latents = [None for _ in range(get_runtime_state().num_pipeline_patch)] recv_timesteps = (num_timesteps - 1 if is_pipeline_first_stage() else num_timesteps) # construct receive tasks for each patch for _ in range(recv_timesteps): for patch_idx in range(get_runtime_state().num_pipeline_patch): get_pp_group().add_pipeline_recv_task(patch_idx) return patch_latents Iterate Over Timesteps å¯¹äºæ¯ä¸ª timestepï¼ˆå³æ¯ä¸ªå»å™ªæ­¥éª¤ï¼‰ï¼Œä¼šå¯¹æ¯ä¸ª patch æ‰§è¡Œï¼š\nå¦‚æœå½“å‰è®¾å¤‡æ˜¯æµæ°´çº¿çš„æœ€åä¸€é˜¶æ®µ (is_pipeline_last_stage())ï¼Œå°†å½“å‰ patch çš„æ•°æ®ä¿å­˜åˆ° last_patch_latents ä¸­ã€‚ å¦‚æœä¸æ˜¯ç¬¬ä¸€é˜¶æ®µçš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ (i == 0)ï¼Œè°ƒç”¨ recv_next() æ¥å¼‚æ­¥æ¥æ”¶æ¥è‡ªä¸Šä¸€è®¾å¤‡çš„ patch æ•°æ®ï¼ˆéé˜»å¡æ“ä½œï¼Œé€šè¿‡ irecv å®Œæˆï¼‰ã€‚ å¯¹æ¯ä¸ª patch æ‰§è¡Œæ¨¡å‹çš„å‰å‘ä¼ æ’­ _backbone_forwardï¼Œæ ¹æ®å½“å‰æ—¶é—´æ­¥ t è¿›è¡Œæ¨ç†å’Œè®¡ç®—ã€‚ å¦‚æœå½“å‰è®¾å¤‡æ˜¯æœ€åä¸€é˜¶æ®µï¼Œè°ƒç”¨ _scheduler_step æ¥æ ¹æ®å™ªå£°è¿›è¡Œå»å™ªï¼Œå¹¶å°†æ•°æ®å‘é€ç»™ä¸‹ä¸€ä¸ªè®¾å¤‡ pipeline_isendã€‚ å¯¹äºéæœ€åé˜¶æ®µçš„è®¾å¤‡ï¼Œç»§ç»­å°†å½“å‰ patch çš„è®¡ç®—ç»“æœå‘é€åˆ°ä¸‹ä¸€è®¾å¤‡ã€‚ get_pp_group().pipeline_isend ç”¨äºå°†å½“å‰ patch å‘é€åˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡ï¼Œä½¿ç”¨çš„æ˜¯ torch.distributed.isendï¼Œè¿™æ˜¯éé˜»å¡å‘é€ã€‚ get_pp_group().recv_next ä¼šå‡†å¤‡å¥½æ¥æ”¶æ¥è‡ªä¸Šä¸€ä¸ªè®¾å¤‡çš„æ•°æ®ï¼Œrecv_buffer ç”¨æ¥å­˜æ”¾æ¥æ”¶åˆ°çš„æ•°æ®ã€‚irecv å®ç°éé˜»å¡æ¥æ”¶ï¼Œå¯ä»¥åœ¨ç­‰å¾…æ•°æ®çš„åŒæ—¶è¿›è¡Œå…¶ä»–æ“ä½œã€‚\nWarning\nscheduler_step åªå¯¹å•ç‹¬çš„ patch è¿›è¡Œï¼ŒåŸå› æœªçŸ¥ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 first_async_recv = True for i, t in enumerate(timesteps): for patch_idx in range(num_pipeline_patch): if is_pipeline_last_stage(): last_patch_latents[patch_idx] = patch_latents[patch_idx] if is_pipeline_first_stage() and i == 0: pass else: if first_async_recv: get_pp_group().recv_next() first_async_recv = False patch_latents[patch_idx] = get_pp_group().get_pipeline_recv_data( idx=patch_idx ) patch_latents[patch_idx] = self._backbone_forward( latents=patch_latents[patch_idx], prompt_embeds=prompt_embeds, prompt_attention_mask=prompt_attention_mask, added_cond_kwargs=added_cond_kwargs, t=t, guidance_scale=guidance_scale, ) if is_pipeline_last_stage(): patch_latents[patch_idx] = self._scheduler_step( patch_latents[patch_idx], # pred noise last_patch_latents[patch_idx], # last timestep noise t, extra_step_kwargs, ) if i != len(timesteps) - 1: get_pp_group().pipeline_isend( patch_latents[patch_idx], segment_idx=patch_idx ) else: get_pp_group().pipeline_isend( patch_latents[patch_idx], segment_idx=patch_idx ) if is_pipeline_first_stage() and i == 0: pass else: if i == len(timesteps) - 1 and patch_idx == num_pipeline_patch - 1: pass else: get_pp_group().recv_next() get_runtime_state().next_patch() # switch to next: (self.pipeline_patch_idx + 1) % self.num_pipeline_patch if i == len(timesteps) - 1 or ( (i + num_pipeline_warmup_steps + 1) \u003e num_warmup_steps and (i + num_pipeline_warmup_steps + 1) % self.scheduler.order == 0 ): progress_bar.update() assert callback is None, \"callback not supported in async \" \"pipeline\" if ( callback is not None and i + num_pipeline_warmup_steps % callback_steps == 0 ): step_idx = (i + num_pipeline_warmup_steps) // getattr( self.scheduler, \"order\", 1 ) callback(step_idx, t, patch_latents[patch_idx]) Construct Final Latents timestep éå†å®Œæˆåï¼Œä»ç„¶æœ‰æœ€åçš„æ“ä½œè¦è¿›è¡Œï¼Œè¿™äº›æ“ä½œçš„ä¸»è¦ç›®çš„æ˜¯å°†æµæ°´çº¿å¹¶è¡Œä¸­å„ä¸ª patch çš„ç»“æœæ‹¼æ¥èµ·æ¥ï¼Œå½¢æˆå®Œæ•´çš„è¾“å‡ºç»“æœã€‚å°¤å…¶æ˜¯å¯¹äºæœ€åä¸€ä¸ªè®¾å¤‡ï¼Œè¿˜éœ€è¦å¤„ç† åºåˆ—å¹¶è¡Œï¼ˆsequence parallelismï¼‰ çš„åˆå¹¶æ“ä½œã€‚é€šè¿‡ all_gather æ“ä½œå°†æ¯ä¸ªè®¾å¤‡ä¸Šå¤„ç†çš„ patch ç»“æœæ”¶é›†èµ·æ¥ï¼Œç„¶åä»æ¯ä¸ªè®¾å¤‡çš„ sp_latents_list ä¸­ï¼Œæå–å‡ºå¯¹åº”äº pp_patch_idx çš„ patch æ•°æ®å¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥ã€‚\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 latents = None if is_pipeline_last_stage(): latents = torch.cat(patch_latents, dim=2) if get_sequence_parallel_world_size() \u003e 1: sp_degree = get_sequence_parallel_world_size() sp_latents_list = get_sp_group().all_gather( latents, separate_tensors=True ) latents_list = [] for pp_patch_idx in range(get_runtime_state().num_pipeline_patch): latents_list += [ sp_latents_list[sp_patch_idx][ ..., get_runtime_state().pp_patches_start_idx_local[pp_patch_idx] : get_runtime_state().pp_patches_start_idx_local[pp_patch_idx + 1], :, ] for sp_patch_idx in range(sp_degree) ] latents = torch.cat(latents_list, dim=-2) return latents Decode Latents ä¸ºäº†é¿å… VAE ä¸­çš„ Decoder åœ¨å¯¹ 8192px åˆ†è¾¨ç‡å›¾åƒè¿›è¡Œ conv2D çš„è¿‡ç¨‹ä¸­å‡ºç° OOM çš„é—®é¢˜ï¼Œ xDiT ä½¿ç”¨äº†åºåˆ—å¹¶è¡Œå’Œ patch å¹¶è¡Œçš„ PatchConv2d å’Œ PatchGroupNorm æ¥æ›¿æ¢æ‰åŸæœ‰ Decoder ä¸­çš„ UpDecoderBlock2D å¯¹åº”çš„å±‚ã€‚\nPatchGroupNorm PatchGroupNorm åœ¨ H ç»´åº¦ä¸Šåˆ’åˆ†ä¸ºå¤šä¸ª patchï¼Œæ¯ä¸ªè®¾å¤‡æ±‚è‡ªå·±æ‰€è´Ÿè´£çš„éƒ¨åˆ†å’Œã€‚ GroupNorm Principles å‡è®¾è¾“å…¥å¼ é‡ x çš„å½¢çŠ¶ä¸º [N, C, H, W]ï¼Œå…¶ä¸­ N è¡¨ç¤ºæ‰¹é‡å¤§å°ï¼ˆBatch Sizeï¼‰ï¼ŒC è¡¨ç¤ºé€šé“æ•°ï¼ˆChannelsï¼‰ï¼ŒH å’Œ W åˆ†åˆ«è¡¨ç¤ºé«˜åº¦å’Œå®½åº¦ã€‚åœ¨ GN ä¸­ï¼Œé€šé“æ•° C è¢«åˆ’åˆ†ä¸º G ç»„ï¼Œæ¯ä¸ªç»„åŒ…å« C/G ä¸ªé€šé“ã€‚è®¡ç®—æ¯ä¸ªç»„å†…å³ [C/G, H, W] ç»´åº¦ä¸Šçš„å‡å€¼å’Œæ–¹å·®ã€‚ç‰¹åˆ«çš„ G=1 æ—¶ï¼ŒGN é€€åŒ–ä¸º BNã€‚G=C æ—¶ï¼ŒGN é€€åŒ–ä¸º LNã€‚ è·å–é«˜åº¦ä¿¡æ¯ 1 2 3 4 5 6 7 class PatchGroupNorm(nn.Module): ''' def __init__(self, ...)''' def forward(self, x: Tensor) -\u003e Tensor: height = torch.tensor(x.shape[-2], dtype=torch.int64, device=x.device) dist.all_reduce(height) # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„é«˜åº¦å¹¶æ±‡æ€»ã€‚æœ€ç»ˆæ¯ä¸ªè¿›ç¨‹çš„ height éƒ½å°†è¡¨ç¤ºå…¨å±€çš„é«˜åº¦å’Œã€‚ è®¡ç®—æ¯ä¸ªç»„çš„é€šé“æ•°é‡ä»¥åŠæ¯ä¸ªè¿›ç¨‹å†…çš„å…ƒç´ æ•°é‡ 1 2 3 channels_per_group = x.shape[1] // self.num_groups # æ¯ä¸ªç»„çš„é€šé“æ•°é‡ nelements_rank = channels_per_group * x.shape[-2] * x.shape[-1] # å½“å‰è¿›ç¨‹è´Ÿè´£çš„æ¯ä¸ªç»„ä¸­çš„å…ƒç´ æ€» nelements = channels_per_group * height * x.shape[-1] # æ‰€æœ‰è¿›ç¨‹çš„æ¯ä¸ªç»„ä¸­çš„å…ƒç´ æ€»æ•° è®¡ç®—æ¯ä¸ªç»„çš„å‡å€¼ 1 2 3 4 5 x = x.view(x.shape[0], self.num_groups, -1, x.shape[-2], x.shape[-1]) # [batch_size, num_groups, channels_per_group, height, width] group_sum = x.mean(dim=(2,3,4), dtype=torch.float32) # å¯¹æ¯ä¸ªç»„çš„æ‰€æœ‰å…ƒç´  (channels_per_group, height, width) æ±‚å¹³å‡ group_sum = group_sum * nelements_rank # åŠ æƒåçš„å±€éƒ¨å’Œ = å±€éƒ¨å‡å€¼ * å½“å‰è¿›ç¨‹çš„å…ƒç´ æ•°é‡ dist.all_reduce(group_sum) # æ”¶é›†å¹¶æ±‡æ€»æ‰€æœ‰è¿›ç¨‹çš„å±€éƒ¨å’Œï¼Œå¾—åˆ°å…¨å±€å’Œ E = (group_sum / nelements)[:, :, None, None, None].to(x.dtype) # è®¡ç®—å…¨å±€çš„å‡å€¼ E è®¡ç®—æ¯ä¸ªç»„çš„æ–¹å·® 1 2 3 4 5 6 # å’Œè®¡ç®—å‡å€¼åŒæ ·çš„æ“ä½œ group_var_sum = torch.empty((x.shape[0], self.num_groups), dtype=torch.float32, device=x.device) torch.var(x, dim=(2,3,4), out=group_var_sum) group_var_sum = group_var_sum * nelements_rank dist.all_reduce(group_var_sum) var = (group_var_sum / nelements)[:, :, None, None, None].to(x.dtype) å½’ä¸€åŒ–å¹¶ç¼©æ”¾ $y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$ 1 2 3 x = (x - E) / torch.sqrt(var + self.eps) x = x * self.weight[:, :, None, None, None] + self.bias[:, :, None, None, None] return x PatchConv2d PatchConv2d å°†æ½œåœ¨ç©ºé—´ä¸­çš„ç‰¹å¾æ˜ å°„åˆ†å‰²æˆå¤šä¸ª patchï¼Œè·¨ä¸åŒè®¾å¤‡è¿›è¡Œåºåˆ—å¹¶è¡Œ VAE è§£ç ã€‚è¿™ç§æŠ€æœ¯å°†ä¸­é—´æ¿€æ´»æ‰€éœ€çš„å³°å€¼å†…å­˜å‡å°‘åˆ° 1/Nï¼Œå…¶ä¸­ N æ˜¯æ‰€ä½¿ç”¨çš„è®¾å¤‡æ•°é‡ã€‚å¯¹äº VAE ä¸­çš„å·ç§¯ç®—å­ï¼Œéœ€è¦å¯¹å¦‚ä¸‹å›¾æ‰€ç¤ºçš„ halo åŒºåŸŸæ•°æ®è¿›è¡Œé€šä¿¡ã€‚\nPatch VAE Conv\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class PatchConv2d(nn.Conv2d): def __init__( self, in_channels: int, out_channels: int, kernel_size: _size_2_t, stride: _size_2_t = 1, padding: Union[str, _size_2_t] = 0, dilation: _size_2_t = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros', # TODO: refine this type device=None, dtype=None, block_size: Union[int, Tuple[int, int]] = 0 ) -\u003e None: if isinstance(dilation, int): assert dilation == 1, \"dilation is not supported in PatchConv2d\" else: for i in dilation: assert i == 1, \"dilation is not supported in PatchConv2d\" self.block_size = block_size super().__init__( in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype) _conv_forward å‡½æ•°æ˜¯ PatchConv2d ç±»çš„æ ¸å¿ƒï¼Œå®ƒè´Ÿè´£åœ¨è¾“å…¥å¼ é‡ä¸Šæ‰§è¡Œå·ç§¯æ“ä½œï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒå¼è®¡ç®—åœºæ™¯ä¸‹å¤„ç†è·¨è¿›ç¨‹çš„è¾“å…¥åˆ‡åˆ†ã€halo åŒºåŸŸçš„ä¼ é€’å’Œè®¡ç®—ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨çš„è¾…åŠ©å‡½æ•°çš„ç®€è¦åŠŸèƒ½è¯´æ˜\n_get_world_size_and_rank ï¼šè·å–å½“å‰åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„è¿›ç¨‹æ€»æ•° world_size å’Œå½“å‰è¿›ç¨‹çš„ç¼–å· rank _calc_patch_height_indexï¼šæ ¹æ®æ¯ä¸ªè¿›ç¨‹çš„è¾“å…¥é«˜åº¦ï¼Œè®¡ç®—æ‰€æœ‰è¿›ç¨‹çš„èµ·å§‹å’Œç»“æŸé«˜åº¦ç´¢å¼•ã€‚ _calc_halo_width_in_h_dimï¼šè®¡ç®—å½“å‰è¿›ç¨‹åœ¨ h ç»´åº¦ä¸Šæ‰€éœ€çš„ä¸Šæ–¹å’Œä¸‹æ–¹çš„ halo åŒºåŸŸå®½åº¦ã€‚ _calc_bottom_halo_widthï¼šè®¡ç®—å½“å‰è¿›ç¨‹ä»ä¸‹æ–¹ç›¸é‚»è¿›ç¨‹éœ€è¦æ¥æ”¶çš„ halo åŒºåŸŸçš„å®½åº¦ã€‚ _calc_top_halo_widthï¼šè®¡ç®—å½“å‰è¿›ç¨‹ä»ä¸Šæ–¹ç›¸é‚»è¿›ç¨‹éœ€è¦æ¥æ”¶çš„ halo åŒºåŸŸçš„å®½åº¦ã€‚ _adjust_padding_for_patchï¼šæ ¹æ®å½“å‰è¿›ç¨‹çš„ rank å’Œæ€»è¿›ç¨‹æ•°è°ƒæ•´è¾“å…¥æ•°æ®çš„å¡«å……æ–¹å¼ï¼Œé˜²æ­¢è¾¹ç•Œé‡å¤è®¡ç®—ã€‚ è·å–è¾“å…¥ä¿¡æ¯ä»¥åŠé€šä¿¡ç»„ä¿¡æ¯ 1 2 3 4 5 6 7 8 9 10 11 12 def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]): bs, channels, h, w = input.shape world_size, rank = self._get_world_size_and_rank() if (world_size == 1): # å¤„ç†éåˆ†å¸ƒå¼æƒ…å†µ if self.padding_mode != 'zeros': return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode), weight, bias, self.stride, _pair(0), self.dilation, self.groups) return F.conv2d(input, weight, bias, self.stride, self.padding, self.dilation, self.groups) è·å–è¾“å…¥çš„å…ƒæ•°æ® 1 2 3 4 patch_height_list = [torch.zeros(1, dtype=torch.int64, device=f\"cuda:{rank}\") for _ in range(dist.get_world_size())] dist.all_gather(patch_height_list, torch.tensor([h], dtype=torch.int64, device=f\"cuda:{rank}\")) # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„è¾“å…¥é«˜åº¦ patch_height_index = self._calc_patch_height_index(patch_height_list) # è®¡ç®—æ¯ä¸ªè¿›ç¨‹å—çš„èµ·å§‹é«˜åº¦å’Œç»“æŸé«˜åº¦çš„ç´¢å¼• halo_width = self._calc_halo_width_in_h_dim(rank, patch_height_index, self.kernel_size[0], self.padding[0], self.stride[0]) # è®¡ç®—å½“å‰è¿›ç¨‹å—çš„ä¸Šä¸‹ halo åŒºåŸŸçš„å®½åº¦ è®¡ç®—ç›¸é‚»è¿›ç¨‹çš„ halo åŒºåŸŸ (ä¹Ÿå°±æ˜¯è‡ªå·±éœ€è¦æ¥å‘é€çš„éƒ¨åˆ†) é€šè¿‡è®¡ç®—å‰ä¸€ä¸ªè¿›ç¨‹çš„ bottom_halo_width å’Œåä¸€ä¸ªè¿›ç¨‹çš„ top_halo_width å¾—å‡ºè‡ªå·±éœ€è¦å‘é€çš„éƒ¨åˆ†\n1 2 3 4 5 6 7 prev_bottom_halo_width: int = 0 next_top_halo_width: int = 0 if rank != 0: prev_bottom_halo_width = self._calc_bottom_halo_width(rank - 1, patch_height_index, self.kernel_size[0], self.padding[0], self.stride[0]) if rank != world_size - 1: next_top_halo_width = self._calc_top_halo_width(rank + 1, patch_height_index, self.kernel_size[0], self.padding[0], self.stride[0]) next_top_halo_width = max(0, next_top_halo_width) è¿›è¡Œ halo åŒºåŸŸçš„å‘é€ä¸æ¥æ”¶ å¼‚æ­¥å‘é€ï¼ŒåŒæ­¥æ¥æ”¶\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 to_next = None to_prev = None top_halo_recv = None bottom_halo_recv = None if next_top_halo_width \u003e 0: bottom_halo_send = input[:, :, -next_top_halo_width:, :].contiguous() to_next = dist.isend(bottom_halo_send, rank + 1) if halo_width[0] \u003e 0: # not rank 0 top_halo_recv = torch.empty([bs, channels, halo_width[0], w], dtype=input.dtype, device=f\"cuda:{rank}\") dist.recv(top_halo_recv, rank - 1) if prev_bottom_halo_width \u003e 0: # not rank N-1 top_halo_send = input[:, :, :prev_bottom_halo_width, :].contiguous() to_prev = dist.isend(top_halo_send, rank - 1) if halo_width[1] \u003e 0: bottom_halo_recv = torch.empty([bs, channels, halo_width[1], w], dtype=input.dtype, device=f\"cuda:{rank}\") dist.recv(bottom_halo_recv, rank + 1) æ‹¼æ¥ halo åŒºåŸŸ 1 2 3 4 5 6 7 if halo_width[0] \u003c 0: # Remove redundancy at the top of the input input = input[:, :, -halo_width[0]:, :] if top_halo_recv is not None: # concat the halo region to the input tensor input = torch.cat([top_halo_recv, input], dim=-2) if bottom_halo_recv is not None: input = torch.cat([input, bottom_halo_recv], dim=-2) ç­‰å¾…å‘é€å®Œæˆå†å¼€å§‹è®¡ç®— 1 2 3 4 if to_next is not None: to_next.wait() if to_prev is not None: to_prev.wait() è¿›è¡Œå·ç§¯å’Œåå¤„ç† ä¸ºäº†å‡å°‘ memory spike ä¸€æ¬¡è®¡ç®— block_size*block_size çš„åŒºåŸŸï¼Œå¹¶å°†ç»“æœæ‹¼æ¥èµ·æ¥\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 padding = self._adjust_padding_for_patch(self._reversed_padding_repeated_twice, rank=rank, world_size=world_size) if self.block_size == 0 or (h \u003c= self.block_size and w \u003c= self.block_size): if self.padding_mode != 'zeros': conv_res = F.conv2d(F.pad(input, padding, mode=self.padding_mode), weight, bias, self.stride, _pair(0), self.dilation, self.groups) else: conv_res = F.conv2d(input, weight, bias, self.stride, self.padding, self.dilation, self.groups) return conv_res else: if self.padding_mode != \"zeros\": input = F.pad(input, padding, mode=self.padding_mode) elif self.padding != 0: input = F.pad(input, padding, mode=\"constant\") _, _, h, w = input.shape num_chunks_in_h = (h + self.block_size - 1) // self.block_size # h ç»´åº¦çš„ block æ•°é‡ num_chunks_in_w = (w + self.block_size - 1) // self.block_size # w ... unit_chunk_size_h = h // num_chunks_in_h unit_chunk_size_w = w // num_chunks_in_w outputs = [] for idx_h in range(num_chunks_in_h): inner_output = [] for idx_w in range(num_chunks_in_w): start_w = idx_w * unit_chunk_size_w start_h = idx_h * unit_chunk_size_h end_w = (idx_w + 1) * unit_chunk_size_w end_h = (idx_h + 1) * unit_chunk_size_h # è®¡ç®—æ¯ä¸ªå—çš„å¼€å§‹å’Œç»“æŸç´¢å¼•ï¼Œè°ƒæ•´å—çš„è¾¹ç•Œ # ... # å¯¹å½“å‰å—æ‰§è¡Œå·ç§¯æ“ä½œ inner_output.append( F.conv2d( input[:, :, start_h:end_h, start_w:end_w], weight, bias, self.stride, 0, self.dilation, self.groups, ) ) outputs.append(torch.cat(inner_output, dim=-1)) return torch.cat(outputs, dim=-2) ",
  "wordCount" : "7008",
  "inLanguage": "en",
  "datePublished": "2025-06-07T20:44:50+08:00",
  "dateModified": "2025-06-07T23:40:58+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/xdit/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="ç®€ä½“ä¸­æ–‡"
                            aria-label="ç®€ä½“ä¸­æ–‡">ç®€ä½“ä¸­æ–‡</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="ğŸ  Home">
                    <span>ğŸ  Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="ğŸ™‹ğŸ»â€â™‚ï¸ Me">
                    <span>ğŸ™‹ğŸ»â€â™‚ï¸ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="ğŸ“š Blogs">
                    <span>ğŸ“š Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="ğŸ§© Categories">
                    <span>ğŸ§© Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="ğŸ”– Tags">
                    <span>ğŸ”– Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="â± Archive">
                    <span>â± Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="ğŸ” Search (Alt &#43; /)" accesskey=/>
                    <span>ğŸ” Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="ğŸ¤ Friends">
                    <span>ğŸ¤ Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      xDiT Principle
    </h1>
    <div class="post-description">
      This is a brief introduction to the xDiT Principle.
    </div>
    <div class="post-meta"><span title='2025-06-07 20:44:50 +0800 CST'>Jun-07-2025</span>&nbsp;Â·&nbsp;14 min&nbsp;Â·&nbsp;7008 words&nbsp;Â·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#parse-config-arguments" aria-label="Parse Config Arguments">Parse Config Arguments</a></li>
                    <li>
                        <a href="#construct-pipeline" aria-label="Construct Pipeline">Construct Pipeline</a><ul>
                            
                    <li>
                        <a href="#initialize_model_parallel" aria-label="initialize_model_parallel">initialize_model_parallel</a></li>
                    <li>
                        <a href="#hybrid-parallelsim-design" aria-label="Hybrid Parallelsim Design">Hybrid Parallelsim Design</a></li>
                    <li>
                        <a href="#convert-model" aria-label="Convert Model">Convert Model</a></li></ul>
                    </li>
                    <li>
                        <a href="#pipeline-execution" aria-label="Pipeline Execution">Pipeline Execution</a></li>
                    <li>
                        <a href="#async-pipeline" aria-label="Async Pipeline">Async Pipeline</a><ul>
                            
                    <li>
                        <a href="#initialize-pipeline" aria-label="Initialize Pipeline">Initialize Pipeline</a></li></ul>
                    </li>
                    <li>
                        <a href="#iterate-over-timesteps" aria-label="Iterate Over Timesteps">Iterate Over Timesteps</a><ul>
                            
                    <li>
                        <a href="#construct-final-latents" aria-label="Construct Final Latents">Construct Final Latents</a></li></ul>
                    </li>
                    <li>
                        <a href="#decode-latents" aria-label="Decode Latents">Decode Latents</a><ul>
                            
                    <li>
                        <a href="#patchgroupnorm" aria-label="PatchGroupNorm">PatchGroupNorm</a></li>
                    <li>
                        <a href="#patchconv2d" aria-label="PatchConv2d">PatchConv2d</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="parse-config-arguments">Parse Config Arguments<a hidden class="anchor" aria-hidden="true" href="#parse-config-arguments">#</a></h1>
<p>ä¼šä»å‘½ä»¤è¡Œå‚æ•°ä¸­è·å–æœ‰å…³ Model, Runtime, Parallel Processing &amp; Input æœ‰å…³çš„ä¿¡æ¯ã€‚å‰ä¸‰è€…è¢«åŒ…å«åœ¨ <code>engine_config</code> ä¸­ï¼Œè€Œæœ€åè€…åˆ™è¢«åŒ…å«åœ¨ <code>input_config</code> ä¸­ã€‚åœ¨ <code>create_config()</code> å‡½æ•°ä¸­ï¼Œä¼šåˆå§‹åŒ– <code>_WORLD</code> å…¨å±€å˜é‡ï¼Œå®ƒæ˜¯ä¸€ä¸ª <code>GroupCoordinator</code> å®ä¾‹ã€‚å¾ˆæ˜æ˜¾å®ƒåªæœ‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰çš„è®¾å¤‡è¿›ç¨‹ç»„ã€‚
<details class="custom-details">
    <summary class="custom-summary">GroupCoordinator</summary>
    <div><p><code>GroupCoordinator</code> ç±»æ˜¯ä¸€ä¸ª PyTorch çš„è¿›ç¨‹ç»„å°è£…å™¨ï¼Œä¸»è¦ç”¨äºç®¡ç†ä¸€ç»„è¿›ç¨‹ä¹‹é—´çš„é€šä¿¡ã€‚å®ƒå¯ä»¥æ ¹æ®ä¸åŒçš„é€šä¿¡åç«¯ï¼ˆå¦‚ NCCLã€Glooã€MPI ç­‰ï¼‰æ¥åè°ƒè¿›ç¨‹ä¹‹é—´çš„æ“ä½œã€‚åŒ…å«ä»¥ä¸‹ä¿¡æ¯</p>
<ul>
<li><code>rank</code>: å½“å‰è¿›ç¨‹çš„å…¨å±€ç´¢å¼•ï¼ˆå…¨å±€å”¯ä¸€ï¼‰ã€‚</li>
<li><code>ranks</code>: ç»„å†…æ‰€æœ‰è¿›ç¨‹çš„å…¨å±€ç´¢å¼•åˆ—è¡¨ã€‚</li>
<li><code>world_size</code>: ç»„çš„å¤§å°ï¼Œå³è¿›ç¨‹çš„æ•°é‡ <code>len(ranks)</code></li>
<li><code>local_rank</code>: å½“å‰è¿›ç¨‹åœ¨æœ¬åœ°èŠ‚ç‚¹ä¸­çš„ç´¢å¼•ã€‚</li>
<li><code>rank_in_group</code>: å½“å‰è¿›ç¨‹åœ¨ç»„å†…çš„ç´¢å¼•ã€‚</li>
<li><code>cpu_group</code>: ç”¨äº CPU é€šä¿¡çš„è¿›ç¨‹ç»„ã€‚</li>
<li><code>device_group</code>: ç”¨äºè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰é€šä¿¡çš„è¿›ç¨‹ç»„ã€‚</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">we</span> <span class="n">have</span> <span class="n">a</span> <span class="n">group</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span> <span class="n">across</span> <span class="n">two</span> <span class="n">nodes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">Process</span> <span class="o">|</span> <span class="n">Node</span> <span class="o">|</span> <span class="n">Rank</span> <span class="o">|</span> <span class="n">Local</span> <span class="n">Rank</span> <span class="o">|</span> <span class="n">Rank</span> <span class="ow">in</span> <span class="n">Group</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span>     <span class="o">|</span>   <span class="mi">0</span>  <span class="o">|</span>  <span class="mi">0</span>   <span class="o">|</span>     <span class="mi">0</span>      <span class="o">|</span>       <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span>     <span class="o">|</span>   <span class="mi">0</span>  <span class="o">|</span>  <span class="mi">1</span>   <span class="o">|</span>     <span class="mi">1</span>      <span class="o">|</span>       <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="mi">2</span>     <span class="o">|</span>   <span class="mi">1</span>  <span class="o">|</span>  <span class="mi">2</span>   <span class="o">|</span>     <span class="mi">0</span>      <span class="o">|</span>       <span class="mi">2</span>
</span></span><span class="line"><span class="cl">  <span class="mi">3</span>     <span class="o">|</span>   <span class="mi">1</span>  <span class="o">|</span>  <span class="mi">3</span>   <span class="o">|</span>     <span class="mi">1</span>      <span class="o">|</span>       <span class="mi">3</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>__init__</code> æ–¹æ³•æ¥æ”¶ä»¥ä¸‹å‚æ•°ï¼š</p>
<ul>
<li><code>group_ranks</code>: ä¸€ä¸ªåŒ…å«å¤šä¸ªè¿›ç¨‹ç´¢å¼•åˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨è¡¨ç¤ºä¸€ä¸ªè¿›ç¨‹ç»„ã€‚</li>
<li><code>local_rank</code>: å½“å‰è¿›ç¨‹çš„æœ¬åœ°ç´¢å¼•ã€‚</li>
<li><code>torch_distributed_backend</code>: æŒ‡å®šç”¨äºé€šä¿¡çš„åç«¯ç±»å‹ (å¦‚ &ldquo;gloo&rdquo; æˆ– &ldquo;nccl&rdquo;).</li>
</ul>
<p>åˆå§‹åŒ–è¿‡ç¨‹ï¼š</p>
<ol>
<li>ä½¿ç”¨ <code>torch.distributed.get_rank()</code> è·å–å½“å‰è¿›ç¨‹çš„å…¨å±€ç´¢å¼•ã€‚</li>
<li>éå†ä¼ å…¥çš„ <code>group_ranks</code> åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªå­åˆ—è¡¨åˆ›å»ºä¸€ä¸ªæ–°çš„è®¾å¤‡ç»„å’Œ CPU ç»„ã€‚</li>
<li>å¦‚æœå½“å‰è¿›ç¨‹çš„ç´¢å¼•åœ¨å½“å‰å­åˆ—è¡¨ä¸­ï¼Œåˆ™è®¾ç½®è¯¥è¿›ç¨‹çš„ç»„å†…ä¿¡æ¯ (åŒ…æ‹¬ <code>ranks</code>ã€<code>world_size</code> å’Œ <code>rank_in_group</code>).</li>
<li>ç¡®ä¿ CPU ç»„å’Œè®¾å¤‡ç»„éƒ½å·²æˆåŠŸåˆ›å»ºã€‚</li>
<li>æ ¹æ®æ˜¯å¦å¯ç”¨ CUDA è®¾ç½®å½“å‰è®¾å¤‡ä¸º GPU æˆ– CPU.</li>
</ol>
</div>
</details><br></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span> <span class="o">=</span> <span class="n">FlexibleArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&#34;xFuser Arguments&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">xFuserArgs</span><span class="o">.</span><span class="n">add_cli_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>  <span class="c1"># Add Command Line Interface (CLI) arguments</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_args</span> <span class="o">=</span> <span class="n">xFuserArgs</span><span class="o">.</span><span class="n">from_cli_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Extract CLI args and pass them to xFuserArgs Constructor</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_config</span><span class="p">,</span> <span class="n">input_config</span> <span class="o">=</span> <span class="n">engine_args</span><span class="o">.</span><span class="n">create_config</span><span class="p">()</span>  <span class="c1"># Init _WORLD. engine_config: model, run_time &amp; parallel infos, input_config: input shape, prompt &amp; sampler infos</span>
</span></span><span class="line"><span class="cl">    <span class="n">local_rank</span> <span class="o">=</span> <span class="n">get_world_group</span><span class="p">()</span><span class="o">.</span><span class="n">local_rank</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>å…³äºå¯ä»¥æ”¯æŒçš„å¹¶è¡Œç­–ç•¥å¦‚ä¸‹ï¼ŒåŒ…æ‹¬ Data Parallel, Sequence Parallel, Pipefusion Parallel &amp; Tensor Parallel.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Parallel Processing Options:
</span></span><span class="line"><span class="cl">  --use_cfg_parallel    Use split batch in classifier_free_guidance. cfg_degree will be <span class="m">2</span> <span class="k">if</span> <span class="nb">set</span>
</span></span><span class="line"><span class="cl">  --data_parallel_degree DATA_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Data parallel degree.
</span></span><span class="line"><span class="cl">  --ulysses_degree ULYSSES_DEGREE
</span></span><span class="line"><span class="cl">                        Ulysses sequence parallel degree. Used in attention layer.
</span></span><span class="line"><span class="cl">  --ring_degree RING_DEGREE
</span></span><span class="line"><span class="cl">                        Ring sequence parallel degree. Used in attention layer.
</span></span><span class="line"><span class="cl">  --pipefusion_parallel_degree PIPEFUSION_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Pipefusion parallel degree. Indicates the number of pipeline stages.
</span></span><span class="line"><span class="cl">  --num_pipeline_patch NUM_PIPELINE_PATCH
</span></span><span class="line"><span class="cl">                        Number of patches the feature map should be segmented in pipefusion parallel.
</span></span><span class="line"><span class="cl">  --attn_layer_num_for_pp <span class="o">[</span>ATTN_LAYER_NUM_FOR_PP ...<span class="o">]</span>
</span></span><span class="line"><span class="cl">                        List representing the number of layers per stage of the pipeline in pipefusion parallel
</span></span><span class="line"><span class="cl">  --tensor_parallel_degree TENSOR_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Tensor parallel degree.
</span></span><span class="line"><span class="cl">  --split_scheme SPLIT_SCHEME
</span></span><span class="line"><span class="cl">                        Split scheme <span class="k">for</span> tensor parallel.
</span></span></code></pre></td></tr></table>
</div>
</div><p>ä» CLI è§£æçš„å‚æ•°åä¼šåœ¨ <code>create_config()</code> ä¸­ç»„æˆå¦‚ä¸‹çš„ <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/config/config.py#L185">ParallelConfig</a>.</p>
<ul>
<li><code>DataParallelConfig</code>: æ€»çš„å¹¶è¡Œåº¦ä¸º <code>dp_degree * cfg_degree</code>.
<ul>
<li><code>dp_degree</code>: ç›¸å½“äºå¯¹ batch ç»´åº¦è¿›è¡Œåˆ‡åˆ†ï¼Œ</li>
<li><code>cfg_degree</code>: Class-free Guidance(cfg) ç”¨äºæ§åˆ¶æ— æ¡ä»¶çš„å›¾ç‰‡ç”Ÿæˆ (è‹¥ä½¿ç”¨ç›¸å½“äº <code>batchsize *= 2</code>).</li>
</ul>
</li>
<li><code>SequenceParallelConfig</code>: æ€»çš„å¹¶è¡Œåº¦ä¸º <code>sp_degree = ulysses_degree * ring_degree</code>
<ul>
<li><code>ulysses_degree</code>: ç”¨äºæ§åˆ¶ <a href="https://arxiv.org/abs/2309.14509">DeepSeed-Ulesses</a> çš„åºåˆ—å¹¶è¡Œåº¦ã€‚</li>
<li><code>ring_degree</code>: ç”¨äºæ§åˆ¶è®¡ç®— Ring Attention æ—¶å¯¹ Q K V æ²¿ç€ Sequence ç»´åº¦çš„åˆ‡åˆ†å—æ•°ã€‚</li>
</ul>
</li>
<li><code>TensorParallelConfig</code>: æ€»çš„å¹¶è¡Œåº¦ä¸º <code>tp_degree</code>.
<ul>
<li><code>tp_degree</code>: ç”¨äºæ§åˆ¶ <a href="https://arxiv.org/abs/2104.05343">2D Tensor Parallel</a> çš„å¹¶è¡Œåº¦ã€‚</li>
<li><code>split_scheme</code>: ç”¨äºæ§åˆ¶å¼ é‡åˆ‡åˆ†æ–¹å¼.</li>
</ul>
</li>
<li><code>PipeFusionParallelConfig</code>: æ€»çš„å¹¶è¡Œåº¦ä¸º <code>pp_degree=num_pipeline_patch</code>.
<ul>
<li><code>pp_degree</code>: ç”¨äºæ§åˆ¶ <a href="https://arxiv.org/abs/2112.11446">PipeFusion</a> ä¸­æ¨¡å‹ Transoformer Blocks çš„åˆ‡åˆ†ä¸ªæ•°ã€‚</li>
<li><code>num_pipeline_patch</code>: ç”¨äºæ§åˆ¶å¯¹ latent feature map çš„åˆ‡åˆ†å—æ•°.</li>
<li><code>attn_layer_num_for_pp</code>: æ˜¯ä¸€ä¸ª listï¼Œè¡¨ç¤º <code>pp_degree</code> é‡Œæ¯ä¸ª stage çš„ Transformer å±‚æ•°ã€‚</li>
</ul>
</li>
</ul>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>å…³äº PipeFusionï¼ŒåŸæ–‡è¯´åˆ‡åˆ†çš„ patch æ•°å’Œ pipeline å¤§å°å¯ä»¥ä¸åŒï¼Œä½†è¿™é‡Œè¦æ±‚ <code>len(attn_layer_num_for_pp)=pp_degree</code></p></div>

<div class="notice info" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="92 59.5 300 300">
  <path d="M292 303.25V272c0-3.516-2.734-6.25-6.25-6.25H267v-100c0-3.516-2.734-6.25-6.25-6.25h-62.5c-3.516 0-6.25 2.734-6.25 6.25V197c0 3.516 2.734 6.25 6.25 6.25H217v62.5h-18.75c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h87.5c3.516 0 6.25-2.734 6.25-6.25Zm-25-175V97c0-3.516-2.734-6.25-6.25-6.25h-37.5c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h37.5c3.516 0 6.25-2.734 6.25-6.25Zm125 81.25c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Info</p><p>è®¾å¤‡æ•°å¿…é¡»ç­‰äº <code>dp_degree * cfg_degree * sp_degree * tp_degree * num_pipeline_patch</code>ï¼Œå¹¶ä¸” <code>pp_degree</code> å¿…é¡»å°äºç­‰äºè®¾å¤‡æ•°ã€‚
<code>ulysses_degree</code> å¿…é¡»è¦å¤§äºä¸”èƒ½è¢« attention çš„å¤´æ•°æ•´é™¤ã€‚</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">ParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">dp_config</span><span class="o">=</span><span class="n">DataParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">dp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cfg_parallel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_cfg_parallel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">sp_config</span><span class="o">=</span><span class="n">SequenceParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">ulysses_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ulysses_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">ring_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ring_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">tp_config</span><span class="o">=</span><span class="n">TensorParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">tp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">split_scheme</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">split_scheme</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_config</span><span class="o">=</span><span class="n">PipeFusionParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">pp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pipefusion_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_pipeline_patch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_layer_num_for_pp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layer_num_for_pp</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="construct-pipeline">Construct Pipeline<a hidden class="anchor" aria-hidden="true" href="#construct-pipeline">#</a></h1>
<p>è§£æå®Œé…ç½®å‚æ•°å¹¶æ„å»ºäº† <code>engine_config</code> åï¼Œä¸‹ä¸€æ­¥æ˜¯æ„å»ºæ¨¡å‹çš„ pipeline.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">pipe</span> <span class="o">=</span> <span class="n">xFuserPixArtAlphaPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>  <span class="c1"># First construct a PixArtAlphaPipeline, then pass it and engine_config to xFuserPipelineBaseWrapper</span>
</span></span><span class="line"><span class="cl">        <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">engine_config</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">engine_config</span><span class="o">=</span><span class="n">engine_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pipe</span><span class="o">.</span><span class="n">prepare_run</span><span class="p">(</span><span class="n">input_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>xFuserPixArtAlphaPipeline ç»§æ‰¿è‡ª <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/model_executor/pipelines/base_pipeline.py#L61">xFuserPipelineBaseWrapper</a>ï¼Œ_init_runtime_state å‡½æ•°ç»è¿‡ä¸€ç•ªè°ƒç”¨åä¼šä½¿ç”¨ <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/core/distributed/parallel_state.py#L265">initialize_model_parallel</a> åˆå§‹åŒ– <code>_RUNTIME</code> æœ‰å…³æ¨¡å‹å‚æ•°çš„éƒ¨åˆ†å’Œæ¨¡å‹å¹¶è¡Œçš„å…¨å±€å˜é‡ <code>_DP, _CFG, _PP, _SP, _TP</code>ï¼Œå®ƒæ˜¯ä¸€ä¸ª DiTRuntimeState (ç»§æ‰¿ RuntimeState) å®ä¾‹ï¼Œè®°å½•äº†æ¯ä¸ª Group åŒ…å«çš„è®¾å¤‡ç´¢å¼•ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜åŒ…æ‹¬ PipeFusionParallel ä¸­æœ‰å…³ patch ç´¢å¼•çš„å‚æ•° (åœ¨ç¨å pipeline æ‰§è¡Œçš„æ—¶å€™è®¡ç®—).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xFuserPipelineBaseWrapper</span><span class="p">(</span><span class="n">xFuserBaseWrapper</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pipeline</span><span class="p">:</span> <span class="n">DiffusionPipeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">engine_config</span><span class="p">:</span> <span class="n">EngineConfig</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">:</span> <span class="n">DiffusionPipeline</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_init_runtime_state</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">engine_config</span><span class="o">=</span><span class="n">engine_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># backbone</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;transformer&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;unet&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># vae</span>
</span></span><span class="line"><span class="cl">        <span class="n">vae</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;vae&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scheduler</span>
</span></span><span class="line"><span class="cl">        <span class="n">scheduler</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;scheduler&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">transformer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_transformer_backbone</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">unet</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">unet</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_unet_backbone</span><span class="p">(</span><span class="n">unet</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_scheduler</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">pipeline</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   
</span></span><span class="line"><span class="cl">   
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_convert_transformer_backbone</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">				<span class="c1">#...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&#34;Transformer backbone found, paralleling transformer...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wrapper</span> <span class="o">=</span> <span class="o">**</span><span class="n">xFuserTransformerWrappersRegister</span><span class="o">.</span><span class="n">get_wrapper</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span><span class="o">**</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span> <span class="o">=</span> <span class="n">wrapper</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">transformer</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="initialize_model_parallel">initialize_model_parallel<a hidden class="anchor" aria-hidden="true" href="#initialize_model_parallel">#</a></h2>
<p>è¯¥å‡½æ•°ä¸­ä¼šåˆå§‹åŒ–ä¸€ä¸ª <code>RankGenerator</code>ï¼Œå®ƒæ¥æ”¶æ¯ä¸ªå¹¶è¡Œæ–¹æ³•çš„è®¾å¤‡ç»„å¤§å°å’Œå¹¶è¡Œåº¦å¤§å°é¡ºåºã€‚å…¶ä¸»è¦çš„æ–¹æ³•æ˜¯é€šè¿‡ <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/core/distributed/utils.py#L4">generate_masked_orthogonal_rank_groups</a> å‡½æ•°ç¡®å®šæ¯ä¸ªå¹¶è¡Œç»„ç”±åŒ…å«å“ªäº›è®¾å¤‡ï¼Œå…ˆæŠŠå¹¶è¡Œæ–¹æ³•æŒ‰ç…§å¹¶è¡Œåº¦ä»å°åˆ°å¤§æ’åˆ—æˆ <code>tp-sp-pp-cfg-dp</code>. å†æ ¹æ®è¦ç”Ÿæˆçš„å¹¶è¡Œç»„äº§ç”Ÿå¯¹åº”çš„ <code>mask</code>. å³å¦‚æœè¦ç”Ÿæˆ <code>pp</code> ç»„å¯¹åº”çš„ rankï¼Œé‚£ä¹ˆ <code>mask = [0, 0, 1, 0, 0]</code></p>
<p>è¯¥å‡½æ•°é¦–å…ˆä¼šç”Ÿæˆéœ€è¦ç”Ÿæˆçš„å¹¶è¡Œç»„çš„å¤§å°ç»„æˆçš„ masked_shape å’Œä¸éœ€è¦ç”Ÿæˆçš„ unmasked_shape. é¦–å…ˆè¦ç”¨ prefix_product è®¡ç®— <code>global_stride</code>ï¼Œå³æ¯ä¸ªå¹¶è¡Œåº¦çš„è®¾å¤‡ç»„åŒ…å«å‡ ä¸ªè®¾å¤‡ã€‚å†æ ¹æ® <code>mask</code> å–å‡ºå¯¹åº”çš„ <code>mask_stride</code> å’Œ <code>unmaskd_stride</code>. <code>group_size = mask_stride[-1]</code> å³ä¸ºæœ€å¤§å¹¶è¡Œåº¦çš„ç»„åŒ…å«çš„è®¾å¤‡æ•°ã€‚<code>num_of_group = num_of_device / mask_stride[-1]</code> å³ä¸ºè¦ç”Ÿæˆå‡ ä¸ªå¹¶è¡Œåº¦æœ€å¤§çš„ç»„ã€‚å…ˆéå†è¦ç”Ÿæˆçš„æ¯ä¸ªè®¾å¤‡ç»„ï¼Œå¹¶ç”¨ decompose å‡½æ•°ç¡®å®šè¯¥è®¾å¤‡ç»„åœ¨ä¸éœ€è¦å¹¶è¡Œç»´åº¦ä¸Šçš„ç´¢å¼•ï¼›å†éå†è¯¥ç»„ä¸­çš„æ¯ä¸ªè®¾å¤‡çš„ lock rankï¼Œç¡®å®šè¯¥è®¾å¤‡åœ¨éœ€è¦å¹¶è¡Œç»´åº¦ä¸Šçš„ç´¢å¼•ï¼Œæœ€åç”¨ inner_product ç¡®å®šè¯¥è®¾å¤‡çš„ global rank.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_masked_orthogonal_rank_groups</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">parallel_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">prefix_product</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">init</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>  <span class="c1"># Exclusive</span>
</span></span><span class="line"><span class="cl">        <span class="n">r</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">a</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">init</span> <span class="o">=</span> <span class="n">init</span> <span class="o">*</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">            <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">r</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">inner_product</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">b</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">decompose</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># index: ç¬¬å‡ ä¸ªå¹¶è¡Œç»„  # shape: å¹¶è¡Œç»„å¤§å°çš„ list</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function solve the math problem below:
</span></span></span><span class="line"><span class="cl"><span class="s2">            There is an equation: index = sum(idx[i] * stride[i])
</span></span></span><span class="line"><span class="cl"><span class="s2">            And given the value of index, stride.
</span></span></span><span class="line"><span class="cl"><span class="s2">            Return the idx.
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function will used to get the pp/dp/pp_rank from group_index and rank_in_group.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">stride</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">idx</span> <span class="o">=</span> <span class="p">[(</span><span class="n">index</span> <span class="o">//</span> <span class="n">d</span><span class="p">)</span> <span class="o">%</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">)]</span>  <span class="c1">#  è®¡ç®—åœ¨æ¯ä¸ªå¹¶è¡Œç»´åº¦ä¸Šçš„ç´¢å¼•</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># stride is a prefix_product result. And the value of stride[-1]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># is not used.</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stride</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])])</span> <span class="o">==</span> <span class="n">index</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span> <span class="s2">&#34;idx </span><span class="si">{}</span><span class="s2"> with shape </span><span class="si">{}</span><span class="s2"> mismatch the return idx </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">masked_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="p">]</span>  <span class="c1"># éœ€è¦é‡‡å–å¹¶è¡Œçš„ç»´åº¦</span>
</span></span><span class="line"><span class="cl">    <span class="n">unmasked_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="p">]</span>  <span class="c1"># ä¸éœ€è¦çš„</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">global_stride</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">)</span>  <span class="c1"># exclusive å‰ç¼€ç§¯ è¡¨ç¤ºå¤§çš„å¹¶è¡Œç»´åº¦åŒ…æ‹¬å‡ ä¸ªè®¾å¤‡</span>
</span></span><span class="line"><span class="cl">    <span class="n">masked_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_stride</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">unmasked_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_stride</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">group_size</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">masked_shape</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># æœ€å¤§çš„ä¸€ä¸ªå¹¶è¡Œç»´åº¦åŒ…æ‹¬å‡ ä¸ªè®¾å¤‡</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_of_group</span> <span class="o">=</span> <span class="n">world_size</span> <span class="o">//</span> <span class="n">group_size</span>  <span class="c1"># åˆ†æˆå‡ ä¸ªå¤§ç»„</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>  
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">group_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_group</span><span class="p">):</span>  <span class="c1"># éå†æ¯ä¸ªè®¾å¤‡ç»„</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get indices from unmaksed for group_index.</span>
</span></span><span class="line"><span class="cl">        <span class="n">decomposed_group_idx</span> <span class="o">=</span> <span class="n">decompose</span><span class="p">(</span><span class="n">group_index</span><span class="p">,</span> <span class="n">unmasked_shape</span><span class="p">)</span>  <span class="c1"># å¾—åˆ°åœ¨ä¸éœ€è¦é‡‡å–å¹¶è¡Œçš„ç»´åº¦ä¸Šçš„ç´¢å¼•</span>
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rank_in_group</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">):</span>  <span class="c1"># éå†è¯¥ç»„ä¸­çš„æ¯ä¸ªè®¾å¤‡ local rank</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># get indices from masked for rank_in_group.</span>
</span></span><span class="line"><span class="cl">            <span class="n">decomposed_rank_idx</span> <span class="o">=</span> <span class="n">decompose</span><span class="p">(</span><span class="n">rank_in_group</span><span class="p">,</span> <span class="n">masked_shape</span><span class="p">)</span>  <span class="c1"># å¾—åˆ°æœ€å¤§å¹¶è¡Œç»„çš„æ¯ä¸ªè®¾å¤‡åœ¨é‡‡å–å¹¶è¡Œçš„ç»´åº¦ä¸Šçš„ç´¢å¼•</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>  <span class="o">//</span> <span class="n">ç›¸åŠ å¾—åˆ°å…¨å±€rank</span>
</span></span><span class="line"><span class="cl">                <span class="n">inner_product</span><span class="p">(</span><span class="n">decomposed_rank_idx</span><span class="p">,</span> <span class="n">masked_stride</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">                <span class="o">+</span> <span class="n">inner_product</span><span class="p">(</span><span class="n">decomposed_group_idx</span><span class="p">,</span> <span class="n">unmasked_stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">ranks</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="hybrid-parallelsim-design">Hybrid Parallelsim Design<a hidden class="anchor" aria-hidden="true" href="#hybrid-parallelsim-design">#</a></h2>
<p>xDiTæ”¯æŒå››ç§å¹¶è¡Œæ–¹å¼ï¼šPipeFusionã€Sequenceã€Data å’Œ CFG Parallelã€‚å…¶ä¸­ï¼ŒData å’Œ CFG Parallelåœ¨å›¾åƒé—´å¹¶è¡Œç›¸å¯¹ç®€å•ï¼Œè€Œ PipeFusionå’Œ Sequence åœ¨å›¾åƒå†…éƒ¨çš„ä¸åŒ Patch é—´å¹¶è¡Œåˆ™è¾ƒä¸ºå¤æ‚ã€‚èƒ½</p>
<p>PipeFusion åˆ©ç”¨ Input Tempor Redundancyç‰¹ç‚¹ï¼Œä½¿ç”¨è¿‡æ—¶çš„ KVï¼ˆStale KVï¼‰è¿›è¡Œ Attention è®¡ç®—ï¼Œè¿™ä½¿å¾— PipeFusion æ— æ³•åƒå¤§å‹è¯­è¨€æ¨¡å‹é‚£æ ·è½»æ¾åœ°å®ç°å¹¶è¡Œç­–ç•¥çš„æ··åˆã€‚ä½¿ç”¨æ ‡å‡†çš„åºåˆ—å¹¶è¡Œæ¥å£ï¼Œå¦‚RingAttentionã€Ulyssesæˆ– USPï¼Œæ— æ³•æ»¡è¶³ SP ä¸PipeFusionæ··åˆå¹¶è¡Œçš„éœ€æ±‚ã€‚</p>
<p>æˆ‘ä»¬å¯¹è¿™ä¸ªé—®é¢˜å…·ä½“è¯´æ˜ï¼Œä¸‹å›¾å±•ç¤ºäº†pipe_degree=4ï¼Œsp_degree=2çš„æ··åˆå¹¶è¡Œæ–¹æ³•ã€‚è®¾ç½® <code>num_pipeline_patch</code>=4ï¼Œå›¾ç‰‡åˆ‡åˆ†ä¸º M=<code>num_pipeline_patch*sp_degree</code>=8 ä¸ª Patchï¼Œåˆ†åˆ«æ˜¯ P0~P7.</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/hybrid_pp_scheme.png" alt="hybrid process group config"  width="60%">
</div>
<p>Standard SP Attention çš„è¾“å…¥Qï¼ŒKï¼ŒV å’Œè¾“å‡º O éƒ½æ˜¯æ²¿ç€åºåˆ—ç»´åº¦åˆ‡åˆ†ï¼Œä¸”åˆ‡åˆ†æ–¹å¼ä¸€è‡´ã€‚å¦‚æœä¸åŒ rank çš„è¾“å…¥ patch æ²¡æœ‰é‡å ï¼Œæ¯ä¸ª micro step è®¡ç®—å‡º fresh KV æ›´æ–°çš„ä½ç½®åœ¨ä¸åŒ rank é—´ä¹Ÿæ²¡æœ‰é‡å ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œstandard SP çš„ KV Buffer ä¸­é»„è‰²éƒ¨åˆ†æ˜¯ SP0 rank=0 æ‹¥æœ‰çš„ fresh KVï¼Œç»¿è‰²éƒ¨åˆ†æ˜¯ SP1 rank=1 æ‹¥æœ‰çš„fresh KVï¼ŒäºŒè€…å¹¶ä¸ç›¸åŒã€‚åœ¨è¿™ä¸ª diffusion step å†…ï¼Œdevice=0 æ— æ³•æ‹¿åˆ° P1,3,5,7 çš„ fresh KV è¿›è¡Œè®¡ç®—ï¼Œä½†æ˜¯ PipeFusion åˆ™éœ€è¦åœ¨ä¸‹ä¸€ä¸ª diffusion step ä¸­ï¼Œæ‹¥æœ‰ä¸Šä¸€ä¸ªdiffusion step å…¨éƒ¨çš„ KV. standard SP åªæ‹¥æœ‰ 1/sp_degree çš„ fresh kv bufferï¼Œå› æ­¤æ— æ³•è·å¾—æ··åˆå¹¶è¡Œæ¨ç†æ­£ç¡®çš„ç»“æœã€‚</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/hybrid_workflow.png" alt="hybrid parallel workflow">
</div>
<p>xDiTä¸“é—¨å®šåˆ¶äº†åºåˆ—å¹¶è¡Œçš„å®ç°æ–¹å¼ï¼Œä»¥é€‚åº”è¿™ç§æ··åˆå¹¶è¡Œçš„éœ€æ±‚ã€‚xDiTä½¿ç”¨ <code>xFuserLongContextAttention</code> æŠŠSPçš„ä¸­é—´ç»“æœå­˜åœ¨ KV Buffer å†…ã€‚æ•ˆæœå¦‚ä¸‹å›¾ï¼Œæ¯ä¸ª micro-step SP æ‰§è¡Œå®Œæ¯•åï¼ŒSP Group å†…ä¸åŒ rank è®¾å¤‡çš„ fresh KVæ˜¯ replicate çš„ã€‚è¿™æ ·ä¸€ä¸ª diffusion step åï¼ŒSP Group æ‰€æœ‰è®¾å¤‡çš„ KV Buffer éƒ½æ›´æ–°æˆæœ€æ–°ï¼Œä¾›ä¸‹ä¸€ä¸ª Diffusion Step ä½¿ç”¨ã€‚</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/kvbuffer_hybrid.png" alt="kvbuffer in hybrid parallel">
</div>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>å‡è®¾ä¸€å…±æœ‰ 16 ä¸ª GPUï¼Œç´¢å¼•è¡¨ç¤ºä¸º g0 &hellip; g15ï¼Œå¹¶è¡Œæ–¹æ³•å’Œå¹¶è¡Œåº¦è®¾ç½®å¦‚ä¸‹</p>
<p><code>dp_degree (2) * cfg_degree (2) * pp_degree (2) * sp_degree (2) = 16</code>.</p>
<p>é‚£ä¹ˆä¸€å…±ä¼šåˆ›å»º 2 data parallel-groups, 8 CFG groups, 8 pipeline-parallel groups &amp; 8 sequence-parallel groups:</p>
<ul>
<li>2 data-parallel groups:
[g0, g1, g2, g3, g4, g5, g6, g7],
[g8, g9, g10, g11, g12, g13, g14, g15]</li>
<li>8 CFG-parallel groups:
[g0, g4], [g1, g5], [g2, g6], [g3, g7],
[g8, g12], [g9, g13], [g10, g14], [g11, g15]</li>
<li>8 pipeline-parallel groups:
[g0, g2], [g4, g6], [g8, g10], [g12, g14],
[g1, g3], [g5, g7], [g9, g11], [g13, g15]</li>
<li>8 sequence-parallel groups:
[g0, g1], [g2, g3], [g4, g5], [g6, g7],
[g8, g9], [g10, g11], [g12, g13], [g14, g15]</li>
</ul></div>

<h2 id="convert-model">Convert Model<a hidden class="anchor" aria-hidden="true" href="#convert-model">#</a></h2>
<p><a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/model_executor/models/transformers/base_transformer.py#L76">_split_transformer_blocks</a> ä¼šå¯¹ transformer block è¿›è¡Œåˆ†é…ï¼Œå¦‚æœ parallel_config æŒ‡å®šäº† attn_layer_num_for_ppï¼Œå³å­˜æœ‰æ¯ä¸ª pipeFusion çš„è®¾å¤‡è¢«åˆ†é…çš„ transformer block æ•°é‡çš„åˆ—è¡¨ï¼ŒæŒ‰å…¶è¿›è¡Œåˆ†é…ï¼›å¦åˆ™å¹³å‡åˆ†ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_split_transformer_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># omit</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># transformer layer split</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_layer_num_for_pp</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># è·å–æ¯ä¸ª pipeFusion çš„è®¾å¤‡è¢«åˆ†é…çš„ transformer block æ•°é‡</span>
</span></span><span class="line"><span class="cl">        <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pp_config</span><span class="o">.</span><span class="n">attn_layer_num_for_pp</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_rank</span> <span class="o">=</span> <span class="n">get_pipeline_parallel_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_world_size</span> <span class="o">=</span> <span class="n">get_pipeline_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attn_layer_num_for_pp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span> <span class="p">:</span> <span class="n">attn_layer_num_for_pp</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">attn_layer_num_for_pp</span><span class="p">[:</span> <span class="n">pp_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="p">:</span> 
</span></span><span class="line"><span class="cl">                                                                            <span class="nb">sum</span><span class="p">(</span><span class="n">attn_layer_num_for_pp</span><span class="p">[:</span><span class="n">pp_rank</span><span class="p">])]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>  <span class="c1"># æ²¡æœ‰æŒ‡å®šåˆ™å¹³å‡åˆ†</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_blocks_per_stage</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">)</span> <span class="o">+</span> <span class="n">pp_world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">pp_world_size</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">pp_rank</span> <span class="o">*</span> <span class="n">num_blocks_per_stage</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">pp_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_blocks_per_stage</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">),)</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># position embedding</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">norm_out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">transformer</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>åŒæ—¶ä¹Ÿä¼š convert åŸå…ˆçš„ transformer backbone ä¸º <a href="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/models/transformers/pixart_transformer_2d.py#L21">xFuserPixArtTransformer2DWrapper</a>ï¼Œå…·ä½“è¡¨ç°ä¸ºåªæœ‰ pipeline çš„ç¬¬ä¸€é˜¶æ®µè¿›è¡Œ position embeddingï¼Œæœ€åä¸€é˜¶æ®µè¿›è¡Œ unpatchify å˜ä¸ºåŸæ¥çš„å›¾åƒå½¢çŠ¶ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@xFuserTransformerWrappersRegister.register</span><span class="p">(</span><span class="n">PixArtTransformer2DModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xFuserPixArtTransformer2DWrapper</span><span class="p">(</span><span class="n">xFuserTransformerBaseWrapper</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="p">:</span> <span class="n">PixArtTransformer2DModel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">submodule_classes_to_wrap</span><span class="o">=</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">PatchEmbed</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">submodule_name_to_wrap</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;attn1&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@xFuserBaseWrapper.forward_check_condition</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">added_cond_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cross_attention_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>  
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_patch_height_width</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># * only pp rank 0 needs pos_embed (patchify)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">	    <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">output</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">Transformer2DModelOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="pipeline-execution">Pipeline Execution<a hidden class="anchor" aria-hidden="true" href="#pipeline-execution">#</a></h1>
<p>åœ¨è¿›è¡Œ warm up åä¾¿ä¼šè¿›è¡Œæ¨¡å‹æ¨ç†å’Œé‡‡æ ·å™¨çš„å»å™ªè¿‡ç¨‹ã€‚æ¨¡å‹æ¨ç†é€šè¿‡è°ƒç”¨ pipeline çš„ <code>__call__</code> æ–¹æ³•å®ç°ã€‚åœ¨åŸå…ˆ diffusers åŒ…ä¸­çš„ <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py">PixaeArtAlphaPipeline</a> åŸºç¡€ä¸Šåšäº†ä¸€äº›ä¿®æ”¹ã€‚æˆ‘ä»¬ç›´æ¥çœ‹ä¿®æ”¹çš„éƒ¨åˆ†ã€‚</p>
<p><code>get_runtime_state()</code> è¿”å› <code>_RUNTIME</code> ï¼Œå†è°ƒç”¨ <code>set_input_parameters</code> æ–¹æ³•ï¼Œè®¾ç½®è¾“å…¥å‚æ•°å’Œè®¡ç®— PipeFusionParallel ä¸­æœ‰å…³ patch ç´¢å¼•çš„å‚æ•°ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">set_input_parameters</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">num_inference_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>è¯¥å‡½æ•°ä¼šè®¡ç®—</p>
<ul>
<li>pipeline parallel ä¸­æ¯ä¸ª patch çš„é«˜åº¦ï¼Œå¿…é¡»æ˜¯ <code>patch_size * num_sp_patches</code> çš„æ•´æ•°å€ã€‚</li>
<li>å°†æ¯ä¸ªæµæ°´çº¿é˜¶æ®µçš„ patch é«˜åº¦å‡åŒ€åœ°åˆ†é…ç»™ <code>num_sp_patches</code> ä¸ªåºåˆ—å¹¶è¡Œè®¾å¤‡ï¼Œè®¡ç®—æ¯ä¸ªè®¾å¤‡çš„ patch é«˜åº¦å’Œèµ·å§‹ç´¢å¼•ã€‚</li>
</ul>
<p>ç„¶åä¼šå¯¹ prompt åµŒå…¥åçš„æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬åœ¨ cfg parallel ç»„ä¸­çš„è®¾å¤‡è¿›è¡Œåˆ†å‰², rank 0 è´Ÿæ ·æœ¬ï¼Œrank 1 æ­£æ ·æœ¬ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">prompt_embeds</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">prompt_attention_mask</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_cfg_split_batch</span><span class="p">(</span><span class="n">negative_prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">negative_prompt_attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">prompt_attention_mask</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_process_cfg_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_0_negative</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_0</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_1_negative</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">get_classifier_free_guidance_world_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">concat_group_0_negative</span><span class="p">,</span> <span class="n">concat_group_0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">concat_group_1_negative</span><span class="p">,</span> <span class="n">concat_group_1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">get_classifier_free_guidance_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">concat_group_0_negative</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">concat_group_1_negative</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">get_classifier_free_guidance_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">concat_group_0</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">concat_group_1</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid classifier free guidance rank&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">concat_group_0</span><span class="p">,</span> <span class="n">concat_group_1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="async-pipeline">Async Pipeline<a hidden class="anchor" aria-hidden="true" href="#async-pipeline">#</a></h1>
<h2 id="initialize-pipeline">Initialize Pipeline<a hidden class="anchor" aria-hidden="true" href="#initialize-pipeline">#</a></h2>
<p>é¦–å…ˆä¼šåˆå§‹åŒ– pipelineï¼Œrank 0 ä¼šæ¥æ”¶ warmup é˜¶æ®µçš„ latents ç„¶åæ²¿ç€ H ç»´åº¦è¿›è¡Œåˆ†å—ï¼Œrank -1 ä¹Ÿä¼šæ²¿ç€ H ç»´åº¦è¿›è¡Œåˆ†å—ã€‚ç„¶åä¸ºæ¯ä¸ª patch åˆ›å»ºæ¥æ”¶çš„ä»»åŠ¡ï¼Œæ³¨æ„ rank 0 ç¬¬ä¸€æ¬¡æ˜¯ä» warmup é˜¶æ®µæ¥æ”¶ latentsï¼Œæ‰€ä»¥ä»–çš„éœ€è¦æ¥æ”¶çš„ timestep å°‘ä¸€ä¸ªã€‚
<code>patch_latents</code> è¡¨ç¤ºå½“å‰è®¾å¤‡æ­£åœ¨å¤„ç†çš„ patch æ•°æ®ï¼Œå®ƒä¼šåœ¨æµæ°´çº¿çš„æ¯ä¸€é˜¶æ®µè¿›è¡Œå¤„ç†å’Œä¼ é€’ã€‚<code>last_patch_latents</code> åªåœ¨æµæ°´çº¿çš„æœ€åé˜¶æ®µè®¾å¤‡ä¸­ä½¿ç”¨ï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ª patch çš„æœ€ç»ˆè®¡ç®—ç»“æœã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">latents</span>
</span></span><span class="line"><span class="cl"><span class="n">num_pipeline_patch</span> <span class="o">=</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span>
</span></span><span class="line"><span class="cl"><span class="n">num_pipeline_warmup_steps</span> <span class="o">=</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">warmup_steps</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_async_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_timesteps</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span><span class="o">=</span><span class="n">latents</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_pipeline_warmup_steps</span><span class="o">=</span><span class="n">num_pipeline_warmup_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">last_patch_latents</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># æ¯ä¸ª pipeline group æœ€åçš„è®¾å¤‡æ¥æ”¶æ‰€æœ‰çš„ patch</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pipeline_patch</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">is_pipeline_last_stage</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_init_async_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_timesteps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_pipeline_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">set_patched_mode</span><span class="p">(</span><span class="n">patch_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get latents computed in warmup stage</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ignore latents after the last timestep</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents</span> <span class="o">=</span> <span class="p">(</span><span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_recv</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                  <span class="k">if</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">                  <span class="k">else</span> <span class="n">latents</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">latents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_height</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">latents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_height</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">recv_timesteps</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_timesteps</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="k">else</span> <span class="n">num_timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># construct receive tasks for each patch</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">recv_timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">add_pipeline_recv_task</span><span class="p">(</span><span class="n">patch_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">patch_latents</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="iterate-over-timesteps">Iterate Over Timesteps<a hidden class="anchor" aria-hidden="true" href="#iterate-over-timesteps">#</a></h1>
<p>å¯¹äºæ¯ä¸ª <code>timestep</code>ï¼ˆå³æ¯ä¸ªå»å™ªæ­¥éª¤ï¼‰ï¼Œä¼šå¯¹æ¯ä¸ª patch æ‰§è¡Œï¼š</p>
<ol>
<li>å¦‚æœå½“å‰è®¾å¤‡æ˜¯æµæ°´çº¿çš„æœ€åä¸€é˜¶æ®µ (<code>is_pipeline_last_stage()</code>)ï¼Œå°†å½“å‰ patch çš„æ•°æ®ä¿å­˜åˆ° <code>last_patch_latents</code> ä¸­ã€‚</li>
<li>å¦‚æœä¸æ˜¯ç¬¬ä¸€é˜¶æ®µçš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ (<code>i == 0</code>)ï¼Œè°ƒç”¨ <code>recv_next()</code> æ¥å¼‚æ­¥æ¥æ”¶æ¥è‡ªä¸Šä¸€è®¾å¤‡çš„ patch æ•°æ®ï¼ˆéé˜»å¡æ“ä½œï¼Œé€šè¿‡ <code>irecv</code> å®Œæˆï¼‰ã€‚</li>
<li>å¯¹æ¯ä¸ª patch æ‰§è¡Œæ¨¡å‹çš„å‰å‘ä¼ æ’­ <code>_backbone_forward</code>ï¼Œæ ¹æ®å½“å‰æ—¶é—´æ­¥ <code>t</code> è¿›è¡Œæ¨ç†å’Œè®¡ç®—ã€‚</li>
<li>å¦‚æœå½“å‰è®¾å¤‡æ˜¯æœ€åä¸€é˜¶æ®µï¼Œè°ƒç”¨ <code>_scheduler_step</code> æ¥æ ¹æ®å™ªå£°è¿›è¡Œå»å™ªï¼Œå¹¶å°†æ•°æ®å‘é€ç»™ä¸‹ä¸€ä¸ªè®¾å¤‡ <code>pipeline_isend</code>ã€‚</li>
<li>å¯¹äºéæœ€åé˜¶æ®µçš„è®¾å¤‡ï¼Œç»§ç»­å°†å½“å‰ patch çš„è®¡ç®—ç»“æœå‘é€åˆ°ä¸‹ä¸€è®¾å¤‡ã€‚</li>
</ol>
<p><code>get_pp_group().pipeline_isend</code> ç”¨äºå°†å½“å‰ patch å‘é€åˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡ï¼Œä½¿ç”¨çš„æ˜¯ torch.distributed.isendï¼Œè¿™æ˜¯éé˜»å¡å‘é€ã€‚
<code>get_pp_group().recv_next</code> ä¼šå‡†å¤‡å¥½æ¥æ”¶æ¥è‡ªä¸Šä¸€ä¸ªè®¾å¤‡çš„æ•°æ®ï¼Œrecv_buffer ç”¨æ¥å­˜æ”¾æ¥æ”¶åˆ°çš„æ•°æ®ã€‚irecv å®ç°éé˜»å¡æ¥æ”¶ï¼Œå¯ä»¥åœ¨ç­‰å¾…æ•°æ®çš„åŒæ—¶è¿›è¡Œå…¶ä»–æ“ä½œã€‚</p>
<div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>scheduler_step åªå¯¹å•ç‹¬çš„ patch è¿›è¡Œï¼ŒåŸå› æœªçŸ¥ã€‚</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">first_async_recv</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">last_patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">first_async_recv</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">recv_next</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">                <span class="n">first_async_recv</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">get_pipeline_recv_data</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents</span><span class="o">=</span><span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">prompt_attention_mask</span><span class="o">=</span><span class="n">prompt_attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">added_cond_kwargs</span><span class="o">=</span><span class="n">added_cond_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">guidance_scale</span><span class="o">=</span><span class="n">guidance_scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>  <span class="c1"># pred noise</span>
</span></span><span class="line"><span class="cl">                <span class="n">last_patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>  <span class="c1"># last timestep noise</span>
</span></span><span class="line"><span class="cl">                <span class="n">t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">extra_step_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_isend</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span> <span class="n">segment_idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_isend</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span> <span class="n">segment_idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">patch_idx</span> <span class="o">==</span> <span class="n">num_pipeline_patch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">pass</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">recv_next</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">next_patch</span><span class="p">()</span>  <span class="c1"># switch to next: (self.pipeline_patch_idx + 1) % self.num_pipeline_patch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">or</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_warmup_steps</span>
</span></span><span class="line"><span class="cl">        <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">callback</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&#34;callback not supported in async &#34;</span> <span class="s2">&#34;pipeline&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">            <span class="ow">and</span> <span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">%</span> <span class="n">callback_steps</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">step_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span><span class="p">)</span> <span class="o">//</span> <span class="nb">getattr</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&#34;order&#34;</span><span class="p">,</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">callback</span><span class="p">(</span><span class="n">step_idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="construct-final-latents">Construct Final Latents<a hidden class="anchor" aria-hidden="true" href="#construct-final-latents">#</a></h2>
<p>timestep éå†å®Œæˆåï¼Œä»ç„¶æœ‰æœ€åçš„æ“ä½œè¦è¿›è¡Œï¼Œè¿™äº›æ“ä½œçš„ä¸»è¦ç›®çš„æ˜¯å°†æµæ°´çº¿å¹¶è¡Œä¸­å„ä¸ª patch çš„ç»“æœæ‹¼æ¥èµ·æ¥ï¼Œå½¢æˆå®Œæ•´çš„è¾“å‡ºç»“æœã€‚å°¤å…¶æ˜¯å¯¹äºæœ€åä¸€ä¸ªè®¾å¤‡ï¼Œè¿˜éœ€è¦å¤„ç† åºåˆ—å¹¶è¡Œï¼ˆsequence parallelismï¼‰ çš„åˆå¹¶æ“ä½œã€‚é€šè¿‡ all_gather æ“ä½œå°†æ¯ä¸ªè®¾å¤‡ä¸Šå¤„ç†çš„ patch ç»“æœæ”¶é›†èµ·æ¥ï¼Œç„¶åä»æ¯ä¸ªè®¾å¤‡çš„ <code>sp_latents_list</code> ä¸­ï¼Œæå–å‡ºå¯¹åº”äº <code>pp_patch_idx</code> çš„ patch æ•°æ®å¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥ã€‚</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">latents</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">patch_latents</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">get_sequence_parallel_world_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">sp_degree</span> <span class="o">=</span> <span class="n">get_sequence_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">sp_latents_list</span> <span class="o">=</span> <span class="n">get_sp_group</span><span class="p">()</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents</span><span class="p">,</span> <span class="n">separate_tensors</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">pp_patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents_list</span> <span class="o">+=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="n">sp_latents_list</span><span class="p">[</span><span class="n">sp_patch_idx</span><span class="p">][</span>
</span></span><span class="line"><span class="cl">                    <span class="o">...</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_start_idx_local</span><span class="p">[</span><span class="n">pp_patch_idx</span><span class="p">]</span> <span class="p">:</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_start_idx_local</span><span class="p">[</span><span class="n">pp_patch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="p">:,</span>
</span></span><span class="line"><span class="cl">                <span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">sp_patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sp_degree</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">latents_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">latents</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="decode-latents">Decode Latents<a hidden class="anchor" aria-hidden="true" href="#decode-latents">#</a></h1>
<p>ä¸ºäº†é¿å… VAE ä¸­çš„ <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/vae.py#L185">Decoder</a> åœ¨å¯¹ 8192px åˆ†è¾¨ç‡å›¾åƒè¿›è¡Œ conv2D çš„è¿‡ç¨‹ä¸­å‡ºç° OOM çš„é—®é¢˜ï¼Œ xDiT ä½¿ç”¨äº†åºåˆ—å¹¶è¡Œå’Œ patch å¹¶è¡Œçš„ <a href="https://github.com/xdit-project/DistVAE/blob/a7e7ee7ec222f45af1214984561c8c645be8aece/distvae/models/layers/conv2d.py#L13">PatchConv2d</a> å’Œ <a href="https://github.com/xdit-project/DistVAE/blob/a7e7ee7ec222f45af1214984561c8c645be8aece/distvae/models/layers/normalization.py#L59">PatchGroupNorm</a> æ¥æ›¿æ¢æ‰åŸæœ‰ Decoder ä¸­çš„ <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d_blocks.py#L2682">UpDecoderBlock2D</a> å¯¹åº”çš„å±‚ã€‚</p>
<h2 id="patchgroupnorm">PatchGroupNorm<a hidden class="anchor" aria-hidden="true" href="#patchgroupnorm">#</a></h2>
<p>PatchGroupNorm åœ¨ H ç»´åº¦ä¸Šåˆ’åˆ†ä¸ºå¤šä¸ª patchï¼Œæ¯ä¸ªè®¾å¤‡æ±‚è‡ªå·±æ‰€è´Ÿè´£çš„éƒ¨åˆ†å’Œã€‚
<details class="custom-details">
    <summary class="custom-summary">GroupNorm Principles</summary>
    <div>å‡è®¾è¾“å…¥å¼ é‡ x çš„å½¢çŠ¶ä¸º [N, C, H, W]ï¼Œå…¶ä¸­ N è¡¨ç¤ºæ‰¹é‡å¤§å°ï¼ˆBatch Sizeï¼‰ï¼ŒC è¡¨ç¤ºé€šé“æ•°ï¼ˆChannelsï¼‰ï¼ŒH å’Œ W åˆ†åˆ«è¡¨ç¤ºé«˜åº¦å’Œå®½åº¦ã€‚åœ¨ GN ä¸­ï¼Œé€šé“æ•° C è¢«åˆ’åˆ†ä¸º G ç»„ï¼Œæ¯ä¸ªç»„åŒ…å« C/G ä¸ªé€šé“ã€‚è®¡ç®—æ¯ä¸ªç»„å†…å³ [C/G, H, W] ç»´åº¦ä¸Šçš„å‡å€¼å’Œæ–¹å·®ã€‚ç‰¹åˆ«çš„ G=1 æ—¶ï¼ŒGN é€€åŒ–ä¸º BNã€‚G=C æ—¶ï¼ŒGN é€€åŒ–ä¸º LNã€‚</div>
</details><br></p>
<ol>
<li>è·å–é«˜åº¦ä¿¡æ¯</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchGroupNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; def __init__(self, ...)&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">height</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">height</span><span class="p">)</span>  <span class="c1"># æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„é«˜åº¦å¹¶æ±‡æ€»ã€‚æœ€ç»ˆæ¯ä¸ªè¿›ç¨‹çš„ height éƒ½å°†è¡¨ç¤ºå…¨å±€çš„é«˜åº¦å’Œã€‚</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>è®¡ç®—æ¯ä¸ªç»„çš„é€šé“æ•°é‡ä»¥åŠæ¯ä¸ªè¿›ç¨‹å†…çš„å…ƒç´ æ•°é‡</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">channels_per_group</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span>  <span class="c1"># æ¯ä¸ªç»„çš„é€šé“æ•°é‡</span>
</span></span><span class="line"><span class="cl"><span class="n">nelements_rank</span> <span class="o">=</span> <span class="n">channels_per_group</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># å½“å‰è¿›ç¨‹è´Ÿè´£çš„æ¯ä¸ªç»„ä¸­çš„å…ƒç´ æ€»</span>
</span></span><span class="line"><span class="cl"><span class="n">nelements</span> <span class="o">=</span> <span class="n">channels_per_group</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># æ‰€æœ‰è¿›ç¨‹çš„æ¯ä¸ªç»„ä¸­çš„å…ƒç´ æ€»æ•°</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>è®¡ç®—æ¯ä¸ªç»„çš„å‡å€¼</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1">#  [batch_size, num_groups, channels_per_group, height, width]</span>
</span></span><span class="line"><span class="cl"><span class="n">group_sum</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># å¯¹æ¯ä¸ªç»„çš„æ‰€æœ‰å…ƒç´  (channels_per_group, height, width) æ±‚å¹³å‡</span>
</span></span><span class="line"><span class="cl"><span class="n">group_sum</span> <span class="o">=</span> <span class="n">group_sum</span> <span class="o">*</span> <span class="n">nelements_rank</span>  <span class="c1"># åŠ æƒåçš„å±€éƒ¨å’Œ = å±€éƒ¨å‡å€¼ * å½“å‰è¿›ç¨‹çš„å…ƒç´ æ•°é‡</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">group_sum</span><span class="p">)</span>  <span class="c1"># æ”¶é›†å¹¶æ±‡æ€»æ‰€æœ‰è¿›ç¨‹çš„å±€éƒ¨å’Œï¼Œå¾—åˆ°å…¨å±€å’Œ</span>
</span></span><span class="line"><span class="cl"><span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_sum</span> <span class="o">/</span> <span class="n">nelements</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># è®¡ç®—å…¨å±€çš„å‡å€¼ E</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>è®¡ç®—æ¯ä¸ªç»„çš„æ–¹å·®</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># å’Œè®¡ç®—å‡å€¼åŒæ ·çš„æ“ä½œ</span>
</span></span><span class="line"><span class="cl"><span class="n">group_var_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">out</span><span class="o">=</span><span class="n">group_var_sum</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="n">group_var_sum</span> <span class="o">=</span> <span class="n">group_var_sum</span> <span class="o">*</span> <span class="n">nelements_rank</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">group_var_sum</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">var</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_var_sum</span> <span class="o">/</span> <span class="n">nelements</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>å½’ä¸€åŒ–å¹¶ç¼©æ”¾ $y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">E</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchconv2d">PatchConv2d<a hidden class="anchor" aria-hidden="true" href="#patchconv2d">#</a></h2>
<p><code>PatchConv2d</code> å°†æ½œåœ¨ç©ºé—´ä¸­çš„ç‰¹å¾æ˜ å°„åˆ†å‰²æˆå¤šä¸ª patchï¼Œè·¨ä¸åŒè®¾å¤‡è¿›è¡Œåºåˆ—å¹¶è¡Œ VAE è§£ç ã€‚è¿™ç§æŠ€æœ¯å°†ä¸­é—´æ¿€æ´»æ‰€éœ€çš„å³°å€¼å†…å­˜å‡å°‘åˆ° 1/Nï¼Œå…¶ä¸­ N æ˜¯æ‰€ä½¿ç”¨çš„è®¾å¤‡æ•°é‡ã€‚å¯¹äº VAE ä¸­çš„å·ç§¯ç®—å­ï¼Œéœ€è¦å¯¹å¦‚ä¸‹å›¾æ‰€ç¤ºçš„ halo åŒºåŸŸæ•°æ®è¿›è¡Œé€šä¿¡ã€‚</p>
<p>
<figure class="post-figure">
    <a href="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/patchvaeconv.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/patchvaeconv.png" alt="Patch VAE Conv">
    </a><figcaption>Patch VAE Conv</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>  <span class="c1"># TODO: refine this type</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">assert</span> <span class="n">dilation</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;dilation is not supported in PatchConv2d&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">assert</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;dilation is not supported in PatchConv2d&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>_conv_forward</code> å‡½æ•°æ˜¯ <code>PatchConv2d</code> ç±»çš„æ ¸å¿ƒï¼Œå®ƒè´Ÿè´£åœ¨è¾“å…¥å¼ é‡ä¸Šæ‰§è¡Œå·ç§¯æ“ä½œï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒå¼è®¡ç®—åœºæ™¯ä¸‹å¤„ç†è·¨è¿›ç¨‹çš„è¾“å…¥åˆ‡åˆ†ã€halo åŒºåŸŸçš„ä¼ é€’å’Œè®¡ç®—ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨çš„è¾…åŠ©å‡½æ•°çš„ç®€è¦åŠŸèƒ½è¯´æ˜</p>
<ul>
<li><code>_get_world_size_and_rank </code>ï¼šè·å–å½“å‰åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„è¿›ç¨‹æ€»æ•° <code>world_size</code> å’Œå½“å‰è¿›ç¨‹çš„ç¼–å· <code>rank</code></li>
<li><code>_calc_patch_height_index</code>ï¼šæ ¹æ®æ¯ä¸ªè¿›ç¨‹çš„è¾“å…¥é«˜åº¦ï¼Œè®¡ç®—æ‰€æœ‰è¿›ç¨‹çš„èµ·å§‹å’Œç»“æŸé«˜åº¦ç´¢å¼•ã€‚</li>
<li><code>_calc_halo_width_in_h_dim</code>ï¼šè®¡ç®—å½“å‰è¿›ç¨‹åœ¨ h ç»´åº¦ä¸Šæ‰€éœ€çš„ä¸Šæ–¹å’Œä¸‹æ–¹çš„ halo åŒºåŸŸå®½åº¦ã€‚</li>
<li><code>_calc_bottom_halo_width</code>ï¼šè®¡ç®—å½“å‰è¿›ç¨‹ä»ä¸‹æ–¹ç›¸é‚»è¿›ç¨‹éœ€è¦æ¥æ”¶çš„ halo åŒºåŸŸçš„å®½åº¦ã€‚</li>
<li><code>_calc_top_halo_width</code>ï¼šè®¡ç®—å½“å‰è¿›ç¨‹ä»ä¸Šæ–¹ç›¸é‚»è¿›ç¨‹éœ€è¦æ¥æ”¶çš„ halo åŒºåŸŸçš„å®½åº¦ã€‚</li>
<li><code>_adjust_padding_for_patch</code>ï¼šæ ¹æ®å½“å‰è¿›ç¨‹çš„ <code>rank</code> å’Œæ€»è¿›ç¨‹æ•°è°ƒæ•´è¾“å…¥æ•°æ®çš„å¡«å……æ–¹å¼ï¼Œé˜²æ­¢è¾¹ç•Œé‡å¤è®¡ç®—ã€‚</li>
</ul>
<ol>
<li>è·å–è¾“å…¥ä¿¡æ¯ä»¥åŠé€šä¿¡ç»„ä¿¡æ¯</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_world_size_and_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">world_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># å¤„ç†éåˆ†å¸ƒå¼æƒ…å†µ</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                                <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>è·å–è¾“å…¥çš„å…ƒæ•°æ®</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">patch_height_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">patch_height_list</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">h</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">))</span>  <span class="c1"># æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„è¾“å…¥é«˜åº¦</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_height_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_patch_height_index</span><span class="p">(</span><span class="n">patch_height_list</span><span class="p">)</span>  <span class="c1"># è®¡ç®—æ¯ä¸ªè¿›ç¨‹å—çš„èµ·å§‹é«˜åº¦å’Œç»“æŸé«˜åº¦çš„ç´¢å¼•</span>
</span></span><span class="line"><span class="cl"><span class="n">halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_halo_width_in_h_dim</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># è®¡ç®—å½“å‰è¿›ç¨‹å—çš„ä¸Šä¸‹ halo åŒºåŸŸçš„å®½åº¦</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>è®¡ç®—ç›¸é‚»è¿›ç¨‹çš„ halo åŒºåŸŸ (ä¹Ÿå°±æ˜¯è‡ªå·±éœ€è¦æ¥å‘é€çš„éƒ¨åˆ†)</li>
</ol>
<p>é€šè¿‡è®¡ç®—å‰ä¸€ä¸ªè¿›ç¨‹çš„ bottom_halo_width å’Œåä¸€ä¸ªè¿›ç¨‹çš„ top_halo_width å¾—å‡ºè‡ªå·±éœ€è¦å‘é€çš„éƒ¨åˆ†</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">prev_bottom_halo_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="n">next_top_halo_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">prev_bottom_halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_bottom_halo_width</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="n">world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_top_halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_top_halo_width</span><span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_top_halo_width</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">next_top_halo_width</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>è¿›è¡Œ halo åŒºåŸŸçš„å‘é€ä¸æ¥æ”¶</li>
</ol>
<p>å¼‚æ­¥å‘é€ï¼ŒåŒæ­¥æ¥æ”¶</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">to_next</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">to_prev</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">top_halo_recv</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">bottom_halo_recv</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">next_top_halo_width</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bottom_halo_send</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">next_top_halo_width</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_next</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">bottom_halo_send</span><span class="p">,</span> <span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not rank 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_halo_recv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">top_halo_recv</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">prev_bottom_halo_width</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not rank N-1</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_halo_send</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">prev_bottom_halo_width</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_prev</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">top_halo_send</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bottom_halo_recv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">bottom_halo_recv</span><span class="p">,</span> <span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>æ‹¼æ¥ halo åŒºåŸŸ</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Remove redundancy at the top of the input</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]:,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">top_halo_recv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># concat the halo region to the input tensor </span>
</span></span><span class="line"><span class="cl">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">top_halo_recv</span><span class="p">,</span> <span class="nb">input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">bottom_halo_recv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="nb">input</span><span class="p">,</span> <span class="n">bottom_halo_recv</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="6">
<li>ç­‰å¾…å‘é€å®Œæˆå†å¼€å§‹è®¡ç®—</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">to_next</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_next</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">to_prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_prev</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="7">
<li>è¿›è¡Œå·ç§¯å’Œåå¤„ç†</li>
</ol>
<p>ä¸ºäº†å‡å°‘ memory spike ä¸€æ¬¡è®¡ç®— block_size*block_size çš„åŒºåŸŸï¼Œå¹¶å°†ç»“æœæ‹¼æ¥èµ·æ¥</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjust_padding_for_patch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">h</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">and</span> <span class="n">w</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                            <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">conv_res</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s2">&#34;zeros&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks_in_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>  <span class="c1"># h ç»´åº¦çš„ block æ•°é‡</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks_in_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>  <span class="c1"># w ...</span>
</span></span><span class="line"><span class="cl">    <span class="n">unit_chunk_size_h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">//</span> <span class="n">num_chunks_in_h</span>
</span></span><span class="line"><span class="cl">    <span class="n">unit_chunk_size_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">//</span> <span class="n">num_chunks_in_w</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">idx_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks_in_h</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">inner_output</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx_w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks_in_w</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_w</span> <span class="o">=</span> <span class="n">idx_w</span> <span class="o">*</span> <span class="n">unit_chunk_size_w</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_h</span> <span class="o">=</span> <span class="n">idx_h</span> <span class="o">*</span> <span class="n">unit_chunk_size_h</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">unit_chunk_size_w</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">unit_chunk_size_h</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># è®¡ç®—æ¯ä¸ªå—çš„å¼€å§‹å’Œç»“æŸç´¢å¼•ï¼Œè°ƒæ•´å—çš„è¾¹ç•Œ</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># å¯¹å½“å‰å—æ‰§è¡Œå·ç§¯æ“ä½œ</span>
</span></span><span class="line"><span class="cl">        <span class="n">inner_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_h</span><span class="p">:</span><span class="n">end_h</span><span class="p">,</span> <span class="n">start_w</span><span class="p">:</span><span class="n">end_w</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">inner_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/xdit/">XDiT</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/zero/">
    <span class="title">Â« Prev</span>
    <br>
    <span>ZeRO, ZeRO-Offload, ZeRO-Infinity</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/vllm/">
    <span class="title">Next Â»</span>
    <br>
    <span>VLLM Sourse Code Reading</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>Â© 2024-2025 WITHER</span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
