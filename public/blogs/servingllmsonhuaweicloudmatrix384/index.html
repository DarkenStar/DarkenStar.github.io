<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>ServingLLMsOnHuaweiCloudMatrix384 | WITHER</title>
<meta name="keywords" content="Distributed Training, LLM">
<meta name="description" content="Paper Reading of Serving Large Language Models on Huawei CloudMatrix384">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="ServingLLMsOnHuaweiCloudMatrix384">
  <meta property="og:description" content="Paper Reading of Serving Large Language Models on Huawei CloudMatrix384">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-06-17T21:50:16+08:00">
    <meta property="article:modified_time" content="2025-06-17T21:50:16+08:00">
    <meta property="article:tag" content="Distributed Training">
    <meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ServingLLMsOnHuaweiCloudMatrix384">
<meta name="twitter:description" content="Paper Reading of Serving Large Language Models on Huawei CloudMatrix384">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "ServingLLMsOnHuaweiCloudMatrix384",
      "item": "http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "ServingLLMsOnHuaweiCloudMatrix384",
  "name": "ServingLLMsOnHuaweiCloudMatrix384",
  "description": "Paper Reading of Serving Large Language Models on Huawei CloudMatrix384",
  "keywords": [
    "Distributed Training", "LLM"
  ],
  "articleBody": "Abstract CloudMatrix384硬件架构:\n论文提出了一种 peer-to-peer 的硬件设计，包含 384 个 Ascend 910C NPU 和 192 个 Kunpeng CPU，通过 Unified Bus (UB) Network 互联。UB 网络支持高带宽 (392 GB/s单向带宽)和低延迟 (1.9 µs)的全局通信，解决了传统AI集群中跨节点通信的瓶颈问题。 架构特点包括: 资源解耦和池化: 计算、存储和网络资源可以动态分配，支持灵活的并行策略 (如专家并行EP、数据并行DP). 三层网络平面: UB平面 (超节点内通信)、RDMA平面 (跨超节点通信)和VPC平面 (数据中心网络接入)。 CloudMatrix-Infer软件优化:\n预填充-解码-缓存 (PDC)解耦架构: 将LLM推理拆分为Prefill, Decode \u0026 Caching 三个子系统，通过 UB网络实现高效协同。 large-scale expert parallelism (EP) strategy: 支持高达EP320的专家并行度，每个NPU芯片承载一个专家，减少MoE模型中的通信开销。 UB驱动的分布式缓存: 利用 弹性内存服务 (EMS) 构建全局缓存池，支持KV缓存和模型权重的快速访问。 性能优化技术:\n** micro-batch 流水线 (Microbatch Pipeline)**: 重叠计算和通信，提升资源利用率。 INT8量化: 在Ascend 910C 上实现高效的8位推理，保持模型精度。 论文使用DeepSeek-R1 (671B参数MoE模型)验证了 CloudMatrix-Infer 的性能:\n预填充吞吐量: 6,688 tokens/s/NPU (4.45 tokens/s/TFLOPS)，优于 NVIDIA H100 的 SGLang (3.75 tokens/s/TFLOPS). 解码吞吐量: 1,943 tokens/s/NPU (1.29 tokens/s/TFLOPS)，在TPOT \u003c50 ms约束下仍能保持高吞吐。 缓存命中率提升: 上下文缓存 (Context Caching)在 90% 重用率时，预填充时间减少 59%. 1. Introduction LLM 的发展趋势有以下几点:\n参数规模的指数级增长。DeepSeek-R1, LLaMA-4 和 Qwen-3 通常扩展到数千亿甚至数万亿参数. 专家混合 (MoE)架构的广泛采用。MoE 通过每个 token 选择性激活少数专家，引入了结构稀疏性，实现了更大模型下的效率提升，但同时在专家路由和同步方面带来了新的系统级挑战。 上下文长度的大幅提高。上下文窗口从数万 token 扩展到超过一百万 token，对注意力计算和 KV cache 存储施加了巨大压力。 KV Cache 大小随着并发用户数量线性增长，这对其的分布、放置和访问方式提出了重大限制。 LLM 服务系统必须适应可变长度的用户输入、跨 token 的不平衡专家激活以及高度突发的用户查询，同时保持严格的延迟和吞吐量目标。满足这些需求不仅仅是简单地扩展硬件资源，而是需要全面的软硬件协同设计，包括紧密集成的计算、内存和网络硬件资源，辅以智能任务调度、自适应运行时编排以及弹性资源管理策略，能够动态响应不断演变的模型结构和波动的工作负载。\n为了应对这些挑战，论文引入了 华为CloudMatrix，一个旨在重塑 AI 基础设施基础的下一代 AI 数据中心架构。其首个生产级实现是 CloudMatrix384，它具备以下核心特点:\n硬件构成: 它是一个集成了384个昇腾 (Ascend)910C NPU、192个 鲲鹏 (Kunpeng)CPU 的AI超级节点。 核心网络: 所有组件通过一个超高带宽、低延迟的 UB 网络互联，实现了硬件上的 **完全点对点 (peer-to-peer)**通信。 架构优势: 与传统层级式设计不同，UB网络允许计算、内存等资源被动态池化、统一访问和独立扩展，特别适合处理MoE专家并行和分布式KV缓存访问这类通信密集型操作。 基于CloudMatrix384硬件，论文提出了一个名为 CloudMatrix-Infer 的综合性LLM服务解决方案，其包含了三大核心创新:\n点对点服务架构: 该架构将推理系统解耦为 预填充 (prefill), 解码 (decode) 和 缓存 (caching) 三个可独立扩展的资源池。借助UB网络，所有 NPU 都能统一访问共享的缓存数据，从而摆脱了传统架构中因数据局部性限制而导致的调度复杂性和效率低下问题。\n大规模专家并行 (LEP)策略: 该策略专为MoE模型优化，利用UB网络高效地进行 token 分发和专家输出合并。它支持极高的专家并行度 (如 EP320 )，允许每个 NPU Die 只承载一个专家，从而显著降低解码延迟。\n硬件感知优化: 包括为昇腾芯片高度优化的算子、基于 micro-batch 的流水线技术 (以重叠计算和通信)以及INT8量化 (以提升计算效率和减少内存消耗)。\n在 DeepSeek-R1 模型上的测试表明，CloudMatrix-Infer 实现了业界领先的性能和效率:\n预填充性能: 每个NPU的吞吐量为 6,688 tokens/s，计算效率为 4.45 tokens/s/TFLOPS. 解码性能: 在低于50ms的单Token输出延迟 (TPOT)下，每个NPU的吞吐量为1,943 tokens/s，计算效率为 1.29 tokens/s/TFLOPS. 性能对比: 预填充和解码的计算效率均超过了在NVIDIA H100上运行SGLang和在NVIDIA H800上运行DeepSeek的公开数据. 准确性: 在昇腾910C上进行的 INT8 量化，其模型准确率与官方 DeepSeek-R1 API 在 16 个基准测试中相当。 2. LLM Trends and Their Challenges for Datacenter Infrastructure 2.1. LLM Trends Ever-Larger Parameter Counts. scaling law 表明，增加 LLM 的参数数量可以提升其在各种任务上的表现。这一趋势的典型代表包括:\nMeta 的 Llama 4 Behemoth: 拥有近 2 万亿参数。 DeepSeek-V3: 包含 6710 亿参数。 Google 的 PaLM: 包含 5400 亿参数。 xAI 的 Grok-1: 拥有 3140 亿参数。 Sparsity through MoE. 为了控制不断攀升的训练和推理成本，现代 LLM 越来越多地采用稀疏激活的 MoE 架构。这种架构将模型的总大小与处理每个 token 所需的计算量解耦:\nMixtral 8x7B: 总参数量为 467 亿，但每个 token 只激活 129 亿参数。 Databricks 的 DBRX: 总参数量为 1320 亿，每个 token 激活 360 亿参数。 Meta 的 Llama 4 系列: Llama 4 Maverick 使用 128 个专家，而 Llama 4 Scout 使用 16 个专家。 DeepSeek-V3: 将每层的专家数量从 160 个增加到 256 个，从而在不显著增加计算负载的情况下提升了模型容量。 阿里巴巴的 Qwen3-235B: 集成了 128 个专家，每个 token 激活 220 亿参数。 华为的盘古 Ultra MoE: 总参数量达 7180 亿，每个 token 激活 390 亿参数。 这些模型共同凸显了 LLM 扩展策略的范式转变，即更强调通过架构稀疏性而非单纯的参数数量来提升性能和效率。\nExtension of Context Windows. LLM 上下文窗口的扩大使其能够处理更长的序列，这对于需要扩展推理和连贯性的任务至关重要。近期的进展包括:\nOpenAI 的 GPT-4.5: 支持 128,000 个 token 的上下文窗口。 Google 的 Gemini 2.5 Pro: 提供高达 100 万个 token 的上下文窗口。 然而处理长文本会显著增加计算成本和推理延迟。为了缓解这一问题，生产系统普遍采用上下文缓存 (context caching) 技术。该技术通过存储和复用由先前提示片段生成的 KV block，来消除对提示的冗余注意力计算，从而降低延迟并提高效率。\n2.2. Challenges for Datacenter Infrastructure 上述 LLM 的发展趋势对底层的数据中心基础设施提出了严峻的新要求。随着模型能力的扩展，它们催生了如强化学习、交互式媒体生成和自主 AI 代理等日益复杂的工作负载。这些应用不仅需要海量的计算和内存资源，还需要对基础设施进行根本性的重新架构，以支持高带宽通信和低延迟存储，从而在动态、异构的真实世界条件下满足严格的服务水平目标。\nScaling Communication-Intensive Parallelism.. 随着模型规模的增长，单个计算节点已无法容纳最先进的 AI 模型，必须使用多节点并行策略。尽管现有的 AI 集群通过 RDMA 网络支持跨节点通信，但其带宽和拓扑结构通常只为数据并行 (DP) 或流水线并行 (PP) 等通信量较小的场景优化。然而，张量并行 (Tensor Parallelism, TP) 和 专家并行 (Expert Parallelism, EP) 需要频繁、细粒度且低延迟的通信，这种通信模式难以在节点之间高效扩展。这迫使许多部署方案将 TP 和 EP 限制在单个计算节点内，从而限制了可扩展性。\nMaintaining High Utilization under Heterogeneous AI Workloads. 现代 AI 工作负载表现出高度多样化和动态的资源需求:\n训练任务通常是计算密集型的。 LLM 推理的解码阶段往往受限内存带宽。 自动驾驶模型训练等任务涉及大量的 CPU 端数据预处理。 固定的节点配置无法高效地适应这种多样性，常常导致资源过配或利用不足。为了最大化效率和适应性，现代 AI 基础设施必须能够根据每种工作负载的特定需求，动态、细粒度地组合异构资源 (如 NPU, CPU 和内存).\nEnabling Converged Execution of AI and Data-Intensive Workloads. AI 工作流与传统数据密集型操作 (如数据摄取、预处理、检索、分析和模拟) 的交叉越来越频繁。同时，数据库、大数据和高性能计算 (HPC) 等通用工作负载本身也在不断集成 AI 功能。这种融合执行模式要求高吞吐、低延迟的通信和灵活的资源编排。然而，主要为传统通用工作负载优化的老旧数据中心基础设施难以满足这些苛刻的要求。\nDelivering Memory-class Storage Performance. 现代 AI 流水线操作的数据规模已远超传统存储系统的能力。诸如摄取 PB 级数据集、管理 TB 级模型检查点以及支持延迟敏感的推理 (特别是在使用大型 KV cache 和检索增强生成 RAG 模块时) 等任务，都需要存储子系统具备内存级的带宽、延迟和 IOPS (Input/Output Operations Per Second). 围绕磁盘访问模式设计的传统存储层次结构频繁地成为性能瓶颈，因数据供给不足而导致 NPU 利用率低下。\n3 Huawei CloudMatrix 3.1. Vision for Huawei CloudMatrix 如下图所示，CloudMatrix 超越了传统以 CPU 为中心的分层设计，实现了无需 CPU 介质下包括 NPU、CPU、DRAM、SSD、NIC 及领域专用加速器在内的所有异构系统组件之间的直接高性能通信。该架构的核心是超高带宽、低延迟的 UB 网络。\nScalable Communication for TP/EP. UB 网络可以为张量并行 (TP) 和专家并行 (EP) 提供强大的通信支持，使其能够轻松扩展到单节点边界之外。\nFlexible Resource Composition for Heterogeneous Workloads. 将 CPU、NPU、内存等资源解耦成独立的资源池，允许根据工作负载按需、细粒度地进行组合。\nUnified Infrastructure for Converged Workloads. 在单一架构内同时支持 AI 和数据密集型工作负载的融合执行。\nMemory-class Storage via Disaggregated Memory Pool. 通过聚合集群中所有 CPU 挂载的 DRAM，形成一个可通过 UB 访问的共享高性能内存池，为 KV cache 复用、参数加载等提供加速。\n3.2. CloudMatrix384 Overview: A Fully Peer-to-Peer Hardware Architecture CloudMatrix384 是一个集成了 384 个昇腾 (Ascend) 910C NPU 和 192 个鲲鹏 (Kunpeng) CPU 的 AI 超级节点。其最显著的特点是，通过 UB 网络实现了跨节点通信性能与节点内性能的高度一致 (带宽下降 \u003c 3%，延迟增加 \u003c 1µs). CloudMatrix384 包含三个互补的网络平面:\nUB 平面: 超级节点内的主要纵向扩展 (Scale-Up) 网络，以全互联、无阻塞的方式连接所有 384 个 NPU 和 192 个 CPU. 每个昇腾 910C NPU 为该平面贡献超过 392 GB/s 的单向带宽。 RDMA 平面: 用于超级节点间的横向扩展 (Scale-Out) 通信，采用 RoCE 协议，确保与现有 RDMA 生态的兼容性。每个 NPU 为该平面贡献高达 400 Gbps 的单向带宽。 VPC 平面: 通过擎天卡 (Qingtian Card) 将超级节点接入更广泛的数据中心网络，用于管理、控制和访问持久化存储等操作。 3.3. Hardware Components 昇腾 910C 芯片: 作为系统的核心，它采用双晶粒 (dual-die) 封装。每个芯片包提供 752 TFLOPS 的 BF16/FP16 算力，支持 INT8 数据类型。它集成 128 GB 片上内存，总带宽高达 3.2 TB/s. 昇腾 910C 节点: 每个计算节点集成 8 个昇腾 910C NPU 和 4 个鲲鹏 CPU. UB 交换系统: 采用两级 (L1/L2) 无阻塞交换网络拓扑，将所有节点紧密连接成一个统一的超级节点。 3.4. Software Stack CANN (神经网络计算架构): 华为为昇腾 NPU 开发的完整软件生态系统，类似于 NVIDIA 的 CUDA. 它包含驱动层、运行时层 (Runtime) 和库层 (如用于分布式通信的 HCCL)，并通过图引擎 (Graph Engine, GE) 对上层框架 (如 PyTorch, TensorFlow) 的计算图进行编译和优化。 云部署基础设施软件: 包括 MatrixResource、MatrixLink、MatrixCompute 和 MatrixContainer 等一系列软件，用于在云环境中对 CloudMatrix 集群进行资源管理、网络配置和容器化部署。上层的 ModelArts 平台则提供端到端的 AI 开发和 MLOps 服务。 3.5. Suitability Analysis for DeepSeek Models DeepSeek Models and Their Deployment on NVIDIA H800 DeepSeek 在由 NVIDIA H800 GPU 组成的集群上部署其 V3 和 R1 模型，每个 GPU 内存 80 GB，节点内通过 NVLink 连接，节点间通过 400 Gbps InfiniBand 连接。该部署采用了分离式预填充-解码架构。在预填充阶段，DeepSeek 将四个 H800 节点（共 32 个 GPU）组织成一个部署单元。在每个单元内，256 个路由专家被策略性地分布在 GPU 上，每个 GPU 负责 9 个路由专家和 1 个共享专家。该配置标为 DP32+EP32，利用 32 个 GPU 之间的 EP，同时共享专家和 MLA 机制通过 DP 在同一组 GPU 上复制。在解码阶段，DeepSeek 进一步扩展并行度至 DP144+EP144，将 18 个节点组合成总计 144 个 GPU。在这一更大规模的部署中，每个 GPU 管理两个路由专家和一个共享专家，保持系统范围内 32 个路由专家副本的冗余。\nDeepSeek 采用了 DualPipe 策略用于重叠计算和 All-to-all 通信。当一个 micro-batch 正在进行 MoE 相关的 dispatch 和 combine 时，下一个 micro-batch 则同时进行局部注意力或 MLP 计算。\n每个 H800 GPU 在 prefill 阶段达到最高 9,213 token/s (6.3% 的上下文缓存命中率). 剔除缓存命中后有效吞吐量为 4,026 token/s. Decode 阶段，每个 GPU 维持平均 1,850 token/s 的吞吐量。\n本节分析了 CloudMatrix384 的架构特性为何与大规模 MoE 模型 (以 DeepSeek-R1 为例) 的需求高度协同。\nMoE 通信协同: 高带宽、低延迟的 UB 网络非常适合 MoE 模型中通信开销巨大的 token dispatch 和专家输出 combine 阶段。 内存容量与管理: 整个超级节点提供高达 49.2 TB 的 NPU 内存，足以容纳像 DeepSeek-R1 (671B 参数) 这样的巨型模型及其庞大的 KV cache. 上下文缓存复用: UB 网络使 NPU 能够以内存级的速度直接访问由 CPU DRAM 构成的解耦内存池，极大地加速了历史 KV cache 的读取，从而降低了首 token 生成延迟 (TTFT). 量化支持: 昇腾 910C 对 INT8 计算的原生支持，为通过量化来降低模型内存占用、减少计算开销和提升推理性能提供了宝贵的机会。 4. DeepSeek Serving on Huawei CloudMatrix384 4.1. Overview: A Peer-to-Peer Serving Architecture with PDC Disaggregation 如下图所示，文中提出了一种独特的点对点服务架构，将系统划分为三个功能子系统，prefill, decode 和 caching (PDC)，每个子系统独立运行，并通过显式的 KV cache 传输接口进行通信。\nKVCache-centric vs. Peer-to-Peer Serving Architectures: 现有的 LLM 服务系统如 NVIDIA Dynamo（NVIDIA Corporation，2025）和 Mooncake（Qin 等，2025）采用 KVCache 为中心的设计，其中请求调度与 KV cache 的局部性紧密耦合。在这些系统中，请求通常被路由到已经持有对应 KV cache 的特定计算节点。此类缓存感知调度对于缓解远程内存访问带来的显著性能损失至关重要，因为节点内存访问（例如通过 PCIe，约 256 GB/s）远远快于节点间带宽（通常约 25 GB/s 或 200 Gbps）。因此，远程 KV cache 加载通常会带来较大延迟。然而，这种设计引入了复杂的调度难题，并且在动态工作负载下可能导致负载均衡恶化。此外，该设计限制了全局资源效率，因为解码节点上的 DRAM 通常处于孤立状态且利用率低，无法有效贡献于共享缓存容量。\nCloudMatrix-Infer 中的点对点服务架构充分利用了 CloudMatrix384 的超高带宽 UB 互联。这使得基于解耦内存池构建的分布式缓存集群 (4.4) 能够实现统一访问。无论 NPU 是执行预填充任务还是解码任务，都可以直接访问共享解耦内存池。这种完全的点对点设计有效地扁平化了内存层次结构，弥补了本地访问和远程访问延迟之间的传统差距。这种将请求调度与 KV 缓存放置解耦带来了几个关键优势。\n使推理请求可以调度到任何可用的 NPU 实例，而不受数据局部性的限制。显著提升了系统范围内的负载均衡和 NPU 利用率。 消除了对复杂的亲和性调度机制的需求，从而降低了架构复杂性，简化了系统维护。 通过在预填充和解码节点之间共享 DRAM 资源，系统形成了一个统一的弹性缓存底层，提高了内存利用率，增加了缓存命中率，并在负载失衡或突发情况下提供了更强的弹性。 每个预填充实例在 CloudMatrix384 上配备 16 个 Ascend 910C NPU（32 个芯片），并以 32 路专家并行（EP32）运行。每个 rank 上放置 10 个专家: 1 个共享专家、8个路由专家和 1 个冗余路由专家，以支持专家并行负载均衡（EPLB）。为了进一步提高效率，对 MLA 计算采用混合并行策略，并应用基于 micro-batch 的流水线以重叠通信开销（4.3）。\n每个解码实例分配了 160 个 Ascend 910C NPU（320 个芯片）应于 MoE 层的 320 路专家并行（EP320）. 每个 rank 承载一个专家，整体配置包括 32 个共享专家、256 个独立路由专家和 32 个冗余路由专家，以支持 EPLB。为了进一步加速解码，引入了优化的 Ascend 原生算子、流水线解码策略以及 MTP (4.2).\n4.2. Tightly-Coupled Decode with Large-scale Expert Parallelism 4.2.1 Fused Communication Operators for LEP MoE 计算流程为: 在 gate network 为每个 token 选择 Top-K 专家后，进入 FFN 阶段前需要两次 all-to-all 通信。第一次 all-to-all 操作交换路由信息 (如 token 到专家的分配信息). 第二次 all-to-all 操作交换实际的 token 数据。该数据最初以 BF16 格式存储，为减少通信和计算开销每个 NPU 进行量化为 INT8 格式，然后由分配的 FFN 处理。计算完成后，第三次 all-to-all 通信将专家输出发送回其原先的 rank，每个 NPU 执行最终的 token 合并以重构输出。该流程存在三个低效问题:\n通信开销大: 三次 all-to-all 通信引加上通信横跨几百个 NPU 导致延迟很大。 动态形状: 因为每次解码迭代中分配给每个专家的 token 数量不同，导致 all2ll 通信中数据形状不固定。需要动态内存分配和频繁的 CPU-NPU 同步，降低了执行效率。 顺序依赖:MoE 计算的顺序执行特性导致步骤之间存在依赖关系，降低了资源利用率和吞吐量。 为了解决这些问题，本文开发了 FusedDispatch 和 FusedCombine 两个融合算子，将通信和计算集成在一起，专门设计用于在 CloudMatrix384 上实现最佳的解码性能。\nAIV-Direct Communication across NPUs: AIV-Direct 使 AI vector 核心能够通过 UB 互连直接将数据写入远程 NPU 的内存，完全绕过了易产生延迟的 Serial Direct Memory Access (SDMA) 路径 (下图蓝线).\nEarly Quantization: Dispatch 的时候不再发送 BF16 数据，而是传输 INT8 量化后的数据及其缩放因子。INT8 表示每个 token (7,168 维) 需要 7 KB。 缩放因子占用 4 字节（INT32），但为了对齐分配 512 B 给缩放因子。因此，每个 token 的传输消息大小为 7.5 KB.\nStatic Execution via Shared‑Memory Pre‑allocation: 在每个 NPU rank 中静态预分配共享内存缓冲区。\n$$\rbuffer_size = rank_num × max_tokens × msg_size\r$$ 其中 $max_tokens = local_batch × \\min(topK, experts_per_die)$. msg_size 是每个 token 的消息长度（INT8 量化后，dispatch 为 7.5 KB，combine 为 14 KB）\n由于 FusedDispatch 和 FusedCombine 是连续执行的，共用一个缓冲区会产生竞争，因此采用双缓冲机制来避免写入覆盖。本文设置中每个芯片处理最多 local_batch=96，并最多放置 2 专家，产生 max_tokens=96×min⁡(8,1)=96. 在包含 320 个设备的通信域中，分发缓冲区占用 320×96×7.5⁢KB≈225⁢MB ，合并缓冲区占用 320×96×14⁢KB≈420⁢MB.\nData-Sending Pipeline: 远程数据写入需要计算目标 NPU 预分配缓冲区的偏移量，但顺序执行此计算和传输会导致执行阻塞。因此文中将执行分成三阶段流水线\n将下一个 micro-batch 复制到本地 UBuffer. 计算远程缓冲区偏移量，并进行 INT8 量化。 向对应 NPU 的内存发起 AIV-Direct 写入。 完整的 FusedDispatch 如下:\n每个设备会检查自己有哪些 token 要发给其他设备的专家。 AIV 核心从内存里把 token 读到本地的 UBuffer. 把数据量化成 INT8 格式，同时记录缩放因子。给每个 token 加上标签，包括：源 rank ID，token 属于哪个批次（batch-slot ID）以及 token 在数据里的位置（key offset）.通过 AIV-direct 把打包好的数据写到目标节点的预分配内存里。 等所有 token 数据都通过 AIV-direct 发完后，系统会设置一个 barrier，确保每个设备的数据都写完了。设备计算每个专家收到了多少 token 后会互相同步，确保计数没出错。最后设备通过 AIV-direct 再发一个完成标志和设备上每个专家的 token 计数 (每个专家发了几个 token). 每个设备会一直检查别人发来的完成标志，等着所有标志都变成 1. 收到所有标志后，设备会读取每个专家的 token 计数，算出数据在内存里的偏移。然后，设备里的 AIV 核心会并行工作，把收到的数据从共享内存里取出来，整理成一个连续的的输出缓冲区。 完整的 FusedCombine 如下:\n结合 AIV 核心遍历其负责的 peer 设备，根据接收计数，从内存中提取 FFN 结果数据，存入本地 UBuffer。核心利用 token 元数据计算原始设备的接收地址，通过 AIV-direct 通道将数据传输至原始节点的预分配缓冲区。 标志更新 核心根据 token 元数据推算目标设备的标志地址，通过 AIV-direct 发出原子加操作，在对目标节点上的标志增量计数。 每个节点的核心会一直检查自己的标志，等待所有标志都变成 1. 随后从共享内存收集 FFN 输出，提取对应缩放因子，进行逐元素缩放并求和。将合并结果加到共享的 FFN 输出中，生成最终的 token 输出。 4.2.2 MLA Optimization ",
  "wordCount" : "7372",
  "inLanguage": "en",
  "datePublished": "2025-06-17T21:50:16+08:00",
  "dateModified": "2025-06-17T21:50:16+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      ServingLLMsOnHuaweiCloudMatrix384
    </h1>
    <div class="post-description">
      Paper Reading of Serving Large Language Models on Huawei CloudMatrix384
    </div>
    <div class="post-meta"><span title='2025-06-17 21:50:16 +0800 CST'>Jun-17-2025</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;7372 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                    <li>
                        <a href="#1-introduction" aria-label="1. Introduction">1. Introduction</a></li>
                    <li>
                        <a href="#2-llm-trends-and-their-challenges-for-datacenter-infrastructure" aria-label="2. LLM Trends and Their Challenges for Datacenter Infrastructure">2. LLM Trends and Their Challenges for Datacenter Infrastructure</a></li>
                    <li>
                        <a href="#21-llm-trends" aria-label="2.1. LLM Trends">2.1. LLM Trends</a><ul>
                            
                    <li>
                        <a href="#22-challenges-for-datacenter-infrastructure" aria-label="2.2. Challenges for Datacenter Infrastructure">2.2. Challenges for Datacenter Infrastructure</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-huawei-cloudmatrix" aria-label="3 Huawei CloudMatrix">3 Huawei CloudMatrix</a><ul>
                            
                    <li>
                        <a href="#31-vision-for-huawei-cloudmatrix" aria-label="3.1. Vision for Huawei CloudMatrix">3.1. Vision for Huawei CloudMatrix</a></li>
                    <li>
                        <a href="#32-cloudmatrix384-overview-a-fully-peer-to-peer-hardware-architecture" aria-label="3.2. CloudMatrix384 Overview: A Fully Peer-to-Peer Hardware Architecture">3.2. CloudMatrix384 Overview: A Fully Peer-to-Peer Hardware Architecture</a></li>
                    <li>
                        <a href="#33-hardware-components" aria-label="3.3. Hardware Components">3.3. Hardware Components</a></li>
                    <li>
                        <a href="#34-software-stack" aria-label="3.4. Software Stack">3.4. Software Stack</a></li>
                    <li>
                        <a href="#35-suitability-analysis-for-deepseek-models" aria-label="3.5. Suitability Analysis for DeepSeek Models">3.5. Suitability Analysis for DeepSeek Models</a></li></ul>
                    </li>
                    <li>
                        <a href="#4-deepseek-serving-on-huawei-cloudmatrix384" aria-label="4. DeepSeek Serving on Huawei CloudMatrix384">4. DeepSeek Serving on Huawei CloudMatrix384</a><ul>
                            
                    <li>
                        <a href="#41-overview-a-peer-to-peer-serving-architecture-with-pdc-disaggregation" aria-label="4.1. Overview: A Peer-to-Peer Serving Architecture with PDC Disaggregation">4.1. Overview: A Peer-to-Peer Serving Architecture with PDC Disaggregation</a></li>
                    <li>
                        <a href="#42-tightly-coupled-decode-with-large-scale-expert-parallelism" aria-label="4.2. Tightly-Coupled Decode with Large-scale Expert Parallelism">4.2. Tightly-Coupled Decode with Large-scale Expert Parallelism</a><ul>
                            
                    <li>
                        <a href="#421-fused-communication-operators-for-lep" aria-label="4.2.1 Fused Communication Operators for LEP">4.2.1 Fused Communication Operators for LEP</a></li>
                    <li>
                        <a href="#422-mla-optimization" aria-label="4.2.2 MLA Optimization">4.2.2 MLA Optimization</a>
                    </li>
                </ul>
                </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<ol>
<li>
<p><strong>CloudMatrix384硬件架构</strong>:</p>
<ul>
<li>论文提出了一种 <strong>peer-to-peer</strong> 的硬件设计，包含 384 个 <strong>Ascend 910C NPU</strong> 和 192 个 <strong>Kunpeng CPU</strong>，通过 <strong>Unified Bus (UB) Network</strong> 互联。UB 网络支持高带宽 (392 GB/s单向带宽)和低延迟 (1.9 µs)的全局通信，解决了传统AI集群中跨节点通信的瓶颈问题。</li>
<li>架构特点包括:
<ul>
<li><strong>资源解耦和池化</strong>: 计算、存储和网络资源可以动态分配，支持灵活的并行策略 (如专家并行EP、数据并行DP).</li>
<li><strong>三层网络平面</strong>: UB平面 (超节点内通信)、RDMA平面 (跨超节点通信)和VPC平面 (数据中心网络接入)。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CloudMatrix-Infer软件优化</strong>:</p>
<ul>
<li><strong>预填充-解码-缓存 (PDC)解耦架构</strong>: 将LLM推理拆分为Prefill, Decode &amp; Caching 三个子系统，通过 UB网络实现高效协同。</li>
<li><strong>large-scale expert parallelism (EP) strategy</strong>: 支持高达<strong>EP320</strong>的专家并行度，每个NPU芯片承载一个专家，减少MoE模型中的通信开销。</li>
<li><strong>UB驱动的分布式缓存</strong>: 利用 <strong>弹性内存服务 (EMS)</strong> 构建全局缓存池，支持KV缓存和模型权重的快速访问。</li>
</ul>
</li>
<li>
<p><strong>性能优化技术</strong>:</p>
<ul>
<li>** micro-batch 流水线 (Microbatch Pipeline)**: 重叠计算和通信，提升资源利用率。</li>
<li><strong>INT8量化</strong>: 在Ascend 910C 上实现高效的8位推理，保持模型精度。</li>
</ul>
</li>
</ol>
<p>论文使用<strong>DeepSeek-R1</strong> (671B参数MoE模型)验证了 CloudMatrix-Infer 的性能:</p>
<ul>
<li><strong>预填充吞吐量</strong>: 6,688 tokens/s/NPU (4.45 tokens/s/TFLOPS)，优于 NVIDIA H100 的 SGLang (3.75 tokens/s/TFLOPS).</li>
<li><strong>解码吞吐量</strong>: 1,943 tokens/s/NPU (1.29 tokens/s/TFLOPS)，在TPOT &lt;50 ms约束下仍能保持高吞吐。</li>
<li><strong>缓存命中率提升</strong>: 上下文缓存 (Context Caching)在 90% 重用率时，预填充时间减少 59%.</li>
</ul>
<h1 id="1-introduction">1. Introduction<a hidden class="anchor" aria-hidden="true" href="#1-introduction">#</a></h1>
<p>LLM 的发展趋势有以下几点:</p>
<ol>
<li>参数规模的指数级增长。DeepSeek-R1, LLaMA-4 和 Qwen-3 通常扩展到数千亿甚至数万亿参数.</li>
<li>专家混合 (MoE)架构的广泛采用。MoE 通过每个 token 选择性激活少数专家，引入了结构稀疏性，实现了更大模型下的效率提升，但同时在专家路由和同步方面带来了新的系统级挑战。</li>
<li>上下文长度的大幅提高。上下文窗口从数万 token 扩展到超过一百万 token，对注意力计算和 KV cache 存储施加了巨大压力。 KV Cache 大小随着并发用户数量线性增长，这对其的分布、放置和访问方式提出了重大限制。</li>
</ol>
<p>LLM 服务系统必须适应可变长度的用户输入、跨 token 的不平衡专家激活以及高度突发的用户查询，同时保持严格的延迟和吞吐量目标。满足这些需求不仅仅是简单地扩展硬件资源，而是需要全面的软硬件协同设计，包括紧密集成的计算、内存和网络硬件资源，辅以智能任务调度、自适应运行时编排以及弹性资源管理策略，能够动态响应不断演变的模型结构和波动的工作负载。</p>
<p>为了应对这些挑战，论文引入了 <strong>华为CloudMatrix</strong>，一个旨在重塑 AI 基础设施基础的下一代 AI 数据中心架构。其首个生产级实现是 <strong>CloudMatrix384</strong>，它具备以下核心特点:</p>
<ul>
<li><strong>硬件构成</strong>: 它是一个集成了384个<strong>昇腾 (Ascend)910C NPU</strong>、192个 <strong>鲲鹏 (Kunpeng)CPU</strong> 的AI超级节点。</li>
<li><strong>核心网络</strong>: 所有组件通过一个超高带宽、低延迟的 UB 网络互联，实现了硬件上的 **完全点对点 (peer-to-peer)**通信。</li>
<li><strong>架构优势</strong>: 与传统层级式设计不同，UB网络允许计算、内存等资源被动态池化、统一访问和独立扩展，特别适合处理MoE专家并行和分布式KV缓存访问这类通信密集型操作。</li>
</ul>
<p>基于CloudMatrix384硬件，论文提出了一个名为 <strong>CloudMatrix-Infer</strong> 的综合性LLM服务解决方案，其包含了三大核心创新:</p>
<ol>
<li>
<p><strong>点对点服务架构</strong>: 该架构将推理系统解耦为 <strong>预填充 (prefill)</strong>, <strong>解码 (decode)</strong> 和 <strong>缓存 (caching)</strong> 三个可独立扩展的资源池。借助UB网络，所有 NPU 都能统一访问共享的缓存数据，从而摆脱了传统架构中因数据局部性限制而导致的调度复杂性和效率低下问题。</p>
</li>
<li>
<p><strong>大规模专家并行 (LEP)策略</strong>: 该策略专为MoE模型优化，利用UB网络高效地进行 token 分发和专家输出合并。它支持极高的专家并行度 (如 <strong>EP320</strong> )，允许每个 NPU Die 只承载一个专家，从而显著降低解码延迟。</p>
</li>
<li>
<p><strong>硬件感知优化</strong>: 包括为昇腾芯片高度优化的算子、基于 micro-batch 的流水线技术 (以重叠计算和通信)以及INT8量化 (以提升计算效率和减少内存消耗)。</p>
</li>
</ol>
<p>在 DeepSeek-R1 模型上的测试表明，CloudMatrix-Infer 实现了业界领先的性能和效率:</p>
<ul>
<li><strong>预填充性能</strong>: 每个NPU的吞吐量为 <strong>6,688 tokens/s</strong>，计算效率为 <strong>4.45 tokens/s/TFLOPS</strong>.</li>
<li><strong>解码性能</strong>: 在低于50ms的单Token输出延迟 (TPOT)下，每个NPU的吞吐量为<strong>1,943 tokens/s</strong>，计算效率为 <strong>1.29 tokens/s/TFLOPS</strong>.</li>
<li><strong>性能对比</strong>: 预填充和解码的计算效率均<strong>超过了在NVIDIA H100上运行SGLang和在NVIDIA H800上运行DeepSeek的公开数据</strong>.</li>
<li><strong>准确性</strong>: 在昇腾910C上进行的 INT8 量化，其模型准确率与官方 DeepSeek-R1 API 在 16 个基准测试中相当。</li>
</ul>
<h1 id="2-llm-trends-and-their-challenges-for-datacenter-infrastructure">2. LLM Trends and Their Challenges for Datacenter Infrastructure<a hidden class="anchor" aria-hidden="true" href="#2-llm-trends-and-their-challenges-for-datacenter-infrastructure">#</a></h1>
<h1 id="21-llm-trends">2.1. LLM Trends<a hidden class="anchor" aria-hidden="true" href="#21-llm-trends">#</a></h1>
<p><strong>Ever-Larger Parameter Counts.</strong> scaling law 表明，增加 LLM 的参数数量可以提升其在各种任务上的表现。这一趋势的典型代表包括:</p>
<ul>
<li><strong>Meta 的 Llama 4 Behemoth</strong>: 拥有近 2 万亿参数。</li>
<li><strong>DeepSeek-V3</strong>: 包含 6710 亿参数。</li>
<li><strong>Google 的 PaLM</strong>: 包含 5400 亿参数。</li>
<li><strong>xAI 的 Grok-1</strong>: 拥有 3140 亿参数。</li>
</ul>
<p><strong>Sparsity through MoE.</strong> 为了控制不断攀升的训练和推理成本，现代 LLM 越来越多地采用稀疏激活的 MoE 架构。这种架构将模型的总大小与处理每个 token 所需的计算量解耦:</p>
<ul>
<li><strong>Mixtral 8x7B</strong>: 总参数量为 467 亿，但每个 token 只激活 129 亿参数。</li>
<li><strong>Databricks 的 DBRX</strong>: 总参数量为 1320 亿，每个 token 激活 360 亿参数。</li>
<li><strong>Meta 的 Llama 4 系列</strong>: Llama 4 Maverick 使用 128 个专家，而 Llama 4 Scout 使用 16 个专家。</li>
<li><strong>DeepSeek-V3</strong>: 将每层的专家数量从 160 个增加到 256 个，从而在不显著增加计算负载的情况下提升了模型容量。</li>
<li><strong>阿里巴巴的 Qwen3-235B</strong>: 集成了 128 个专家，每个 token 激活 220 亿参数。</li>
<li><strong>华为的盘古 Ultra MoE</strong>: 总参数量达 7180 亿，每个 token 激活 390 亿参数。</li>
</ul>
<p>这些模型共同凸显了 LLM 扩展策略的范式转变，即更强调通过架构稀疏性而非单纯的参数数量来提升性能和效率。</p>
<p><strong>Extension of Context Windows.</strong> LLM 上下文窗口的扩大使其能够处理更长的序列，这对于需要扩展推理和连贯性的任务至关重要。近期的进展包括:</p>
<ul>
<li><strong>OpenAI 的 GPT-4.5</strong>: 支持 128,000 个 token 的上下文窗口。</li>
<li><strong>Google 的 Gemini 2.5 Pro</strong>: 提供高达 100 万个 token 的上下文窗口。</li>
</ul>
<p>然而处理长文本会显著增加计算成本和推理延迟。为了缓解这一问题，生产系统普遍采用<strong>上下文缓存 (context caching)</strong> 技术。该技术通过存储和复用由先前提示片段生成的 KV block，来消除对提示的冗余注意力计算，从而降低延迟并提高效率。</p>
<h2 id="22-challenges-for-datacenter-infrastructure">2.2. Challenges for Datacenter Infrastructure<a hidden class="anchor" aria-hidden="true" href="#22-challenges-for-datacenter-infrastructure">#</a></h2>
<p>上述 LLM 的发展趋势对底层的数据中心基础设施提出了严峻的新要求。随着模型能力的扩展，它们催生了如强化学习、交互式媒体生成和自主 AI 代理等日益复杂的工作负载。这些应用不仅需要海量的计算和内存资源，还需要对基础设施进行根本性的重新架构，以支持高带宽通信和低延迟存储，从而在动态、异构的真实世界条件下满足严格的服务水平目标。</p>
<p><strong>Scaling Communication-Intensive Parallelism.</strong>. 随着模型规模的增长，单个计算节点已无法容纳最先进的 AI 模型，必须使用多节点并行策略。尽管现有的 AI 集群通过 RDMA 网络支持跨节点通信，但其带宽和拓扑结构通常只为数据并行 (DP) 或流水线并行 (PP) 等通信量较小的场景优化。然而，张量并行 (Tensor Parallelism, TP) 和 专家并行 (Expert Parallelism, EP) 需要频繁、细粒度且低延迟的通信，这种通信模式难以在节点之间高效扩展。这迫使许多部署方案将 TP 和 EP 限制在单个计算节点内，从而限制了可扩展性。</p>
<p><strong>Maintaining High Utilization under Heterogeneous AI Workloads.</strong> 现代 AI 工作负载表现出高度多样化和动态的资源需求:</p>
<ul>
<li>训练任务通常是计算密集型的。</li>
<li>LLM 推理的解码阶段往往受限内存带宽。</li>
<li>自动驾驶模型训练等任务涉及大量的 CPU 端数据预处理。</li>
</ul>
<p>固定的节点配置无法高效地适应这种多样性，常常导致资源过配或利用不足。为了最大化效率和适应性，现代 AI 基础设施必须能够根据每种工作负载的特定需求，动态、细粒度地组合异构资源 (如 NPU, CPU 和内存).</p>
<p><strong>Enabling Converged Execution of AI and Data-Intensive Workloads.</strong> AI 工作流与传统数据密集型操作 (如数据摄取、预处理、检索、分析和模拟) 的交叉越来越频繁。同时，数据库、大数据和高性能计算 (HPC) 等通用工作负载本身也在不断集成 AI 功能。这种融合执行模式要求高吞吐、低延迟的通信和灵活的资源编排。然而，主要为传统通用工作负载优化的老旧数据中心基础设施难以满足这些苛刻的要求。</p>
<p><strong>Delivering Memory-class Storage Performance.</strong> 现代 AI 流水线操作的数据规模已远超传统存储系统的能力。诸如摄取 PB 级数据集、管理 TB 级模型检查点以及支持延迟敏感的推理 (特别是在使用大型 KV cache 和检索增强生成 RAG 模块时) 等任务，都需要存储子系统具备<strong>内存级的带宽、延迟和 IOPS (Input/Output Operations Per Second)</strong>. 围绕磁盘访问模式设计的传统存储层次结构频繁地成为性能瓶颈，因数据供给不足而导致 NPU 利用率低下。</p>
<h1 id="3-huawei-cloudmatrix">3 Huawei CloudMatrix<a hidden class="anchor" aria-hidden="true" href="#3-huawei-cloudmatrix">#</a></h1>
<h2 id="31-vision-for-huawei-cloudmatrix">3.1. Vision for Huawei CloudMatrix<a hidden class="anchor" aria-hidden="true" href="#31-vision-for-huawei-cloudmatrix">#</a></h2>
<p>如下图所示，CloudMatrix 超越了传统以 CPU 为中心的分层设计，实现了无需 CPU 介质下包括 NPU、CPU、DRAM、SSD、NIC 及领域专用加速器在内的所有异构系统组件之间的直接高性能通信。该架构的核心是超高带宽、低延迟的 UB 网络。</p>
<p><strong>Scalable Communication for TP/EP.</strong> UB 网络可以为张量并行 (TP) 和专家并行 (EP) 提供强大的通信支持，使其能够轻松扩展到单节点边界之外。</p>
<p><strong>Flexible Resource Composition for Heterogeneous Workloads.</strong> 将 CPU、NPU、内存等资源解耦成独立的资源池，允许根据工作负载按需、细粒度地进行组合。</p>
<p><strong>Unified Infrastructure for Converged Workloads.</strong> 在单一架构内同时支持 AI 和数据密集型工作负载的融合执行。</p>
<p><strong>Memory-class Storage via Disaggregated Memory Pool.</strong> 通过聚合集群中所有 CPU 挂载的 DRAM，形成一个可通过 UB 访问的共享高性能内存池，为 KV cache 复用、参数加载等提供加速。</p>
<h2 id="32-cloudmatrix384-overview-a-fully-peer-to-peer-hardware-architecture">3.2. CloudMatrix384 Overview: A Fully Peer-to-Peer Hardware Architecture<a hidden class="anchor" aria-hidden="true" href="#32-cloudmatrix384-overview-a-fully-peer-to-peer-hardware-architecture">#</a></h2>
<p>CloudMatrix384 是一个集成了 <strong>384 个昇腾 (Ascend) 910C NPU</strong> 和 <strong>192 个鲲鹏 (Kunpeng) CPU</strong> 的 AI 超级节点。其最显著的特点是，通过 UB 网络实现了跨节点通信性能与节点内性能的高度一致 (带宽下降 &lt; 3%，延迟增加 &lt; 1µs). CloudMatrix384 包含三个互补的网络平面:</p>
<ol>
<li><strong>UB 平面</strong>: 超级节点内的主要<strong>纵向扩展 (Scale-Up)</strong> 网络，以全互联、无阻塞的方式连接所有 384 个 NPU 和 192 个 CPU. 每个昇腾 910C NPU 为该平面贡献超过 392 GB/s 的单向带宽。</li>
<li><strong>RDMA 平面</strong>: 用于超级节点间的<strong>横向扩展 (Scale-Out)</strong> 通信，采用 RoCE 协议，确保与现有 RDMA 生态的兼容性。每个 NPU 为该平面贡献高达 400 Gbps 的单向带宽。</li>
<li><strong>VPC 平面</strong>: 通过擎天卡 (Qingtian Card) 将超级节点接入更广泛的数据中心网络，用于管理、控制和访问持久化存储等操作。</li>
</ol>
<h2 id="33-hardware-components">3.3. Hardware Components<a hidden class="anchor" aria-hidden="true" href="#33-hardware-components">#</a></h2>
<ul>
<li><strong>昇腾 910C 芯片</strong>: 作为系统的核心，它采用双晶粒 (dual-die) 封装。每个芯片包提供 752 TFLOPS 的 BF16/FP16 算力，支持 INT8 数据类型。它集成 128 GB 片上内存，总带宽高达 3.2 TB/s.</li>
<li><strong>昇腾 910C 节点</strong>: 每个计算节点集成 8 个昇腾 910C NPU 和 4 个鲲鹏 CPU.</li>
<li><strong>UB 交换系统</strong>: 采用两级 (L1/L2) 无阻塞交换网络拓扑，将所有节点紧密连接成一个统一的超级节点。</li>
</ul>
<h2 id="34-software-stack">3.4. Software Stack<a hidden class="anchor" aria-hidden="true" href="#34-software-stack">#</a></h2>
<ul>
<li><strong>CANN (神经网络计算架构)</strong>: 华为为昇腾 NPU 开发的完整软件生态系统，类似于 NVIDIA 的 CUDA. 它包含驱动层、运行时层 (Runtime) 和库层 (如用于分布式通信的 HCCL)，并通过图引擎 (Graph Engine, GE) 对上层框架 (如 PyTorch, TensorFlow) 的计算图进行编译和优化。</li>
<li><strong>云部署基础设施软件</strong>: 包括 MatrixResource、MatrixLink、MatrixCompute 和 MatrixContainer 等一系列软件，用于在云环境中对 CloudMatrix 集群进行资源管理、网络配置和容器化部署。上层的 <strong>ModelArts</strong> 平台则提供端到端的 AI 开发和 MLOps 服务。</li>
</ul>
<h2 id="35-suitability-analysis-for-deepseek-models">3.5. Suitability Analysis for DeepSeek Models<a hidden class="anchor" aria-hidden="true" href="#35-suitability-analysis-for-deepseek-models">#</a></h2>
<details class="custom-details">
    <summary class="custom-summary">DeepSeek Models and Their Deployment on NVIDIA H800</summary>
    <div><p>DeepSeek 在由 NVIDIA H800 GPU 组成的集群上部署其 V3 和 R1 模型，每个 GPU 内存 80 GB，节点内通过 NVLink 连接，节点间通过 400 Gbps InfiniBand 连接。该部署采用了分离式预填充-解码架构。在预填充阶段，DeepSeek 将四个 H800 节点（共 32 个 GPU）组织成一个部署单元。在每个单元内，256 个路由专家被策略性地分布在 GPU 上，每个 GPU 负责 9 个路由专家和 1 个共享专家。该配置标为 DP32+EP32，利用 32 个 GPU 之间的 EP，同时共享专家和 MLA 机制通过 DP 在同一组 GPU 上复制。在解码阶段，DeepSeek 进一步扩展并行度至 DP144+EP144，将 18 个节点组合成总计 144 个 GPU。在这一更大规模的部署中，每个 GPU 管理两个路由专家和一个共享专家，保持系统范围内 32 个路由专家副本的冗余。</p>
<p>DeepSeek 采用了 DualPipe 策略用于重叠计算和 All-to-all 通信。当一个 micro-batch 正在进行 MoE 相关的 dispatch 和 combine 时，下一个 micro-batch 则同时进行局部注意力或 MLP 计算。</p>
<p>每个 H800 GPU 在 prefill 阶段达到最高 9,213 token/s (6.3% 的上下文缓存命中率). 剔除缓存命中后有效吞吐量为 4,026 token/s. Decode 阶段，每个 GPU 维持平均 1,850 token/s 的吞吐量。</p>
</div>
</details><br>
<p>本节分析了 CloudMatrix384 的架构特性为何与大规模 MoE 模型 (以 DeepSeek-R1 为例) 的需求高度协同。</p>
<ul>
<li><strong>MoE 通信协同</strong>: 高带宽、低延迟的 UB 网络非常适合 MoE 模型中通信开销巨大的 token dispatch 和专家输出 combine 阶段。</li>
<li><strong>内存容量与管理</strong>: 整个超级节点提供高达 49.2 TB 的 NPU 内存，足以容纳像 DeepSeek-R1 (671B 参数) 这样的巨型模型及其庞大的 KV cache.</li>
<li><strong>上下文缓存复用</strong>: UB 网络使 NPU 能够以内存级的速度直接访问由 CPU DRAM 构成的解耦内存池，极大地加速了历史 KV cache 的读取，从而降低了首 token 生成延迟 (TTFT).</li>
<li><strong>量化支持</strong>: 昇腾 910C 对 INT8 计算的原生支持，为通过量化来降低模型内存占用、减少计算开销和提升推理性能提供了宝贵的机会。</li>
</ul>
<h1 id="4-deepseek-serving-on-huawei-cloudmatrix384">4. DeepSeek Serving on Huawei CloudMatrix384<a hidden class="anchor" aria-hidden="true" href="#4-deepseek-serving-on-huawei-cloudmatrix384">#</a></h1>
<h2 id="41-overview-a-peer-to-peer-serving-architecture-with-pdc-disaggregation">4.1. Overview: A Peer-to-Peer Serving Architecture with PDC Disaggregation<a hidden class="anchor" aria-hidden="true" href="#41-overview-a-peer-to-peer-serving-architecture-with-pdc-disaggregation">#</a></h2>
<p>如下图所示，文中提出了一种独特的点对点服务架构，将系统划分为三个功能子系统，prefill, decode 和 caching (PDC)，每个子系统独立运行，并通过显式的 KV cache 传输接口进行通信。</p>
<p><strong>KVCache-centric vs. Peer-to-Peer Serving Architectures:</strong> 现有的 LLM 服务系统如 NVIDIA Dynamo（NVIDIA Corporation，2025）和 Mooncake（Qin 等，2025）采用 KVCache 为中心的设计，其中请求调度与 KV cache 的局部性紧密耦合。在这些系统中，请求通常被路由到已经持有对应 KV cache 的特定计算节点。此类缓存感知调度对于缓解远程内存访问带来的显著性能损失至关重要，因为节点内存访问（例如通过 PCIe，约 256 GB/s）远远快于节点间带宽（通常约 25 GB/s 或 200 Gbps）。因此，远程 KV cache 加载通常会带来较大延迟。然而，这种设计引入了复杂的调度难题，并且在动态工作负载下可能导致负载均衡恶化。此外，该设计限制了全局资源效率，因为解码节点上的 DRAM 通常处于孤立状态且利用率低，无法有效贡献于共享缓存容量。</p>
<p>CloudMatrix-Infer 中的点对点服务架构充分利用了 CloudMatrix384 的超高带宽 UB 互联。这使得基于解耦内存池构建的分布式缓存集群 (4.4) 能够实现统一访问。无论 NPU 是执行预填充任务还是解码任务，都可以直接访问共享解耦内存池。这种完全的点对点设计有效地扁平化了内存层次结构，弥补了本地访问和远程访问延迟之间的传统差距。这种将请求调度与 KV 缓存放置解耦带来了几个关键优势。</p>
<ol>
<li>使推理请求可以调度到任何可用的 NPU 实例，而不受数据局部性的限制。显著提升了系统范围内的负载均衡和 NPU 利用率。</li>
<li>消除了对复杂的亲和性调度机制的需求，从而降低了架构复杂性，简化了系统维护。</li>
<li>通过在预填充和解码节点之间共享 DRAM 资源，系统形成了一个统一的弹性缓存底层，提高了内存利用率，增加了缓存命中率，并在负载失衡或突发情况下提供了更强的弹性。</li>
</ol>
<p>每个预填充实例在 CloudMatrix384 上配备 16 个 Ascend 910C NPU（32 个芯片），并以 32 路专家并行（EP32）运行。每个 rank 上放置 10 个专家: 1 个共享专家、8个路由专家和 1 个冗余路由专家，以支持专家并行负载均衡（EPLB）。为了进一步提高效率，对 MLA 计算采用混合并行策略，并应用基于 micro-batch 的流水线以重叠通信开销（4.3）。</p>
<p>每个解码实例分配了 160 个 Ascend 910C NPU（320 个芯片）应于 MoE 层的 320 路专家并行（EP320）. 每个 rank 承载一个专家，整体配置包括 32 个共享专家、256 个独立路由专家和 32 个冗余路由专家，以支持 EPLB。为了进一步加速解码，引入了优化的 Ascend 原生算子、流水线解码策略以及 MTP (4.2).</p>
<h2 id="42-tightly-coupled-decode-with-large-scale-expert-parallelism">4.2. Tightly-Coupled Decode with Large-scale Expert Parallelism<a hidden class="anchor" aria-hidden="true" href="#42-tightly-coupled-decode-with-large-scale-expert-parallelism">#</a></h2>
<h3 id="421-fused-communication-operators-for-lep">4.2.1 Fused Communication Operators for LEP<a hidden class="anchor" aria-hidden="true" href="#421-fused-communication-operators-for-lep">#</a></h3>
<p>MoE 计算流程为: 在 gate network 为每个 token 选择 Top-K 专家后，进入 FFN 阶段前需要两次 all-to-all 通信。第一次 all-to-all 操作交换路由信息 (如 token 到专家的分配信息). 第二次 all-to-all 操作交换实际的 token 数据。该数据最初以 BF16 格式存储，为减少通信和计算开销每个 NPU 进行量化为 INT8 格式，然后由分配的 FFN 处理。计算完成后，第三次 all-to-all 通信将专家输出发送回其原先的 rank，每个 NPU 执行最终的 token 合并以重构输出。该流程存在三个低效问题:</p>
<ol>
<li>通信开销大: 三次 all-to-all 通信引加上通信横跨几百个 NPU 导致延迟很大。</li>
<li>动态形状: 因为每次解码迭代中分配给每个专家的 token 数量不同，导致 all2ll 通信中数据形状不固定。需要动态内存分配和频繁的 CPU-NPU 同步，降低了执行效率。</li>
<li>顺序依赖:MoE 计算的顺序执行特性导致步骤之间存在依赖关系，降低了资源利用率和吞吐量。</li>
</ol>
<p>为了解决这些问题，本文开发了 FusedDispatch 和 FusedCombine 两个融合算子，将通信和计算集成在一起，专门设计用于在 CloudMatrix384 上实现最佳的解码性能。</p>
<p><strong>AIV-Direct Communication across NPUs</strong>: AIV-Direct 使 AI vector 核心能够通过 UB 互连直接将数据写入远程 NPU 的内存，完全绕过了易产生延迟的 Serial Direct Memory Access (SDMA) 路径 (下图蓝线).</p>
<p><strong>Early Quantization</strong>: Dispatch 的时候不再发送 BF16 数据，而是传输 INT8 量化后的数据及其缩放因子。INT8 表示每个 token (7,168 维) 需要 7 KB。 缩放因子占用 4 字节（INT32），但为了对齐分配 512 B 给缩放因子。因此，每个 token 的传输消息大小为 7.5 KB.</p>
<p><strong>Static Execution via Shared‑Memory Pre‑allocation</strong>: 在每个 NPU rank 中静态预分配共享内存缓冲区。</p>
$$
buffer_size = rank_num × max_tokens × msg_size
$$<p>
其中 $max_tokens = local_batch × \min(topK, experts_per_die)$. msg_size 是每个 token 的消息长度（INT8 量化后，dispatch 为 7.5 KB，combine 为 14 KB）</p>
<p>由于 FusedDispatch 和 FusedCombine 是连续执行的，共用一个缓冲区会产生竞争，因此采用双缓冲机制来避免写入覆盖。本文设置中每个芯片处理最多 local_batch=96，并最多放置 2 专家，产生 max_tokens=96×min⁡(8,1)=96. 在包含 320 个设备的通信域中，分发缓冲区占用 320×96×7.5⁢KB≈225⁢MB ，合并缓冲区占用 320×96×14⁢KB≈420⁢MB.</p>
<p><strong>Data-Sending Pipeline</strong>: 远程数据写入需要计算目标 NPU 预分配缓冲区的偏移量，但顺序执行此计算和传输会导致执行阻塞。因此文中将执行分成三阶段流水线</p>
<ol>
<li>将下一个 micro-batch 复制到本地 UBuffer.</li>
<li>计算远程缓冲区偏移量，并进行 INT8 量化。</li>
<li>向对应 NPU 的内存发起 AIV-Direct 写入。</li>
</ol>
<p>完整的 FusedDispatch 如下:</p>
<ol>
<li>每个设备会检查自己有哪些 token 要发给其他设备的专家。 AIV 核心从内存里把 token 读到本地的 UBuffer. 把数据量化成 INT8 格式，同时记录缩放因子。给每个 token 加上标签，包括：源 rank ID，token 属于哪个批次（batch-slot ID）以及 token 在数据里的位置（key offset）.通过 AIV-direct 把打包好的数据写到目标节点的预分配内存里。</li>
<li>等所有 token 数据都通过 AIV-direct 发完后，系统会设置一个 barrier，确保每个设备的数据都写完了。设备计算每个专家收到了多少 token 后会互相同步，确保计数没出错。最后设备通过 AIV-direct 再发一个完成标志和设备上每个专家的 token 计数 (每个专家发了几个 token).</li>
<li>每个设备会一直检查别人发来的完成标志，等着所有标志都变成 1. 收到所有标志后，设备会读取每个专家的 token 计数，算出数据在内存里的偏移。然后，设备里的 AIV 核心会并行工作，把收到的数据从共享内存里取出来，整理成一个连续的的输出缓冲区。</li>
</ol>
<p>完整的 FusedCombine 如下:</p>
<ol>
<li>结合 AIV 核心遍历其负责的 peer 设备，根据接收计数，从内存中提取 FFN 结果数据，存入本地 UBuffer。核心利用 token 元数据计算原始设备的接收地址，通过 AIV-direct 通道将数据传输至原始节点的预分配缓冲区。
标志更新</li>
<li>核心根据 token 元数据推算目标设备的标志地址，通过 AIV-direct 发出原子加操作，在对目标节点上的标志增量计数。</li>
<li>每个节点的核心会一直检查自己的标志，等待所有标志都变成 1. 随后从共享内存收集 FFN 输出，提取对应缩放因子，进行逐元素缩放并求和。将合并结果加到共享的 FFN 输出中，生成最终的 token 输出。</li>
</ol>
<h3 id="422-mla-optimization">4.2.2 MLA Optimization<a hidden class="anchor" aria-hidden="true" href="#422-mla-optimization">#</a></h3>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/distributed-training/">Distributed Training</a></li>
      <li><a href="http://localhost:1313/tags/llm/">LLM</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/deepseek/deepseekmla/">
    <span class="title">« Prev</span>
    <br>
    <span>DeepSeekMLA</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/internlm3-8b-instruct/">
    <span class="title">Next »</span>
    <br>
    <span>InternLM3 8B Instruct</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
