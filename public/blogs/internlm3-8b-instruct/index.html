<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>InternLM3 8B Instruct | WITHER</title>
<meta name="keywords" content="word 1, word 2">
<meta name="description" content="model &amp; config
Network Definition
{
    &#34;architectures&#34;: [
        &#34;InternLM3ForCausalLM&#34;
    ],
    &#34;attention_dropout&#34;: 0.0,
    &#34;auto_map&#34;: {
        &#34;AutoConfig&#34;: &#34;configuration_internlm3.InternLM3Config&#34;,
        &#34;AutoModel&#34;: &#34;modeling_internlm3.InternLM3Model&#34;,
        &#34;AutoModelForCausalLM&#34;: &#34;modeling_internlm3.InternLM3ForCausalLM&#34;
    },
    &#34;bias&#34;: false,
    &#34;bos_token_id&#34;: 1,
    &#34;eos_token_id&#34;: 2,
    &#34;head_dim&#34;: 128,
    &#34;hidden_act&#34;: &#34;silu&#34;,
    &#34;hidden_size&#34;: 4096,
    &#34;initializer_range&#34;: 0.02,
    &#34;intermediate_size&#34;: 10240,
    &#34;max_position_embeddings&#34;: 32768,
    &#34;model_type&#34;: &#34;internlm3&#34;,
    &#34;num_attention_heads&#34;: 32,
    &#34;num_hidden_layers&#34;: 48,
    &#34;num_key_value_heads&#34;: 2,
    &#34;pad_token_id&#34;: 2,
    &#34;qkv_bias&#34;: false,
    &#34;rms_norm_eps&#34;: 1e-05,
    &#34;rope_scaling&#34;: {
        &#34;factor&#34;: 6.0,
        &#34;rope_type&#34;: &#34;dynamic&#34;
    },
    &#34;rope_theta&#34;: 50000000,
    &#34;tie_word_embeddings&#34;: false,
    &#34;torch_dtype&#34;: &#34;bfloat16&#34;,
    &#34;transformers_version&#34;: &#34;4.47.1&#34;,
    &#34;use_cache&#34;: true,
    &#34;vocab_size&#34;: 128512
}

  
      
          æ¨¡å— (Module)
          å­æ¨¡å— (Sub-module)
          åŠŸèƒ½æè¿°
          é…ç½®å‚æ•°
      
  
  
      
          InternLM3ForCausalLM
          model: InternLM3Model
          æ¨¡å‹ä¸»å¹²ï¼ŒåŒ…å«è¯åµŒå…¥å’Œè§£ç å™¨å±‚ã€‚
          -
      
      
          
          lm_head: Linear
          çº¿æ€§è¾“å‡ºå±‚ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ï¼Œç”Ÿæˆé¢„æµ‹ logitsã€‚
          hidden_size: 4096, vocab_size: 128512
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3Model
          embed_tokens: Embedding
          å°†è¾“å…¥çš„ token IDs è½¬æ¢ä¸ºç¨ å¯†å‘é‡ï¼ˆEmbeddingsï¼‰ã€‚
          vocab_size: 128512, hidden_size: 4096
      
      
          
          layers: ModuleList[InternLM3DecoderLayer]
          åŒ…å«å¤šä¸ªï¼ˆ48ä¸ªï¼‰Transformerè§£ç å™¨å±‚ã€‚
          num_hidden_layers: 48
      
      
          
          norm: RMSNorm
          åœ¨æ‰€æœ‰è§£ç å™¨å±‚ä¹‹åï¼Œå¯¹æœ€ç»ˆçš„éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ã€‚
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          
          rotary_emb: RotaryEmbedding
          è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ä½ç½®ä¿¡æ¯ã€‚
          head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {&quot;factor&quot;: 6.0, &quot;rope_type&quot;: &quot;dynamic&quot;}
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3DecoderLayer
          self_attn: InternLM3Attention
          å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚
          -
      
      
          
          mlp: InternLM3MLP
          å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºå¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚
          -
      
      
          
          input_layernorm: RMSNorm
          åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹å‰å¯¹è¾“å…¥è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          
          post_attention_layernorm: RMSNorm
          åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹åã€MLPæ¨¡å—ä¹‹å‰è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3Attention
          qkv_proj: Linear
          å°†è¾“å…¥çš„éšè—çŠ¶æ€çº¿æ€§å˜æ¢ä¸ºæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰ã€‚é‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ã€‚
          hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V)
      
      
          
          o_proj: Linear
          å°†æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºçº¿æ€§å˜æ¢å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚
          hidden_size: 4096
      
      
          
          apply_rotary_pos_emb
          å°†æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰åº”ç”¨äºQå’ŒKã€‚
          -
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3MLP
          gate_up_proj: Linear
          ä¸¤ä¸ªå¹¶è¡Œçš„çº¿æ€§å±‚ï¼ˆgate å’Œ upï¼‰ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°ä¸­é—´ç»´åº¦ã€‚
          hidden_size: 4096, intermediate_size: 10240
      
      
          
          act_fn: SiluAndMul
          ä½¿ç”¨ SiLU (Swish) æ¿€æ´»å‡½æ•°å¹¶è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚
          hidden_act: &quot;silu&quot;
      
      
          
          down_proj: Linear
          å°†æ¿€æ´»åçš„ä¸­é—´çŠ¶æ€æ˜ å°„å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚
          intermediate_size: 10240, hidden_size: 4096
      
  

InternLM3DecoderLayer è¿ç®—æµç¨‹
å‡è®¾è¾“å…¥ä¸º hidden_states å’Œ residual (åœ¨å‰ä¸€å±‚è®¡ç®—å¾—å‡ºï¼Œç¬¬ä¸€å±‚æ—¶ residual ç­‰äº hidden_states)ã€‚">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/internlm3-8b-instruct/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/internlm3-8b-instruct/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/internlm3-8b-instruct/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="InternLM3 8B Instruct">
  <meta property="og:description" content="model &amp; config Network Definition
{ &#34;architectures&#34;: [ &#34;InternLM3ForCausalLM&#34; ], &#34;attention_dropout&#34;: 0.0, &#34;auto_map&#34;: { &#34;AutoConfig&#34;: &#34;configuration_internlm3.InternLM3Config&#34;, &#34;AutoModel&#34;: &#34;modeling_internlm3.InternLM3Model&#34;, &#34;AutoModelForCausalLM&#34;: &#34;modeling_internlm3.InternLM3ForCausalLM&#34; }, &#34;bias&#34;: false, &#34;bos_token_id&#34;: 1, &#34;eos_token_id&#34;: 2, &#34;head_dim&#34;: 128, &#34;hidden_act&#34;: &#34;silu&#34;, &#34;hidden_size&#34;: 4096, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 10240, &#34;max_position_embeddings&#34;: 32768, &#34;model_type&#34;: &#34;internlm3&#34;, &#34;num_attention_heads&#34;: 32, &#34;num_hidden_layers&#34;: 48, &#34;num_key_value_heads&#34;: 2, &#34;pad_token_id&#34;: 2, &#34;qkv_bias&#34;: false, &#34;rms_norm_eps&#34;: 1e-05, &#34;rope_scaling&#34;: { &#34;factor&#34;: 6.0, &#34;rope_type&#34;: &#34;dynamic&#34; }, &#34;rope_theta&#34;: 50000000, &#34;tie_word_embeddings&#34;: false, &#34;torch_dtype&#34;: &#34;bfloat16&#34;, &#34;transformers_version&#34;: &#34;4.47.1&#34;, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 128512 } æ¨¡å— (Module) å­æ¨¡å— (Sub-module) åŠŸèƒ½æè¿° é…ç½®å‚æ•° InternLM3ForCausalLM model: InternLM3Model æ¨¡å‹ä¸»å¹²ï¼ŒåŒ…å«è¯åµŒå…¥å’Œè§£ç å™¨å±‚ã€‚ - lm_head: Linear çº¿æ€§è¾“å‡ºå±‚ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ï¼Œç”Ÿæˆé¢„æµ‹ logitsã€‚ hidden_size: 4096, vocab_size: 128512 â€” â€” â€” â€” InternLM3Model embed_tokens: Embedding å°†è¾“å…¥çš„ token IDs è½¬æ¢ä¸ºç¨ å¯†å‘é‡ï¼ˆEmbeddingsï¼‰ã€‚ vocab_size: 128512, hidden_size: 4096 layers: ModuleList[InternLM3DecoderLayer] åŒ…å«å¤šä¸ªï¼ˆ48ä¸ªï¼‰Transformerè§£ç å™¨å±‚ã€‚ num_hidden_layers: 48 norm: RMSNorm åœ¨æ‰€æœ‰è§£ç å™¨å±‚ä¹‹åï¼Œå¯¹æœ€ç»ˆçš„éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 rotary_emb: RotaryEmbedding è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ä½ç½®ä¿¡æ¯ã€‚ head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {&#34;factor&#34;: 6.0, &#34;rope_type&#34;: &#34;dynamic&#34;} â€” â€” â€” â€” InternLM3DecoderLayer self_attn: InternLM3Attention å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚ - mlp: InternLM3MLP å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºå¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚ - input_layernorm: RMSNorm åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹å‰å¯¹è¾“å…¥è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 post_attention_layernorm: RMSNorm åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹åã€MLPæ¨¡å—ä¹‹å‰è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 â€” â€” â€” â€” InternLM3Attention qkv_proj: Linear å°†è¾“å…¥çš„éšè—çŠ¶æ€çº¿æ€§å˜æ¢ä¸ºæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰ã€‚é‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ã€‚ hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V) o_proj: Linear å°†æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºçº¿æ€§å˜æ¢å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚ hidden_size: 4096 apply_rotary_pos_emb å°†æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰åº”ç”¨äºQå’ŒKã€‚ - â€” â€” â€” â€” InternLM3MLP gate_up_proj: Linear ä¸¤ä¸ªå¹¶è¡Œçš„çº¿æ€§å±‚ï¼ˆgate å’Œ upï¼‰ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°ä¸­é—´ç»´åº¦ã€‚ hidden_size: 4096, intermediate_size: 10240 act_fn: SiluAndMul ä½¿ç”¨ SiLU (Swish) æ¿€æ´»å‡½æ•°å¹¶è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚ hidden_act: &#34;silu&#34; down_proj: Linear å°†æ¿€æ´»åçš„ä¸­é—´çŠ¶æ€æ˜ å°„å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚ intermediate_size: 10240, hidden_size: 4096 InternLM3DecoderLayer è¿ç®—æµç¨‹ å‡è®¾è¾“å…¥ä¸º hidden_states å’Œ residual (åœ¨å‰ä¸€å±‚è®¡ç®—å¾—å‡ºï¼Œç¬¬ä¸€å±‚æ—¶ residual ç­‰äº hidden_states)ã€‚">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-06-17T10:08:03+08:00">
    <meta property="article:modified_time" content="2025-06-17T10:08:03+08:00">
    <meta property="article:tag" content="Tag 1">
    <meta property="article:tag" content="Tag 2">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="InternLM3 8B Instruct">
<meta name="twitter:description" content="model &amp; config
Network Definition
{
    &#34;architectures&#34;: [
        &#34;InternLM3ForCausalLM&#34;
    ],
    &#34;attention_dropout&#34;: 0.0,
    &#34;auto_map&#34;: {
        &#34;AutoConfig&#34;: &#34;configuration_internlm3.InternLM3Config&#34;,
        &#34;AutoModel&#34;: &#34;modeling_internlm3.InternLM3Model&#34;,
        &#34;AutoModelForCausalLM&#34;: &#34;modeling_internlm3.InternLM3ForCausalLM&#34;
    },
    &#34;bias&#34;: false,
    &#34;bos_token_id&#34;: 1,
    &#34;eos_token_id&#34;: 2,
    &#34;head_dim&#34;: 128,
    &#34;hidden_act&#34;: &#34;silu&#34;,
    &#34;hidden_size&#34;: 4096,
    &#34;initializer_range&#34;: 0.02,
    &#34;intermediate_size&#34;: 10240,
    &#34;max_position_embeddings&#34;: 32768,
    &#34;model_type&#34;: &#34;internlm3&#34;,
    &#34;num_attention_heads&#34;: 32,
    &#34;num_hidden_layers&#34;: 48,
    &#34;num_key_value_heads&#34;: 2,
    &#34;pad_token_id&#34;: 2,
    &#34;qkv_bias&#34;: false,
    &#34;rms_norm_eps&#34;: 1e-05,
    &#34;rope_scaling&#34;: {
        &#34;factor&#34;: 6.0,
        &#34;rope_type&#34;: &#34;dynamic&#34;
    },
    &#34;rope_theta&#34;: 50000000,
    &#34;tie_word_embeddings&#34;: false,
    &#34;torch_dtype&#34;: &#34;bfloat16&#34;,
    &#34;transformers_version&#34;: &#34;4.47.1&#34;,
    &#34;use_cache&#34;: true,
    &#34;vocab_size&#34;: 128512
}

  
      
          æ¨¡å— (Module)
          å­æ¨¡å— (Sub-module)
          åŠŸèƒ½æè¿°
          é…ç½®å‚æ•°
      
  
  
      
          InternLM3ForCausalLM
          model: InternLM3Model
          æ¨¡å‹ä¸»å¹²ï¼ŒåŒ…å«è¯åµŒå…¥å’Œè§£ç å™¨å±‚ã€‚
          -
      
      
          
          lm_head: Linear
          çº¿æ€§è¾“å‡ºå±‚ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ï¼Œç”Ÿæˆé¢„æµ‹ logitsã€‚
          hidden_size: 4096, vocab_size: 128512
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3Model
          embed_tokens: Embedding
          å°†è¾“å…¥çš„ token IDs è½¬æ¢ä¸ºç¨ å¯†å‘é‡ï¼ˆEmbeddingsï¼‰ã€‚
          vocab_size: 128512, hidden_size: 4096
      
      
          
          layers: ModuleList[InternLM3DecoderLayer]
          åŒ…å«å¤šä¸ªï¼ˆ48ä¸ªï¼‰Transformerè§£ç å™¨å±‚ã€‚
          num_hidden_layers: 48
      
      
          
          norm: RMSNorm
          åœ¨æ‰€æœ‰è§£ç å™¨å±‚ä¹‹åï¼Œå¯¹æœ€ç»ˆçš„éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ã€‚
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          
          rotary_emb: RotaryEmbedding
          è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ä½ç½®ä¿¡æ¯ã€‚
          head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {&quot;factor&quot;: 6.0, &quot;rope_type&quot;: &quot;dynamic&quot;}
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3DecoderLayer
          self_attn: InternLM3Attention
          å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚
          -
      
      
          
          mlp: InternLM3MLP
          å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºå¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚
          -
      
      
          
          input_layernorm: RMSNorm
          åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹å‰å¯¹è¾“å…¥è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          
          post_attention_layernorm: RMSNorm
          åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹åã€MLPæ¨¡å—ä¹‹å‰è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3Attention
          qkv_proj: Linear
          å°†è¾“å…¥çš„éšè—çŠ¶æ€çº¿æ€§å˜æ¢ä¸ºæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰ã€‚é‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ã€‚
          hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V)
      
      
          
          o_proj: Linear
          å°†æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºçº¿æ€§å˜æ¢å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚
          hidden_size: 4096
      
      
          
          apply_rotary_pos_emb
          å°†æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰åº”ç”¨äºQå’ŒKã€‚
          -
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3MLP
          gate_up_proj: Linear
          ä¸¤ä¸ªå¹¶è¡Œçš„çº¿æ€§å±‚ï¼ˆgate å’Œ upï¼‰ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°ä¸­é—´ç»´åº¦ã€‚
          hidden_size: 4096, intermediate_size: 10240
      
      
          
          act_fn: SiluAndMul
          ä½¿ç”¨ SiLU (Swish) æ¿€æ´»å‡½æ•°å¹¶è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚
          hidden_act: &quot;silu&quot;
      
      
          
          down_proj: Linear
          å°†æ¿€æ´»åçš„ä¸­é—´çŠ¶æ€æ˜ å°„å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚
          intermediate_size: 10240, hidden_size: 4096
      
  

InternLM3DecoderLayer è¿ç®—æµç¨‹
å‡è®¾è¾“å…¥ä¸º hidden_states å’Œ residual (åœ¨å‰ä¸€å±‚è®¡ç®—å¾—å‡ºï¼Œç¬¬ä¸€å±‚æ—¶ residual ç­‰äº hidden_states)ã€‚">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "InternLM3 8B Instruct",
      "item": "http://localhost:1313/blogs/internlm3-8b-instruct/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "InternLM3 8B Instruct",
  "name": "InternLM3 8B Instruct",
  "description": "model \u0026amp; config Network Definition\n{ \u0026#34;architectures\u0026#34;: [ \u0026#34;InternLM3ForCausalLM\u0026#34; ], \u0026#34;attention_dropout\u0026#34;: 0.0, \u0026#34;auto_map\u0026#34;: { \u0026#34;AutoConfig\u0026#34;: \u0026#34;configuration_internlm3.InternLM3Config\u0026#34;, \u0026#34;AutoModel\u0026#34;: \u0026#34;modeling_internlm3.InternLM3Model\u0026#34;, \u0026#34;AutoModelForCausalLM\u0026#34;: \u0026#34;modeling_internlm3.InternLM3ForCausalLM\u0026#34; }, \u0026#34;bias\u0026#34;: false, \u0026#34;bos_token_id\u0026#34;: 1, \u0026#34;eos_token_id\u0026#34;: 2, \u0026#34;head_dim\u0026#34;: 128, \u0026#34;hidden_act\u0026#34;: \u0026#34;silu\u0026#34;, \u0026#34;hidden_size\u0026#34;: 4096, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 10240, \u0026#34;max_position_embeddings\u0026#34;: 32768, \u0026#34;model_type\u0026#34;: \u0026#34;internlm3\u0026#34;, \u0026#34;num_attention_heads\u0026#34;: 32, \u0026#34;num_hidden_layers\u0026#34;: 48, \u0026#34;num_key_value_heads\u0026#34;: 2, \u0026#34;pad_token_id\u0026#34;: 2, \u0026#34;qkv_bias\u0026#34;: false, \u0026#34;rms_norm_eps\u0026#34;: 1e-05, \u0026#34;rope_scaling\u0026#34;: { \u0026#34;factor\u0026#34;: 6.0, \u0026#34;rope_type\u0026#34;: \u0026#34;dynamic\u0026#34; }, \u0026#34;rope_theta\u0026#34;: 50000000, \u0026#34;tie_word_embeddings\u0026#34;: false, \u0026#34;torch_dtype\u0026#34;: \u0026#34;bfloat16\u0026#34;, \u0026#34;transformers_version\u0026#34;: \u0026#34;4.47.1\u0026#34;, \u0026#34;use_cache\u0026#34;: true, \u0026#34;vocab_size\u0026#34;: 128512 } æ¨¡å— (Module) å­æ¨¡å— (Sub-module) åŠŸèƒ½æè¿° é…ç½®å‚æ•° InternLM3ForCausalLM model: InternLM3Model æ¨¡å‹ä¸»å¹²ï¼ŒåŒ…å«è¯åµŒå…¥å’Œè§£ç å™¨å±‚ã€‚ - lm_head: Linear çº¿æ€§è¾“å‡ºå±‚ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ï¼Œç”Ÿæˆé¢„æµ‹ logitsã€‚ hidden_size: 4096, vocab_size: 128512 \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; InternLM3Model embed_tokens: Embedding å°†è¾“å…¥çš„ token IDs è½¬æ¢ä¸ºç¨ å¯†å‘é‡ï¼ˆEmbeddingsï¼‰ã€‚ vocab_size: 128512, hidden_size: 4096 layers: ModuleList[InternLM3DecoderLayer] åŒ…å«å¤šä¸ªï¼ˆ48ä¸ªï¼‰Transformerè§£ç å™¨å±‚ã€‚ num_hidden_layers: 48 norm: RMSNorm åœ¨æ‰€æœ‰è§£ç å™¨å±‚ä¹‹åï¼Œå¯¹æœ€ç»ˆçš„éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 rotary_emb: RotaryEmbedding è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ä½ç½®ä¿¡æ¯ã€‚ head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {\u0026quot;factor\u0026quot;: 6.0, \u0026quot;rope_type\u0026quot;: \u0026quot;dynamic\u0026quot;} \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; InternLM3DecoderLayer self_attn: InternLM3Attention å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚ - mlp: InternLM3MLP å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºå¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚ - input_layernorm: RMSNorm åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹å‰å¯¹è¾“å…¥è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 post_attention_layernorm: RMSNorm åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹åã€MLPæ¨¡å—ä¹‹å‰è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; InternLM3Attention qkv_proj: Linear å°†è¾“å…¥çš„éšè—çŠ¶æ€çº¿æ€§å˜æ¢ä¸ºæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰ã€‚é‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ã€‚ hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V) o_proj: Linear å°†æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºçº¿æ€§å˜æ¢å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚ hidden_size: 4096 apply_rotary_pos_emb å°†æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰åº”ç”¨äºQå’ŒKã€‚ - \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; InternLM3MLP gate_up_proj: Linear ä¸¤ä¸ªå¹¶è¡Œçš„çº¿æ€§å±‚ï¼ˆgate å’Œ upï¼‰ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°ä¸­é—´ç»´åº¦ã€‚ hidden_size: 4096, intermediate_size: 10240 act_fn: SiluAndMul ä½¿ç”¨ SiLU (Swish) æ¿€æ´»å‡½æ•°å¹¶è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚ hidden_act: \u0026quot;silu\u0026quot; down_proj: Linear å°†æ¿€æ´»åçš„ä¸­é—´çŠ¶æ€æ˜ å°„å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚ intermediate_size: 10240, hidden_size: 4096 InternLM3DecoderLayer è¿ç®—æµç¨‹ å‡è®¾è¾“å…¥ä¸º hidden_states å’Œ residual (åœ¨å‰ä¸€å±‚è®¡ç®—å¾—å‡ºï¼Œç¬¬ä¸€å±‚æ—¶ residual ç­‰äº hidden_states)ã€‚\n",
  "keywords": [
    "word 1", "word 2"
  ],
  "articleBody": "model \u0026 config Network Definition\n{ \"architectures\": [ \"InternLM3ForCausalLM\" ], \"attention_dropout\": 0.0, \"auto_map\": { \"AutoConfig\": \"configuration_internlm3.InternLM3Config\", \"AutoModel\": \"modeling_internlm3.InternLM3Model\", \"AutoModelForCausalLM\": \"modeling_internlm3.InternLM3ForCausalLM\" }, \"bias\": false, \"bos_token_id\": 1, \"eos_token_id\": 2, \"head_dim\": 128, \"hidden_act\": \"silu\", \"hidden_size\": 4096, \"initializer_range\": 0.02, \"intermediate_size\": 10240, \"max_position_embeddings\": 32768, \"model_type\": \"internlm3\", \"num_attention_heads\": 32, \"num_hidden_layers\": 48, \"num_key_value_heads\": 2, \"pad_token_id\": 2, \"qkv_bias\": false, \"rms_norm_eps\": 1e-05, \"rope_scaling\": { \"factor\": 6.0, \"rope_type\": \"dynamic\" }, \"rope_theta\": 50000000, \"tie_word_embeddings\": false, \"torch_dtype\": \"bfloat16\", \"transformers_version\": \"4.47.1\", \"use_cache\": true, \"vocab_size\": 128512 } æ¨¡å— (Module) å­æ¨¡å— (Sub-module) åŠŸèƒ½æè¿° é…ç½®å‚æ•° InternLM3ForCausalLM model: InternLM3Model æ¨¡å‹ä¸»å¹²ï¼ŒåŒ…å«è¯åµŒå…¥å’Œè§£ç å™¨å±‚ã€‚ - lm_head: Linear çº¿æ€§è¾“å‡ºå±‚ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ï¼Œç”Ÿæˆé¢„æµ‹ logitsã€‚ hidden_size: 4096, vocab_size: 128512 â€” â€” â€” â€” InternLM3Model embed_tokens: Embedding å°†è¾“å…¥çš„ token IDs è½¬æ¢ä¸ºç¨ å¯†å‘é‡ï¼ˆEmbeddingsï¼‰ã€‚ vocab_size: 128512, hidden_size: 4096 layers: ModuleList[InternLM3DecoderLayer] åŒ…å«å¤šä¸ªï¼ˆ48ä¸ªï¼‰Transformerè§£ç å™¨å±‚ã€‚ num_hidden_layers: 48 norm: RMSNorm åœ¨æ‰€æœ‰è§£ç å™¨å±‚ä¹‹åï¼Œå¯¹æœ€ç»ˆçš„éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 rotary_emb: RotaryEmbedding è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ä½ç½®ä¿¡æ¯ã€‚ head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {\"factor\": 6.0, \"rope_type\": \"dynamic\"} â€” â€” â€” â€” InternLM3DecoderLayer self_attn: InternLM3Attention å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚ - mlp: InternLM3MLP å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºå¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚ - input_layernorm: RMSNorm åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹å‰å¯¹è¾“å…¥è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 post_attention_layernorm: RMSNorm åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹åã€MLPæ¨¡å—ä¹‹å‰è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚ hidden_size: 4096, rms_norm_eps: 1e-05 â€” â€” â€” â€” InternLM3Attention qkv_proj: Linear å°†è¾“å…¥çš„éšè—çŠ¶æ€çº¿æ€§å˜æ¢ä¸ºæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰ã€‚é‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ã€‚ hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V) o_proj: Linear å°†æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºçº¿æ€§å˜æ¢å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚ hidden_size: 4096 apply_rotary_pos_emb å°†æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰åº”ç”¨äºQå’ŒKã€‚ - â€” â€” â€” â€” InternLM3MLP gate_up_proj: Linear ä¸¤ä¸ªå¹¶è¡Œçš„çº¿æ€§å±‚ï¼ˆgate å’Œ upï¼‰ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°ä¸­é—´ç»´åº¦ã€‚ hidden_size: 4096, intermediate_size: 10240 act_fn: SiluAndMul ä½¿ç”¨ SiLU (Swish) æ¿€æ´»å‡½æ•°å¹¶è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚ hidden_act: \"silu\" down_proj: Linear å°†æ¿€æ´»åçš„ä¸­é—´çŠ¶æ€æ˜ å°„å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚ intermediate_size: 10240, hidden_size: 4096 InternLM3DecoderLayer è¿ç®—æµç¨‹ å‡è®¾è¾“å…¥ä¸º hidden_states å’Œ residual (åœ¨å‰ä¸€å±‚è®¡ç®—å¾—å‡ºï¼Œç¬¬ä¸€å±‚æ—¶ residual ç­‰äº hidden_states)ã€‚\næ­¥éª¤ æ¨¡å—/æ“ä½œ è¾“å…¥ è¿ç®—æè¿° è¾“å‡º 1. è¾“å…¥å½’ä¸€åŒ– input_layernorm (RMSNorm) hidden_states, residual å¯¹ hidden_states è¿›è¡Œ RMS å½’ä¸€åŒ–ã€‚åŒæ—¶ï¼Œå°† hidden_states åŠ ä¸Šä¸Šä¸€å±‚çš„æ®‹å·® residualï¼Œä¸ºåç»­çš„æ®‹å·®è¿æ¥åšå‡†å¤‡ã€‚ norm_hidden_states, new_residual 2. è‡ªæ³¨æ„åŠ› (Self-Attention) self_attn (InternLM3Attention) norm_hidden_states è¿™æ˜¯æœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œå†…å«å¤šä¸ªå­æ­¥éª¤ï¼š\na. QKV æŠ•å°„: qkv_proj å°† norm_hidden_states çº¿æ€§å˜æ¢ï¼Œç”ŸæˆæŸ¥è¯¢ Qã€é”® K å’Œå€¼ Vã€‚\nb. ä½ç½®ç¼–ç : apply_rotary_pos_emb å°†æ—‹è½¬ä½ç½®ç¼–ç  (RoPE) åº”ç”¨äº Q å’Œ Kï¼Œæ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚\nc. æ³¨æ„åŠ›è®¡ç®—: attn_fwd æ ¹æ® Qã€Kã€V è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¹¶ç”ŸæˆåŠ æƒå’Œã€‚\nd. è¾“å‡ºæŠ•å°„: o_proj å°†æ³¨æ„åŠ›è®¡ç®—ç»“æœçº¿æ€§å˜æ¢å› hidden_size ç»´åº¦ã€‚ attn_output 3. ç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥ + attn_output, new_residual å°†æ­¥éª¤ 2 çš„ attn_output ä¸æ­¥éª¤ 1 çš„ new_residual é€å…ƒç´ ç›¸åŠ ã€‚ attn_residual_output 4. æ³¨æ„åŠ›åå½’ä¸€åŒ– post_attention_layernorm (RMSNorm) attn_residual_output å¯¹æ®‹å·®è¿æ¥åçš„ç»“æœè¿›è¡Œç¬¬äºŒæ¬¡ RMS å½’ä¸€åŒ–ã€‚ norm_attn_output 5. MLP (å‰é¦ˆç½‘ç»œ) mlp (InternLM3MLP) norm_attn_output åŒ…å«ä¸‰ä¸ªå­æ­¥éª¤ï¼š\na. Gate \u0026 Up æŠ•å°„: gate_up_proj åŒæ—¶å°† norm_attn_output çº¿æ€§å˜æ¢åˆ° intermediate_sizeï¼Œå¾—åˆ° gate å’Œ up ä¸¤ä¸ªå¼ é‡ã€‚\nb. æ¿€æ´»: act_fn (SiLU and Multiply) å¯¹ gate åº”ç”¨ SiLU æ¿€æ´»å‡½æ•°ï¼Œç„¶åä¸ up é€å…ƒç´ ç›¸ä¹˜ã€‚\nc. Down æŠ•å°„: down_proj å°†æ¿€æ´»åçš„ç»“æœä» intermediate_size çº¿æ€§å˜æ¢å› hidden_sizeã€‚ mlp_output 6. ç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥ + mlp_output, norm_attn_output å°†æ­¥éª¤ 5 çš„ mlp_output ä¸æ­¥éª¤ 4 çš„ norm_attn_output é€å…ƒç´ ç›¸åŠ ã€‚ final_hidden_states 7. è¾“å‡º - - è¿™ä¸€å±‚çš„æœ€ç»ˆè¾“å‡º hidden_states å°†ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œè€Œ final_residual å°†ä½œä¸ºä¸‹ä¸€å±‚çš„æ®‹å·®è¾“å…¥ã€‚ hidden_states (ç”¨äºä¸‹ä¸€å±‚), residual (ç”¨äºä¸‹ä¸€å±‚) graph TD\rsubgraph \"InternLM3DecoderLayer å†…éƒ¨æµç¨‹\"\rdirection LR\r%% å®šä¹‰è¾“å…¥\rInput[Input: hidden_states\nInput: residual_in] --\u003e Norm1\r%% ç¬¬ä¸€ä¸ªæ¨¡å—ï¼šæ³¨æ„åŠ›\rsubgraph \"æ¨¡å—1: æ³¨æ„åŠ› (Pre-Norm)\"\rNorm1(RMSNorm:\ninput_layernorm) --\u003e Attention[Self-Attention]\rInput -- residual --o Add1\rAttention -- attn_output --o Add1\rend\r%% ç¬¬äºŒä¸ªæ¨¡å—ï¼šMLP\rsubgraph \"æ¨¡å—2: MLP (Pre-Norm)\"\rAdd1(ç¬¬ä¸€æ¬¡\næ®‹å·®è¿æ¥ +) --\u003e Norm2(RMSNorm:\npost_attention_layernorm)\rNorm2 --\u003e MLP[MLP Block]\rAdd1 -- residual --o Add2\rMLP -- mlp_output --o Add2\rend\r%% å®šä¹‰è¾“å‡º\rAdd2(ç¬¬äºŒæ¬¡\næ®‹å·®è¿æ¥ +) --\u003e Output[Output: hidden_states\nOutput: residual_out]\rend\r%% æ ·å¼å®šä¹‰\rclassDef default fill:#f9f9f9,stroke:#333,stroke-width:2px;\rclassDef subgraph_style fill:#eef,stroke:#333,stroke-width:2px,color:#333;\rclass Input,Output,Add1,Add2,Norm1,Norm2,Attention,MLP subgraph_style Group Pattern åœ¨ include/tx8be_mlir/OpHelper.h ä¸­æ·»åŠ ä½ è‡ªå·±å®šä¹‰çš„ pattern åˆ°\ntypedef enum { // ... GROUP_NAME = id, } GroupPatternMode; åœ¨ include/tx8be_mlir/Transforms/LayerGroup/GroupPattern.h å®šä¹‰å¥½ä½ è‡ªå·±çš„\nconst std::map\u003cstd::vector\u003cTX8BE_OPS\u003e, int\u003e PATTERN_NAME { // ... }; ç„¶åæ·»åŠ è¿› patternConfigMap ä¸­\nconst std::map\u003cint, std::map\u003cstd::vector\u003cTX8BE_OPS\u003e\u003e patternConfigMap { {GROUP_NAME, PATTERN_NAME}, } æœ€ååœ¨ lib/Support/OpHelper.cpp çš„ getGroupPatternMode å‡½æ•°é‡Œæ·»åŠ \nelse if (lowerOption.find(\"opt_group_name\")) { mode = GROUP_NAME; } opt_group_name ç”± run_codegen_layer å‘½ä»¤çš„ --opt_group=opt_group_name å‚æ•°æŒ‡å®š\n",
  "wordCount" : "1375",
  "inLanguage": "en",
  "datePublished": "2025-06-17T10:08:03+08:00",
  "dateModified": "2025-06-17T10:08:03+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/internlm3-8b-instruct/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="ç®€ä½“ä¸­æ–‡"
                            aria-label="ç®€ä½“ä¸­æ–‡">ç®€ä½“ä¸­æ–‡</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="ğŸ  Home">
                    <span>ğŸ  Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="ğŸ™‹ğŸ»â€â™‚ï¸ Me">
                    <span>ğŸ™‹ğŸ»â€â™‚ï¸ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="ğŸ“š Blogs">
                    <span>ğŸ“š Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="ğŸ§© Categories">
                    <span>ğŸ§© Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="ğŸ”– Tags">
                    <span>ğŸ”– Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="â± Archive">
                    <span>â± Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="ğŸ” Search (Alt &#43; /)" accesskey=/>
                    <span>ğŸ” Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="ğŸ¤ Friends">
                    <span>ğŸ¤ Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      InternLM3 8B Instruct
    </h1>
    <div class="post-meta"><span title='2025-06-17 10:08:03 +0800 CST'>Jun-17-2025</span>&nbsp;Â·&nbsp;3 min&nbsp;Â·&nbsp;1375 words&nbsp;Â·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#model--config" aria-label="model &amp; config">model &amp; config</a><ul>
                            <ul>
                            
                    <li>
                        <a href="#internlm3decoderlayer-%e8%bf%90%e7%ae%97%e6%b5%81%e7%a8%8b" aria-label="InternLM3DecoderLayer è¿ç®—æµç¨‹">InternLM3DecoderLayer è¿ç®—æµç¨‹</a></li></ul>
                        </ul>
                    </li>
                    <li>
                        <a href="#group-pattern" aria-label="Group Pattern">Group Pattern</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="model--config">model &amp; config<a hidden class="anchor" aria-hidden="true" href="#model--config">#</a></h1>
<p><a href="https://github.com/InternLM/lmdeploy/blob/7ca466599f01e5ef93e8951771c62163136e21b2/lmdeploy/pytorch/models/internlm3.py#L304">Network Definition</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;architectures&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;InternLM3ForCausalLM&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;attention_dropout&#34;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;auto_map&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoConfig&#34;</span><span class="p">:</span> <span class="s2">&#34;configuration_internlm3.InternLM3Config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoModel&#34;</span><span class="p">:</span> <span class="s2">&#34;modeling_internlm3.InternLM3Model&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoModelForCausalLM&#34;</span><span class="p">:</span> <span class="s2">&#34;modeling_internlm3.InternLM3ForCausalLM&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;bias&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;bos_token_id&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;eos_token_id&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;head_dim&#34;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;hidden_act&#34;</span><span class="p">:</span> <span class="s2">&#34;silu&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;hidden_size&#34;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;initializer_range&#34;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;intermediate_size&#34;</span><span class="p">:</span> <span class="mi">10240</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;max_position_embeddings&#34;</span><span class="p">:</span> <span class="mi">32768</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;model_type&#34;</span><span class="p">:</span> <span class="s2">&#34;internlm3&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_attention_heads&#34;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_hidden_layers&#34;</span><span class="p">:</span> <span class="mi">48</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_key_value_heads&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;pad_token_id&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;qkv_bias&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rms_norm_eps&#34;</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rope_scaling&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;factor&#34;</span><span class="p">:</span> <span class="mf">6.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;rope_type&#34;</span><span class="p">:</span> <span class="s2">&#34;dynamic&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rope_theta&#34;</span><span class="p">:</span> <span class="mi">50000000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;tie_word_embeddings&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;torch_dtype&#34;</span><span class="p">:</span> <span class="s2">&#34;bfloat16&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;transformers_version&#34;</span><span class="p">:</span> <span class="s2">&#34;4.47.1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;use_cache&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;vocab_size&#34;</span><span class="p">:</span> <span class="mi">128512</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>æ¨¡å— (Module)</strong></th>
          <th style="text-align: left"><strong>å­æ¨¡å— (Sub-module)</strong></th>
          <th style="text-align: left"><strong>åŠŸèƒ½æè¿°</strong></th>
          <th style="text-align: left"><strong>é…ç½®å‚æ•°</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><code>InternLM3ForCausalLM</code></td>
          <td style="text-align: left"><code>model: InternLM3Model</code></td>
          <td style="text-align: left">æ¨¡å‹ä¸»å¹²ï¼ŒåŒ…å«è¯åµŒå…¥å’Œè§£ç å™¨å±‚ã€‚</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>lm_head: Linear</code></td>
          <td style="text-align: left">çº¿æ€§è¾“å‡ºå±‚ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°è¯æ±‡è¡¨å¤§å°ï¼Œç”Ÿæˆé¢„æµ‹ logitsã€‚</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>vocab_size: 128512</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3Model</code></td>
          <td style="text-align: left"><code>embed_tokens: Embedding</code></td>
          <td style="text-align: left">å°†è¾“å…¥çš„ token IDs è½¬æ¢ä¸ºç¨ å¯†å‘é‡ï¼ˆEmbeddingsï¼‰ã€‚</td>
          <td style="text-align: left"><code>vocab_size: 128512</code>, <code>hidden_size: 4096</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>layers: ModuleList[InternLM3DecoderLayer]</code></td>
          <td style="text-align: left">åŒ…å«å¤šä¸ªï¼ˆ48ä¸ªï¼‰Transformerè§£ç å™¨å±‚ã€‚</td>
          <td style="text-align: left"><code>num_hidden_layers: 48</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>norm: RMSNorm</code></td>
          <td style="text-align: left">åœ¨æ‰€æœ‰è§£ç å™¨å±‚ä¹‹åï¼Œå¯¹æœ€ç»ˆçš„éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ã€‚</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>rotary_emb: RotaryEmbedding</code></td>
          <td style="text-align: left">è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œç”¨äºåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­èå…¥ä½ç½®ä¿¡æ¯ã€‚</td>
          <td style="text-align: left"><code>head_dim: 128</code>, <code>max_position_embeddings: 32768</code>, <code>rope_theta: 50000000</code>, <code>rope_scaling: {&quot;factor&quot;: 6.0, &quot;rope_type&quot;: &quot;dynamic&quot;}</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3DecoderLayer</code></td>
          <td style="text-align: left"><code>self_attn: InternLM3Attention</code></td>
          <td style="text-align: left">å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ•æ‰è¾“å…¥åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>mlp: InternLM3MLP</code></td>
          <td style="text-align: left">å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºå¯¹æ³¨æ„åŠ›è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>input_layernorm: RMSNorm</code></td>
          <td style="text-align: left">åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹å‰å¯¹è¾“å…¥è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>post_attention_layernorm: RMSNorm</code></td>
          <td style="text-align: left">åœ¨è‡ªæ³¨æ„åŠ›æ¨¡å—ä¹‹åã€MLPæ¨¡å—ä¹‹å‰è¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3Attention</code></td>
          <td style="text-align: left"><code>qkv_proj: Linear</code></td>
          <td style="text-align: left">å°†è¾“å…¥çš„éšè—çŠ¶æ€çº¿æ€§å˜æ¢ä¸ºæŸ¥è¯¢ï¼ˆQï¼‰ã€é”®ï¼ˆKï¼‰ã€å€¼ï¼ˆVï¼‰ã€‚é‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ã€‚</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>num_attention_heads: 32</code> (Q), <code>num_key_value_heads: 2</code> (K, V)</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>o_proj: Linear</code></td>
          <td style="text-align: left">å°†æ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºçº¿æ€§å˜æ¢å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚</td>
          <td style="text-align: left"><code>hidden_size: 4096</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>apply_rotary_pos_emb</code></td>
          <td style="text-align: left">å°†æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰åº”ç”¨äºQå’ŒKã€‚</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3MLP</code></td>
          <td style="text-align: left"><code>gate_up_proj: Linear</code></td>
          <td style="text-align: left">ä¸¤ä¸ªå¹¶è¡Œçš„çº¿æ€§å±‚ï¼ˆgate å’Œ upï¼‰ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„åˆ°ä¸­é—´ç»´åº¦ã€‚</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>intermediate_size: 10240</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>act_fn: SiluAndMul</code></td>
          <td style="text-align: left">ä½¿ç”¨ SiLU (Swish) æ¿€æ´»å‡½æ•°å¹¶è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚</td>
          <td style="text-align: left"><code>hidden_act: &quot;silu&quot;</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>down_proj: Linear</code></td>
          <td style="text-align: left">å°†æ¿€æ´»åçš„ä¸­é—´çŠ¶æ€æ˜ å°„å›éšè—çŠ¶æ€çš„ç»´åº¦ã€‚</td>
          <td style="text-align: left"><code>intermediate_size: 10240</code>, <code>hidden_size: 4096</code></td>
      </tr>
  </tbody>
</table>
<h3 id="internlm3decoderlayer-è¿ç®—æµç¨‹">InternLM3DecoderLayer è¿ç®—æµç¨‹<a hidden class="anchor" aria-hidden="true" href="#internlm3decoderlayer-è¿ç®—æµç¨‹">#</a></h3>
<p>å‡è®¾è¾“å…¥ä¸º <code>hidden_states</code> å’Œ <code>residual</code> (åœ¨å‰ä¸€å±‚è®¡ç®—å¾—å‡ºï¼Œç¬¬ä¸€å±‚æ—¶ <code>residual</code> ç­‰äº <code>hidden_states</code>)ã€‚</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>æ­¥éª¤</strong></th>
          <th style="text-align: left"><strong>æ¨¡å—/æ“ä½œ</strong></th>
          <th style="text-align: left"><strong>è¾“å…¥</strong></th>
          <th style="text-align: left"><strong>è¿ç®—æè¿°</strong></th>
          <th style="text-align: left"><strong>è¾“å‡º</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>1. è¾“å…¥å½’ä¸€åŒ–</strong></td>
          <td style="text-align: left"><code>input_layernorm</code> (RMSNorm)</td>
          <td style="text-align: left"><code>hidden_states</code>, <code>residual</code></td>
          <td style="text-align: left">å¯¹ <code>hidden_states</code> è¿›è¡Œ RMS å½’ä¸€åŒ–ã€‚åŒæ—¶ï¼Œå°† <code>hidden_states</code> åŠ ä¸Šä¸Šä¸€å±‚çš„æ®‹å·® <code>residual</code>ï¼Œä¸ºåç»­çš„æ®‹å·®è¿æ¥åšå‡†å¤‡ã€‚</td>
          <td style="text-align: left"><code>norm_hidden_states</code>, <code>new_residual</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>2. è‡ªæ³¨æ„åŠ› (Self-Attention)</strong></td>
          <td style="text-align: left"><code>self_attn</code> (InternLM3Attention)</td>
          <td style="text-align: left"><code>norm_hidden_states</code></td>
          <td style="text-align: left"><strong>è¿™æ˜¯æœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œå†…å«å¤šä¸ªå­æ­¥éª¤ï¼š</strong><br>a. <strong>QKV æŠ•å°„</strong>: <code>qkv_proj</code> å°† <code>norm_hidden_states</code> çº¿æ€§å˜æ¢ï¼Œç”ŸæˆæŸ¥è¯¢ <code>Q</code>ã€é”® <code>K</code> å’Œå€¼ <code>V</code>ã€‚<br>b. <strong>ä½ç½®ç¼–ç </strong>: <code>apply_rotary_pos_emb</code> å°†æ—‹è½¬ä½ç½®ç¼–ç  (RoPE) åº”ç”¨äº <code>Q</code> å’Œ <code>K</code>ï¼Œæ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚<br>c. <strong>æ³¨æ„åŠ›è®¡ç®—</strong>: <code>attn_fwd</code> æ ¹æ® <code>Q</code>ã€<code>K</code>ã€<code>V</code> è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¹¶ç”ŸæˆåŠ æƒå’Œã€‚<br>d. <strong>è¾“å‡ºæŠ•å°„</strong>: <code>o_proj</code> å°†æ³¨æ„åŠ›è®¡ç®—ç»“æœçº¿æ€§å˜æ¢å› <code>hidden_size</code> ç»´åº¦ã€‚</td>
          <td style="text-align: left"><code>attn_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>3. ç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥</strong></td>
          <td style="text-align: left"><code>+</code></td>
          <td style="text-align: left"><code>attn_output</code>, <code>new_residual</code></td>
          <td style="text-align: left">å°†æ­¥éª¤ 2 çš„ <code>attn_output</code> ä¸æ­¥éª¤ 1 çš„ <code>new_residual</code> é€å…ƒç´ ç›¸åŠ ã€‚</td>
          <td style="text-align: left"><code>attn_residual_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>4. æ³¨æ„åŠ›åå½’ä¸€åŒ–</strong></td>
          <td style="text-align: left"><code>post_attention_layernorm</code> (RMSNorm)</td>
          <td style="text-align: left"><code>attn_residual_output</code></td>
          <td style="text-align: left">å¯¹æ®‹å·®è¿æ¥åçš„ç»“æœè¿›è¡Œç¬¬äºŒæ¬¡ RMS å½’ä¸€åŒ–ã€‚</td>
          <td style="text-align: left"><code>norm_attn_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>5. MLP (å‰é¦ˆç½‘ç»œ)</strong></td>
          <td style="text-align: left"><code>mlp</code> (InternLM3MLP)</td>
          <td style="text-align: left"><code>norm_attn_output</code></td>
          <td style="text-align: left"><strong>åŒ…å«ä¸‰ä¸ªå­æ­¥éª¤ï¼š</strong><br>a. <strong>Gate &amp; Up æŠ•å°„</strong>: <code>gate_up_proj</code> åŒæ—¶å°† <code>norm_attn_output</code> çº¿æ€§å˜æ¢åˆ° <code>intermediate_size</code>ï¼Œå¾—åˆ° <code>gate</code> å’Œ <code>up</code> ä¸¤ä¸ªå¼ é‡ã€‚<br>b. <strong>æ¿€æ´»</strong>: <code>act_fn</code> (SiLU and Multiply) å¯¹ <code>gate</code> åº”ç”¨ SiLU æ¿€æ´»å‡½æ•°ï¼Œç„¶åä¸ <code>up</code> é€å…ƒç´ ç›¸ä¹˜ã€‚<br>c. <strong>Down æŠ•å°„</strong>: <code>down_proj</code> å°†æ¿€æ´»åçš„ç»“æœä» <code>intermediate_size</code> çº¿æ€§å˜æ¢å› <code>hidden_size</code>ã€‚</td>
          <td style="text-align: left"><code>mlp_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>6. ç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥</strong></td>
          <td style="text-align: left"><code>+</code></td>
          <td style="text-align: left"><code>mlp_output</code>, <code>norm_attn_output</code></td>
          <td style="text-align: left">å°†æ­¥éª¤ 5 çš„ <code>mlp_output</code> ä¸æ­¥éª¤ 4 çš„ <code>norm_attn_output</code> é€å…ƒç´ ç›¸åŠ ã€‚</td>
          <td style="text-align: left"><code>final_hidden_states</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>7. è¾“å‡º</strong></td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">è¿™ä¸€å±‚çš„æœ€ç»ˆè¾“å‡º <code>hidden_states</code> å°†ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œè€Œ <code>final_residual</code> å°†ä½œä¸ºä¸‹ä¸€å±‚çš„æ®‹å·®è¾“å…¥ã€‚</td>
          <td style="text-align: left"><code>hidden_states</code> (ç”¨äºä¸‹ä¸€å±‚), <code>residual</code> (ç”¨äºä¸‹ä¸€å±‚)</td>
      </tr>
  </tbody>
</table>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">graph TD
    subgraph &#34;InternLM3DecoderLayer å†…éƒ¨æµç¨‹&#34;
    
    direction LR
    
    %% å®šä¹‰è¾“å…¥
    Input[Input: hidden_states&lt;br&gt;Input: residual_in] --&gt; Norm1
    
    %% ç¬¬ä¸€ä¸ªæ¨¡å—ï¼šæ³¨æ„åŠ›
    subgraph &#34;æ¨¡å—1: æ³¨æ„åŠ› (Pre-Norm)&#34;
        Norm1(RMSNorm:&lt;br&gt;input_layernorm) --&gt; Attention[Self-Attention]
        Input -- residual --o Add1
        Attention -- attn_output --o Add1
    end
    
    %% ç¬¬äºŒä¸ªæ¨¡å—ï¼šMLP
    subgraph &#34;æ¨¡å—2: MLP (Pre-Norm)&#34;
        Add1(ç¬¬ä¸€æ¬¡&lt;br&gt;æ®‹å·®è¿æ¥ +) --&gt; Norm2(RMSNorm:&lt;br&gt;post_attention_layernorm)
        Norm2 --&gt; MLP[MLP Block]
        Add1 -- residual --o Add2
        MLP -- mlp_output --o Add2
    end

    %% å®šä¹‰è¾“å‡º
    Add2(ç¬¬äºŒæ¬¡&lt;br&gt;æ®‹å·®è¿æ¥ +) --&gt; Output[Output: hidden_states&lt;br&gt;Output: residual_out]

    end

    %% æ ·å¼å®šä¹‰
    classDef default fill:#f9f9f9,stroke:#333,stroke-width:2px;
    classDef subgraph_style fill:#eef,stroke:#333,stroke-width:2px,color:#333;
    class Input,Output,Add1,Add2,Norm1,Norm2,Attention,MLP subgraph_style
</code></pre><h1 id="group-pattern">Group Pattern<a hidden class="anchor" aria-hidden="true" href="#group-pattern">#</a></h1>
<p>åœ¨ <code>include/tx8be_mlir/OpHelper.h</code> ä¸­æ·»åŠ ä½ è‡ªå·±å®šä¹‰çš„ pattern åˆ°</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">enum</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">GROUP_NAME</span> <span class="o">=</span> <span class="n">id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">GroupPatternMode</span><span class="p">;</span>
</span></span></code></pre></div><p>åœ¨ <code>include/tx8be_mlir/Transforms/LayerGroup/GroupPattern.h</code> å®šä¹‰å¥½ä½ è‡ªå·±çš„</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">PATTERN_NAME</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><p>ç„¶åæ·»åŠ è¿› <code>patternConfigMap</code> ä¸­</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;&gt;</span> <span class="n">patternConfigMap</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="n">GROUP_NAME</span><span class="p">,</span> <span class="n">PATTERN_NAME</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>æœ€ååœ¨ <code>lib/Support/OpHelper.cpp</code> çš„ <code>getGroupPatternMode</code> å‡½æ•°é‡Œæ·»åŠ </p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">lowerOption</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">&#34;opt_group_name&#34;</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">mode</span> <span class="o">=</span> <span class="n">GROUP_NAME</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>opt_group_name</code> ç”± <code>run_codegen_layer</code> å‘½ä»¤çš„ <code>--opt_group=opt_group_name</code> å‚æ•°æŒ‡å®š</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/tag-1/">Tag 1</a></li>
      <li><a href="http://localhost:1313/tags/tag-2/">Tag 2</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/">
    <span class="title">Â« Prev</span>
    <br>
    <span>ServingLLMsOnHuaweiCloudMatrix384</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/pipefusion/">
    <span class="title">Next Â»</span>
    <br>
    <span>PipeFusion</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>Â© 2024-2025 WITHER</span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
