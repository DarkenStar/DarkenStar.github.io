<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>InternLM3 8B Instruct | WITHER</title>
<meta name="keywords" content="word 1, word 2">
<meta name="description" content="model &amp; config
Network Definition
{
    &#34;architectures&#34;: [
        &#34;InternLM3ForCausalLM&#34;
    ],
    &#34;attention_dropout&#34;: 0.0,
    &#34;auto_map&#34;: {
        &#34;AutoConfig&#34;: &#34;configuration_internlm3.InternLM3Config&#34;,
        &#34;AutoModel&#34;: &#34;modeling_internlm3.InternLM3Model&#34;,
        &#34;AutoModelForCausalLM&#34;: &#34;modeling_internlm3.InternLM3ForCausalLM&#34;
    },
    &#34;bias&#34;: false,
    &#34;bos_token_id&#34;: 1,
    &#34;eos_token_id&#34;: 2,
    &#34;head_dim&#34;: 128,
    &#34;hidden_act&#34;: &#34;silu&#34;,
    &#34;hidden_size&#34;: 4096,
    &#34;initializer_range&#34;: 0.02,
    &#34;intermediate_size&#34;: 10240,
    &#34;max_position_embeddings&#34;: 32768,
    &#34;model_type&#34;: &#34;internlm3&#34;,
    &#34;num_attention_heads&#34;: 32,
    &#34;num_hidden_layers&#34;: 48,
    &#34;num_key_value_heads&#34;: 2,
    &#34;pad_token_id&#34;: 2,
    &#34;qkv_bias&#34;: false,
    &#34;rms_norm_eps&#34;: 1e-05,
    &#34;rope_scaling&#34;: {
        &#34;factor&#34;: 6.0,
        &#34;rope_type&#34;: &#34;dynamic&#34;
    },
    &#34;rope_theta&#34;: 50000000,
    &#34;tie_word_embeddings&#34;: false,
    &#34;torch_dtype&#34;: &#34;bfloat16&#34;,
    &#34;transformers_version&#34;: &#34;4.47.1&#34;,
    &#34;use_cache&#34;: true,
    &#34;vocab_size&#34;: 128512
}

  
      
          模块 (Module)
          子模块 (Sub-module)
          功能描述
          配置参数
      
  
  
      
          InternLM3ForCausalLM
          model: InternLM3Model
          模型主干，包含词嵌入和解码器层。
          -
      
      
          
          lm_head: Linear
          线性输出层，将隐藏状态映射到词汇表大小，生成预测 logits。
          hidden_size: 4096, vocab_size: 128512
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3Model
          embed_tokens: Embedding
          将输入的 token IDs 转换为稠密向量（Embeddings）。
          vocab_size: 128512, hidden_size: 4096
      
      
          
          layers: ModuleList[InternLM3DecoderLayer]
          包含多个（48个）Transformer解码器层。
          num_hidden_layers: 48
      
      
          
          norm: RMSNorm
          在所有解码器层之后，对最终的隐藏状态进行归一化。
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          
          rotary_emb: RotaryEmbedding
          计算旋转位置编码（RoPE），用于在注意力机制中融入位置信息。
          head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {&quot;factor&quot;: 6.0, &quot;rope_type&quot;: &quot;dynamic&quot;}
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3DecoderLayer
          self_attn: InternLM3Attention
          多头自注意力模块，用于捕捉输入序列中的依赖关系。
          -
      
      
          
          mlp: InternLM3MLP
          前馈神经网络，用于对注意力输出进行非线性变换。
          -
      
      
          
          input_layernorm: RMSNorm
          在自注意力模块之前对输入进行层归一化。
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          
          post_attention_layernorm: RMSNorm
          在自注意力模块之后、MLP模块之前进行层归一化。
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3Attention
          qkv_proj: Linear
          将输入的隐藏状态线性变换为查询（Q）、键（K）、值（V）。采用分组查询注意力（GQA）。
          hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V)
      
      
          
          o_proj: Linear
          将注意力模块的输出线性变换回隐藏状态的维度。
          hidden_size: 4096
      
      
          
          apply_rotary_pos_emb
          将旋转位置编码（RoPE）应用于Q和K。
          -
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3MLP
          gate_up_proj: Linear
          两个并行的线性层（gate 和 up），将隐藏状态映射到中间维度。
          hidden_size: 4096, intermediate_size: 10240
      
      
          
          act_fn: SiluAndMul
          使用 SiLU (Swish) 激活函数并进行逐元素相乘。
          hidden_act: &quot;silu&quot;
      
      
          
          down_proj: Linear
          将激活后的中间状态映射回隐藏状态的维度。
          intermediate_size: 10240, hidden_size: 4096
      
  

InternLM3DecoderLayer 运算流程
假设输入为 hidden_states 和 residual (在前一层计算得出，第一层时 residual 等于 hidden_states)。">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/internlm3-8b-instruct/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/internlm3-8b-instruct/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/internlm3-8b-instruct/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="InternLM3 8B Instruct">
  <meta property="og:description" content="model &amp; config Network Definition
{ &#34;architectures&#34;: [ &#34;InternLM3ForCausalLM&#34; ], &#34;attention_dropout&#34;: 0.0, &#34;auto_map&#34;: { &#34;AutoConfig&#34;: &#34;configuration_internlm3.InternLM3Config&#34;, &#34;AutoModel&#34;: &#34;modeling_internlm3.InternLM3Model&#34;, &#34;AutoModelForCausalLM&#34;: &#34;modeling_internlm3.InternLM3ForCausalLM&#34; }, &#34;bias&#34;: false, &#34;bos_token_id&#34;: 1, &#34;eos_token_id&#34;: 2, &#34;head_dim&#34;: 128, &#34;hidden_act&#34;: &#34;silu&#34;, &#34;hidden_size&#34;: 4096, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 10240, &#34;max_position_embeddings&#34;: 32768, &#34;model_type&#34;: &#34;internlm3&#34;, &#34;num_attention_heads&#34;: 32, &#34;num_hidden_layers&#34;: 48, &#34;num_key_value_heads&#34;: 2, &#34;pad_token_id&#34;: 2, &#34;qkv_bias&#34;: false, &#34;rms_norm_eps&#34;: 1e-05, &#34;rope_scaling&#34;: { &#34;factor&#34;: 6.0, &#34;rope_type&#34;: &#34;dynamic&#34; }, &#34;rope_theta&#34;: 50000000, &#34;tie_word_embeddings&#34;: false, &#34;torch_dtype&#34;: &#34;bfloat16&#34;, &#34;transformers_version&#34;: &#34;4.47.1&#34;, &#34;use_cache&#34;: true, &#34;vocab_size&#34;: 128512 } 模块 (Module) 子模块 (Sub-module) 功能描述 配置参数 InternLM3ForCausalLM model: InternLM3Model 模型主干，包含词嵌入和解码器层。 - lm_head: Linear 线性输出层，将隐藏状态映射到词汇表大小，生成预测 logits。 hidden_size: 4096, vocab_size: 128512 — — — — InternLM3Model embed_tokens: Embedding 将输入的 token IDs 转换为稠密向量（Embeddings）。 vocab_size: 128512, hidden_size: 4096 layers: ModuleList[InternLM3DecoderLayer] 包含多个（48个）Transformer解码器层。 num_hidden_layers: 48 norm: RMSNorm 在所有解码器层之后，对最终的隐藏状态进行归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 rotary_emb: RotaryEmbedding 计算旋转位置编码（RoPE），用于在注意力机制中融入位置信息。 head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {&#34;factor&#34;: 6.0, &#34;rope_type&#34;: &#34;dynamic&#34;} — — — — InternLM3DecoderLayer self_attn: InternLM3Attention 多头自注意力模块，用于捕捉输入序列中的依赖关系。 - mlp: InternLM3MLP 前馈神经网络，用于对注意力输出进行非线性变换。 - input_layernorm: RMSNorm 在自注意力模块之前对输入进行层归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 post_attention_layernorm: RMSNorm 在自注意力模块之后、MLP模块之前进行层归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 — — — — InternLM3Attention qkv_proj: Linear 将输入的隐藏状态线性变换为查询（Q）、键（K）、值（V）。采用分组查询注意力（GQA）。 hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V) o_proj: Linear 将注意力模块的输出线性变换回隐藏状态的维度。 hidden_size: 4096 apply_rotary_pos_emb 将旋转位置编码（RoPE）应用于Q和K。 - — — — — InternLM3MLP gate_up_proj: Linear 两个并行的线性层（gate 和 up），将隐藏状态映射到中间维度。 hidden_size: 4096, intermediate_size: 10240 act_fn: SiluAndMul 使用 SiLU (Swish) 激活函数并进行逐元素相乘。 hidden_act: &#34;silu&#34; down_proj: Linear 将激活后的中间状态映射回隐藏状态的维度。 intermediate_size: 10240, hidden_size: 4096 InternLM3DecoderLayer 运算流程 假设输入为 hidden_states 和 residual (在前一层计算得出，第一层时 residual 等于 hidden_states)。">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2025-06-17T10:08:03+08:00">
    <meta property="article:modified_time" content="2025-06-17T10:08:03+08:00">
    <meta property="article:tag" content="Tag 1">
    <meta property="article:tag" content="Tag 2">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="InternLM3 8B Instruct">
<meta name="twitter:description" content="model &amp; config
Network Definition
{
    &#34;architectures&#34;: [
        &#34;InternLM3ForCausalLM&#34;
    ],
    &#34;attention_dropout&#34;: 0.0,
    &#34;auto_map&#34;: {
        &#34;AutoConfig&#34;: &#34;configuration_internlm3.InternLM3Config&#34;,
        &#34;AutoModel&#34;: &#34;modeling_internlm3.InternLM3Model&#34;,
        &#34;AutoModelForCausalLM&#34;: &#34;modeling_internlm3.InternLM3ForCausalLM&#34;
    },
    &#34;bias&#34;: false,
    &#34;bos_token_id&#34;: 1,
    &#34;eos_token_id&#34;: 2,
    &#34;head_dim&#34;: 128,
    &#34;hidden_act&#34;: &#34;silu&#34;,
    &#34;hidden_size&#34;: 4096,
    &#34;initializer_range&#34;: 0.02,
    &#34;intermediate_size&#34;: 10240,
    &#34;max_position_embeddings&#34;: 32768,
    &#34;model_type&#34;: &#34;internlm3&#34;,
    &#34;num_attention_heads&#34;: 32,
    &#34;num_hidden_layers&#34;: 48,
    &#34;num_key_value_heads&#34;: 2,
    &#34;pad_token_id&#34;: 2,
    &#34;qkv_bias&#34;: false,
    &#34;rms_norm_eps&#34;: 1e-05,
    &#34;rope_scaling&#34;: {
        &#34;factor&#34;: 6.0,
        &#34;rope_type&#34;: &#34;dynamic&#34;
    },
    &#34;rope_theta&#34;: 50000000,
    &#34;tie_word_embeddings&#34;: false,
    &#34;torch_dtype&#34;: &#34;bfloat16&#34;,
    &#34;transformers_version&#34;: &#34;4.47.1&#34;,
    &#34;use_cache&#34;: true,
    &#34;vocab_size&#34;: 128512
}

  
      
          模块 (Module)
          子模块 (Sub-module)
          功能描述
          配置参数
      
  
  
      
          InternLM3ForCausalLM
          model: InternLM3Model
          模型主干，包含词嵌入和解码器层。
          -
      
      
          
          lm_head: Linear
          线性输出层，将隐藏状态映射到词汇表大小，生成预测 logits。
          hidden_size: 4096, vocab_size: 128512
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3Model
          embed_tokens: Embedding
          将输入的 token IDs 转换为稠密向量（Embeddings）。
          vocab_size: 128512, hidden_size: 4096
      
      
          
          layers: ModuleList[InternLM3DecoderLayer]
          包含多个（48个）Transformer解码器层。
          num_hidden_layers: 48
      
      
          
          norm: RMSNorm
          在所有解码器层之后，对最终的隐藏状态进行归一化。
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          
          rotary_emb: RotaryEmbedding
          计算旋转位置编码（RoPE），用于在注意力机制中融入位置信息。
          head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {&quot;factor&quot;: 6.0, &quot;rope_type&quot;: &quot;dynamic&quot;}
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3DecoderLayer
          self_attn: InternLM3Attention
          多头自注意力模块，用于捕捉输入序列中的依赖关系。
          -
      
      
          
          mlp: InternLM3MLP
          前馈神经网络，用于对注意力输出进行非线性变换。
          -
      
      
          
          input_layernorm: RMSNorm
          在自注意力模块之前对输入进行层归一化。
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          
          post_attention_layernorm: RMSNorm
          在自注意力模块之后、MLP模块之前进行层归一化。
          hidden_size: 4096, rms_norm_eps: 1e-05
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3Attention
          qkv_proj: Linear
          将输入的隐藏状态线性变换为查询（Q）、键（K）、值（V）。采用分组查询注意力（GQA）。
          hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V)
      
      
          
          o_proj: Linear
          将注意力模块的输出线性变换回隐藏状态的维度。
          hidden_size: 4096
      
      
          
          apply_rotary_pos_emb
          将旋转位置编码（RoPE）应用于Q和K。
          -
      
      
          &mdash;
          &mdash;
          &mdash;
          &mdash;
      
      
          InternLM3MLP
          gate_up_proj: Linear
          两个并行的线性层（gate 和 up），将隐藏状态映射到中间维度。
          hidden_size: 4096, intermediate_size: 10240
      
      
          
          act_fn: SiluAndMul
          使用 SiLU (Swish) 激活函数并进行逐元素相乘。
          hidden_act: &quot;silu&quot;
      
      
          
          down_proj: Linear
          将激活后的中间状态映射回隐藏状态的维度。
          intermediate_size: 10240, hidden_size: 4096
      
  

InternLM3DecoderLayer 运算流程
假设输入为 hidden_states 和 residual (在前一层计算得出，第一层时 residual 等于 hidden_states)。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "InternLM3 8B Instruct",
      "item": "http://localhost:1313/blogs/internlm3-8b-instruct/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "InternLM3 8B Instruct",
  "name": "InternLM3 8B Instruct",
  "description": "model \u0026amp; config Network Definition\n{ \u0026#34;architectures\u0026#34;: [ \u0026#34;InternLM3ForCausalLM\u0026#34; ], \u0026#34;attention_dropout\u0026#34;: 0.0, \u0026#34;auto_map\u0026#34;: { \u0026#34;AutoConfig\u0026#34;: \u0026#34;configuration_internlm3.InternLM3Config\u0026#34;, \u0026#34;AutoModel\u0026#34;: \u0026#34;modeling_internlm3.InternLM3Model\u0026#34;, \u0026#34;AutoModelForCausalLM\u0026#34;: \u0026#34;modeling_internlm3.InternLM3ForCausalLM\u0026#34; }, \u0026#34;bias\u0026#34;: false, \u0026#34;bos_token_id\u0026#34;: 1, \u0026#34;eos_token_id\u0026#34;: 2, \u0026#34;head_dim\u0026#34;: 128, \u0026#34;hidden_act\u0026#34;: \u0026#34;silu\u0026#34;, \u0026#34;hidden_size\u0026#34;: 4096, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;intermediate_size\u0026#34;: 10240, \u0026#34;max_position_embeddings\u0026#34;: 32768, \u0026#34;model_type\u0026#34;: \u0026#34;internlm3\u0026#34;, \u0026#34;num_attention_heads\u0026#34;: 32, \u0026#34;num_hidden_layers\u0026#34;: 48, \u0026#34;num_key_value_heads\u0026#34;: 2, \u0026#34;pad_token_id\u0026#34;: 2, \u0026#34;qkv_bias\u0026#34;: false, \u0026#34;rms_norm_eps\u0026#34;: 1e-05, \u0026#34;rope_scaling\u0026#34;: { \u0026#34;factor\u0026#34;: 6.0, \u0026#34;rope_type\u0026#34;: \u0026#34;dynamic\u0026#34; }, \u0026#34;rope_theta\u0026#34;: 50000000, \u0026#34;tie_word_embeddings\u0026#34;: false, \u0026#34;torch_dtype\u0026#34;: \u0026#34;bfloat16\u0026#34;, \u0026#34;transformers_version\u0026#34;: \u0026#34;4.47.1\u0026#34;, \u0026#34;use_cache\u0026#34;: true, \u0026#34;vocab_size\u0026#34;: 128512 } 模块 (Module) 子模块 (Sub-module) 功能描述 配置参数 InternLM3ForCausalLM model: InternLM3Model 模型主干，包含词嵌入和解码器层。 - lm_head: Linear 线性输出层，将隐藏状态映射到词汇表大小，生成预测 logits。 hidden_size: 4096, vocab_size: 128512 \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; InternLM3Model embed_tokens: Embedding 将输入的 token IDs 转换为稠密向量（Embeddings）。 vocab_size: 128512, hidden_size: 4096 layers: ModuleList[InternLM3DecoderLayer] 包含多个（48个）Transformer解码器层。 num_hidden_layers: 48 norm: RMSNorm 在所有解码器层之后，对最终的隐藏状态进行归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 rotary_emb: RotaryEmbedding 计算旋转位置编码（RoPE），用于在注意力机制中融入位置信息。 head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {\u0026quot;factor\u0026quot;: 6.0, \u0026quot;rope_type\u0026quot;: \u0026quot;dynamic\u0026quot;} \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; InternLM3DecoderLayer self_attn: InternLM3Attention 多头自注意力模块，用于捕捉输入序列中的依赖关系。 - mlp: InternLM3MLP 前馈神经网络，用于对注意力输出进行非线性变换。 - input_layernorm: RMSNorm 在自注意力模块之前对输入进行层归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 post_attention_layernorm: RMSNorm 在自注意力模块之后、MLP模块之前进行层归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; InternLM3Attention qkv_proj: Linear 将输入的隐藏状态线性变换为查询（Q）、键（K）、值（V）。采用分组查询注意力（GQA）。 hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V) o_proj: Linear 将注意力模块的输出线性变换回隐藏状态的维度。 hidden_size: 4096 apply_rotary_pos_emb 将旋转位置编码（RoPE）应用于Q和K。 - \u0026mdash; \u0026mdash; \u0026mdash; \u0026mdash; InternLM3MLP gate_up_proj: Linear 两个并行的线性层（gate 和 up），将隐藏状态映射到中间维度。 hidden_size: 4096, intermediate_size: 10240 act_fn: SiluAndMul 使用 SiLU (Swish) 激活函数并进行逐元素相乘。 hidden_act: \u0026quot;silu\u0026quot; down_proj: Linear 将激活后的中间状态映射回隐藏状态的维度。 intermediate_size: 10240, hidden_size: 4096 InternLM3DecoderLayer 运算流程 假设输入为 hidden_states 和 residual (在前一层计算得出，第一层时 residual 等于 hidden_states)。\n",
  "keywords": [
    "word 1", "word 2"
  ],
  "articleBody": "model \u0026 config Network Definition\n{ \"architectures\": [ \"InternLM3ForCausalLM\" ], \"attention_dropout\": 0.0, \"auto_map\": { \"AutoConfig\": \"configuration_internlm3.InternLM3Config\", \"AutoModel\": \"modeling_internlm3.InternLM3Model\", \"AutoModelForCausalLM\": \"modeling_internlm3.InternLM3ForCausalLM\" }, \"bias\": false, \"bos_token_id\": 1, \"eos_token_id\": 2, \"head_dim\": 128, \"hidden_act\": \"silu\", \"hidden_size\": 4096, \"initializer_range\": 0.02, \"intermediate_size\": 10240, \"max_position_embeddings\": 32768, \"model_type\": \"internlm3\", \"num_attention_heads\": 32, \"num_hidden_layers\": 48, \"num_key_value_heads\": 2, \"pad_token_id\": 2, \"qkv_bias\": false, \"rms_norm_eps\": 1e-05, \"rope_scaling\": { \"factor\": 6.0, \"rope_type\": \"dynamic\" }, \"rope_theta\": 50000000, \"tie_word_embeddings\": false, \"torch_dtype\": \"bfloat16\", \"transformers_version\": \"4.47.1\", \"use_cache\": true, \"vocab_size\": 128512 } 模块 (Module) 子模块 (Sub-module) 功能描述 配置参数 InternLM3ForCausalLM model: InternLM3Model 模型主干，包含词嵌入和解码器层。 - lm_head: Linear 线性输出层，将隐藏状态映射到词汇表大小，生成预测 logits。 hidden_size: 4096, vocab_size: 128512 — — — — InternLM3Model embed_tokens: Embedding 将输入的 token IDs 转换为稠密向量（Embeddings）。 vocab_size: 128512, hidden_size: 4096 layers: ModuleList[InternLM3DecoderLayer] 包含多个（48个）Transformer解码器层。 num_hidden_layers: 48 norm: RMSNorm 在所有解码器层之后，对最终的隐藏状态进行归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 rotary_emb: RotaryEmbedding 计算旋转位置编码（RoPE），用于在注意力机制中融入位置信息。 head_dim: 128, max_position_embeddings: 32768, rope_theta: 50000000, rope_scaling: {\"factor\": 6.0, \"rope_type\": \"dynamic\"} — — — — InternLM3DecoderLayer self_attn: InternLM3Attention 多头自注意力模块，用于捕捉输入序列中的依赖关系。 - mlp: InternLM3MLP 前馈神经网络，用于对注意力输出进行非线性变换。 - input_layernorm: RMSNorm 在自注意力模块之前对输入进行层归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 post_attention_layernorm: RMSNorm 在自注意力模块之后、MLP模块之前进行层归一化。 hidden_size: 4096, rms_norm_eps: 1e-05 — — — — InternLM3Attention qkv_proj: Linear 将输入的隐藏状态线性变换为查询（Q）、键（K）、值（V）。采用分组查询注意力（GQA）。 hidden_size: 4096, num_attention_heads: 32 (Q), num_key_value_heads: 2 (K, V) o_proj: Linear 将注意力模块的输出线性变换回隐藏状态的维度。 hidden_size: 4096 apply_rotary_pos_emb 将旋转位置编码（RoPE）应用于Q和K。 - — — — — InternLM3MLP gate_up_proj: Linear 两个并行的线性层（gate 和 up），将隐藏状态映射到中间维度。 hidden_size: 4096, intermediate_size: 10240 act_fn: SiluAndMul 使用 SiLU (Swish) 激活函数并进行逐元素相乘。 hidden_act: \"silu\" down_proj: Linear 将激活后的中间状态映射回隐藏状态的维度。 intermediate_size: 10240, hidden_size: 4096 InternLM3DecoderLayer 运算流程 假设输入为 hidden_states 和 residual (在前一层计算得出，第一层时 residual 等于 hidden_states)。\n步骤 模块/操作 输入 运算描述 输出 1. 输入归一化 input_layernorm (RMSNorm) hidden_states, residual 对 hidden_states 进行 RMS 归一化。同时，将 hidden_states 加上上一层的残差 residual，为后续的残差连接做准备。 norm_hidden_states, new_residual 2. 自注意力 (Self-Attention) self_attn (InternLM3Attention) norm_hidden_states 这是最复杂的部分，内含多个子步骤：\na. QKV 投射: qkv_proj 将 norm_hidden_states 线性变换，生成查询 Q、键 K 和值 V。\nb. 位置编码: apply_rotary_pos_emb 将旋转位置编码 (RoPE) 应用于 Q 和 K，注入位置信息。\nc. 注意力计算: attn_fwd 根据 Q、K、V 计算注意力分数，并生成加权和。\nd. 输出投射: o_proj 将注意力计算结果线性变换回 hidden_size 维度。 attn_output 3. 第一次残差连接 + attn_output, new_residual 将步骤 2 的 attn_output 与步骤 1 的 new_residual 逐元素相加。 attn_residual_output 4. 注意力后归一化 post_attention_layernorm (RMSNorm) attn_residual_output 对残差连接后的结果进行第二次 RMS 归一化。 norm_attn_output 5. MLP (前馈网络) mlp (InternLM3MLP) norm_attn_output 包含三个子步骤：\na. Gate \u0026 Up 投射: gate_up_proj 同时将 norm_attn_output 线性变换到 intermediate_size，得到 gate 和 up 两个张量。\nb. 激活: act_fn (SiLU and Multiply) 对 gate 应用 SiLU 激活函数，然后与 up 逐元素相乘。\nc. Down 投射: down_proj 将激活后的结果从 intermediate_size 线性变换回 hidden_size。 mlp_output 6. 第二次残差连接 + mlp_output, norm_attn_output 将步骤 5 的 mlp_output 与步骤 4 的 norm_attn_output 逐元素相加。 final_hidden_states 7. 输出 - - 这一层的最终输出 hidden_states 将作为下一层的输入，而 final_residual 将作为下一层的残差输入。 hidden_states (用于下一层), residual (用于下一层) graph TD\rsubgraph \"InternLM3DecoderLayer 内部流程\"\rdirection LR\r%% 定义输入\rInput[Input: hidden_states\nInput: residual_in] --\u003e Norm1\r%% 第一个模块：注意力\rsubgraph \"模块1: 注意力 (Pre-Norm)\"\rNorm1(RMSNorm:\ninput_layernorm) --\u003e Attention[Self-Attention]\rInput -- residual --o Add1\rAttention -- attn_output --o Add1\rend\r%% 第二个模块：MLP\rsubgraph \"模块2: MLP (Pre-Norm)\"\rAdd1(第一次\n残差连接 +) --\u003e Norm2(RMSNorm:\npost_attention_layernorm)\rNorm2 --\u003e MLP[MLP Block]\rAdd1 -- residual --o Add2\rMLP -- mlp_output --o Add2\rend\r%% 定义输出\rAdd2(第二次\n残差连接 +) --\u003e Output[Output: hidden_states\nOutput: residual_out]\rend\r%% 样式定义\rclassDef default fill:#f9f9f9,stroke:#333,stroke-width:2px;\rclassDef subgraph_style fill:#eef,stroke:#333,stroke-width:2px,color:#333;\rclass Input,Output,Add1,Add2,Norm1,Norm2,Attention,MLP subgraph_style Group Pattern 在 include/tx8be_mlir/OpHelper.h 中添加你自己定义的 pattern 到\ntypedef enum { // ... GROUP_NAME = id, } GroupPatternMode; 在 include/tx8be_mlir/Transforms/LayerGroup/GroupPattern.h 定义好你自己的\nconst std::map\u003cstd::vector\u003cTX8BE_OPS\u003e, int\u003e PATTERN_NAME { // ... }; 然后添加进 patternConfigMap 中\nconst std::map\u003cint, std::map\u003cstd::vector\u003cTX8BE_OPS\u003e\u003e patternConfigMap { {GROUP_NAME, PATTERN_NAME}, } 最后在 lib/Support/OpHelper.cpp 的 getGroupPatternMode 函数里添加\nelse if (lowerOption.find(\"opt_group_name\")) { mode = GROUP_NAME; } opt_group_name 由 run_codegen_layer 命令的 --opt_group=opt_group_name 参数指定\n",
  "wordCount" : "1375",
  "inLanguage": "en",
  "datePublished": "2025-06-17T10:08:03+08:00",
  "dateModified": "2025-06-17T10:08:03+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/internlm3-8b-instruct/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      InternLM3 8B Instruct
    </h1>
    <div class="post-meta"><span title='2025-06-17 10:08:03 +0800 CST'>Jun-17-2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;1375 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#model--config" aria-label="model &amp; config">model &amp; config</a><ul>
                            <ul>
                            
                    <li>
                        <a href="#internlm3decoderlayer-%e8%bf%90%e7%ae%97%e6%b5%81%e7%a8%8b" aria-label="InternLM3DecoderLayer 运算流程">InternLM3DecoderLayer 运算流程</a></li></ul>
                        </ul>
                    </li>
                    <li>
                        <a href="#group-pattern" aria-label="Group Pattern">Group Pattern</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="model--config">model &amp; config<a hidden class="anchor" aria-hidden="true" href="#model--config">#</a></h1>
<p><a href="https://github.com/InternLM/lmdeploy/blob/7ca466599f01e5ef93e8951771c62163136e21b2/lmdeploy/pytorch/models/internlm3.py#L304">Network Definition</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;architectures&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;InternLM3ForCausalLM&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;attention_dropout&#34;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;auto_map&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoConfig&#34;</span><span class="p">:</span> <span class="s2">&#34;configuration_internlm3.InternLM3Config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoModel&#34;</span><span class="p">:</span> <span class="s2">&#34;modeling_internlm3.InternLM3Model&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoModelForCausalLM&#34;</span><span class="p">:</span> <span class="s2">&#34;modeling_internlm3.InternLM3ForCausalLM&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;bias&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;bos_token_id&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;eos_token_id&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;head_dim&#34;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;hidden_act&#34;</span><span class="p">:</span> <span class="s2">&#34;silu&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;hidden_size&#34;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;initializer_range&#34;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;intermediate_size&#34;</span><span class="p">:</span> <span class="mi">10240</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;max_position_embeddings&#34;</span><span class="p">:</span> <span class="mi">32768</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;model_type&#34;</span><span class="p">:</span> <span class="s2">&#34;internlm3&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_attention_heads&#34;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_hidden_layers&#34;</span><span class="p">:</span> <span class="mi">48</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_key_value_heads&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;pad_token_id&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;qkv_bias&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rms_norm_eps&#34;</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rope_scaling&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;factor&#34;</span><span class="p">:</span> <span class="mf">6.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;rope_type&#34;</span><span class="p">:</span> <span class="s2">&#34;dynamic&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rope_theta&#34;</span><span class="p">:</span> <span class="mi">50000000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;tie_word_embeddings&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;torch_dtype&#34;</span><span class="p">:</span> <span class="s2">&#34;bfloat16&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;transformers_version&#34;</span><span class="p">:</span> <span class="s2">&#34;4.47.1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;use_cache&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;vocab_size&#34;</span><span class="p">:</span> <span class="mi">128512</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>模块 (Module)</strong></th>
          <th style="text-align: left"><strong>子模块 (Sub-module)</strong></th>
          <th style="text-align: left"><strong>功能描述</strong></th>
          <th style="text-align: left"><strong>配置参数</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><code>InternLM3ForCausalLM</code></td>
          <td style="text-align: left"><code>model: InternLM3Model</code></td>
          <td style="text-align: left">模型主干，包含词嵌入和解码器层。</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>lm_head: Linear</code></td>
          <td style="text-align: left">线性输出层，将隐藏状态映射到词汇表大小，生成预测 logits。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>vocab_size: 128512</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3Model</code></td>
          <td style="text-align: left"><code>embed_tokens: Embedding</code></td>
          <td style="text-align: left">将输入的 token IDs 转换为稠密向量（Embeddings）。</td>
          <td style="text-align: left"><code>vocab_size: 128512</code>, <code>hidden_size: 4096</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>layers: ModuleList[InternLM3DecoderLayer]</code></td>
          <td style="text-align: left">包含多个（48个）Transformer解码器层。</td>
          <td style="text-align: left"><code>num_hidden_layers: 48</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>norm: RMSNorm</code></td>
          <td style="text-align: left">在所有解码器层之后，对最终的隐藏状态进行归一化。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>rotary_emb: RotaryEmbedding</code></td>
          <td style="text-align: left">计算旋转位置编码（RoPE），用于在注意力机制中融入位置信息。</td>
          <td style="text-align: left"><code>head_dim: 128</code>, <code>max_position_embeddings: 32768</code>, <code>rope_theta: 50000000</code>, <code>rope_scaling: {&quot;factor&quot;: 6.0, &quot;rope_type&quot;: &quot;dynamic&quot;}</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3DecoderLayer</code></td>
          <td style="text-align: left"><code>self_attn: InternLM3Attention</code></td>
          <td style="text-align: left">多头自注意力模块，用于捕捉输入序列中的依赖关系。</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>mlp: InternLM3MLP</code></td>
          <td style="text-align: left">前馈神经网络，用于对注意力输出进行非线性变换。</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>input_layernorm: RMSNorm</code></td>
          <td style="text-align: left">在自注意力模块之前对输入进行层归一化。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>post_attention_layernorm: RMSNorm</code></td>
          <td style="text-align: left">在自注意力模块之后、MLP模块之前进行层归一化。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3Attention</code></td>
          <td style="text-align: left"><code>qkv_proj: Linear</code></td>
          <td style="text-align: left">将输入的隐藏状态线性变换为查询（Q）、键（K）、值（V）。采用分组查询注意力（GQA）。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>num_attention_heads: 32</code> (Q), <code>num_key_value_heads: 2</code> (K, V)</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>o_proj: Linear</code></td>
          <td style="text-align: left">将注意力模块的输出线性变换回隐藏状态的维度。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>apply_rotary_pos_emb</code></td>
          <td style="text-align: left">将旋转位置编码（RoPE）应用于Q和K。</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3MLP</code></td>
          <td style="text-align: left"><code>gate_up_proj: Linear</code></td>
          <td style="text-align: left">两个并行的线性层（gate 和 up），将隐藏状态映射到中间维度。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>intermediate_size: 10240</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>act_fn: SiluAndMul</code></td>
          <td style="text-align: left">使用 SiLU (Swish) 激活函数并进行逐元素相乘。</td>
          <td style="text-align: left"><code>hidden_act: &quot;silu&quot;</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>down_proj: Linear</code></td>
          <td style="text-align: left">将激活后的中间状态映射回隐藏状态的维度。</td>
          <td style="text-align: left"><code>intermediate_size: 10240</code>, <code>hidden_size: 4096</code></td>
      </tr>
  </tbody>
</table>
<h3 id="internlm3decoderlayer-运算流程">InternLM3DecoderLayer 运算流程<a hidden class="anchor" aria-hidden="true" href="#internlm3decoderlayer-运算流程">#</a></h3>
<p>假设输入为 <code>hidden_states</code> 和 <code>residual</code> (在前一层计算得出，第一层时 <code>residual</code> 等于 <code>hidden_states</code>)。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>步骤</strong></th>
          <th style="text-align: left"><strong>模块/操作</strong></th>
          <th style="text-align: left"><strong>输入</strong></th>
          <th style="text-align: left"><strong>运算描述</strong></th>
          <th style="text-align: left"><strong>输出</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>1. 输入归一化</strong></td>
          <td style="text-align: left"><code>input_layernorm</code> (RMSNorm)</td>
          <td style="text-align: left"><code>hidden_states</code>, <code>residual</code></td>
          <td style="text-align: left">对 <code>hidden_states</code> 进行 RMS 归一化。同时，将 <code>hidden_states</code> 加上上一层的残差 <code>residual</code>，为后续的残差连接做准备。</td>
          <td style="text-align: left"><code>norm_hidden_states</code>, <code>new_residual</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>2. 自注意力 (Self-Attention)</strong></td>
          <td style="text-align: left"><code>self_attn</code> (InternLM3Attention)</td>
          <td style="text-align: left"><code>norm_hidden_states</code></td>
          <td style="text-align: left"><strong>这是最复杂的部分，内含多个子步骤：</strong><br>a. <strong>QKV 投射</strong>: <code>qkv_proj</code> 将 <code>norm_hidden_states</code> 线性变换，生成查询 <code>Q</code>、键 <code>K</code> 和值 <code>V</code>。<br>b. <strong>位置编码</strong>: <code>apply_rotary_pos_emb</code> 将旋转位置编码 (RoPE) 应用于 <code>Q</code> 和 <code>K</code>，注入位置信息。<br>c. <strong>注意力计算</strong>: <code>attn_fwd</code> 根据 <code>Q</code>、<code>K</code>、<code>V</code> 计算注意力分数，并生成加权和。<br>d. <strong>输出投射</strong>: <code>o_proj</code> 将注意力计算结果线性变换回 <code>hidden_size</code> 维度。</td>
          <td style="text-align: left"><code>attn_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>3. 第一次残差连接</strong></td>
          <td style="text-align: left"><code>+</code></td>
          <td style="text-align: left"><code>attn_output</code>, <code>new_residual</code></td>
          <td style="text-align: left">将步骤 2 的 <code>attn_output</code> 与步骤 1 的 <code>new_residual</code> 逐元素相加。</td>
          <td style="text-align: left"><code>attn_residual_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>4. 注意力后归一化</strong></td>
          <td style="text-align: left"><code>post_attention_layernorm</code> (RMSNorm)</td>
          <td style="text-align: left"><code>attn_residual_output</code></td>
          <td style="text-align: left">对残差连接后的结果进行第二次 RMS 归一化。</td>
          <td style="text-align: left"><code>norm_attn_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>5. MLP (前馈网络)</strong></td>
          <td style="text-align: left"><code>mlp</code> (InternLM3MLP)</td>
          <td style="text-align: left"><code>norm_attn_output</code></td>
          <td style="text-align: left"><strong>包含三个子步骤：</strong><br>a. <strong>Gate &amp; Up 投射</strong>: <code>gate_up_proj</code> 同时将 <code>norm_attn_output</code> 线性变换到 <code>intermediate_size</code>，得到 <code>gate</code> 和 <code>up</code> 两个张量。<br>b. <strong>激活</strong>: <code>act_fn</code> (SiLU and Multiply) 对 <code>gate</code> 应用 SiLU 激活函数，然后与 <code>up</code> 逐元素相乘。<br>c. <strong>Down 投射</strong>: <code>down_proj</code> 将激活后的结果从 <code>intermediate_size</code> 线性变换回 <code>hidden_size</code>。</td>
          <td style="text-align: left"><code>mlp_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>6. 第二次残差连接</strong></td>
          <td style="text-align: left"><code>+</code></td>
          <td style="text-align: left"><code>mlp_output</code>, <code>norm_attn_output</code></td>
          <td style="text-align: left">将步骤 5 的 <code>mlp_output</code> 与步骤 4 的 <code>norm_attn_output</code> 逐元素相加。</td>
          <td style="text-align: left"><code>final_hidden_states</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>7. 输出</strong></td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">这一层的最终输出 <code>hidden_states</code> 将作为下一层的输入，而 <code>final_residual</code> 将作为下一层的残差输入。</td>
          <td style="text-align: left"><code>hidden_states</code> (用于下一层), <code>residual</code> (用于下一层)</td>
      </tr>
  </tbody>
</table>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">graph TD
    subgraph &#34;InternLM3DecoderLayer 内部流程&#34;
    
    direction LR
    
    %% 定义输入
    Input[Input: hidden_states&lt;br&gt;Input: residual_in] --&gt; Norm1
    
    %% 第一个模块：注意力
    subgraph &#34;模块1: 注意力 (Pre-Norm)&#34;
        Norm1(RMSNorm:&lt;br&gt;input_layernorm) --&gt; Attention[Self-Attention]
        Input -- residual --o Add1
        Attention -- attn_output --o Add1
    end
    
    %% 第二个模块：MLP
    subgraph &#34;模块2: MLP (Pre-Norm)&#34;
        Add1(第一次&lt;br&gt;残差连接 +) --&gt; Norm2(RMSNorm:&lt;br&gt;post_attention_layernorm)
        Norm2 --&gt; MLP[MLP Block]
        Add1 -- residual --o Add2
        MLP -- mlp_output --o Add2
    end

    %% 定义输出
    Add2(第二次&lt;br&gt;残差连接 +) --&gt; Output[Output: hidden_states&lt;br&gt;Output: residual_out]

    end

    %% 样式定义
    classDef default fill:#f9f9f9,stroke:#333,stroke-width:2px;
    classDef subgraph_style fill:#eef,stroke:#333,stroke-width:2px,color:#333;
    class Input,Output,Add1,Add2,Norm1,Norm2,Attention,MLP subgraph_style
</code></pre><h1 id="group-pattern">Group Pattern<a hidden class="anchor" aria-hidden="true" href="#group-pattern">#</a></h1>
<p>在 <code>include/tx8be_mlir/OpHelper.h</code> 中添加你自己定义的 pattern 到</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">enum</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">GROUP_NAME</span> <span class="o">=</span> <span class="n">id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">GroupPatternMode</span><span class="p">;</span>
</span></span></code></pre></div><p>在 <code>include/tx8be_mlir/Transforms/LayerGroup/GroupPattern.h</code> 定义好你自己的</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">PATTERN_NAME</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><p>然后添加进 <code>patternConfigMap</code> 中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;&gt;</span> <span class="n">patternConfigMap</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="n">GROUP_NAME</span><span class="p">,</span> <span class="n">PATTERN_NAME</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>最后在 <code>lib/Support/OpHelper.cpp</code> 的 <code>getGroupPatternMode</code> 函数里添加</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">lowerOption</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">&#34;opt_group_name&#34;</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">mode</span> <span class="o">=</span> <span class="n">GROUP_NAME</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>opt_group_name</code> 由 <code>run_codegen_layer</code> 命令的 <code>--opt_group=opt_group_name</code> 参数指定</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/tag-1/">Tag 1</a></li>
      <li><a href="http://localhost:1313/tags/tag-2/">Tag 2</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/">
    <span class="title">« Prev</span>
    <br>
    <span>ServingLLMsOnHuaweiCloudMatrix384</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/pipefusion/">
    <span class="title">Next »</span>
    <br>
    <span>PipeFusion</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
