<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DistriFusion | WITHER</title>
<meta name="keywords" content="DistriFusion">
<meta name="description" content="Paper reading about DistriFusion.">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/distrifusion/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/distrifusion/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/distrifusion/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="DistriFusion">
  <meta property="og:description" content="Paper reading about DistriFusion.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2024-10-23T14:28:37+08:00">
    <meta property="article:modified_time" content="2025-06-07T23:40:58+08:00">
    <meta property="article:tag" content="Diffusion Models">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DistriFusion">
<meta name="twitter:description" content="Paper reading about DistriFusion.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DistriFusion",
      "item": "http://localhost:1313/blogs/distrifusion/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DistriFusion",
  "name": "DistriFusion",
  "description": "Paper reading about DistriFusion.",
  "keywords": [
    "DistriFusion"
  ],
  "articleBody": "Abstract DistriFusion 将模型输入分割成多个 patch 后分配给 GPU。但是直接实现这样的算法会破坏 patch 之间的交互并失去保真度，而同步 GPU 之间的激活将产生巨大的通信开销。为了克服这一困境，根据观察到的相邻扩散步输入之间的高度相似性提出了 displaced patch parallelism，该方法通过重用前一个时间步骤中预先计算的 feature map 来利用扩散过程的顺序性，为当前步提供 context. 该方法支持异步通信，可以通过计算实现流水线化。\nIntroduction Original, Navie Patch \u0026 DistriFusion\n加速扩散模型推理主要集中在两种方法上：减少采样步骤和优化网络推理。随着计算资源的快速增长，利用多个 GPU 来加速推理是很有吸引力的。例如在 NLP 中， LLM 已经成功地利用了 GPU 之间的张量并行性，从而显著降低了延迟。然而，对于扩散模型，由于激活尺寸大，张量并行这样的技术不太适合扩散模型。多个 GPU 通常只用于 batch 推理，当生成单个图像时，通常只涉及一个GPU.\nTechniques like tensor parallelism are less suitable for diffusion models due to the large activation size, as communication costs outweigh savings from distributed computation.\n自然而然的一种方法是将图像分成几个 patch 后分配给不同的设备进行生成。由于各个 patch 之间缺乏相互作用，它在每个 patch 的边界处都有一个清晰可见的分界线。\nDistriFusion 也是基于 patch parallelism. 关键在于扩散模型中相邻去噪步骤的输入是相似的，因此，只在第一步采用同步通信。后续步骤重用前一步中预先计算的激活，为当前步骤提供全局上下文和 patch 交互。通过异步通信有效地隐藏了计算中的通信开销。并且还稀疏地在指定的区域上进行卷积和注意力计算，从而按比例减少每个设备的计算量。\nMethod Displaced Patch Parallelism. 在预测 $\\epsilon_{\\theta}(\\mathbf{x}_{t})$ 时 (忽略条件 c 和时间步 t 的输入) ，首先将 $\\mathbf{x}_{t}$ 分割成多个 patch $\\mathbf{x}_t^{(1)},\\mathbf{x}_t^{(2)},\\ldots,\\mathbf{x}_t^{(N)}$ ，对于每一层 l 和设备 i，在获得输入激活 patch $\\mathbf{A}_{t}^{l,(i)}$ 后异步处理两个操作：首先，对于设备i， 激活 $\\mathbf{A}_{t}^{l,(i)}$ 首先 scatter 到上一步旧的激活 $\\mathbf{A}_{t+1}^{l}$ 中。然后将此分散操作的输出送入稀疏算子 Fl (线性、卷积或注意层)，该算子专门对新区域执行计算并产生相应的输出。同时，对 $\\mathbf{A}_{t}^{l,(i)}$ 执行 AllGather 操作，为下一步的全尺寸激活 $\\mathbf{A}_{t}^{l}$ 做准备。\nOverview of DistriFusion\n我们对除第一层 (采用同步通信获得其他设备上的输入) 外的每一层重复这个过程。然后将最终输出 Gather 在一起以近似 $\\epsilon_{\\theta}(\\mathbf{x}_{t})$，用于计算 $\\mathbf{x}_{t-1}$\nTimeline Visualization on Each Device\nSparse Operations 对于每一层 l，如果原始算子 Fl 是一个卷积层、线性层或交叉注意层，调整使其专门作用于新激活的区域。这可以通过从 scatter 输出中提取最新部分并将其输入到 Fl 中来实现。对于 self-attention，将其转换为 cross-attention，仅在设备上保留来自新激活的 Q，而 KV 仍然包含整个特征图。\nCorrected Asynchronous GroupNorm 仅对新 patch 进行归一化或重用旧特征都会降低图像质量。同步 AllGather 所有均值和方差将产生相当大的开销。为了解决这一困境，DistriFusion 在陈旧的统计数据中引入了一个校正项。计算公式如下\n$$\r\\mathbb{E}[\\mathbf{A}_t]\\approx\\underbrace{\\mathbb{E}[\\mathbf{A}_{t+1}]}_{\\text{stale global mean}}+\\underbrace{\\mathbb{E}[\\mathbf{A}_t^{(i)}]-\\mathbb{E}[\\mathbf{A}_{t+1}^{(i)}]}_{\\text{correction}}\r$$同样对二阶矩 $\\mathbb{E}[\\mathbf{A}^2_t]$ 也采用这种计算方式，然后通过 $\\mathbb{E}[\\mathbf{A}^2_t] - \\mathbb{E}[\\mathbf{A}_t]^2$ 来计算方差。对于方差结果为负的部分，将使用新鲜 patch 的局部方差代替。\nCode Implementation Distrifusion 中主要就是将 UNet2DConditionModel 中的 Conv2d, Attention 和 GroupNorm 替换成对应的 patch 实现的网络结构 DistriUNetPP. 这里继承的 BaseModel 类为集成了 PatchParallelismCommManager 类 (介绍见后文) 的网络。\nUNet2DConditionModel\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class DistriUNetPP(BaseModel): # for Patch Parallelism def __init__(self, model: UNet2DConditionModel, distri_config: DistriConfig): assert isinstance(model, UNet2DConditionModel) if distri_config.world_size \u003e 1 and distri_config.n_device_per_batch \u003e 1: for name, module in model.named_modules(): if isinstance(module, BaseModule): continue ''' Substitute Conv2d, Attention, GroupNorm with DistriConv2dPP, DistriSelfAttentionPP, DistriCrossAttentionPP, DistriGroupNorm ''' for subname, submodule in module.named_children(): if isinstance(submodule, nn.Conv2d): kernel_size = submodule.kernel_size if kernel_size == (1, 1) or kernel_size == 1: continue wrapped_submodule = DistriConv2dPP( submodule, distri_config, is_first_layer=subname == \"conv_in\" ) setattr(module, subname, wrapped_submodule) elif isinstance(submodule, Attention): if subname == \"attn1\": # self attention wrapped_submodule = DistriSelfAttentionPP(submodule, distri_config) else: # cross attention assert subname == \"attn2\" wrapped_submodule = DistriCrossAttentionPP(submodule, distri_config) setattr(module, subname, wrapped_submodule) elif isinstance(submodule, nn.GroupNorm): wrapped_submodule = DistriGroupNorm(submodule, distri_config) setattr(module, subname, wrapped_submodule) super(DistriUNetPP, self).__init__(model, distri_config) PatchParallelismCommManager PatchParallelismCommManager 类主要处理异步通信的部分。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class PatchParallelismCommManager: def __init__(self, distri_config: DistriConfig): self.distri_config = distri_config self.torch_dtype = None self.numel = 0 # 已经注册的张量的累计总元素数量 self.numel_dict = {} # 记录每个 layer_type 所注册的张量的累计元素数量 self.buffer_list = None # 在每个设备上存储所有注册张量的数据，通信所用的 buffer self.starts = [] # 记录每个注册张量的起始位置 (在 buffer_list 中的起始索引) self.ends = [] # 结束 结束 self.shapes = [] # 记录每个注册张量的 shape self.idx_queue = [] # 需要进行通信的张量索引的队列 self.handles = None # 存储每个设备通信操作的句柄的 list, 用于检查通信是否完成 成员函数功能介绍如下\nregister_tensor(self, shape: tuple[int, ...] or list[int], torch_dtype: torch.dtype, layer_type: str = None) -\u003e int: 用于注册张量的形状和数据类型，同时计算并记录张量在缓冲区中的起始位置和结束位置。\n如果尚未指定 torch_dtype，则将传入的 torch_dtype 设为类成员的默认数据类型。 计算传入张量形状的总元素数 numel，并更新 starts、ends 和 shapes 列表。 如果指定了 layer_type，更新 numel_dict 中该层类型对应的元素数目。 create_buffer(self) : 每个设备上为所有注册的张量创建一个统一的缓冲区。\n为每个设备创建一个形状为 (numel,) 的张量，并将其放入 buffer_list 中。 输出在各设备上创建的缓冲区总参数量。 get_buffer_list(self, idx: int) -\u003e list[torch.Tensor]: 返回每个设备上对应于指定索引 idx 的缓冲区张量。\n根据 starts 和 ends 信息，从 buffer_list 中提取指定索引 idx 的张量片段并调整其形状。 communicate(self): 调用 dist.all_gather 将缓冲区中的张量在不同设备间进行广播。\n确定当前需要通信的张量范围 (根据 idx_queue 中的索引). 调用 dist.all_gather 在设备组内进行异步广播通信，并将句柄存储在 handles 中。 enqueue(self, idx: int, tensor: torch.Tensor): 将指定索引 idx 处的张量数据复制到 buffer_list 中，并将索引添加到通信队列 idx_queue。\n如果通信队列不为空且索引为 0，则先执行一次通信操作。 将张量数据复制到 buffer_list 中的对应位置。 当通信队列长度达到 distri_config 中设定的通信检查点值时，进行通信。 clear(self): 执行一次所有待通信张量的通信，并等待所有异步操作完成。\n如果通信队列不为空，则进行通信操作。 遍历所有句柄，等待所有异步操作完成后，将句柄设为 None. DistriConv2dPP DistriConv2dPP 计算自己负责 patch 部分的卷积，需要通信其他设备需要自己负责 patch 的上下 padding 部分。\n__init__：构造函数，初始化成员变量，设置是否为第一层卷积。 naive_forward：执行标准的前向传播，不进行任何切片操作。这是单个设备处理时的普通卷积操作。 sliced_forward：处理输入张量的切片操作。根据当前设备索引 (split_idx) 计算输入张量在高度方向的起始和结束位置，并在必要时为切片后的张量添加 padding 后进行卷积操作。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class DistriConv2dPP(BaseModule): def __init__(self, module: nn.Conv2d, distri_config: DistriConfig, is_first_layer: bool = False): super(DistriConv2dPP, self).__init__(module, distri_config) self.is_first_layer = is_first_layer def naive_forward(self, x: torch.Tensor) -\u003e torch.Tensor: # x: [B, C, H, W] output = self.module(x) return output def sliced_forward(self, x: torch.Tensor) -\u003e torch.Tensor: '''...''' def forward(self, x: torch.Tensor, *args, **kwargs) -\u003e torch.Tensor: distri_config = self.distri_config # 等待上一步通信完成 if self.comm_manager is not None and self.comm_manager.handles is not None and self.idx is not None: if self.comm_manager.handles[self.idx] is not None: self.comm_manager.handles[self.idx].wait() self.comm_manager.handles[self.idx] = None boundary_size = self.module.padding[0] if self.buffer_list is None: # buffer_list 存储的是每个 devive 进行卷积所需要的其他 devive 的数据 if self.comm_manager.buffer_list is None: self.idx = self.comm_manager.register_tensor( shape=[2, x.shape[0], x.shape[1], boundary_size, x.shape[3]], torch_dtype=x.dtype, layer_type=\"conv2d\", ) else: self.buffer_list = self.comm_manager.get_buffer_list(self.idx) def create_padded_x(): '''拼接接收到的数据''' if distri_config.split_idx() == 0: # rank 0 concat_x = torch.cat([x, self.buffer_list[distri_config.split_idx() + 1][0]], dim=2) padded_x = F.pad(concat_x, [0, 0, boundary_size, 0], mode=\"constant\") elif distri_config.split_idx() == distri_config.n_device_per_batch - 1: # rank n-1 concat_x = torch.cat([self.buffer_list[distri_config.split_idx() - 1][1], x], dim=2) padded_x = F.pad(concat_x, [0, 0, 0, boundary_size], mode=\"constant\") else: # other ranks padded_x = torch.cat( [ self.buffer_list[distri_config.split_idx() - 1][1], x, self.buffer_list[distri_config.split_idx() + 1][0], ], dim=2, ) return padded_x # 提取当前输入张量需要发送给其他设备的部分 boundary = torch.stack([x[:, :, :boundary_size, :], x[:, :, -boundary_size:, :]], dim=0) # 直接用上一步的 buffer 拼接 padded_x = create_padded_x() output = F.conv2d( padded_x, self.module.weight, self.module.bias, stride=self.module.stride[0], padding=(0, self.module.padding[1]), ) if distri_config.mode != \"no_sync\": self.comm_manager.enqueue(self.idx, boundary) # 插入自己要发送的数据 self.counter += 1 return output DistriSelfAttentionPP DistriSelfAttentionPP 只负责计算自己 patch 的输出，需要完整的 KV，将 self attention 运算变成 cross-attention 计算。需要通信自己的 KV.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class DistriSelfAttentionPP(DistriAttentionPP): def __init__(self, module: Attention, distri_config: DistriConfig): super(DistriSelfAttentionPP, self).__init__(module, distri_config) def _forward(self, hidden_states: torch.FloatTensor, scale: float = 1.0): attn = self.module # 获取 Attention 模块 distri_config = self.distri_config residual = hidden_states # 残差连接 batch_size, sequence_length, _ = hidden_states.shape args = () if USE_PEFT_BACKEND else (scale,) query = attn.to_q(hidden_states, *args) # Q Projection encoder_hidden_states = hidden_states kv = self.to_kv(encoder_hidden_states) # KV Projection if self.buffer_list is None: # 如果缓冲区未创建 full_kv = torch.cat([kv for _ in range(distri_config.n_device_per_batch)], dim=1) new_buffer_list = [buffer for buffer in self.buffer_list] new_buffer_list[distri_config.split_idx()] = kv full_kv = torch.cat(new_buffer_list, dim=1) if distri_config.mode != \"no_sync\": self.comm_manager.enqueue(self.idx, kv) # 将 full_kv 分割为 key 和 value key, value = torch.split(full_kv, full_kv.shape[-1] // 2, dim=-1) inner_dim = key.shape[-1] head_dim = inner_dim // attn.heads # multi-head attention query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2) hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False) hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim) hidden_states = hidden_states.to(query.dtype) hidden_states = attn.to_out[0](hidden_states, *args) # O Projection hidden_states = attn.to_out[1](hidden_states) # Dropout if attn.residual_connection: hidden_states = hidden_states + residual hidden_states = hidden_states / attn.rescale_output_factor return hidden_states DistriGroupNorm DistriGroupNorm 根据上一步全特征图的以及当前步 patch 的均值和二阶矩近似当前步的全特征图均值和方差。需要通信 patch 均值和二阶矩。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 class DistriGroupNorm(BaseModule): def __init__(self, module: nn.GroupNorm, distri_config: DistriConfig): assert isinstance(module, nn.GroupNorm) super(DistriGroupNorm, self).__init__(module, distri_config) def forward(self, x: torch.Tensor) -\u003e torch.Tensor: module = self.module distri_config = self.distri_config if self.comm_manager is not None and self.comm_manager.handles is not None and self.idx is not None: if self.comm_manager.handles[self.idx] is not None: self.comm_manager.handles[self.idx].wait() self.comm_manager.handles[self.idx] = None assert x.ndim == 4 n, c, h, w = x.shape num_groups = module.num_groups group_size = c // num_groups if self.buffer_list is None: if self.comm_manager.buffer_list is None: n, c, h, w = x.shape self.idx = self.comm_manager.register_tensor( # register for E[x], E[x^2] shape=[2, n, num_groups, 1, 1, 1], torch_dtype=x.dtype, layer_type=\"gn\" ) else: self.buffer_list = self.comm_manager.get_buffer_list(self.idx) x = x.view([n, num_groups, group_size, h, w]) # 计算 patch 均值和二阶矩 x_mean = x.mean(dim=[2, 3, 4], keepdim=True) # [1, num_groups, 1, 1, 1] x2_mean = (x**2).mean(dim=[2, 3, 4], keepdim=True) # [1, num_groups, 1, 1, 1] slice_mean = torch.stack([x_mean, x2_mean], dim=0) if self.buffer_list is None: full_mean = slice_mean else: # Equation 2 in the paper E[A_t] = E[A_(t+1)] + (E[A^i_t] - E[A^i_(t+1)]), same for E[A^2_t] correction = slice_mean - self.buffer_list[distri_config.split_idx()] full_mean = sum(self.buffer_list) / distri_config.n_device_per_batch + correction self.comm_manager.enqueue(self.idx, slice_mean) full_x_mean, full_x2_mean = full_mean[0], full_mean[1] var = full_x2_mean - full_x_mean**2 # 计算方差 slice_x_mean, slice_x2_mean = slice_mean[0], slice_mean[1] slice_var = slice_x2_mean - slice_x_mean**2 var = torch.where(var \u003c 0, slice_var, var) # Correct negative variance num_elements = group_size * h * w var = var * (num_elements / (num_elements - 1)) std = (var + module.eps).sqrt() output = (x - full_x_mean) / std output = output.view([n, c, h, w]) # scale and shift if module.affine: output = output * module.weight.view([1, -1, 1, 1]) output = output + module.bias.view([1, -1, 1, 1]) self.counter += 1 return output ",
  "wordCount" : "3399",
  "inLanguage": "en",
  "datePublished": "2024-10-23T14:28:37+08:00",
  "dateModified": "2025-06-07T23:40:58+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/distrifusion/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="简体中文"
                            aria-label="简体中文">简体中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="🏠 Home">
                    <span>🏠 Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="🙋🏻‍♂️ Me">
                    <span>🙋🏻‍♂️ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="📚 Blogs">
                    <span>📚 Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="🧩 Categories">
                    <span>🧩 Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="🔖 Tags">
                    <span>🔖 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="⏱ Archive">
                    <span>⏱ Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="🔍 Search (Alt &#43; /)" accesskey=/>
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="🤝 Friends">
                    <span>🤝 Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      DistriFusion
    </h1>
    <div class="post-description">
      Paper reading about DistriFusion.
    </div>
    <div class="post-meta"><span title='2024-10-23 14:28:37 +0800 CST'>Oct-23-2024</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;3399 words&nbsp;·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                    <li>
                        <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                    <li>
                        <a href="#method" aria-label="Method">Method</a><ul>
                            
                    <li>
                        <a href="#displaced-patch-parallelism" aria-label="Displaced Patch Parallelism.">Displaced Patch Parallelism.</a></li>
                    <li>
                        <a href="#sparse-operations" aria-label="Sparse Operations">Sparse Operations</a></li>
                    <li>
                        <a href="#corrected-asynchronous-groupnorm" aria-label="Corrected Asynchronous GroupNorm">Corrected Asynchronous GroupNorm</a></li></ul>
                    </li>
                    <li>
                        <a href="#code-implementation" aria-label="Code Implementation">Code Implementation</a><ul>
                            
                    <li>
                        <a href="#patchparallelismcommmanager" aria-label="PatchParallelismCommManager">PatchParallelismCommManager</a></li>
                    <li>
                        <a href="#districonv2dpp" aria-label="DistriConv2dPP">DistriConv2dPP</a></li>
                    <li>
                        <a href="#distriselfattentionpp" aria-label="DistriSelfAttentionPP">DistriSelfAttentionPP</a></li>
                    <li>
                        <a href="#distrigroupnorm" aria-label="DistriGroupNorm">DistriGroupNorm</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<p>DistriFusion 将模型输入分割成多个 patch 后分配给 GPU。但是直接实现这样的算法会破坏 patch 之间的交互并失去保真度，而同步 GPU 之间的激活将产生巨大的通信开销。为了克服这一困境，根据观察到的相邻扩散步输入之间的高度相似性提出了 <strong>displaced patch parallelism</strong>，该方法通过重用前一个时间步骤中预先计算的 feature map 来利用扩散过程的顺序性，为当前步提供 context. 该方法支持异步通信，可以通过计算实现流水线化。</p>
<h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" alt="Original, Navie Patch &amp; DistriFusion">
    </a><figcaption>Original, Navie Patch &amp; DistriFusion</figcaption></figure></p>
<p>加速扩散模型推理主要集中在两种方法上：减少采样步骤和优化网络推理。随着计算资源的快速增长，利用多个 GPU 来加速推理是很有吸引力的。例如在 NLP 中， LLM 已经成功地利用了 GPU 之间的张量并行性，从而显著降低了延迟。然而，对于扩散模型，由于激活尺寸大，张量并行这样的技术不太适合扩散模型。多个 GPU 通常只用于 batch 推理，当生成单个图像时，通常只涉及一个GPU.</p>
<blockquote>
<p>Techniques like tensor parallelism are less suitable for diffusion models due to the large activation size, as communication costs outweigh savings from distributed computation.</p></blockquote>
<p>自然而然的一种方法是将图像分成几个 patch 后分配给不同的设备进行生成。由于各个 patch 之间缺乏相互作用，它在每个 patch 的边界处都有一个清晰可见的分界线。</p>
<p>DistriFusion 也是基于 patch parallelism. 关键在于扩散模型中相邻去噪步骤的输入是相似的，因此，只在第一步采用同步通信。后续步骤重用前一步中预先计算的激活，为当前步骤提供全局上下文和 patch 交互。通过异步通信有效地隐藏了计算中的通信开销。并且还稀疏地在指定的区域上进行卷积和注意力计算，从而按比例减少每个设备的计算量。</p>
<h1 id="method">Method<a hidden class="anchor" aria-hidden="true" href="#method">#</a></h1>
<h2 id="displaced-patch-parallelism">Displaced Patch Parallelism.<a hidden class="anchor" aria-hidden="true" href="#displaced-patch-parallelism">#</a></h2>
<p>在预测 $\epsilon_{\theta}(\mathbf{x}_{t})$ 时 (忽略条件 c 和时间步 t 的输入) ，首先将 $\mathbf{x}_{t}$ 分割成多个 patch $\mathbf{x}_t^{(1)},\mathbf{x}_t^{(2)},\ldots,\mathbf{x}_t^{(N)}$ ，对于每一层 l 和设备 i，在获得输入激活 patch $\mathbf{A}_{t}^{l,(i)}$ 后异步处理两个操作：首先，对于设备i， 激活 $\mathbf{A}_{t}^{l,(i)}$ 首先 scatter 到上一步旧的激活 $\mathbf{A}_{t+1}^{l}$ 中。然后将此分散操作的输出送入稀疏算子 Fl (线性、卷积或注意层)，该算子专门对新区域执行计算并产生相应的输出。同时，对 $\mathbf{A}_{t}^{l,(i)}$ 执行 AllGather 操作，为下一步的全尺寸激活 $\mathbf{A}_{t}^{l}$ 做准备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" alt="Overview of DistriFusion">
    </a><figcaption>Overview of DistriFusion</figcaption></figure></p>
<p>我们对除第一层 (采用同步通信获得其他设备上的输入) 外的每一层重复这个过程。然后将最终输出 Gather 在一起以近似 $\epsilon_{\theta}(\mathbf{x}_{t})$，用于计算 $\mathbf{x}_{t-1}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" alt="Timeline Visualization on Each Device">
    </a><figcaption>Timeline Visualization on Each Device</figcaption></figure></p>
<h2 id="sparse-operations">Sparse Operations<a hidden class="anchor" aria-hidden="true" href="#sparse-operations">#</a></h2>
<p>对于每一层 l，如果原始算子 Fl 是一个卷积层、线性层或交叉注意层，调整使其专门作用于新激活的区域。这可以通过从 scatter 输出中提取最新部分并将其输入到 Fl 中来实现。对于 self-attention，将其转换为 cross-attention，仅在设备上保留来自新激活的 Q，而 KV 仍然包含整个特征图。</p>
<h2 id="corrected-asynchronous-groupnorm">Corrected Asynchronous GroupNorm<a hidden class="anchor" aria-hidden="true" href="#corrected-asynchronous-groupnorm">#</a></h2>
<p>仅对新 patch 进行归一化或重用旧特征都会降低图像质量。同步 AllGather 所有均值和方差将产生相当大的开销。为了解决这一困境，DistriFusion 在陈旧的统计数据中引入了一个校正项。计算公式如下</p>
$$
\mathbb{E}[\mathbf{A}_t]\approx\underbrace{\mathbb{E}[\mathbf{A}_{t+1}]}_{\text{stale global mean}}+\underbrace{\mathbb{E}[\mathbf{A}_t^{(i)}]-\mathbb{E}[\mathbf{A}_{t+1}^{(i)}]}_{\text{correction}}
$$<p>同样对二阶矩 $\mathbb{E}[\mathbf{A}^2_t]$ 也采用这种计算方式，然后通过 $\mathbb{E}[\mathbf{A}^2_t] - \mathbb{E}[\mathbf{A}_t]^2$ 来计算方差。对于方差结果为负的部分，将使用新鲜 patch 的局部方差代替。</p>
<h1 id="code-implementation">Code Implementation<a hidden class="anchor" aria-hidden="true" href="#code-implementation">#</a></h1>
<p>Distrifusion 中主要就是将 <a href="https://github.com/huggingface/diffusers/blob/9366c8f84bfe47099ff047272661786ebb54721d/src/diffusers/models/unets/unet_2d_condition.py#L71">UNet2DConditionModel</a> 中的 Conv2d, Attention 和 GroupNorm 替换成对应的 patch 实现的网络结构 <a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L15">DistriUNetPP</a>. 这里继承的 BaseModel 类为集成了 PatchParallelismCommManager 类 (介绍见后文) 的网络。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" alt="UNet2DConditionModel">
    </a><figcaption>UNet2DConditionModel</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriUNetPP</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>  <span class="c1"># for Patch Parallelism</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">UNet2DConditionModel</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">UNet2DConditionModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39; 
</span></span></span><span class="line"><span class="cl"><span class="s1">                Substitute Conv2d, Attention, GroupNorm with DistriConv2dPP, DistriSelfAttentionPP, DistriCrossAttentionPP, DistriGroupNorm 
</span></span></span><span class="line"><span class="cl"><span class="s1">                &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">subname</span><span class="p">,</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>  
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">kernel_size</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriConv2dPP</span><span class="p">(</span>  
</span></span><span class="line"><span class="cl">                            <span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="o">=</span><span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;conv_in&#34;</span>
</span></span><span class="line"><span class="cl">                        <span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">Attention</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn1&#34;</span><span class="p">:</span>  <span class="c1"># self attention</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="k">else</span><span class="p">:</span>  <span class="c1"># cross attention</span>
</span></span><span class="line"><span class="cl">                            <span class="k">assert</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn2&#34;</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriCrossAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriGroupNorm</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriUNetPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchparallelismcommmanager">PatchParallelismCommManager<a hidden class="anchor" aria-hidden="true" href="#patchparallelismcommmanager">#</a></h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/utils.py#L112">PatchParallelismCommManager</a> 类主要处理异步通信的部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchParallelismCommManager</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span> <span class="o">=</span> <span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 已经注册的张量的累计总元素数量</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel_dict</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># 记录每个 layer_type 所注册的张量的累计元素数量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 在每个设备上存储所有注册张量的数据，通信所用的 buffer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">starts</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的起始位置 (在 buffer_list 中的起始索引)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ends</span> <span class="o">=</span> <span class="p">[]</span>    <span class="c1">#                 结束                       结束</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shapes</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的 shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">idx_queue</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 需要进行通信的张量索引的队列</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 存储每个设备通信操作的句柄的 list, 用于检查通信是否完成</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>成员函数功能介绍如下</p>
<ol>
<li>
<p><code>register_tensor(self, shape: tuple[int, ...] or list[int], torch_dtype: torch.dtype, layer_type: str = None) -&gt; int</code>: 用于注册张量的形状和数据类型，同时计算并记录张量在缓冲区中的起始位置和结束位置。</p>
<ul>
<li>如果尚未指定 <code>torch_dtype</code>，则将传入的 <code>torch_dtype</code> 设为类成员的默认数据类型。</li>
<li>计算传入张量形状的总元素数 <code>numel</code>，并更新 <code>starts</code>、<code>ends</code> 和 <code>shapes</code> 列表。</li>
<li>如果指定了 <code>layer_type</code>，更新 <code>numel_dict</code> 中该层类型对应的元素数目。</li>
</ul>
</li>
<li>
<p><code>create_buffer(self)</code> : 每个设备上为所有注册的张量创建一个统一的缓冲区。</p>
<ul>
<li>为每个设备创建一个形状为 <code>(numel,)</code> 的张量，并将其放入 <code>buffer_list</code> 中。</li>
<li>输出在各设备上创建的缓冲区总参数量。</li>
</ul>
</li>
<li>
<p><code>get_buffer_list(self, idx: int) -&gt; list[torch.Tensor]</code>: 返回每个设备上对应于指定索引 <code>idx</code> 的缓冲区张量。</p>
<ul>
<li>根据 <code>starts</code> 和 <code>ends</code> 信息，从 <code>buffer_list</code> 中提取指定索引 <code>idx</code> 的张量片段并调整其形状。</li>
</ul>
</li>
<li>
<p><code>communicate(self)</code>: 调用 <code>dist.all_gather</code> 将缓冲区中的张量在不同设备间进行广播。</p>
<ul>
<li>确定当前需要通信的张量范围 (根据 <code>idx_queue</code> 中的索引).</li>
<li>调用 <code>dist.all_gather</code> 在设备组内进行异步广播通信，并将句柄存储在 <code>handles</code> 中。</li>
</ul>
</li>
<li>
<p><code>enqueue(self, idx: int, tensor: torch.Tensor)</code>: 将指定索引 <code>idx</code> 处的张量数据复制到 <code>buffer_list</code> 中，并将索引添加到通信队列 <code>idx_queue</code>。</p>
<ul>
<li>如果通信队列不为空且索引为 0，则先执行一次通信操作。</li>
<li>将张量数据复制到 <code>buffer_list</code> 中的对应位置。</li>
<li>当通信队列长度达到 <code>distri_config</code> 中设定的通信检查点值时，进行通信。</li>
</ul>
</li>
<li>
<p><code>clear(self)</code>: 执行一次所有待通信张量的通信，并等待所有异步操作完成。</p>
<ul>
<li>如果通信队列不为空，则进行通信操作。</li>
<li>遍历所有句柄，等待所有异步操作完成后，将句柄设为 <code>None</code>.</li>
</ul>
</li>
</ol>
<h2 id="districonv2dpp">DistriConv2dPP<a hidden class="anchor" aria-hidden="true" href="#districonv2dpp">#</a></h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L10">DistriConv2dPP</a> 计算自己负责 patch 部分的卷积，需要通信其他设备需要自己负责 patch 的上下 padding 部分。</p>
<ul>
<li><code>__init__</code>：构造函数，初始化成员变量，设置是否为第一层卷积。</li>
<li><code>naive_forward</code>：执行标准的前向传播，不进行任何切片操作。这是单个设备处理时的普通卷积操作。</li>
<li><code>sliced_forward</code>：处理输入张量的切片操作。根据当前设备索引 (<code>split_idx</code>) 计算输入张量在高度方向的起始和结束位置，并在必要时为切片后的张量添加 padding 后进行卷积操作。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriConv2dPP</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriConv2dPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_layer</span> <span class="o">=</span> <span class="n">is_first_layer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">naive_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#  x: [B, C, H, W]</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sliced_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;...&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 等待上一步通信完成</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">boundary_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># buffer_list 存储的是每个 devive 进行卷积所需要的其他 devive 的数据</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;conv2d&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">            <span class="k">def</span> <span class="nf">create_padded_x</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39;拼接接收到的数据&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># rank 0</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">elif</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># rank n-1</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>  <span class="c1"># other ranks</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="p">[</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                            <span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">padded_x</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 提取当前输入张量需要发送给其他设备的部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">boundary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">boundary_size</span><span class="p">,</span> <span class="p">:],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">boundary_size</span><span class="p">:,</span> <span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 直接用上一步的 buffer 拼接</span>
</span></span><span class="line"><span class="cl">            <span class="n">padded_x</span> <span class="o">=</span> <span class="n">create_padded_x</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">padded_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">boundary</span><span class="p">)</span>  <span class="c1"># 插入自己要发送的数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distriselfattentionpp">DistriSelfAttentionPP<a hidden class="anchor" aria-hidden="true" href="#distriselfattentionpp">#</a></h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/attn.py#L107">DistriSelfAttentionPP</a> 只负责计算自己 patch 的输出，需要完整的 KV，将 self attention 运算变成 cross-attention 计算。需要通信自己的 KV.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">DistriAttentionPP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriSelfAttentionPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>  <span class="c1"># 获取 Attention 模块</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>  <span class="c1"># 残差连接</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">args</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">USE_PEFT_BACKEND</span> <span class="k">else</span> <span class="p">(</span><span class="n">scale</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Q Projection</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_kv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>  <span class="c1"># KV Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># 如果缓冲区未创建</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">buffer</span> <span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span> <span class="o">=</span> <span class="n">kv</span>
</span></span><span class="line"><span class="cl">        <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_buffer_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将 full_kv 分割为 key 和 value</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">full_kv</span><span class="p">,</span> <span class="n">full_kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># multi-head attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># O Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># Dropout</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">hidden_states</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distrigroupnorm">DistriGroupNorm<a hidden class="anchor" aria-hidden="true" href="#distrigroupnorm">#</a></h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/groupnorm.py#L9">DistriGroupNorm</a> 根据上一步全特征图的以及当前步 patch 的均值和二阶矩近似当前步的全特征图均值和方差。需要通信 patch 均值和二阶矩。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriGroupNorm</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriGroupNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl">        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_groups</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="n">group_size</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>  <span class="c1"># register for E[x], E[x^2]</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;gn&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算 patch 均值和二阶矩</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x2_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x2_mean</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="n">slice_mean</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Equation 2 in the paper E[A_t] = E[A_(t+1)] + (E[A^i_t] - E[A^i_(t+1)]), same for E[A^2_t]</span>
</span></span><span class="line"><span class="cl">            <span class="n">correction</span> <span class="o">=</span> <span class="n">slice_mean</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">)</span> <span class="o">/</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">+</span> <span class="n">correction</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">slice_mean</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">full_x_mean</span><span class="p">,</span> <span class="n">full_x2_mean</span> <span class="o">=</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">full_x2_mean</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算方差</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_x_mean</span><span class="p">,</span> <span class="n">slice_x2_mean</span> <span class="o">=</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_var</span> <span class="o">=</span> <span class="n">slice_x2_mean</span> <span class="o">-</span> <span class="n">slice_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">var</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">slice_var</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>  <span class="c1"># Correct negative variance</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">num_elements</span> <span class="o">=</span> <span class="n">group_size</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scale and shift</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/diffusion-models/">Diffusion Models</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/courselearning/mlir/mlir-ch2-writing-our-first-pass/">
    <span class="title">« Prev</span>
    <br>
    <span>MLIR-Ch2 Writing Our First Pass</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/deepspeedulysses/">
    <span class="title">Next »</span>
    <br>
    <span>DeepSpeedUlysses</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>© 2024-2025 WITHER</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
