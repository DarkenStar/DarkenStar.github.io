<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Megatron-LM | WITHER</title>
<meta name="keywords" content="MegatronLM">
<meta name="description" content="Paper reading about Megatron-LM">
<meta name="author" content="WITHER">
<link rel="canonical" href="http://localhost:1313/blogs/megatronlm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.dd3b5b907a50db3238b81d49d094cf1c04a091227797dc9cfde4e2fa3f35df49.css" integrity="sha256-3TtbkHpQ2zI4uB1J0JTPHASgkSJ3l9yc/eTi&#43;j8130k=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/blogs/megatronlm/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    }
  };
</script>




<script src="https://cdn.bootcdn.net/ajax/libs/mermaid/9.1.6/mermaid.min.js"></script>
<script>
    mermaid.initialize({
        startOnLoad: true,
        theme: localStorage.getItem("pref-theme") === "dark" ? "dark" : "forest" 
    });
</script>

<meta property="og:url" content="http://localhost:1313/blogs/megatronlm/">
  <meta property="og:site_name" content="WITHER">
  <meta property="og:title" content="Megatron-LM">
  <meta property="og:description" content="Paper reading about Megatron-LM">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blogs">
    <meta property="article:published_time" content="2024-10-02T15:51:50+08:00">
    <meta property="article:modified_time" content="2025-06-13T15:12:24+08:00">
    <meta property="article:tag" content="Distributed Training">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Megatron-LM">
<meta name="twitter:description" content="Paper reading about Megatron-LM">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "http://localhost:1313/blogs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Megatron-LM",
      "item": "http://localhost:1313/blogs/megatronlm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Megatron-LM",
  "name": "Megatron-LM",
  "description": "Paper reading about Megatron-LM",
  "keywords": [
    "MegatronLM"
  ],
  "articleBody": "Abstract æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦æ–°çš„ç¼–è¯‘å™¨æˆ–æ›´æ”¹åº“ï¼Œä¸æµæ°´çº¿æ¨¡å‹å¹¶è¡Œ (pipeline model parallelism) æ­£äº¤äº’è¡¥ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡åœ¨åŸç”Ÿ PyTorch ä¸­æ’å…¥ä¸€äº›é€šä¿¡æ“ä½œæ¥å®ç°ã€‚ä¸ºäº†é˜è¿°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨ 512 ä¸ª GPU å°†åŸºäº transformer çš„æ¨¡å‹æ‰©å±•åˆ° 83 äº¿ä¸ªå‚æ•°ã€‚ä¸å¯ä¿æŒ 39 TeraFLOPs (å³°å€¼ FLOPs çš„ 30%) çš„å¼ºå¤§å• GPU åŸºå‡†ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªåº”ç”¨ä¸­ä¿æŒäº† 15.1 PetaFLOPsï¼Œæ‰©å±•æ•ˆç‡é«˜è¾¾ 76%.\nIntroduction éšç€ LLM å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œå®ƒä»¬ä¼šè¶…å‡ºç°ä»£å¤„ç†å™¨çš„å†…å­˜é™åˆ¶ï¼Œå¹¶éœ€è¦å¦‚æ¿€æ´»æ£€æŸ¥ç‚¹ (activation checkpoint) ç­‰é¢å¤–çš„å†…å­˜ç®¡ç†æŠ€æœ¯ã€‚å¹¿æ³›ä½¿ç”¨çš„ä¼˜åŒ–ç®—æ³• (å¦‚ADAM) éœ€è¦æ¯ä¸ªå‚æ•°é¢å¤–çš„å†…å­˜æ¥å­˜å‚¨åŠ¨é‡å’Œå…¶ä»–ä¼˜åŒ–å™¨çŠ¶æ€ã€‚è¿™å‡å°‘äº†å¯ä»¥æœ‰æ•ˆè®­ç»ƒçš„æ¨¡å‹çš„å¤§å°ã€‚æ¨¡å‹å¹¶è¡Œæ€§çš„å‡ ç§æ–¹æ³•å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼Œå®ƒä»¬å¯¹æ¨¡å‹è¿›è¡Œåˆ†åŒºï¼Œä½¿æƒé‡åŠå…¶ç›¸å…³çš„ä¼˜åŒ–å™¨çŠ¶æ€ä¸éœ€è¦å¹¶å‘åœ°é©»ç•™åœ¨å¤„ç†å™¨ä¸Šã€‚\nActivation Checkpoint åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‰å‘ä¼ æ’­ä¼šè®¡ç®—å¹¶å­˜å‚¨æ¯ä¸€å±‚çš„æ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼åœ¨åå‘ä¼ æ’­æ—¶è¢«ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚ç„¶è€Œï¼Œå¯¹äºæ·±åº¦å¾ˆå¤§çš„æ¨¡å‹å› ä¸ºéœ€è¦å­˜å‚¨å¤§é‡çš„æ¿€æ´»å€¼ï¼Œå¯èƒ½ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºã€‚æ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯é€šè¿‡åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­åªå­˜å‚¨ä¸€éƒ¨åˆ†çš„æ¿€æ´»å€¼æ¥è§£å†³å†…å­˜å ç”¨é—®é¢˜ï¼Œå¦‚æœåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦æ²¡æœ‰å­˜å‚¨çš„æ¿€æ´»å€¼å°±è¿›è¡Œé‡æ–°è®¡ç®—ã€‚ ä¸ºäº†è¯æ˜æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼Œé€šè¿‡åœ¨å•ä¸ªè‹±ä¼Ÿè¾¾ V100 32GB GPU ä¸Šè®­ç»ƒä¸€ä¸ªåŒ…å« 12 äº¿ä¸ªå‚æ•°çš„æ¨¡å‹æ¥å»ºç«‹åŸºå‡†ã€‚è®­ç»ƒè¯¥æ¨¡å‹å¯ç»´æŒ 39 TeraFLOPs çš„ç®—åŠ›ï¼Œæ˜¯åœ¨ DGX-2H æœåŠ¡å™¨ä¸­é…ç½®çš„å•ä¸ª GPU ç†è®ºå³°å€¼ FLOPS çš„ 30%. åœ¨ 512 ä¸ª GPU ä¸Šå°†æ¨¡å‹æ‰©å±•åˆ° 83 äº¿ä¸ªå‚æ•°ï¼Œå¹¶é‡‡ç”¨ 8 è·¯æ¨¡å‹å¹¶è¡Œï¼Œåœ¨æ•´ä¸ªåº”ç”¨ä¸­å®ç°äº†é«˜è¾¾ 15.1 PetaFLOPs çš„æŒç»­è¿è¡Œé€Ÿåº¦ã€‚ä¸å• GPU æƒ…å†µç›¸æ¯”ï¼Œæ‰©å±•æ•ˆç‡æé«˜äº† 76%. ä¸‹å›¾å±•ç¤ºäº†æ›´è¯¦ç»†çš„æ‰©å±•ç»“æœã€‚\nModel (blue) and model+data (green) parallel FLOPS\nBackground \u0026 Chanllenges Neural Language Model Pretraining æ—©æœŸçš„é¢„è®­ç»ƒå’Œä¼ é€’è¯­è¨€ç¥ç»è¡¨ç¤ºçš„ä¾‹å­è¡¨æ˜ï¼Œä¸ä»å¤´å¼€å§‹å­¦ä¹ çš„è¯åµŒå…¥è¡¨ç›¸æ¯”ï¼Œé¢„è®­ç»ƒçš„è¯åµŒå…¥è¡¨æ”¹å–„äº†ä¸‹æ¸¸ä»»åŠ¡çš„ç»“æœã€‚ç›®å‰çš„æŠ€æœ¯æ°´å¹³å·²ç»ä»ä¼ è¾“å•è¯åµŒå…¥è¡¨å‘å±•åˆ°ä¼ è¾“æ•´ä¸ªæ•°åäº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•çš„è¿›æ­¥è¦æ±‚ç¡¬ä»¶ã€ç³»ç»ŸæŠ€æœ¯å’Œæ¡†æ¶èƒ½å¤Ÿé«˜æ•ˆåœ°å¤§è§„æ¨¡è¿è¡Œã€‚\nTransformer Language Models and Multi-Head Attention ä¸‹å›¾å±•ç¤ºäº†ä½¿ç”¨çš„ transformer æ¨¡å‹çš„ç¤ºæ„å›¾ã€‚æœ€è¿‘åˆ©ç”¨ transformer è¿›è¡Œè¯­è¨€å»ºæ¨¡çš„å·¥ä½œï¼Œå¦‚ BERT å’Œ GPT-2 æ ¹æ®éœ€è¦åˆ†åˆ«åªä½¿ç”¨ç¼–ç å™¨å’Œè§£ç å™¨ã€‚\nGPT-2 å’Œ BERT éƒ½å¯¹å¤šå¤´æ³¨æ„å’Œ FFN çš„è¾“å…¥ä½¿ç”¨ GeLU éçº¿æ€§å’Œå±‚å½’ä¸€åŒ–ï¼Œè€ŒåŸå§‹ transformer ä½¿ç”¨ ReLU éçº¿æ€§å¹¶å¯¹è¾“å‡ºè¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚\nTransformer Architecture\nData and Model Parallelism in Deep Learning å°†æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒæ‰©å±•åˆ°å¤šç¡¬ä»¶åŠ é€Ÿå™¨æœ‰ä¸¤ç§èŒƒå¼:\nData Parallelism (DP): å°† batch æ‹†åˆ†åˆ°å¤šä¸ª worker Model Parallelism (MP): å°†æ¨¡å‹çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—åˆ†å¸ƒåœ¨å¤šä¸ª worker ä¸­ã€‚ Pipeline Parallelism (PP): ä¸€ç»„æ“ä½œåœ¨ä¸€ä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œï¼Œç„¶åå°†è¾“å‡ºä¼ é€’åˆ°æµæ°´çº¿ä¸­çš„ä¸‹ä¸€ä¸ªè®¾å¤‡æ‰§è¡Œå¦ä¸€ç»„æ“ä½œã€‚ Distributed Tensor Computation: å°†å¼ é‡è¿ç®—åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œä»¥åŠ é€Ÿè®¡ç®—æˆ–å¢åŠ æ¨¡å‹å¤§å°ã€‚ ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯æœ‰ä¸€ä¸ªåŸºæœ¬çš„é™åˆ¶: æ¨¡å‹æƒé‡å¿…é¡»èƒ½åŠ è½½è¿› worker. æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åˆ©ç”¨æ¨¡å‹å¹¶è¡Œæ€§åœ¨å¤šä¸ªåŠ é€Ÿå™¨ä¹‹é—´åˆ†å‰²æ¨¡å‹ã€‚\nModel Parallel Transformers æˆ‘ä»¬åˆ©ç”¨ transformer ç½‘ç»œçš„ç»“æ„ (self-attention å’Œ FFN (2*MLP) ç»„æˆ)ï¼Œé€šè¿‡æ·»åŠ ä¸€äº›åŒæ­¥åŸè¯­ï¼Œåˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„å¹¶è¡Œè®¡ç®—æ¨¡å‹ã€‚ä¸‹é¢åˆ†åˆ«é˜è¿°å¯¹ FFN å’Œ self-attention çš„å¹¶è¡ŒåŒ–ã€‚\nFFN ç¬¬ä¸€ä¸ª MLP ç”±ä¸€ä¸ª GEMMï¼Œåè·Ÿä¸€ä¸ª GeLU éçº¿æ€§ç»„æˆ:\n$$\rY=\\text{GeLU}(XA)\r$$å¹¶è¡ŒåŒ– GEMM çš„ä¸€ç§é€‰æ‹©æ˜¯å°†æƒé‡çŸ©é˜µ A æ²¿ç€è¡Œåˆ‡åˆ†ï¼Œå¹¶å°† X æ²¿ç€å…¶åˆ—åˆ‡åˆ†:\n$$\rX=[X_1,X_2], A=\\begin{bmatrix}A_1\\\\A_2\\end{bmatrix}\r$$\rRow Split of Weight\nå¯ä»¥å¾—å‡º $Y = X_1A_1+X_2A_2$. ç”±äº GeLU æ˜¯éçº¿æ€§å‡½æ•°ï¼Œå› æ­¤è¿™ç§æ–¹æ³•éœ€è¦åœ¨ GeLU å‡½æ•°ä¹‹å‰è¿›è¡ŒåŒæ­¥ã€‚\nå¦ä¸€ä¸ªé€‰æ‹©æ˜¯æ²¿ç€åˆ—åˆ‡åˆ† $A=\\begin{bmatrix}A_1,A_2\\end{bmatrix}$. è¿™æ ·å¯ä»¥è®© GeLU ç‹¬ç«‹åœ°åº”ç”¨äºæ¯ä¸ª GEMM çš„è¾“å‡º\n$[Y_1, Y_2]=\\begin{bmatrix}\\text{GeLU}(XA_1),\\text{GeLU}(XA_2)\\end{bmatrix}$.\nColumn Split of Weight\nè¿™ç§åˆ‡åˆ†æ–¹å¼çš„ä¼˜ç‚¹æ˜¯ä¸éœ€è¦è¿›è¡ŒåŒæ­¥æ“ä½œã€‚\nå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä»¥åˆ—å¹¶è¡Œæ–¹å¼åˆ‡åˆ†ç¬¬ä¸€ä¸ª GEMMï¼Œå¹¶æ²¿ç€è¡Œåˆ‡åˆ†ç¬¬äºŒä¸ªGEMMã€‚ç„¶åï¼Œåœ¨å°†è¾“å‡ºä¼ é€’ç»™ dropout å±‚ä¹‹å‰ï¼Œç¬¬äºŒä¸ªGEMM çš„è¾“å‡ºåœ¨ GPU ä¹‹é—´è¿›è¡Œ all-reduce æ“ä½œã€‚è¿™ç§æ–¹æ³•å°† FFN ä¸­çš„ä¸¤ä¸ª GEMM æ‹†åˆ†åˆ°å¤šä¸ª GPU ä¸Šæ‰§è¡Œï¼Œå¹¶ä¸”åªéœ€è¦åœ¨æ­£å‘ä¼ æ’­ (g æ“ä½œç¬¦) å’Œåå‘ä¼ æ’­ (f æ“ä½œç¬¦) ä¸­åˆ†åˆ«æ‰§è¡Œä¸€æ¬¡ all-reduce æ“ä½œã€‚\nParallelism of MLP\nå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›æ“ä½œä¸­æœ¬èº«å­˜åœ¨çš„å¹¶è¡Œæ€§ï¼Œä»¥åˆ—å¹¶è¡Œçš„æ–¹å¼åˆ’åˆ†ä¸ QKV ç›¸å…³çš„ GEMMï¼Œä»¥ä¾¿æ¯ä¸ªæ³¨æ„åŠ›å¤´å¯¹åº”çš„çŸ©é˜µä¹˜æ³•åœ¨ä¸€ä¸ª GPU ä¸Šç‹¬ç«‹å®Œæˆã€‚è¾“å‡ºçº¿æ€§å±‚çš„ GEMM æ²¿ç€å…¶è¡Œå¹¶è¡ŒåŒ–ï¼Œå¹¶ç›´æ¥è·å–å¹¶è¡Œ attention çš„è¾“å‡ºã€‚\nParallelism of Self-Attention\nå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™ä½¿èƒ½å¤Ÿä»…åœ¨æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­åˆ†åˆ«ä¸­ä½¿ç”¨ä¸¤ä¸ª all-reduce æ“ä½œæ‰§è¡Œ transformer ä¸­æ‰€æœ‰çš„ GEMM.\nParallelism of Transformer Layer\nåŸºäº transformer çš„è¯­è¨€æ¨¡å‹çš„è¾“å‡ºåµŒå…¥ç»´åº¦ä¸ºéšè—å±‚å¤§å° (H) ä¹˜ä»¥è¯æ±‡è¡¨å¤§å° (v). æˆ‘ä»¬æ²¿ç€è¯æ±‡è¡¨ç»´åº¦ $E = \\begin{bmatrix}E_1,E_2\\end{bmatrix}$ å¹¶è¡ŒåŒ–æƒé‡çŸ©é˜µã€‚æ¯ä¸€å—ç°åœ¨åªåŒ…å«åµŒå…¥è¡¨çš„ä¸€éƒ¨åˆ†ï¼Œè¾“å…¥åµŒå…¥åéœ€è¦ä¸€ä¸ª all-reduce (g ç®—å­).\nå¯¹äºè¾“å‡ºåµŒå…¥ï¼Œä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡å¹¶è¡Œ $\\mathrm{GEMM} [Y_{1},Y_{2}]=[XE_{1},XE_{2}]$ æ¥è·å¾— logitsï¼Œå¹¶å¯¹ç»“æœ all-gather åé€å…¥äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œall-gather é€šä¿¡é‡ä¸º bsv ä¸ªå…ƒç´  (b æ˜¯æ‰¹å¤„ç†å¤§å°ï¼Œs æ˜¯åºåˆ—é•¿åº¦). ä¸ºäº†å‡å°é€šä¿¡è§„æ¨¡ï¼Œæˆ‘ä»¬å°†è¾“å‡ºä¸äº¤å‰ç†µæŸå¤±èåˆï¼Œè¿™æ ·é€šä¿¡é‡é™ä¸º bs.\næˆ‘ä»¬åœ¨æ¯ä¸ª GPU ä¸Šç»´æŠ¤å±‚å½’ä¸€åŒ–å‚æ•°çš„å‰¯æœ¬ï¼Œå¹¶åœ¨å°†è¿™äº›å¼ é‡ä½œä¸ºè¾“å…¥é€åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹å¹¶è¡ŒåŒºåŸŸä¹‹å‰ï¼Œåœ¨æœ¬åœ°è¾“å‡ºä¸Šè¿›è¡Œ dropout å’Œæ®‹å·®è¿æ¥ã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹ï¼Œæˆ‘ä»¬å…è®¸æ¯ä¸ªæ¨¡å‹å¹¶è¡Œ worker ä¼˜åŒ–è‡ªå·±çš„ä¸€ç»„å‚æ•°ã€‚å› ä¸ºæ‰€æœ‰çš„å€¼è¦ä¹ˆæ˜¯æœ¬åœ°çš„ï¼Œè¦ä¹ˆæ˜¯åœ¨ GPUä¸Š é‡å¤çš„ï¼Œæ‰€ä»¥åœ¨è¿™ä¸ªå…¬å¼ä¸­ä¸éœ€è¦é€šä¿¡æ›´æ–°çš„å‚æ•°å€¼ã€‚\n",
  "wordCount" : "1866",
  "inLanguage": "en",
  "datePublished": "2024-10-02T15:51:50+08:00",
  "dateModified": "2025-06-13T15:12:24+08:00",
  "author":[{
    "@type": "Person",
    "name": "WITHER"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/blogs/megatronlm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "WITHER",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="WITHER (Alt + H)">WITHER</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="http://localhost:1313/zh/" title="ç®€ä½“ä¸­æ–‡"
                            aria-label="ç®€ä½“ä¸­æ–‡">ç®€ä½“ä¸­æ–‡</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="ğŸ  Home">
                    <span>ğŸ  Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about_me/" title="ğŸ™‹ğŸ»â€â™‚ï¸ Me">
                    <span>ğŸ™‹ğŸ»â€â™‚ï¸ Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/blogs/" title="ğŸ“š Blogs">
                    <span>ğŸ“š Blogs</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="ğŸ§© Categories">
                    <span>ğŸ§© Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="ğŸ”– Tags">
                    <span>ğŸ”– Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="â± Archive">
                    <span>â± Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="ğŸ” Search (Alt &#43; /)" accesskey=/>
                    <span>ğŸ” Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/friends/" title="ğŸ¤ Friends">
                    <span>ğŸ¤ Friends</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/blogs/">Blogs</a></div>
    <h1 class="post-title entry-hint-parent">
      Megatron-LM
    </h1>
    <div class="post-description">
      Paper reading about Megatron-LM
    </div>
    <div class="post-meta"><span title='2024-10-02 15:51:50 +0800 CST'>Oct-02-2024</span>&nbsp;Â·&nbsp;4 min&nbsp;Â·&nbsp;1866 words&nbsp;Â·&nbsp;WITHER

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#abstract" aria-label="Abstract">Abstract</a></li>
                    <li>
                        <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                    <li>
                        <a href="#background--chanllenges" aria-label="Background &amp; Chanllenges">Background &amp; Chanllenges</a><ul>
                            
                    <li>
                        <a href="#neural-language-model-pretraining" aria-label="Neural Language Model Pretraining">Neural Language Model Pretraining</a></li>
                    <li>
                        <a href="#transformer-language-models-and-multi-head-attention" aria-label="Transformer Language Models and Multi-Head Attention">Transformer Language Models and Multi-Head Attention</a></li>
                    <li>
                        <a href="#data-and-model-parallelism-in-deep-learning" aria-label="Data and Model Parallelism in Deep Learning">Data and Model Parallelism in Deep Learning</a></li></ul>
                    </li>
                    <li>
                        <a href="#model-parallel-transformers" aria-label="Model Parallel Transformers">Model Parallel Transformers</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    
    document.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();
    
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        if (elements.length > 0) {
            
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
        }
    
        
        const topLink = document.getElementById('top-link');
        if (topLink) {
            topLink.addEventListener('click', (event) => {
                
                event.preventDefault();
    
                
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        }
    }, false);
    
    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);
    
    window.addEventListener('scroll', () => {
        
        const scrollPosition = window.pageYOffset || document.documentElement.scrollTop;
    
        
        if (scrollPosition === 0) {
            return;
        }
    
        
        if (elements && elements.length > 0) {
            
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - scrollPosition) > 0 && 
                    (getOffsetTop(element) - scrollPosition) < window.innerHeight / 2) {
                    return element;
                }
            }) || activeElement;
    
            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                const tocLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
                if (element === activeElement){
                    tocLink.classList.add('active');
    
                    
                    const tocContainer = document.querySelector('.toc .inner');
                    const linkOffsetTop = tocLink.offsetTop;
                    const containerHeight = tocContainer.clientHeight;
                    const linkHeight = tocLink.clientHeight;
    
                    
                    const scrollPosition = linkOffsetTop - (containerHeight / 2) + (linkHeight / 2);
                    tocContainer.scrollTo({ top: scrollPosition, behavior: 'smooth' });
                } else {
                    tocLink.classList.remove('active');
                }
            });
        }
    }, false);
    
    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);
    
    function checkTocPosition() {
        const width = document.body.scrollWidth;
    
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }
    
    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
    
</script>

  <div class="post-content"><h1 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h1>
<p>æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦æ–°çš„ç¼–è¯‘å™¨æˆ–æ›´æ”¹åº“ï¼Œä¸æµæ°´çº¿æ¨¡å‹å¹¶è¡Œ (<em>pipeline model parallelism</em>) æ­£äº¤äº’è¡¥ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡åœ¨åŸç”Ÿ PyTorch ä¸­æ’å…¥ä¸€äº›é€šä¿¡æ“ä½œæ¥å®ç°ã€‚ä¸ºäº†é˜è¿°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨ 512 ä¸ª GPU å°†åŸºäº transformer çš„æ¨¡å‹æ‰©å±•åˆ° 83 äº¿ä¸ªå‚æ•°ã€‚ä¸å¯ä¿æŒ 39 TeraFLOPs (å³°å€¼ FLOPs çš„ 30%) çš„å¼ºå¤§å• GPU åŸºå‡†ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªåº”ç”¨ä¸­ä¿æŒäº† 15.1 PetaFLOPsï¼Œæ‰©å±•æ•ˆç‡é«˜è¾¾ 76%.</p>
<h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>éšç€ LLM å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œå®ƒä»¬ä¼šè¶…å‡ºç°ä»£å¤„ç†å™¨çš„å†…å­˜é™åˆ¶ï¼Œå¹¶éœ€è¦å¦‚æ¿€æ´»æ£€æŸ¥ç‚¹ (activation checkpoint) ç­‰é¢å¤–çš„å†…å­˜ç®¡ç†æŠ€æœ¯ã€‚å¹¿æ³›ä½¿ç”¨çš„ä¼˜åŒ–ç®—æ³• (å¦‚ADAM) éœ€è¦æ¯ä¸ªå‚æ•°é¢å¤–çš„å†…å­˜æ¥å­˜å‚¨åŠ¨é‡å’Œå…¶ä»–ä¼˜åŒ–å™¨çŠ¶æ€ã€‚è¿™å‡å°‘äº†å¯ä»¥æœ‰æ•ˆè®­ç»ƒçš„æ¨¡å‹çš„å¤§å°ã€‚æ¨¡å‹å¹¶è¡Œæ€§çš„å‡ ç§æ–¹æ³•å…‹æœäº†è¿™ä¸€é™åˆ¶ï¼Œå®ƒä»¬å¯¹æ¨¡å‹è¿›è¡Œåˆ†åŒºï¼Œä½¿æƒé‡åŠå…¶ç›¸å…³çš„ä¼˜åŒ–å™¨çŠ¶æ€ä¸éœ€è¦å¹¶å‘åœ°é©»ç•™åœ¨å¤„ç†å™¨ä¸Šã€‚</p>
<details class="custom-details">
    <summary class="custom-summary">Activation Checkpoint</summary>
    <div>åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå‰å‘ä¼ æ’­ä¼šè®¡ç®—å¹¶å­˜å‚¨æ¯ä¸€å±‚çš„æ¿€æ´»å€¼ï¼Œè¿™äº›æ¿€æ´»å€¼åœ¨åå‘ä¼ æ’­æ—¶è¢«ç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚ç„¶è€Œï¼Œå¯¹äºæ·±åº¦å¾ˆå¤§çš„æ¨¡å‹å› ä¸ºéœ€è¦å­˜å‚¨å¤§é‡çš„æ¿€æ´»å€¼ï¼Œå¯èƒ½ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºã€‚æ¿€æ´»æ£€æŸ¥ç‚¹æŠ€æœ¯é€šè¿‡åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­åªå­˜å‚¨ä¸€éƒ¨åˆ†çš„æ¿€æ´»å€¼æ¥è§£å†³å†…å­˜å ç”¨é—®é¢˜ï¼Œå¦‚æœåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦æ²¡æœ‰å­˜å‚¨çš„æ¿€æ´»å€¼å°±è¿›è¡Œé‡æ–°è®¡ç®—ã€‚</div>
</details><br>
<p>ä¸ºäº†è¯æ˜æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼Œé€šè¿‡åœ¨å•ä¸ªè‹±ä¼Ÿè¾¾ V100 32GB GPU ä¸Šè®­ç»ƒä¸€ä¸ªåŒ…å« 12 äº¿ä¸ªå‚æ•°çš„æ¨¡å‹æ¥å»ºç«‹åŸºå‡†ã€‚è®­ç»ƒè¯¥æ¨¡å‹å¯ç»´æŒ 39 TeraFLOPs çš„ç®—åŠ›ï¼Œæ˜¯åœ¨ DGX-2H æœåŠ¡å™¨ä¸­é…ç½®çš„å•ä¸ª GPU ç†è®ºå³°å€¼ FLOPS çš„ 30%. åœ¨ 512 ä¸ª GPU ä¸Šå°†æ¨¡å‹æ‰©å±•åˆ° 83 äº¿ä¸ªå‚æ•°ï¼Œå¹¶é‡‡ç”¨ 8 è·¯æ¨¡å‹å¹¶è¡Œï¼Œåœ¨æ•´ä¸ªåº”ç”¨ä¸­å®ç°äº†é«˜è¾¾ 15.1 PetaFLOPs çš„æŒç»­è¿è¡Œé€Ÿåº¦ã€‚ä¸å• GPU æƒ…å†µç›¸æ¯”ï¼Œæ‰©å±•æ•ˆç‡æé«˜äº† 76%. ä¸‹å›¾å±•ç¤ºäº†æ›´è¯¦ç»†çš„æ‰©å±•ç»“æœã€‚</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" alt="Model (blue) and model&#43;data (green) parallel FLOPS">
    </a><figcaption>Model (blue) and model&#43;data (green) parallel FLOPS</figcaption></figure></p>
<h1 id="background--chanllenges">Background &amp; Chanllenges<a hidden class="anchor" aria-hidden="true" href="#background--chanllenges">#</a></h1>
<h2 id="neural-language-model-pretraining">Neural Language Model Pretraining<a hidden class="anchor" aria-hidden="true" href="#neural-language-model-pretraining">#</a></h2>
<p>æ—©æœŸçš„é¢„è®­ç»ƒå’Œä¼ é€’è¯­è¨€ç¥ç»è¡¨ç¤ºçš„ä¾‹å­è¡¨æ˜ï¼Œä¸ä»å¤´å¼€å§‹å­¦ä¹ çš„è¯åµŒå…¥è¡¨ç›¸æ¯”ï¼Œé¢„è®­ç»ƒçš„è¯åµŒå…¥è¡¨æ”¹å–„äº†ä¸‹æ¸¸ä»»åŠ¡çš„ç»“æœã€‚ç›®å‰çš„æŠ€æœ¯æ°´å¹³å·²ç»ä»ä¼ è¾“å•è¯åµŒå…¥è¡¨å‘å±•åˆ°ä¼ è¾“æ•´ä¸ªæ•°åäº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•çš„è¿›æ­¥è¦æ±‚ç¡¬ä»¶ã€ç³»ç»ŸæŠ€æœ¯å’Œæ¡†æ¶èƒ½å¤Ÿé«˜æ•ˆåœ°å¤§è§„æ¨¡è¿è¡Œã€‚</p>
<h2 id="transformer-language-models-and-multi-head-attention">Transformer Language Models and Multi-Head Attention<a hidden class="anchor" aria-hidden="true" href="#transformer-language-models-and-multi-head-attention">#</a></h2>
<p>ä¸‹å›¾å±•ç¤ºäº†ä½¿ç”¨çš„ transformer æ¨¡å‹çš„ç¤ºæ„å›¾ã€‚æœ€è¿‘åˆ©ç”¨ transformer è¿›è¡Œè¯­è¨€å»ºæ¨¡çš„å·¥ä½œï¼Œå¦‚ BERT å’Œ GPT-2 æ ¹æ®éœ€è¦åˆ†åˆ«åªä½¿ç”¨ç¼–ç å™¨å’Œè§£ç å™¨ã€‚</p>
<blockquote>
<p>GPT-2 å’Œ BERT éƒ½å¯¹å¤šå¤´æ³¨æ„å’Œ FFN çš„è¾“å…¥ä½¿ç”¨ GeLU éçº¿æ€§å’Œå±‚å½’ä¸€åŒ–ï¼Œè€ŒåŸå§‹ transformer ä½¿ç”¨ ReLU éçº¿æ€§å¹¶å¯¹è¾“å‡ºè¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" alt="Transformer Architecture">
    </a><figcaption>Transformer Architecture</figcaption></figure></p>
<h2 id="data-and-model-parallelism-in-deep-learning">Data and Model Parallelism in Deep Learning<a hidden class="anchor" aria-hidden="true" href="#data-and-model-parallelism-in-deep-learning">#</a></h2>
<p>å°†æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒæ‰©å±•åˆ°å¤šç¡¬ä»¶åŠ é€Ÿå™¨æœ‰ä¸¤ç§èŒƒå¼:</p>
<ul>
<li>Data Parallelism (DP): å°† batch æ‹†åˆ†åˆ°å¤šä¸ª worker</li>
<li>Model Parallelism (MP): å°†æ¨¡å‹çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—åˆ†å¸ƒåœ¨å¤šä¸ª worker ä¸­ã€‚
<ul>
<li>Pipeline Parallelism (PP): ä¸€ç»„æ“ä½œåœ¨ä¸€ä¸ªè®¾å¤‡ä¸Šæ‰§è¡Œï¼Œç„¶åå°†è¾“å‡ºä¼ é€’åˆ°æµæ°´çº¿ä¸­çš„ä¸‹ä¸€ä¸ªè®¾å¤‡æ‰§è¡Œå¦ä¸€ç»„æ“ä½œã€‚</li>
<li>Distributed Tensor Computation: å°†å¼ é‡è¿ç®—åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œä»¥åŠ é€Ÿè®¡ç®—æˆ–å¢åŠ æ¨¡å‹å¤§å°ã€‚</li>
</ul>
</li>
</ul>
<p>ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯æœ‰ä¸€ä¸ªåŸºæœ¬çš„é™åˆ¶: æ¨¡å‹æƒé‡å¿…é¡»èƒ½åŠ è½½è¿› worker. æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åˆ©ç”¨æ¨¡å‹å¹¶è¡Œæ€§åœ¨å¤šä¸ªåŠ é€Ÿå™¨ä¹‹é—´åˆ†å‰²æ¨¡å‹ã€‚</p>
<h1 id="model-parallel-transformers">Model Parallel Transformers<a hidden class="anchor" aria-hidden="true" href="#model-parallel-transformers">#</a></h1>
<p>æˆ‘ä»¬åˆ©ç”¨ transformer ç½‘ç»œçš„ç»“æ„ (self-attention å’Œ FFN (2*MLP) ç»„æˆ)ï¼Œé€šè¿‡æ·»åŠ ä¸€äº›åŒæ­¥åŸè¯­ï¼Œåˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„å¹¶è¡Œè®¡ç®—æ¨¡å‹ã€‚ä¸‹é¢åˆ†åˆ«é˜è¿°å¯¹ FFN å’Œ self-attention çš„å¹¶è¡ŒåŒ–ã€‚</p>
<p>FFN ç¬¬ä¸€ä¸ª MLP ç”±ä¸€ä¸ª GEMMï¼Œåè·Ÿä¸€ä¸ª GeLU éçº¿æ€§ç»„æˆ:</p>
$$
Y=\text{GeLU}(XA)
$$<p>å¹¶è¡ŒåŒ– GEMM çš„ä¸€ç§é€‰æ‹©æ˜¯å°†æƒé‡çŸ©é˜µ A æ²¿ç€è¡Œåˆ‡åˆ†ï¼Œå¹¶å°† X æ²¿ç€å…¶åˆ—åˆ‡åˆ†:</p>
$$
X=[X_1,X_2], A=\begin{bmatrix}A_1\\A_2\end{bmatrix}
$$<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" alt="Row Split of Weight">
    </a><figcaption>Row Split of Weight</figcaption></figure></p>
<p>å¯ä»¥å¾—å‡º $Y = X_1A_1+X_2A_2$. ç”±äº GeLU æ˜¯éçº¿æ€§å‡½æ•°ï¼Œå› æ­¤è¿™ç§æ–¹æ³•éœ€è¦åœ¨ GeLU å‡½æ•°ä¹‹å‰è¿›è¡ŒåŒæ­¥ã€‚</p>
<p>å¦ä¸€ä¸ªé€‰æ‹©æ˜¯æ²¿ç€åˆ—åˆ‡åˆ† $A=\begin{bmatrix}A_1,A_2\end{bmatrix}$. è¿™æ ·å¯ä»¥è®© GeLU ç‹¬ç«‹åœ°åº”ç”¨äºæ¯ä¸ª GEMM çš„è¾“å‡º</p>
<p>$[Y_1, Y_2]=\begin{bmatrix}\text{GeLU}(XA_1),\text{GeLU}(XA_2)\end{bmatrix}$.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" alt="Column Split of Weight">
    </a><figcaption>Column Split of Weight</figcaption></figure></p>
<p>è¿™ç§åˆ‡åˆ†æ–¹å¼çš„ä¼˜ç‚¹æ˜¯ä¸éœ€è¦è¿›è¡ŒåŒæ­¥æ“ä½œã€‚</p>
<p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä»¥åˆ—å¹¶è¡Œæ–¹å¼åˆ‡åˆ†ç¬¬ä¸€ä¸ª GEMMï¼Œå¹¶æ²¿ç€è¡Œåˆ‡åˆ†ç¬¬äºŒä¸ªGEMMã€‚ç„¶åï¼Œåœ¨å°†è¾“å‡ºä¼ é€’ç»™ dropout å±‚ä¹‹å‰ï¼Œç¬¬äºŒä¸ªGEMM çš„è¾“å‡ºåœ¨ GPU ä¹‹é—´è¿›è¡Œ all-reduce æ“ä½œã€‚è¿™ç§æ–¹æ³•å°† FFN ä¸­çš„ä¸¤ä¸ª GEMM æ‹†åˆ†åˆ°å¤šä¸ª GPU ä¸Šæ‰§è¡Œï¼Œå¹¶ä¸”åªéœ€è¦åœ¨æ­£å‘ä¼ æ’­ (g æ“ä½œç¬¦) å’Œåå‘ä¼ æ’­ (f æ“ä½œç¬¦) ä¸­åˆ†åˆ«æ‰§è¡Œä¸€æ¬¡ all-reduce æ“ä½œã€‚</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" alt="Parallelism of MLP">
    </a><figcaption>Parallelism of MLP</figcaption></figure></p>
<p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›æ“ä½œä¸­æœ¬èº«å­˜åœ¨çš„å¹¶è¡Œæ€§ï¼Œä»¥åˆ—å¹¶è¡Œçš„æ–¹å¼åˆ’åˆ†ä¸ QKV ç›¸å…³çš„ GEMMï¼Œä»¥ä¾¿æ¯ä¸ªæ³¨æ„åŠ›å¤´å¯¹åº”çš„çŸ©é˜µä¹˜æ³•åœ¨ä¸€ä¸ª GPU ä¸Šç‹¬ç«‹å®Œæˆã€‚è¾“å‡ºçº¿æ€§å±‚çš„ GEMM æ²¿ç€å…¶è¡Œå¹¶è¡ŒåŒ–ï¼Œå¹¶ç›´æ¥è·å–å¹¶è¡Œ attention çš„è¾“å‡ºã€‚</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" alt="Parallelism of Self-Attention">
    </a><figcaption>Parallelism of Self-Attention</figcaption></figure></p>
<p>å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™ä½¿èƒ½å¤Ÿä»…åœ¨æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­åˆ†åˆ«ä¸­ä½¿ç”¨ä¸¤ä¸ª all-reduce æ“ä½œæ‰§è¡Œ transformer ä¸­æ‰€æœ‰çš„ GEMM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" alt="Parallelism of Transformer Layer">
    </a><figcaption>Parallelism of Transformer Layer</figcaption></figure></p>
<p>åŸºäº transformer çš„è¯­è¨€æ¨¡å‹çš„è¾“å‡ºåµŒå…¥ç»´åº¦ä¸ºéšè—å±‚å¤§å° (H) ä¹˜ä»¥è¯æ±‡è¡¨å¤§å° (v). æˆ‘ä»¬æ²¿ç€è¯æ±‡è¡¨ç»´åº¦ $E = \begin{bmatrix}E_1,E_2\end{bmatrix}$ å¹¶è¡ŒåŒ–æƒé‡çŸ©é˜µã€‚æ¯ä¸€å—ç°åœ¨åªåŒ…å«åµŒå…¥è¡¨çš„ä¸€éƒ¨åˆ†ï¼Œè¾“å…¥åµŒå…¥åéœ€è¦ä¸€ä¸ª all-reduce (g ç®—å­).</p>
<p>å¯¹äºè¾“å‡ºåµŒå…¥ï¼Œä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡å¹¶è¡Œ $\mathrm{GEMM} [Y_{1},Y_{2}]=[XE_{1},XE_{2}]$ æ¥è·å¾— logitsï¼Œå¹¶å¯¹ç»“æœ all-gather åé€å…¥äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œall-gather é€šä¿¡é‡ä¸º bsv ä¸ªå…ƒç´  (b æ˜¯æ‰¹å¤„ç†å¤§å°ï¼Œs æ˜¯åºåˆ—é•¿åº¦). ä¸ºäº†å‡å°é€šä¿¡è§„æ¨¡ï¼Œæˆ‘ä»¬å°†è¾“å‡ºä¸äº¤å‰ç†µæŸå¤±èåˆï¼Œè¿™æ ·é€šä¿¡é‡é™ä¸º bs.</p>
<p>æˆ‘ä»¬åœ¨æ¯ä¸ª GPU ä¸Šç»´æŠ¤å±‚å½’ä¸€åŒ–å‚æ•°çš„å‰¯æœ¬ï¼Œå¹¶åœ¨å°†è¿™äº›å¼ é‡ä½œä¸ºè¾“å…¥é€åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹å¹¶è¡ŒåŒºåŸŸä¹‹å‰ï¼Œåœ¨æœ¬åœ°è¾“å‡ºä¸Šè¿›è¡Œ dropout å’Œæ®‹å·®è¿æ¥ã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹ï¼Œæˆ‘ä»¬å…è®¸æ¯ä¸ªæ¨¡å‹å¹¶è¡Œ worker ä¼˜åŒ–è‡ªå·±çš„ä¸€ç»„å‚æ•°ã€‚å› ä¸ºæ‰€æœ‰çš„å€¼è¦ä¹ˆæ˜¯æœ¬åœ°çš„ï¼Œè¦ä¹ˆæ˜¯åœ¨ GPUä¸Š é‡å¤çš„ï¼Œæ‰€ä»¥åœ¨è¿™ä¸ªå…¬å¼ä¸­ä¸éœ€è¦é€šä¿¡æ›´æ–°çš„å‚æ•°å€¼ã€‚</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/distributed-training/">Distributed Training</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/">
    <span class="title">Â« Prev</span>
    <br>
    <span>Efficient Large-Scale Language Model Training on GPU</span>
  </a>
  <a class="next" href="http://localhost:1313/blogs/ringattention/">
    <span class="title">Next Â»</span>
    <br>
    <span>Ring Attention Principle</span>
  </a>
</nav>

  </footer><script src="https://giscus.app/client.js"
        data-repo="jamesnulliu/jamesnulliu.github.io"
        data-repo-id="R_kgDOMPCQIw"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPCQI84Cgb2t"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>Â© 2024-2025 WITHER</span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
