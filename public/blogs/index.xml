<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blogs on WITHER</title>
    <link>http://localhost:1313/blogs/</link>
    <description>Recent content in Blogs on WITHER</description>
    <generator>Hugo -- 0.148.1</generator>
    <language>en</language>
    <copyright>2024-2025 WITHER</copyright>
    <atom:link href="http://localhost:1313/blogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TX8 Inference Engine</title>
      <link>http://localhost:1313/blogs/tx8_inferenceengine/</link>
      <pubDate>Thu, 07 Aug 2025 21:47:33 +0800</pubDate>
      <guid>http://localhost:1313/blogs/tx8_inferenceengine/</guid>
      <description>TX8 inference engine description.</description>
      <content:encoded><![CDATA[<h1 id="overview">Overview</h1>
<ol>
<li>vLLM 调用 txda 初始化 context. 程序结束后释放 context.</li>
<li>vLLM 根据 hf_dir 中的配置文件创建模型, 其中 config.json 决定使用 TxNN 中定义的哪个 model class; 也决定了这个 class init 时的参数是什么.</li>
<li>调用 TxNN 的 model class 的 load weight 函数和 forward 函数, 具体实现都依赖于 TxDA.</li>
</ol>
<h1 id="argument">Argument</h1>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">parse_args</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--hf_dir&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Huggingface model path&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_tokens&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Max generated tokens&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--tp_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Model TP size&#39;</span><span class="p">)</span>  <span class="c1"># tensor parallel degree, must Keep consistent with model.json</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;--device&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nb">type</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">        <span class="n">default</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">help</span><span class="o">=</span><span class="s2">&#34;Tx device idxs; E.g., --device 0,1,2,3&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&#34;--log_file&#34;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&#34;Log file path&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Get case_dir from huggingface model config</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">hf_dir</span><span class="si">}</span><span class="s2">/config.json&#34;</span><span class="p">,</span> <span class="s2">&#34;r&#34;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&#34;utf-8&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hf_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">args</span><span class="o">.</span><span class="n">case_dir</span> <span class="o">=</span> <span class="n">hf_config</span><span class="p">[</span><span class="s2">&#34;param_dir&#34;</span><span class="p">]</span>  <span class="c1"># kcorebin path, with model_info.json and chip0/, chip1/ ... in the directory</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;Inference info:</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - hf_dir: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">hf_dir</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - case_dir: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">case_dir</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - max_tokens: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">max_tokens</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - tp_size: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">tp_size</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - ENV:TXDA_VISIBLE_DEVICES: &#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;TXDA_VISIBLE_DEVICES&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;Not specified&#39;</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - device: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - log_file: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">log_file</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">args</span>
</span></span></code></pre></div><ul>
<li><code>hf_dir</code>: 指的是模型的 hugging_face 路径。</li>
<li><code>case_dir</code>: 是解析 <code>hf_dir</code> 下的 config.json 文件的字段，指的是后端生成的 kcore 文件夹路径。除此之外还需要在 json 文件中额外配置并行度相关信息。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="nt">&#34;param_dir&#34;</span><span class="p">:</span> <span class="s2">&#34;...&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;tensor_parallel&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;use_tp&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;parallel_size&#34;</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>会根据我们传入的 <code>--device</code> 和 <code>tp_size</code> 参数创建逻辑到实际物理芯片的映射。这里 InternLM3 使用的是 4 卡机器，张量并行度为 2. 计算图中 batchsize=1.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// setDeviceIdx
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;</span> <span class="n">c4_TP2x2</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">}},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;</span> <span class="n">c4_TP2x2_hw</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">}},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">auto</span> <span class="n">set_TP2_C4_Map</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">user_map</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">hw_map</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&amp;</span> <span class="n">user_settings</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it_TP2x2</span> <span class="p">:</span> <span class="n">user_map</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">user_settings</span> <span class="o">==</span> <span class="n">it_TP2x2</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">device_setting_</span> <span class="o">=</span> <span class="n">hw_map</span><span class="p">[</span><span class="n">it_TP2x2</span><span class="p">.</span><span class="n">first</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">TxdATPMode</span><span class="o">::</span><span class="n">C4TP2B1</span> <span class="o">==</span> <span class="n">tp_mode_</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_TP2_C4_Map</span><span class="p">(</span><span class="n">c4_TP2x2</span><span class="p">,</span> <span class="n">c4_TP2x2_hw</span><span class="p">,</span> <span class="n">user_settings</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>TxdaContext::initDevice 的主要功能是初始化设备 (TsmDevice)，并将其封装为 TxdaDevice 对象，最终存储到设备组 (device_group_) 中。</p>
<p>TxdaContext::initChipProperty 的主要功能是初始化芯片相关的属性和配置，包括设备组、模型数量、环境变量解析、内存地址初始化以及设备启动参数的设置。</p>
<p>TxdaContext::initModel：负责初始化模型，解析案例目录，设置 TP 大小和 TP 模式，并调用 TxdaModel::init 初始化每个模型 的基本属性，包括案例目录、JSON 配置和编译选项。</p>
<h1 id="txnn">TxNN</h1>
<p>configuration_internlm3.py 中主要负责模型结构参数的初始化 (根 hugging face 中的 config.json 保持一致) 以及张量并行的设置。</p>
<p>input_output_internlm3.py 负责在 python 端为要从后端加载进来的输入和输出注册张量和开辟空间。-</p>
<ul>
<li>InputBuffer 会创建一个 IntermediateKVCache 实例。这是 KV Cache 的实际存储空间，大小根据序列长度、头的数量预先分配好。还会注册各种输入相关的缓冲区
<ul>
<li>input_ids: 当前需要处理的输入 token ID。</li>
<li>start_index, seq_len: 用于管理当前生成序列的位置信息。</li>
<li>k_gather_end_indices: 为每一层配置 对应的 KV cache 的起始索引。</li>
</ul>
</li>
</ul>
<p>OutputBuffer 和 InputBuffer 类似，它也创建了一个 IntermediateKVCache 实例。这个 Cache 用于存放计算完成后更新的 Key 和 Value，这些更新后的值将在下一步生成token时作为输入使用。</p>
<ul>
<li>注册所有KV Cache层: 它立即将所有层的 KV Cache 缓冲区注册到自己的 _buffer 中。</li>
<li>注册输出缓冲区: 预先分配并注册了用于存放最终结果的内存空间：
<ul>
<li>lm_head_reshape_out: 用于存放语言模型最后的输出（通常是 logits），其维度为 (序列长度, 词汇表大小)。</li>
<li>lm_head_argmax_out: 用于存放最终预测出的 token ID（对 logits 取 argmax 后的结果）。</li>
</ul>
</li>
</ul>
<p>里面负责定义模型，在 python 端为数据开辟好内存，GraphBasedInternlm3ForCausalLM 在初始化时，会创建 InputBuffer 和 OutputBuffer 的实例。它为 tensor_parallel 的每一路都创建了缓冲区。</p>
<p>当 forward 方法接收 input_ids. 不在 Python 中执行复杂的数学运算。而是通过 load_input 把后端编译生成的 bin 文件数据填充到 InputBuffer 中.</p>
<p>通过 TxDA context 把加载好的数据放到硬件上然后 launch kernel将 InputBuffer 和 OutputBuffer 的内存地址传递给底层引擎。底层引擎直接从 InputBuffer 指向的内存中读取输入，执行高效的模型计算，然后将结果直接写入 OutputBuffer 指向的内存中。</p>
<h1 id="vllm">vllm</h1>
<p>主要利用的是其推理框架，在 LLMEngine 里添加了自己的 TxNPUExecutor 里面包含自己的 TxNPUWorker，其中包含 TxNPUCacheEngine 和 TxNPUModelRunner.</p>
<p>TxNPUMModelRunner 定义了专门从后端编译出的 chip_out 文件夹中的 param.bin 文件里加载模型参数到 TxNN 的定义模型的 ModelLoader. 然后推理的时候调用的 forward 流程如上。</p>
]]></content:encoded>
    </item>
    <item>
      <title>TX8 Backend</title>
      <link>http://localhost:1313/blogs/tx8_backend/</link>
      <pubDate>Wed, 23 Jul 2025 11:49:02 +0800</pubDate>
      <guid>http://localhost:1313/blogs/tx8_backend/</guid>
      <description>TX8 backend description.</description>
      <content:encoded><![CDATA[<h1 id="tx8-hardware-overview">TX8 Hardware Overview</h1>
<p>TX8 采用的是空间计算型结构 (Special Computing Architecture)，市面上普遍采用的共享内存结构 (Shared Memory Architecture)，它的数据通信交互主要是依赖于 DDR，一个 thread 把 DDR 的数据改变之后，另外一个 thread 再从 DDR 中才能得知到这个数据已经被改变。这么做有一个很明显的缺陷，就是它瓶颈在于内存容量以及访问内存的带宽延迟。空间计算型的结构它是由中间的NOC (Network On Chip) 来构成模块之间的互联。这样很好的避免了这个 DDR 的瓶颈，同时也有了更好的 scale out 能力。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB04f094c9d80d3990221020a51ce93433?method=download&amp;shareKey=f6149480beae12e20d31f124e425ef84" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB04f094c9d80d3990221020a51ce93433?method=download&amp;shareKey=f6149480beae12e20d31f124e425ef84" alt="(a) Shared Memory Architecture (b) Spatial Computing Architecture">
    </a><figcaption>(a) Shared Memory Architecture (b) Spatial Computing Architecture</figcaption></figure></p>
<p>下图为 TX8 两个芯片互连的逻辑结构。每个芯片由 4x4 总计 16 Tile 以 mesh 拓扑结构进行互连。每一个 Tile 是一个计算核心，是一个图灵完备 (Turing Complete) 的系统，既具有调度控制以及计算通信以及存储的能力。片上 NoC 采用的是 stream (一种轻量级 DMA 技术). 片上 DDR 大小为 64GB，芯片之间是通过 high speed IO 进行互连的。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB98d1d8b5916da28e40cf3c77b63dcbe6?method=download&amp;shareKey=1ca36bb999bfe3f524fc2525e1cb0ab7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB98d1d8b5916da28e40cf3c77b63dcbe6?method=download&amp;shareKey=1ca36bb999bfe3f524fc2525e1cb0ab7" alt="Tile">
    </a><figcaption>Tile</figcaption></figure></p>
<p>单芯片与单卡 A100 性能对比如下表所示</p>
<table>
  <thead>
      <tr>
          <th>TX8</th>
          <th>单卡性能</th>
          <th>最大组网性能</th>
          <th>A100</th>
          <th>单卡性能</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>INT8</strong></td>
          <td>256T</td>
          <td>1E</td>
          <td></td>
          <td>624T</td>
      </tr>
      <tr>
          <td><strong>BF16</strong></td>
          <td>128T</td>
          <td>0.5E</td>
          <td></td>
          <td>312T</td>
      </tr>
      <tr>
          <td><strong>TFP32</strong></td>
          <td>128T</td>
          <td>0.5E</td>
          <td></td>
          <td>156T</td>
      </tr>
      <tr>
          <td><strong>FP32</strong></td>
          <td>21T</td>
          <td>40P</td>
          <td></td>
          <td>19.5T</td>
      </tr>
      <tr>
          <td><strong>内存带宽</strong></td>
          <td>200GB/s</td>
          <td>-</td>
          <td><strong>显存带宽</strong></td>
          <td>1935GB/s</td>
      </tr>
      <tr>
          <td><strong>PCIe</strong></td>
          <td>64GB/s</td>
          <td>-</td>
          <td></td>
          <td>64GB/s</td>
      </tr>
      <tr>
          <td><strong>内存容量</strong></td>
          <td>64GB</td>
          <td>128TB</td>
          <td><strong>显存容量</strong></td>
          <td>80GB</td>
      </tr>
      <tr>
          <td><strong>TsingMicro-Link</strong></td>
          <td>1600Gbps</td>
          <td>-</td>
          <td><strong>NV-Link</strong></td>
          <td>600GB/s</td>
      </tr>
  </tbody>
</table>
<h1 id="single-tile">Single Tile</h1>
<p>下图是单 Tile 的硬件结构，实际上每个 Tile 上会有两个 kernel core 和 special core，图中只画了一个。还有个 neural core，主要是负责计算以及数据搬运等等。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8e08cce47b2de4d373c8dd667a988463?method=download&amp;shareKey=ba34989999d6febf20e7abeda09a81b2" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8e08cce47b2de4d373c8dd667a988463?method=download&amp;shareKey=ba34989999d6febf20e7abeda09a81b2" alt="Tile Microarchitecture">
    </a><figcaption>Tile Microarchitecture</figcaption></figure></p>
<ul>
<li>
<p>kernel core 主要用于下发指令。它会从 DDR 中取址，然后送到这个 neural core 的 NCC controller 里面。NCC controller 又会把根据这个指令的类型下发到 CT/NE/LSU. 他们三个是执行不同种类指令的三个小模块，后面会讲到。这三个小模块会从 SPM (Scratched Pad Memory) 上读取数据，然后再计算，或者再存回 SPM上。值得注意的是，LSU 是用来负责这个数据搬运的，所以它可以把这个 SPM 上的数据直接搬到DDR，或者是从 DDR 搬到 SPM 上。CT 和 NE 都是负责计算的模块，其中 scalar unit 位于 NCC controller，是一个负责标量计算的模块。</p>
</li>
<li>
<p>special core 用来和 NOC 进行连接，它可以从 DDR 中读取数据，然后通过配置 DTE 模块和这个远程的 Tile 进行通信。DTE 模块也可以通过 special core 将本 Tile 上的 SPM 与远程 Tile 上的 SPM 进行通信。</p>
</li>
</ul>
<h1 id="cgra-tensor">CGRA Tensor</h1>
<p>CGRA Tensor 模块支持算术运算，逻辑运算，位操作，激活函数，超越函数，规约，池化，数据搬移，格式转换，辅助计算。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9086e7454a92ac6077331ed5c7f4fc56?method=download&amp;shareKey=834faad50224b52bd4c82ccf738f5293" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9086e7454a92ac6077331ed5c7f4fc56?method=download&amp;shareKey=834faad50224b52bd4c82ccf738f5293" alt="CGRA">
    </a><figcaption>CGRA</figcaption></figure></p>
<p>Neural Core Controller 下发指令到 CTRL_UNIT，然后 CTRL_UNIT 下发指令到 RAM_ACC_UNIT. RAM_ACC_UNIT 读入 SPM 的数据，然后送入 Pipe Unit 进行运算之后把结果存回 SPM.</p>
<p>CGRA 指令格式如下。例如 CGRATensor_ArithOp_V_V_abs，指令操作指的是对向量元素求绝对值。</p>
<table>
  <thead>
      <tr>
          <th>指令格式</th>
          <th>CGRATensor_function_format_name.type</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Function</strong></td>
          <td>描述该单元的主要功能，如算数运算、关系运算、逻辑运算等；</td>
      </tr>
      <tr>
          <td><strong>Format</strong></td>
          <td>描述数据的存储方式，如VV、VS、Tensor、VuV 分别表示</br>向量与向量计算、向量与标量计算、Tensor计算、向量与单元向量计算；</td>
      </tr>
      <tr>
          <td><strong>Name</strong></td>
          <td>描述具体的操作，如加、减、乘、除等；</td>
      </tr>
      <tr>
          <td><strong>Type</strong></td>
          <td>表示数据类型，如 bf16/fp32 等；</td>
      </tr>
  </tbody>
</table>
<p>下面具体讲一下在 BN 算子开发中用到的 <code>CGRATensor_ArithOp_V_VuV_mul_loop (bf16 *src, bf16 *dst, bf16 *unit, int rnd, int src_elem_num, int unit_elem_num, int full_src_elem_num, int  full_unit_elem_num)</code>.</p>
<ul>
<li>src/dst/unit 分别表示 也是原数据/存数/单元向量的地址。</li>
<li>src_elem_num 是做一次这个 VuV 中原数据的个数。</li>
<li>unit_elem_num 是做一次这个 VuV 中单元向量数据的个数。</li>
</ul>
<p>在讲 VuV_mul_loop 之前，先来看一下这个 VuV_mul 也就是没有循环的单次版本。分为两次进行，第一次是前四个蓝色的方块与橙色方块相乘，第二次为后四个蓝色方块与橙色方块相乘。VuV_mul_loop 即把这个过程重复很多次，所以要求 <code>full_src_elem_num/full_unit_elem_num == src_elem_num/unit_elem_num</code>，并且<code>unit_elem_num=64</code>.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB367ed5ab2178c80abdeb160cb55b409d?method=download&amp;shareKey=05e534d23de8ab1be6cef33b8c8e0e4e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB367ed5ab2178c80abdeb160cb55b409d?method=download&amp;shareKey=05e534d23de8ab1be6cef33b8c8e0e4e" alt="VuV_mul_loop">
    </a><figcaption>VuV_mul_loop</figcaption></figure></p>
<h1 id="tensor-layout">Tensor Layout</h1>
<p>layout 可以分为以下几种</p>
<ul>
<li>layout_str: 中端使用
<ul>
<li>CNN Op: 1. Feature (NCHW/NHWC) etc. 2. Weight (OIHW/HWOI) etc.</li>
<li>Non-CNN Op: 大模型中常见，Tensor/NTensor，它们的区别是第 0 维是否为 1.</li>
</ul>
</li>
<li>mem_layout: 后端使用，代表了在芯片上的实际排布
<ul>
<li>Tensor/NTensor: 数据的紧密排布</li>
<li>Cx/NCx: 对 Tensor/NTensor 格式化后的结果，方便易硬件读取。</li>
</ul>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>dtype</th>
          <th>channel</th>
          <th>description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>bf16/fp16 <br>/fp32/tf32</td>
          <td>c &lt;= 32</td>
          <td>NHWC, C向4/8/16/32对齐，N 的起始地址向 2048bit 对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 32</td>
          <td>N[CxHW64, HWC0], C0 向 4/8/16/32 对齐，N 的起始地址向2048bit 对齐<br>在一个 batch 内将 tensor 按 C 分成 Cx*64 和 C0两部分</td>
      </tr>
      <tr>
          <td>int8</td>
          <td>c &lt;= 64</td>
          <td>NHWC, C 向 4/8/16/32/64对齐，N的起始地址向2048bit对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 64</td>
          <td>N[CxHW128, HWC0], C0 向 4/8/16/32/64 对齐，N的起始地址向 2048bit 对齐 <br> 在一个 batch 内将 tensor 按 C 分成 Cx*128 和C0 两部分</td>
      </tr>
  </tbody>
</table>
<p>对于 fp16 的 2x1x2x131 的数据，NTensor 格式存储起始地址为 0x0000 按各存储格式排列如下</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB47732820f52aa7a0bfd0d22bb2e61e00?method=download&amp;shareKey=03c9e8f103abf576755a3e33e5d5cbc5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB47732820f52aa7a0bfd0d22bb2e61e00?method=download&amp;shareKey=03c9e8f103abf576755a3e33e5d5cbc5" alt="NTensor Layout">
    </a><figcaption>NTensor Layout</figcaption></figure></p>
<p>NCx: 131 = 64 x 2 + 3, 将 C 分成 2(Cx) 个 64 和 4(C0). batch0 的结束地址是 0x1080 (4224), batch1 起始地址需对齐到 2048bit，即 4224&ndash;&gt;2048*3=6144 (0x1800).</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB53406947a6dafa2bbe98df922f46a954?method=download&amp;shareKey=3e75616e0238e7360bd6930b98941909" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB53406947a6dafa2bbe98df922f46a954?method=download&amp;shareKey=3e75616e0238e7360bd6930b98941909" alt="NCx Layout">
    </a><figcaption>NCx Layout</figcaption></figure></p>
<h1 id="neural-engine">Neural Engine</h1>
<p>Neural engine 类似于 GPU Tensor Core，主要是完成各种矩阵 (op_Gemm) 和卷积 (op_Conv) 类型的高效并行 Tensor 计算。PE Array 它的进行矩阵运算的部分，一次完成 8x16x8 大小的矩阵乘法。然后它的输入有激活 input，还有 psum，还有 weight，也就是权重。</p>
<p>计算之后，还饿可以进行后处理，对这个结果进行 BN/量化/激活等等，然后再到输出，然后我们要用到neural engine 的算子其实并不多，只有 op_Gemm 和 op_Conv.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBf07a7034750a8723bf35f5cb311251e2?method=download&amp;shareKey=d0c242fdf504441bb4284c72b48908c4" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBf07a7034750a8723bf35f5cb311251e2?method=download&amp;shareKey=d0c242fdf504441bb4284c72b48908c4" alt="Neural Engine">
    </a><figcaption>Neural Engine</figcaption></figure></p>
<h1 id="lsu">LSU</h1>
<p>LSU 是负责数据搬运的 DMA 控制器。具体它有三部分:</p>
<ul>
<li>RDMA: Read DDR &ndash;&gt; SPM，对应指令有 op_loadVar，op_loadConst，op_rdmaGather.</li>
<li>WDMA: Write SPM &ndash;&gt; DDR，对应指令有 op_dma_store，op_wdmaScatter.</li>
<li>TDMA: 对所属 Tile SPM 上的数据进行操作，对应指令有 op_reshape，op_gatherScatter.</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd7f94b1349b939c2c20a37abf5e57bbc?method=download&amp;shareKey=fb7dd9facaf7ca29424c72eaa4991000" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd7f94b1349b939c2c20a37abf5e57bbc?method=download&amp;shareKey=fb7dd9facaf7ca29424c72eaa4991000" alt="LSU">
    </a><figcaption>LSU</figcaption></figure></p>
<p>一种经常使用 TDMA 的情况是进行低精度到高精度的转换。以 fp16 -&gt; fp32 为例，首先会调用 op_gatherScatter 指令把紧密排布的低精度数据读进来然后 scatter 到 SPM 上的对应位置以保留空间存储转换后的数据；然后再调用 CGAR convert_fp16_fp32 指令进行精度转换。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB1079912e78f03abc7a30d0db12ffb046?method=download&amp;shareKey=20f9d8abd3fb47b3194540d639a2f9ee" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB1079912e78f03abc7a30d0db12ffb046?method=download&amp;shareKey=20f9d8abd3fb47b3194540d639a2f9ee" alt="fp16 to fp32 Conversion">
    </a><figcaption>fp16 to fp32 Conversion</figcaption></figure></p>
<h1 id="tx8-compiler">TX8 Compiler</h1>
<p>和一般编译器差不多，先获取前端的 Tensorflow/Pytorch 等等生成的 mhlo 计算图，经过中端的处理，然后转到后端。变成后端 IR. 同时又会调用 OPLIB 算子库中的算子来生成 main.c，就是可以直接放在不同平台上运行的主程序。平台可以选择 RISCV 即真实的硬件，或者是 Cmodel 进行模拟。</p>
<p>BEIR 主要是接过中端传进来的 IR，然后进行各类的图优化的 Pass，包括一些算子切分，还有内存调度等等。最终 codegen 这个可编译执行的 main.c 的文件。然后再放在平台上去编译完再运行。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB95d5755abd98d22b6bcdaca440c0e8c4?method=download&amp;shareKey=e72432cabeea4cbab293db667ecb0648" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB95d5755abd98d22b6bcdaca440c0e8c4?method=download&amp;shareKey=e72432cabeea4cbab293db667ecb0648" alt="TX8 Compiler Workflow">
    </a><figcaption>TX8 Compiler Workflow</figcaption></figure></p>
<h1 id="tx8-be">TX8 BE</h1>
<p>后端 IR 使用的是 MLIR，继承 Dialect，定义了许多 Operations, Attributes, Types.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_Dialect <span class="p">:</span> Dialect <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">name =</span> <span class="s">&#34;tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;A low-level dialect for tx8 backend specification&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">useDefaultAttributePrinterParser =</span> <span class="m">1</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="attribute">Attribute</h2>
<p>下面介绍一些常用的 Attribute.</p>
<p><code>parallel_attr</code> 主要是表示 tensor 每个维度上数据并行和张量并行的切分策略。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_ParallelAttr <span class="p">:</span> Tx8be_Attr<span class="p">&lt;</span><span class="s">&#34;Parallel&#34;</span><span class="p">,</span> <span class="s">&#34;parallel_attr&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;Structure of parallel information.&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">parameters =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        <span class="s">&#34;ParallelModeAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>is_dp_inner<span class="p">,</span>    <span class="c">// dp dimension is in the inner, otherwise tp
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_x<span class="p">,</span>    <span class="c">// data parallel dimension at x axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_y<span class="p">,</span>    <span class="c">// data parallel dimension at y axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_z<span class="p">,</span>    <span class="c">// data parallel dimension at z axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_x<span class="p">,</span>    <span class="c">// tensor parallel dimension at x axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_y<span class="p">,</span>    <span class="c">// tensor parallel dimension at y axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_z<span class="p">,</span>    <span class="c">// tensor parallel dimension at z axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>sharding_is_given<span class="p">,</span>    <span class="c">// true: is given, false: is not
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;::mlir::DenseI32ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>shape_spatial_sharding    <span class="c">// Shape split info
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">assemblyFormat =</span> <span class="s">&#34;`&lt;` struct($params) 1&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>dev_attr</code> 属性包含</p>
<ul>
<li>imm_size，也就是用到的这个辅助空间的大小。</li>
<li>mem_layout 也就是数据的存储数据的排布。</li>
<li>multi_buf_en 指是否使用 double buffer.</li>
<li>out_shape_buf_idx 指的是输出使用第几个缓冲区。</li>
<li>temporal_mem_slice 是单个 Tile 每次处理的数据大小。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_DevAttr <span class="p">:</span> Tx8be_Attr<span class="p">&lt;</span><span class="s">&#34;Dev&#34;</span><span class="p">,</span> <span class="s">&#34;dev_attr&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;Structure of op parameters on device.&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">parameters =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        <span class="s">&#34;uint64_t&#34;</span> <span class="p">:</span> <span class="err">$</span>imm_size<span class="p">,</span>    <span class="c">// Output memory addr offset
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;LayoutModeAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>mem_layout<span class="p">,</span>    <span class="c">// Layout
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>multi_buf_en<span class="p">,</span>    <span class="c">// for double buffering
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int32_t&#34;</span> <span class="p">:</span> <span class="err">$</span>multi_buf_num<span class="p">,</span>    <span class="c">// for double buffering
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>out_shape_buf_idx<span class="p">,</span>    <span class="c">// index for dynamic shape buffer on runtime
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>temporal_mem_slice<span class="p">,</span>    <span class="c">// for compute local buffer size
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int32_t&#34;</span> <span class="p">:</span> <span class="err">$</span>source_type<span class="p">,</span>    <span class="c">// Software pipeline stage
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int64_t&#34;</span> <span class="p">:</span> <span class="err">$</span>imm_addr<span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>mem_addr    <span class="c">// use array for multibuffer
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">assemblyFormat =</span> <span class="s">&#34;`&lt;` struct($params) `&gt;`&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>MemScopeMode</code> 用于描述数据存储在哪里。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_MemScopeMode <span class="p">:</span> I32EnumAttr<span class="p">&lt;</span><span class="s">&#34;MemScopeMode&#34;</span><span class="p">,</span> <span class="s">&#34;Specify the memory scope&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="p">[</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;DDR&#34;</span><span class="p">,</span> <span class="m">0</span><span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;SPM&#34;</span><span class="p">,</span> <span class="m">1</span><span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;3DDRAM&#34;</span><span class="p">,</span> <span class="m">2</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        let <span class="nl">genSpecializedAttr =</span> <span class="m">0</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">        let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></div><h2 id="types">Types</h2>
<p>定义了很多类型，实际上常用的就是 AnyTensorOrNone.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def AnyTensorOrNone<span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">,</span> NoneType<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Tuple <span class="p">:</span> NestedTupleOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def AnyTensorOrTuple <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">,</span> Tx8be_Tuple<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Pred <span class="p">:</span> TypeAlias<span class="p">&lt;</span>I1<span class="p">,</span> <span class="s">&#34;pred (AKA boolean or 1-bit integer)&#34;</span><span class="p">&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_PredTensor <span class="p">:</span> TensorOf<span class="p">&lt;[</span>Tx8be_Pred<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Token <span class="p">:</span> Type<span class="p">&lt;</span>CPred <span class="s">&#34;{$_self-&gt;isa&lt;TokenType&gt;()}&#34;</span><span class="p">,</span> <span class="s">&#34;token&#34;</span><span class="p">&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_TensorOrTokenOrTuple <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyTensor<span class="p">,</span> Tx8be_Token<span class="p">,</span> Tx8be_Tuple<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_SInt <span class="p">:</span> SignlessIntOfWidths<span class="p">&lt;[</span><span class="m">4</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="m">16</span><span class="p">,</span> <span class="m">32</span><span class="p">,</span> <span class="m">64</span><span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_UInt <span class="p">:</span> UnsignedIntOfWidths<span class="p">&lt;[</span><span class="m">4</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="m">16</span><span class="p">,</span> <span class="m">32</span><span class="p">,</span> <span class="m">64</span><span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Int <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>Tx8be_SInt<span class="p">,</span> Tx8be_UInt<span class="p">]&gt;</span><span class="err">;</span>
</span></span></code></pre></div><h2 id="operations">Operations</h2>
<p>以开发的 BatchNorm_InferenceOp 为例讲解一下 Tx8be 中关于算子的定义。首先 batchnorm 是将通道维度视作样本，计算其他维度的平均值和方差后进行归一化的操作。</p>
$$
\begin{aligned}
BatchNorm\colon y&=\gamma\:\frac{x-Mean(x)}{\sqrt{Var(x)+\varepsilon}}+\beta\\
Mean(x)&=\frac{1}{N}\sum_{i=1}^{N}x_{i}\\
Var(x)&=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-Mean(x))^{2}\end{aligned}$$<p>中括号内是一些需要继承的 <a href="https://mlir.llvm.org/docs/Interfaces/">Interface</a>. 其允许 attributes, operations 和 types 公开方法调用接口，而不需要调用者知道特定的派生类型。</p>
<p>arguments 指定了算子需要的输入，包括参数以及之前介绍到的一些属性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_BatchNorm_InferenceOp <span class="p">:</span> Tx8be_Op<span class="p">&lt;</span><span class="s">&#34;BatchNorm_Inference&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span>DeclareOpInterfaceMethods<span class="p">&lt;</span>oplibinterface<span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">    DeclareOpInterfaceMethods<span class="p">&lt;</span>ShardingInterface<span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">    DeclareOpInterfaceMethods<span class="p">&lt;</span>ComputeInterface<span class="p">&gt;]</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;BatchNorm inference&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">        Normalizes the operand <span class="kt">tensor</span> across all dimensions except for the c dimension
</span></span><span class="line"><span class="cl">        and produce a result <span class="kt">tensor</span><span class="p">.</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">arguments =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>input<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>scale<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>offset<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>mean<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>variance<span class="p">,</span>
</span></span><span class="line"><span class="cl">        DefaultValueOptionalStrAttr<span class="p">&lt;</span>StrAttr<span class="p">,</span> <span class="s">&#34;Unknown&#34;</span><span class="p">&gt;:</span><span class="err">$</span>layout_str<span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="c">// The following are backend parameters
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        OptionalAttr<span class="p">&lt;</span>Tx8be_ParallelAttr<span class="p">&gt;:</span><span class="err">$</span>chip_parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        OptionalAttr<span class="p">&lt;</span>Tx8be_ParallelAttr<span class="p">&gt;:</span><span class="err">$</span>tile_parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        OptionalAttr<span class="p">&lt;</span>Tx8be_DevAttr<span class="p">&gt;:</span><span class="err">$</span>dev_info
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">results =</span> <span class="p">(</span>outs AnyTensor<span class="p">:</span><span class="err">$</span>output<span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="interface">Interface</h2>
<p>Interface 定义一些通用的方法或行为，这些方法没有具体实现。要通过继承某个 Interface 来具体实现该接口的方法和行为。tx8中定义了 5 个 Interface: OpLibInterface, ComputeInterface, ShapeInferenceOpInterface, ShardingInterface, StreamConfigInterface.</p>
<p>BatchNorm 算子开发中只用到了前四个，下面依次介绍一下。</p>
<p><code>ShapeInferenceOpInterface</code> 定义了两个方法 <code>inferShapes</code> 和 <code>inferLayout</code>. 继承这个接口的话就需要实现这两种方法。根主要是根据输入来推断输出的形状和布局。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ShapeInferenceOpInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ShapeInferenceOpInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="p">[{</span> <span class="p">}],</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;inferShapes&#34;</span><span class="p">,</span>  <span class="c">// method name
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;DynamicShapeParam&#34;</span> <span class="p">:</span> <span class="err">$</span>shapeParam<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="p">[{</span> <span class="p">}],</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;inferLayout&#34;</span><span class="p">,</span>  <span class="c">// method name
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>由于 batchnorm 不对这两者进行改变，因此输出和输入相同。如果是需要改变的算子比如 transpose 就需要进行改变。</p>
<p><code>input_data &lt;shape=3x4x5x6, layout=NCHW&gt; --&gt; transpose&lt;permutation=(0,2,3,1)&gt; --&gt; output_data&lt;shape=3x5x6x4, layout=NHWC&gt;</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// BatchNorm_Interface.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">inferLayout</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">in_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">getInput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">cur_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">in_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">i_layout</span> <span class="o">=</span> <span class="n">in_op</span><span class="o">-&gt;</span><span class="n">getAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">).</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">StringAttr</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getValue</span><span class="p">().</span><span class="n">str</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">StringAttr</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">i_layout</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">in_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;dev_info&#34;</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">i_dev_layout</span> <span class="o">=</span> <span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">in_op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">cur_op</span><span class="p">,</span> <span class="n">i_dev_layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">inferShapes</span><span class="p">(</span><span class="n">DynamicShapeParam</span> <span class="o">&amp;</span><span class="n">shapeParam</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">getOutput</span><span class="p">().</span><span class="n">setType</span><span class="p">(</span><span class="n">getInput</span><span class="p">().</span><span class="n">getType</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="shardinginterface">ShardingInterface</h2>
<p><code>tileShardingSplit</code> 和前面的 <code>inferShapes</code> 以及 <code>inferLayout</code> 不一样。后两者是从输入信息推出输出的信息。而 <code>tileShardingSplit</code> 是由输出的的切分的因子来推断出各个输入的切分因子。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8d4fed659394922243574186cf74ef3a?method=download&amp;shareKey=d1461507273efadf9613b1496fd1501c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8d4fed659394922243574186cf74ef3a?method=download&amp;shareKey=d1461507273efadf9613b1496fd1501c" alt="BatchNorm ShardingInterface">
    </a><figcaption>BatchNorm ShardingInterface</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ShardingInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ShardingInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*</span><span class="err">/</span><span class="p">[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="c">// vector for diff operand&#39;s info
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::ShardingSplitParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;tileShardingSplit&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;ShardingSplitParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::SliceParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;temporalSliceShape&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;SliceParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::WindowParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;backWindow&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;const WindowParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ul>
<li>Sharding 是空间上的切分，意思是将数据分散到不同的 Tile 上。</li>
<li>Split 是时间上的切分，意思是切分到 Tile 上的将数据按流水线方式轮流进行 load.</li>
</ul>
<p><code>temporalSliceShape</code> 返回的是 sharding + split 后一个 Tile 上单次处理的数据的实际 shape.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB4495c3579079b37d0c2288cc51408601?method=download&amp;shareKey=0513d93d3b51c4782d56c38acb83d0d5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB4495c3579079b37d0c2288cc51408601?method=download&amp;shareKey=0513d93d3b51c4782d56c38acb83d0d5" alt="BatchNorm Sharding Split">
    </a><figcaption>BatchNorm Sharding Split</figcaption></figure>
根据 batchnorm 算子定义 input 只能在通道维度上 sharding.
split 有两种选择</p>
<ol>
<li>对于 input 和 mean，var，scale，shift 都在 C 维度上做相同的切分。</li>
<li>不再 split mean，var，scale，shift，只对 input 的 NHW 进行 split.</li>
</ol>
<p>这里采用的是后者。由于 mean, variance, scale, shift 都是 1x1x1xC 的张量，因此 split 为 (1, 1, 1, 1). 切分搜索得到的符合要求的 ShardingSplitParam (下图中为 cn3) 会继续向上传递。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBc472f9ed0d922130ec05d93efed54186?method=download&amp;shareKey=8bf08e0631cd4c7880b9756969fd4bef" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBc472f9ed0d922130ec05d93efed54186?method=download&amp;shareKey=8bf08e0631cd4c7880b9756969fd4bef" alt="Sharding Split Search">
    </a><figcaption>Sharding Split Search</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ShardingSplitParam</span><span class="o">&gt;</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">tileShardingSplit</span><span class="p">(</span><span class="n">ShardingSplitParam</span> <span class="o">&amp;</span><span class="n">param</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">getOutput</span><span class="p">().</span><span class="n">getType</span><span class="p">().</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">param</span><span class="p">.</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">param</span><span class="p">.</span><span class="n">outSplit</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape_size</span> <span class="o">=</span> <span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ShardingSplitParam</span><span class="o">&gt;</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">param</span><span class="p">);</span> <span class="c1">// input
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">shape_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// can only shard in dim C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">[</span><span class="n">shape_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// can only split except dim C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramMean</span><span class="p">;</span> <span class="c1">// scale/shift/mean/variance
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">paramMean</span><span class="p">.</span><span class="n">outSharding</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">paramMean</span><span class="p">.</span><span class="n">outSplit</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">shape_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// shape is 1x1x1xC，split must be (1, 1, 1, 1)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramVar</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramScale</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramShift</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramScale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramShift</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramMean</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramVar</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="oplibinterface">OpLibInterface</h2>
<p><code>OpLibInterface</code> 有四个方法，</p>
<ul>
<li><code>genOpCode</code>: 生成 main.c 文件的时候所调用的一个接口。</li>
<li><code>getOpClockCycle</code>: 获取 OP 的执行时间。</li>
<li><code>getImmSpSize</code>: 获取 SPM 上临时空间所需要的大小。</li>
<li><code>queryOpAttr</code>: 查询这个 OP 的一些属性。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">def</span> <span class="nl">OpLibInterface</span> <span class="p">:</span> <span class="n">OpInterface</span><span class="o">&lt;</span><span class="s">&#34;OpLibInterface&#34;</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">description</span> <span class="o">=</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">        <span class="n">These</span> <span class="n">are</span> <span class="n">the</span> <span class="n">interfaces</span> <span class="k">for</span> <span class="n">connecting</span> <span class="n">tx8be</span><span class="o">-</span><span class="n">oplib</span>
</span></span><span class="line"><span class="cl">        <span class="n">and</span> <span class="n">codegen</span><span class="p">.</span>
</span></span><span class="line"><span class="cl">    <span class="p">}];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">cppNamespace</span> <span class="o">=</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">methods</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">generate</span> <span class="n">the</span> <span class="n">code</span> <span class="n">of</span> <span class="n">op</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;std::string&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;genOpCode&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span> <span class="s">&#34;OpCodeParam&#34;</span> <span class="o">:</span> <span class="err">$</span><span class="n">param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">clock</span> <span class="n">cycle</span> <span class="n">of</span> <span class="n">the</span> <span class="n">op</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;uint64_t&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;getOpClockCycle&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">the</span> <span class="n">immediate</span> <span class="n">SPM</span> <span class="n">buffer</span> <span class="n">size</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;uint32_t&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s">&#34;getImmSpSize&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">the</span> <span class="n">opAttr</span> <span class="n">info</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;tx8be_mlr::opAttr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;queryOpAttr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>其中 <code>queryOpAttr</code> 接口只需要在对应的接口里给 OpAttr 里的参数赋值。</p>
<ul>
<li><code>alignMode</code>: 算子的对齐要求，有Cx对齐要求，NCx 对齐要求，或者不在意存储格式的。</li>
<li><code>defaultLayout</code>: 算子默认的排布。</li>
<li><code>needPresetToNPU</code>: OP 是否需要进行预设到和硬件匹配的 layout. 当算子用到的指令是带有 NHWC 的配置时候的需要。</li>
<li><code>memInplace</code>: 输入和输出能否使用同一片内存。</li>
<li><code>needLoad</code>: 算子是否需要 load 操作，比如 mask, embedding 就不需要，会跳过loadvar op 生成。bit0 表示 arg idx0，bit1 表示 arg idx1，一共能表示 64 个输入情况。如果是const输入，loadconst 也会跳过codegen 不生成 code.
<blockquote>
<p>一个op可能有多个 input 都没有 load，shape 更新只用最后一个没有 load 的 operand (为 0 的最高位). 如 embedding 的 shape使用最后一个 operand，第一个是 weight 不用管 gshape. scatter有的有load，有的没有，shape 更新只看没有 load 的那个。</p></blockquote>
</li>
<li><code>needStore</code>:  数据是否需要进行 store 操作，会跳过store op 生成。</li>
<li><code>parallel</code>: 是否允许并行模式。</li>
<li><code>alignCx</code>: 最低维度切分是否到 64/128 (i8).</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">OpAttr</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ALIGN_MODE</span> <span class="n">alignMode</span><span class="p">{</span><span class="n">ALIGN_MODE</span><span class="o">::</span><span class="n">NPU_UNKNOWN</span><span class="p">};</span>  <span class="c1">// 算子的对齐要求，有Cx对齐要求，NCx对齐要求，或者不在意存储格式的
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">defaultLayout</span><span class="p">{</span><span class="s">&#34;Tensor&#34;</span><span class="p">};</span>           <span class="c1">// 算子默认的layout
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">needPresetToNPU</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>                   <span class="c1">// op是否需要进行预设到和硬件匹配的layout. 当算子用到的指令是带有 nhwc 的配置时需要
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">ENGINE_TYPE</span> <span class="n">engine</span><span class="p">{</span><span class="n">NPU_ENGINE_CT</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">memInplace</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>                        <span class="c1">// op的输入和输出能否使用同一片memory，比如add的out使用in0的
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint64_t</span> <span class="n">needLoad</span><span class="p">{</span><span class="mh">0xFFFFFFFFFFFFFFFF</span><span class="p">};</span>         <span class="c1">// 算子是否需要load操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint64_t</span> <span class="n">needStore</span><span class="p">{</span><span class="mh">0xFFFFFFFFFFFFFFFF</span><span class="p">};</span>        <span class="c1">// 数据是否需要进行store操作，会跳过store op生成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">parallel</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>                              <span class="c1">// 一般要使能并行模式，不过有的memory可能有问题，就不使能
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">alignCx</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>                               <span class="c1">// 最低维度切分是否到64/128(i8)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><p>batchnorm 允许输入 in 的 layout 为 Cx/NCx，要在 mlir 层的 <code>queryOpAttr()</code> 里将 alignMode 设置为NPU_ALIGN, 维度为 2/3/4，数据类型为 bf16/fp16/fp32/tf32. 其他输入的格式为 fp32. 输出的维度和类型与 in 保持一致。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">OpAttr</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">queryOpAttr</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OpAttr</span> <span class="n">attr</span><span class="p">;</span>  <span class="c1">// 创建一个 OpAttr 对象
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">attr</span><span class="p">.</span><span class="n">alignMode</span> <span class="o">=</span> <span class="n">ALIGN_MODE</span><span class="o">::</span><span class="n">NPU_ALIGN</span><span class="p">;</span>  <span class="c1">// 设置对齐模式为 NPU_ALIGN
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">attr</span><span class="p">.</span><span class="n">needPresetToNPU</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>  <span class="c1">// 设置需要预设到 NPU
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 获取 in 的形状，并判断其第一个维度是否为 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getShape</span><span class="p">()[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">attr</span><span class="p">.</span><span class="n">defaultLayout</span> <span class="o">=</span> <span class="n">batch</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">?</span> <span class="s">&#34;Tensor&#34;</span> <span class="o">:</span> <span class="s">&#34;NTensor&#34;</span><span class="p">;</span>  <span class="c1">// 根据 batch 的值设置默认布局
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">attr</span><span class="p">;</span>  
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>如下图所示，后端编译器会调用 genOpCode 生成相对应的 main.c. 然后 host.cpp 再把 main.c 放到不同的平台上面去编译完再去执行。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBdb6a91e8bb45cbce292f6fdf1fafd0f4?method=download&amp;shareKey=91e74089c887f70b2b508d9a31b877fd" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBdb6a91e8bb45cbce292f6fdf1fafd0f4?method=download&amp;shareKey=91e74089c887f70b2b508d9a31b877fd" alt="OpLibInterface">
    </a><figcaption>OpLibInterface</figcaption></figure></p>
<p>main.c 主要做的就是 load &ndash;&gt; compute &ndash;&gt; store 这三步。伪代码如下，由于进行了时间上的 split，需要循环多次才能读取完整的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">input_done</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// load
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_dma_load</span> <span class="n">Input</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">input_done</span> <span class="o">=</span> <span class="n">Input</span><span class="p">.</span><span class="n">load_finish</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">scale</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">shift</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">mean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">varience</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// compute
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_batchnorm_inference</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">varience</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// store
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_store_var_ncx</span> <span class="n">out</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>op_batchnorm_inference</code> 的定义如下，其中 imm 是辅助空间，此处申请了 2xsizeof(input) Bytes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">uint64_t</span> <span class="nf">op_batchnorm_inference</span><span class="p">(</span><span class="n">BATCHNORM_INFER_PARAM</span> <span class="o">*</span><span class="n">param</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">TSR</span> <span class="o">*</span><span class="n">in</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">shift</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">mean</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">var</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">TSR</span> <span class="o">*</span><span class="n">imm</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">out</span><span class="p">);</span>
</span></span></code></pre></div><p>其中 TSR 是一个自定义的结构体，包括数据格式，地址以及一个 L_shape (load shape). 里面记录了张量完整的大小 shape_whole，以及本 Tile 上每个维度起始下标 shape_start，每个维度加载的大小 shape_slice 和 shape 的维度大小 dim.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">L_SHAPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape_whole</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// the whole shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_start</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// start idx of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_slice</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// length of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_real</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>   <span class="c1">// real length of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">dim</span><span class="p">;</span>                         <span class="c1">// dimension of the shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="n">L_SHAPE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">G_SHAPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">spatial_start</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// [start, end]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">spatial_end</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">dynamic_offset</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">dim</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">done</span><span class="p">;</span>                         <span class="c1">// done for dma load finish
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">batch_offset</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">G_SHAPE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TSR</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Data_Format</span> <span class="n">format</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint64_t</span> <span class="n">addr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">L_SHAPE</span><span class="o">*</span> <span class="n">shape</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TSR</span><span class="p">;</span>
</span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB863424dc56bf5d86b25f817e06a1c716?method=download&amp;shareKey=de577bac9b2b5b108c4a3e8a275d07ea" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB863424dc56bf5d86b25f817e06a1c716?method=download&amp;shareKey=de577bac9b2b5b108c4a3e8a275d07ea" alt="BatchNorm Design">
    </a><figcaption>BatchNorm Design</figcaption></figure></p>
<p>对于非 fp32 类型数据 (以 fp16 为例) 计算过程与空间分配如下图所示。</p>
<ol>
<li>类型转换成 fp32: gatherScatter.</li>
<li>调用 fp16-&gt;fp32 函数进行转换。</li>
<li>循环计算 x-Mean (因为对 in 的 NHW 维度进行了 split)，结果存入 imm_a.</li>
<li>Varience 自加 epsilon(1e-6).</li>
<li>Varience 进行 rsqrt 操作。</li>
<li>Varience 与 x-Mean 进行循环乘。</li>
<li>循环乘 scale.</li>
<li>循环加 shift.</li>
<li>fp32 转回 f16.</li>
<li>gatherScatter 到 out 处。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB427f45ff2571bf94eea6a0b81f897ba1?method=download&amp;shareKey=57a2c93fd8b252f86c47c8e71e325f2c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB427f45ff2571bf94eea6a0b81f897ba1?method=download&amp;shareKey=57a2c93fd8b252f86c47c8e71e325f2c" alt="Batchnorm Computation Flow">
    </a><figcaption>Batchnorm Computation Flow</figcaption></figure></p>
<p>这里需要注意的是 shift(1, 1, 1, C) 和归一化后的 x(N, H, W, C) 相乘的时候，这时候就用到了之前所说的 VuV_mul 和 VuV_mul_loop 指令。</p>
<p>当 C &lt;= 32 时，一个 batch 内的数据排布如下 (以 (4x112x2x30) x (1x1x1x30) 为例)，此时我们在 batch 维度上循环调用 VuV_mul 指令就可以。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB7e47cbd16c9c6777c91a22a0c2685f91?method=download&amp;shareKey=c305ac14d29df963a8884def061ef96f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB7e47cbd16c9c6777c91a22a0c2685f91?method=download&amp;shareKey=c305ac14d29df963a8884def061ef96f" alt="Channel &lt;= 32">
    </a><figcaption>Channel &lt;= 32</figcaption></figure></p>
<p>当 C &gt; 32 时，需要向 64 对齐，一个 batch 内的数据排布如下 (以 (4x112x2x129) x (1x1x1x129) 为例)，每一个 Cx/C0 对应着一次 VuV_mul. 此时我们在 batch 维度上循环调用 VuV_mul_loop 指令就可以。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBfacfc3b8b6722744fa958775ab8a88f4?method=download&amp;shareKey=968a25dfbb972d2e97c7b09f284733be" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBfacfc3b8b6722744fa958775ab8a88f4?method=download&amp;shareKey=968a25dfbb972d2e97c7b09f284733be" alt="Channel &gt; 32">
    </a><figcaption>Channel &gt; 32</figcaption></figure></p>
<p>下面来说明如何调用指令，首先要明确调用的指令是属于哪一个模块的。例如第四步加 epsilon 我们需要调用 addVs 指令，其属于 CGRA 模块。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">enum</span> <span class="nc">OP_INSTR_TYPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_CGRA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_NEUR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_RDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_WDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_TDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_SCALAR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_DTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_CSR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">OP_INSTR_TYPE</span><span class="p">;</span>
</span></span></code></pre></div><p>每个模块下的指令有自己的参数形式，下面列举一些。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// I_CGRA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">CT_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_CT_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_CT_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">CT_Param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_NEUR
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TsmNeInstr</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_NE_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_NE_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TsmNeInstr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_(R/W)DMA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">DMA_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_DMA_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_DMA_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">DMA_Param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_TDMA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TD_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_TDMA_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_TDMA_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TD_Param</span><span class="p">;</span>
</span></span></code></pre></div><p>还是以 AddVS 指令为例，流程如下</p>
<ol>
<li>声明模块的指令参数。</li>
<li>声明对应的指令类型指针，AddVS 属于 arith 类型的指令。getTsmOpPointer()-&gt;arith_pointer;`.</li>
<li>根据调用指令传入参数，指令会根据传入参数配置好 ct_param 上寄存器的值。然后再进行 TsmExecute. 最后再把单词指令的执行时间进行累加。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">CT_Param</span> <span class="n">ct_param</span> <span class="o">=</span> <span class="p">{</span><span class="n">I_CGRA</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">},</span> <span class="p">{</span><span class="mi">0</span><span class="p">}};</span>  <span class="c1">// step 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">TsmArith</span> <span class="o">*</span><span class="n">arith</span> <span class="o">=</span> <span class="p">(</span><span class="n">TsmArith</span> <span class="o">*</span><span class="p">)</span><span class="n">getTsmOpPointer</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">arith_pointer</span><span class="p">;</span>  <span class="c1">// step 2
</span></span></span><span class="line"><span class="cl"><span class="c1">// variance add epsilon
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">arith</span><span class="o">-&gt;</span><span class="n">addVS</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ct_param</span><span class="p">,</span>  <span class="c1">// engine params
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">varAddr</span><span class="p">,</span>  <span class="c1">// vector address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="o">*</span><span class="p">(</span><span class="kt">uint32_t</span> <span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">epsilon</span><span class="p">),</span>  <span class="c1">// scalar address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">varAddr</span><span class="p">,</span>  <span class="c1">// result address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">mid_tensor_info</span><span class="p">.</span><span class="n">total_num</span><span class="p">,</span>  <span class="c1">// vector elements num
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">RND_NEAREST_EVEN</span><span class="p">,</span>  <span class="c1">// round method
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">Fmt_FP32</span><span class="p">);</span>  <span class="c1">// data format
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">cycle_single</span> <span class="o">=</span> <span class="n">TsmExecute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ct_param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">cycle_total</span> <span class="o">=</span> <span class="n">ADD_VALID_CYCLE</span><span class="p">(</span><span class="n">cycle_total</span><span class="p">,</span> <span class="n">cycle_single</span><span class="p">);</span>
</span></span></code></pre></div><h2 id="computeinteface">ComputeInteface</h2>
<p><code>ComputeInterface</code> 这个接口主要是每个 OP 通过 onednn 得到 CPU 代码。或者计算比较简单的 OP 如果在 onednn 的接口中没有找到对应的计算，也可以在 compute 接口中手写当前 OP 的 CPU 实现的 C++代码。最终生成结果会用来检验算子正确性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ComputeInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ComputeInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">description =</span> <span class="p">[]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*</span><span class="err">/</span><span class="p">[],</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;::mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;compute&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;ComputeParam&amp;&#34;</span><span class="p">:</span><span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">  <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h1 id="test-case">Test Case</h1>
<p>TestCase 主要作用是写单算子或者多个 (单算子的上下文算子) 的测试，包括固定配置测试和随机配置测试,随机配置时主要对于算子支持的不同 dim, layout, dtype, shape 这四项做随机。流程主要做以下几件事。</p>
<p><strong>init_param</strong></p>
<p>通过数组来配置固定测试 case 或者随机测试范围，然后通过指定或随机的方式生成对应的输入，输出的 shape， dim 信息，除此之外参与随机的一般还包括数据对齐方式随机，数据类型随机，即在算子可支持的范围内产生随机的 FP16/FP32 不同的数据类型来保证测试的充分和全面。</p>
<p>除此之外还会生成 MLIR Module. 这个 module 是原来就给定的，在这里做的事情是首先新建一个空的 func. 然后在这个 func 中构造一个 block，里面去填入需要测试的这些 OP 的结构。</p>
<ul>
<li>Module：一个程序的容器，包含多个函数。</li>
<li>Func：定义一个函数，包含多个 Block.</li>
<li>Block：定义函数的基本执行单元，包含多个 Operation.</li>
<li>Operation：表示具体的计算或操作，是程序中的基本指令。</li>
</ul>
<h2 id="mlir-structure">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBfdb91efc229999a9b488d5131959f4b0?method=download&amp;shareKey=a3935de13b565d4c37e06857c6c43f90" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBfdb91efc229999a9b488d5131959f4b0?method=download&amp;shareKey=a3935de13b565d4c37e06857c6c43f90" alt="MLIR Structure">
    </a><figcaption>MLIR Structure</figcaption></figure></h2>
<p>init_data</p>
<p>这个方法主要用来通过上面 Param 生成的 dim、输入或者输出 shape、数据类型来生成随机的数据，数据范围一定要根据算子情况配置，不然无效数值可能会在结果中出现 Nan. 还要考虑一些算子的特点，保证测试的充分性，例如创建 relu 的数据时，最好正负值都有覆盖。</p>
<hr>
<p>compile</p>
<p>compile 方法有两个功能</p>
<ol>
<li>调用 Computelnterface 生成 onednn 或者手写 CPU 算子实现的结果。</li>
<li>添加一些配置参数，跑出 tx8be mlir codegen 的结果。这其中会经历一些非常复杂的 pass，稍后再介绍。</li>
</ol>
<hr>
<p>saveInfoFile</p>
<p>saveInfoFile 方法主要是把创建出的 Data 数据写成.bin 文件保存。并把创建出的 module 的信息保存在 json 文件。</p>
<h1 id="overview-of-workflow">Overview of Workflow</h1>
<p>后端接收的是 MLIR 的计算图，然后经过编译器后端的处理，然后生成最后的 BE IR，其中中包含了一些 Oplib 的算子。最终这个 BEIR 会调用 OP 的算子，然后去跑在 C model 或者是实际的硬件芯片上面。后端编译器主要负责四个方面
layout 初始化和传递、const 管理、切分策略及其 SPM 分配和 DDR 分配。</p>
<h1 id="layout-initialization-and-pass">Layout Initialization and Pass.</h1>
<p>layout 可以分为以下几种</p>
<ul>
<li>layout_str: 中端使用
<ul>
<li>CNN Op: 1. Feature (NCHW/NHWC) etc. 2. Weight (OIHW/HWOI) etc.</li>
<li>Non-CNN Op: 大模型中常见，Tensor/NTensor，它们的区别是第 0 维是否为 1.</li>
</ul>
</li>
<li>mem_layout: 后端使用，代表了在芯片上的实际排布
<ul>
<li>Tensor/NTensor: 数据的紧密排布</li>
<li>Cx/NCx: 对 Tensor/NTensor 格式化后的结果，方便易硬件读取。</li>
</ul>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>dtype</th>
          <th>channel</th>
          <th>description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>bf16/fp16 <br>/fp32/tf32</td>
          <td>c &lt;= 32</td>
          <td>NHWC, C向4/8/16/32对齐，N 的起始地址向 2048bit 对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 32</td>
          <td>N[CxHW64, HWC0], C0 向 4/8/16/32 对齐，N 的起始地址向2048bit 对齐<br>在一个 batch 内将 tensor 按 C 分成 Cx*64 和 C0两部分</td>
      </tr>
      <tr>
          <td>int8</td>
          <td>c &lt;= 64</td>
          <td>NHWC, C 向 4/8/16/32/64对齐，N的起始地址向2048bit对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 64</td>
          <td>N[CxHW128, HWC0], C0 向 4/8/16/32/64 对齐，N的起始地址向 2048bit 对齐 <br> 在一个 batch 内将 tensor 按 C 分成 Cx*128 和C0 两部分</td>
      </tr>
  </tbody>
</table>
<h2 id="layoutinitpass">layoutInitPass</h2>
<p>layoutInitPass 用于初始化计算图中 GemmOP 和 ConvOP 的 layout_str，其他的所有算子 layout_str 都设置为 UNKNOWN. 下图中的 <code>GemmOP layout_str = &quot;Tensor-Tensor-Tensor&quot;</code> 分别表示两个输入和输出的数据排布。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB95a6e97d5ae8432b8367189a36987f31?method=download&amp;shareKey=8df50a04c4e2ed29be9e93d3da958e35" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB95a6e97d5ae8432b8367189a36987f31?method=download&amp;shareKey=8df50a04c4e2ed29be9e93d3da958e35" alt="LayoutStr">
    </a><figcaption>LayoutStr</figcaption></figure></p>
<h2 id="layouttransmitpass">layoutTransmitPass</h2>
<p>layoutTransmitPass 会用已知的 GemmOP 和 ConvOP layout 信息进行扩散，得到全图的 layout_str.</p>
<ol>
<li>每个算子初始化为一个节点，有inputNodes容器和outputNodes容器分别存放自己的输入和输出节点。</li>
<li>GemmOp 和 ConvOp 作为起始节点，向前和向后推导 layout (算子的 <code>inferlayout()</code> 接口)，新推出layout 的节点作为下一批起始节点递归推导。</li>
<li>遇到无法推导的节点 (如 Reshape，BroadCast) 则终止推导。将其余无法推导的节点 layout 直接初始化为 Tensor.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9d187e7b17a4b2ee65f01169f8c6a141?method=download&amp;shareKey=c405975eba9c1da95539f89ac8b3de8a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9d187e7b17a4b2ee65f01169f8c6a141?method=download&amp;shareKey=c405975eba9c1da95539f89ac8b3de8a" alt="layoutTransmitPass">
    </a><figcaption>layoutTransmitPass</figcaption></figure></p>
<h2 id="layoutaligntonpupass">layoutAlignToNpuPass</h2>
<p>layoutAlignToNpuPass 用于在数据对齐冲突的地方插入 channelNorm，并将 layout_str 映射到 mem_layout. 在 NPU 上某些算子只支持 <code>COMPACT</code> layout，有些只支持 <code>ALIGN</code> layout，有些则都可以 <code>BOTH</code>.</p>
<ol>
<li>输入默认非对齐排布，从输入出发遍历整图，检查当前算子与其所有 user 之间的对齐要求，若冲突，记录插入点 (算子的对齐要求可以在 <code>OpLibInterface</code> 接口中的 <code>queryOpAttr()</code> 方法中查询到).</li>
<li>根据记录的插入点，再次分析插入点前后的算子对齐要求，以确定channelnorm的方向，插入 channelnorm.</li>
<li>赋值 <code>dev_info</code>，将 <code>layout_str</code> 映射到 <code>mem_layout</code>.</li>
</ol>
<blockquote>
<p>dev_info用来描述数据在设备上的一些属性，有成员：imm_size (辅助空间大小), mem_layout, temporal_mem_slice, imm_addr, mem_addr.</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB41f3c705e0f54e9291a5a2a7916f6045?method=download&amp;shareKey=e9af8cbf6c2e348e4584018bcb4d4782" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB41f3c705e0f54e9291a5a2a7916f6045?method=download&amp;shareKey=e9af8cbf6c2e348e4584018bcb4d4782" alt="layoutAlignToNpuPass">
    </a><figcaption>layoutAlignToNpuPass</figcaption></figure></p>
<p>LayoutAlignOptPass 应用几个 RewritePattern 用于删除冗余的 channelnorm.</p>
<ol>
<li><strong>ConstChannelNormErase</strong>: ConstantOp 维度为 1 并且只有 1 个 user 的时候可以删去并且将 devInfolayout 设置为 Cx.</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">ConstChannelNormErase Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// const can be directly considered to be aligned
</span></span></span><span class="line"><span class="cl"><span class="c1">// constop(dim &lt; 2) -&gt; channelNorm -&gt; constop
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">ConstChannelNormErase</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ConstChannelNormErase</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="mi">1</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">txbe</span><span class="o">::</span><span class="n">ConstantOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// If const has multi user, can not erase
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">hasOneUse</span><span class="p">())</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">user</span> <span class="o">=</span> <span class="o">*</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">userVec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">userVec</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">userVec</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">user</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">user</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">channelNormUser</span> <span class="p">:</span> <span class="n">userVec</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">channelNormUser</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">user</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// set align=true
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getLayoutStr</span><span class="p">().</span><span class="n">str</span><span class="p">(),</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">user</span><span class="o">-&gt;</span><span class="n">use_empty</span><span class="p">())</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<details class="custom-details">
    <summary class="custom-summary">RedudantChannelnormErase Implementation</summary>
    <div><ol start="2">
<li><strong>RedudantChannelnormErase</strong>: 如果该 channelnormOp 的输入是来自一个 constOp 并且只有一个输出，则检查是否还有其他的 channelnormOp 也使用。如果是，则让它们直接使用该 channelnormOp 的结果，以消除多余的 channelnormOp.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// A pass to erase redundant channel normalization operations
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">RedundantChannelNormErase</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">RedundantChannelNormErase</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">)</span> <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="mi">1</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Define the input operation and its defining operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// def represents the operation that generates the op input data
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">def</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getInput</span><span class="p">().</span><span class="n">getDefiningOp</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Check if the defining operation is a ConstantOp and has more than one result
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">def</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">def</span><span class="o">-&gt;</span><span class="n">getNumResults</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span> <span class="c1">// Fail if conditions are not met
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Get the size in bits of the input shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">size</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getInput</span><span class="p">().</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getSizeInBits</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Operation</span> <span class="o">*</span><span class="n">sameOp</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span> <span class="c1">// Pointer to a potentially redundant operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Iterate over all users of the defining operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">def</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">user</span> <span class="o">==</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Skip if the user is the current operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span> <span class="c1">// Check if the user is another ChannelNormOp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">sameOp</span> <span class="o">=</span> <span class="n">user</span><span class="p">;</span> <span class="c1">// Store the redundant operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">sameOp</span><span class="p">)</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span> <span class="c1">// Fail if no redundant operation is found
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Replace all uses of the redundant operation with the current operation&#39;s results
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">op</span><span class="o">-&gt;</span><span class="n">replaceAllUsesWith</span><span class="p">(</span><span class="n">sameOp</span><span class="o">-&gt;</span><span class="n">getOpResults</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">use_empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Erase the current operation if it has no more uses
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span> <span class="c1">// Return success if the rewrite is completed
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div></div>
</details><br>
<h1 id="const-management">Const Management</h1>
<p>常量统一使用 <code>ConstContainer</code> 类来进行管理。通过 map 来记录每个常量对应的 ParamInfo. 一个常量可能被分配到多个芯片上，每个芯片上数据可能相同，也可能不同。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ParamInfo</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;*</span> <span class="n">data_ptr</span><span class="p">;</span>  <span class="c1">// const value
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">set</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">chip_id</span><span class="p">;</span>  <span class="c1">// which chips has this const, -1 indicates all chip has the same param.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint32_t</span> <span class="n">label</span><span class="p">;</span>  <span class="c1">// Indicates whether the data is assigned to a certain chip_id. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// class ConstContainer {
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">ConstContainer</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">ConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">virtual</span> <span class="o">~</span><span class="n">ConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// some public functions
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ParamInfo</span><span class="o">&gt;&gt;</span> <span class="n">_data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="p">,</span> <span class="kt">uint64_t</span><span class="o">&gt;&gt;</span> <span class="n">oidToSize</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="kt">uint32_t</span><span class="o">&gt;</span> <span class="n">oidToNid</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="moveconstantpass">MoveConstantPass</h2>
<p>MoveConstantPass: 创建图的 <code>ConstContainer</code>，然后应用 <code>ConstantToLoadConst</code> Rewrite Pattern. 转换完成后会调用 <code>updateConstContainer</code> 更新 <code>ConstContainer</code> 各个 const 的 ID. 用一个大小为 <code>4*1024*tile_num</code> (DDR_BANK_SIZE) <code>thresholdSize</code> 将大于这个值的 const 全部放在前面，小的放在后面。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MoveConstantPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// create constant container
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">createConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// get module op
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ModuleOp</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="n">RewritePatternSet</span> <span class="nf">patterns</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">patterns</span><span class="p">.</span><span class="n">insert</span><span class="o">&lt;</span><span class="n">ConstantToLoadConst</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="n">FrozenRewritePatternSet</span> <span class="n">frozen_patterns</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">FrozenRewritePatternSet</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">patterns</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set config
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">GreedyRewriteConfig</span> <span class="n">config</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">config</span><span class="p">.</span><span class="n">useTopDownTraversal</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">func</span> <span class="p">:</span> <span class="n">module</span><span class="p">.</span><span class="n">getOps</span><span class="o">&lt;</span><span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span><span class="o">&gt;</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Region</span> <span class="o">&amp;</span><span class="n">body</span> <span class="o">=</span> <span class="n">func</span><span class="p">.</span><span class="n">getBody</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">failed</span><span class="p">(</span><span class="n">applyPatternsAndFoldGreedily</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">frozen_patterns</span><span class="p">,</span> <span class="n">config</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">llvm</span><span class="o">::</span><span class="n">errs</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed when move const in main graph.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">subgraph</span> <span class="p">:</span> <span class="n">module</span><span class="p">.</span><span class="n">getOps</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="o">&gt;</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Region</span> <span class="o">&amp;</span><span class="n">body</span> <span class="o">=</span> <span class="n">subgraph</span><span class="p">.</span><span class="n">getBody</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">failed</span><span class="p">(</span><span class="n">applyPatternsAndFoldGreedily</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">frozen_patterns</span><span class="p">,</span> <span class="n">config</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">llvm</span><span class="o">::</span><span class="n">errs</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed when move const in subgraph.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">TileInfo</span> <span class="n">tinfo</span> <span class="o">=</span> <span class="n">get_tileinfo</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">updateConstContainer</span><span class="p">(</span><span class="n">tinfo</span><span class="p">.</span><span class="n">tile_num</span><span class="p">);</span>  <span class="c1">// update id by thresholdSize
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">updateLdConstop</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>ConstantToLoadConst</code> 首先通过分析该常量的所有 users，来判断这个常量是否需要 LoadConstOp. 如果需要加载，它会将原始常量的数据注册到一个全局容器中并获得一个 ID，然后创建一个新的 LoadConstOp ，并将此 ID 及其他硬件属性赋予它。接着，它会更新所有使用者，将它们的输入从旧的 ConstantOp 重定向到这个新的 LoadConstOp，最后再删除无用的原始常量。最后再更新所有 const 的 ID.</p>
<details class="custom-details">
    <summary class="custom-summary">ConstantToLoadConst Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ConstantToLoadConst</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ConstantToLoadConst</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span>
</span></span><span class="line"><span class="cl">  <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span> <span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Store constant data to constant container 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Determine if this constant operation needs an explicit load instruction.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">needLoad</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">v</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Iterate over all operations that use this output value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user_op</span> <span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Get the argument index of the user op that corresponds to our output value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">int32_t</span> <span class="n">arg_idx</span> <span class="o">=</span> <span class="n">getArgumentIdx</span><span class="p">(</span><span class="n">user_op</span><span class="p">,</span> <span class="n">v</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Assert that the user operation implements our custom OpLibInterface.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">ASSERT</span><span class="p">(</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Get the library attributes for this user operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">opAttr</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">).</span><span class="n">queryOpAttr</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Skip if the user is a TupleOp, which might have special handling.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">TupleOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">opAttr</span><span class="p">.</span><span class="n">needLoad</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">arg_idx</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// Check if the &#39;needLoad&#39; attribute
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">needLoad</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">ASSERT</span><span class="p">(</span><span class="n">needLoad</span> <span class="o">==</span> <span class="nb">false</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Set attributes
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Safely iterate over the users. This is important because we are modifying the use-list inside the loop.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">use</span> <span class="p">:</span> <span class="n">llvm</span><span class="o">::</span><span class="n">make_early_inc_range</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">().</span><span class="n">getUses</span><span class="p">()))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">Operation</span> <span class="o">*</span><span class="n">userOp</span> <span class="o">=</span> <span class="n">use</span><span class="p">.</span><span class="n">getOwner</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Create the new, hardware-specific LoadConst operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">txbe</span><span class="o">::</span><span class="n">LoadConstOp</span> <span class="n">newLoadConst</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">            <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">().</span><span class="n">getType</span><span class="p">(),</span> <span class="n">ValueRange</span><span class="p">{},</span> <span class="n">attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">needLoad</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// this constant does not need an explicit load... 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Get a builder to set attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">OpBuilder</span> <span class="nf">builder</span><span class="p">(</span><span class="n">newLoadConst</span><span class="p">.</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Set a &#39;bypasscodegen&#39; attribute, signaling special handling for this op in later stages.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">newLoadConst</span><span class="p">.</span><span class="n">getOperation</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;bypasscodegen&#34;</span><span class="p">,</span> <span class="n">builder</span><span class="p">.</span><span class="n">getBoolAttr</span><span class="p">(</span><span class="nb">true</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Set the layout string attribute on the new LoadConst op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">newLoadConst</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">,</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// CRITICAL STEP: Rewire the user&#39;s operand to point to the result of the new LoadConst op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">userOp</span><span class="o">-&gt;</span><span class="n">setOperand</span><span class="p">(</span><span class="n">use</span><span class="p">.</span><span class="n">getOperandNumber</span><span class="p">(),</span> <span class="n">newLoadConst</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// After all uses have been replaced, erase the original, now-dead ConstantOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<h2 id="constnormpass">constNormPass</h2>
<p>constNormPass: 遍历图中的 LoadConstOp. 它会寻找一个特定的模式：如果一个 LoadConstOp 的唯一 user 是一个 ChannelNormOp，那么会通过 <code>constChannelNormErase</code> 函数进行消除和将对其信息同步到 LoadConstOp. 最后通过 <code>processMultiUse</code> 确保所有加载同一个底层常量数据的 LoadConstOp 实例，都具有完全相同的内存布局。</p>
<details class="custom-details">
    <summary class="custom-summary">ConstNormPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ConstNormPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ModuleOp</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">mainGraphFunc</span> <span class="o">=</span> <span class="n">getMainFuncOp</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span> <span class="o">*&gt;</span> <span class="n">deletedChannelnorm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Walk the main function to find a specific pattern: LoadConst -&gt; ChannelNorm.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">users</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">            <span class="kt">bool</span> <span class="n">flag</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Check if any user is a ChannelNormOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="n">flag</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1">// If the LoadConst has exactly one user, and that user is a ChannelNormOp,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// mark the ChannelNormOp for deletion.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">(</span><span class="n">flag</span> <span class="o">&amp;&amp;</span> <span class="n">users</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="c1">// The erase logic is commented out, maybe handled by constChannelNormErase or done later.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                    <span class="n">deletedChannelnorm</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">it</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Erase all the marked ChannelNormOps. This is done in a separate loop
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// to avoid iterator invalidation issues.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">deletedChannelnorm</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">op</span><span class="o">-&gt;</span><span class="n">erase</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Set up and run a nested pass pipeline.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">OpPassManager</span> <span class="nf">thisPM</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">getOpName</span><span class="p">().</span><span class="n">value</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// This pipeline will only apply to LoadConstOp operations inside functions.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">OpPassManager</span> <span class="o">&amp;</span><span class="n">loadConstOpPM</span> <span class="o">=</span> <span class="n">thisPM</span><span class="p">.</span><span class="n">nest</span><span class="o">&lt;</span><span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span><span class="o">&gt;</span><span class="p">().</span><span class="n">nest</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Add the ConstNormDoPass to the pipeline.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">loadConstOpPM</span><span class="p">.</span><span class="n">addPass</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">ConstNormDoPass</span><span class="o">&gt;</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Run the newly constructed pipeline on the module.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">result</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">runPipeline</span><span class="p">(</span><span class="n">thisPM</span><span class="p">,</span> <span class="n">getOperation</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// After the pipeline, run a final cleanup/consistency check function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">processMultiUse</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// change unpack input0 qweight shape after ConstNormDoPass. (Original comment)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// This logic is likely inside the runOnOperation() method of ConstNormDoPass.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="c1">// Collect all users of this LoadConstOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">users</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">          
</span></span><span class="line"><span class="cl">          <span class="c1">// Check if any user is an UnpackOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="kt">bool</span> <span class="n">flag</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">UnpackOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="n">flag</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                  <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// If there is exactly one user, and it&#39;s an UnpackOp...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="n">flag</span> <span class="o">&amp;&amp;</span> <span class="n">users</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// This check seems to ensure we are modifying the correct operand.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="k">if</span> <span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">it</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// Get the original shape and type.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="p">,</span> <span class="mi">6</span><span class="o">&gt;</span> <span class="n">oShape</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                  <span class="k">auto</span> <span class="n">type</span> <span class="o">=</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">                  <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">type</span><span class="p">.</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                  <span class="c1">// Apply the shape transformation: e.g., for unpacking packed data.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">oShape</span><span class="p">.</span><span class="n">push_back</span><span class="p">((</span><span class="kt">int32_t</span><span class="p">)</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">4</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                  <span class="n">oShape</span><span class="p">.</span><span class="n">push_back</span><span class="p">((</span><span class="kt">int32_t</span><span class="p">)</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                  <span class="c1">// Create a new tensor type with the new shape.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="k">auto</span> <span class="n">oType</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">::</span><span class="n">RankedTensorType</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="n">oShape</span><span class="p">,</span> <span class="n">type</span><span class="p">.</span><span class="n">getElementType</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// Update the type of the LoadConstOp&#39;s result in-place.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">setType</span><span class="p">(</span><span class="n">oType</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                  <span class="p">}</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>constChannelNormErase</code> 处理 LoadConstOp -&gt; ChannelNormOp 这种模式。让所有原本使用 ChannelNormOp 计算结果的操作，现在改为直接使用 ChannelNormOp 的输入数据。获取 LoadConstOp 当前的设备信息和 layout，计算出一个新的经过对齐的布局 <code>align_dev_layout</code>，然后用这个新布局去更新 LoadConstOp.</p>
<details class="custom-details">
    <summary class="custom-summary">constChannelNormErase Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// This function erases a ChannelNormOp by bypassing it and updating the source constant&#39;s layout.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">constChannelNormErase</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Find the defining operation of the ChannelNorm&#39;s operand, which should be a LoadConstOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">defOp</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast_or_null</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getDefiningOp</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// If the source is not a LoadConstOp, do nothing.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">defOp</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Collect all users of the ChannelNormOp&#39;s result.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">userVec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">userVec</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">userVec</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">userVec</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Replace all uses of the ChannelNormOp&#39;s result with the result of the LoadConstOp..
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">user</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// After bypassing, the layout of the source constant might need to be adjusted
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// to reflect the transformation that the ChannelNormOp was supposed to perform.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// set const layout to cx mode 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">dev_layout</span> <span class="o">=</span> <span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">defOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">align_dev_layout</span> <span class="o">=</span> <span class="n">get_aligned_layout</span><span class="p">((</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">dev_layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">defOp</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">defOp</span><span class="p">,</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LayoutMode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">align_dev_layout</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<p><code>processMultiUse</code> 保证所有对同一份常量数据的引用，其 mem_layout 都是完全一致的。流程如下</p>
<ol>
<li><code>processMultiUse</code> 遍历计算图中的所有 LoadConstOp，以 <code>const_map_id</code> 为 key，将所有指向同一个物理常量的 LoadConstOp 实例分组存放在一起。</li>
<li>遍历这个 map，只处理那些包含多个 LoadConstOp 实例的组 (<code>kv.second.size() &gt; 1</code>).</li>
<li>在每个组内，确定一个正确的布局。代码逻辑是以组内的第一个 LoadConstOp 的布局为基准，但如果发现组内有 <code>is_cx_layout</code>，则会采用这个优先的布局作为标准。</li>
<li>一旦确定了标准布局，会再次遍历该组内的所有 LoadConstOp 实例。调用 <code>setDevInfoWithLayout</code> 函数，强制将每一个实例的布局属性修改为刚才确定的那个标准布局。</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">processMultiUse Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// This function processes multi-use constants to ensure their layouts are consistent.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="n">ConstNormPass</span><span class="o">::</span><span class="n">processMultiUse</span><span class="p">(</span><span class="n">ModuleOp</span> <span class="n">module</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">mainGraphFunc</span> <span class="o">=</span> <span class="n">getMainFuncOp</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// When a const is used by multiple users, multiple loadconsts will be generated,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// but only one loadconst will have its layout set. The others will be skipped.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// We need to go over them uniformly. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// First, find all previous useless constant ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Group all LoadConstOp instances by their underlying constant data ID (const_map_id).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span> <span class="o">*&gt;&gt;</span> <span class="n">allconst</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">auto</span> <span class="n">cOp</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="kt">uint32_t</span> <span class="n">t_map_id</span> <span class="o">=</span> <span class="n">cOp</span><span class="p">.</span><span class="n">getConstMapId</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">      <span class="n">allconst</span><span class="p">[</span><span class="n">t_map_id</span><span class="p">].</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">constOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Based on duplication, find if the layout needs to be changed to cx. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// Check if there is also a Cx with the same layout. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Iterate over each group of LoadConstOps that share the same data.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">kv</span> <span class="p">:</span> <span class="n">allconst</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Process only if there are multiple users.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Assume the layout of the first user is the correct one.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">auto</span> <span class="n">layout</span> <span class="o">=</span> <span class="p">(</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">front</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">      <span class="c1">// This loop is for validation, checking if layouts are inconsistent.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">layout2</span> <span class="o">=</span> <span class="p">(</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">is_cx_layout</span><span class="p">(</span><span class="n">layout2</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ALIGN_NOT</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">layout</span> <span class="o">=</span> <span class="n">layout2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// Force all LoadConstOps in this group to have the same, correct layout.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="n">ASSERT</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;dev_info&#34;</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="s">&#34;Must have dev_info!&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LayoutMode</span><span class="p">)</span><span class="n">layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<h1 id="sharding-search-and-spm-management">Sharding Search and SPM Management</h1>
<p>第一步是对算子进行 Group 划分，插入 load &amp; store. 对每一个 subGraph 会应用如下的 3 个 Pass:</p>
<ul>
<li><strong>GroupPatternPass</strong>：应用配置好的 group config (opt_group).</li>
<li><strong>GroupOptimizationPass</strong>: 如果没有配置，则会为每个 compute op 创建一个 group.</li>
<li><strong>GroupLdStPass</strong>: 为每个需要的 groupOp 插 入loadOp 和 storeOp，并添加 group_tag.
<ul>
<li>group_tag = 0: 需要 load 或 store，意味着该 group 需要后续的切分搜索。</li>
<li>group_tag = 2: 不需要 load 或 store，意味着该 group 的op 都在 DDR 上操作，无需参与后续的切分搜索。</li>
</ul>
</li>
</ul>
<p>SPM 上一定要能放下切分后的结果。Group 是切分搜索和 SPM 分配的基本的单位。思想就是尽量把连续执行的算子组合在一起，一直在 SPM 上运行而不是存回 DDR 再读入，以此来减少访存时间。GroupOp 在 td 文件中定义所包含的输入如下:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-tablegen" data-lang="tablegen"><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">regions</span> <span class="p">=</span> <span class="p">(</span><span class="nv">region</span> <span class="nv">SizedRegion</span><span class="p">&lt;</span><span class="m">1</span><span class="p">&gt;:</span><span class="nv">$body</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">arguments</span> <span class="p">=</span> <span class="p">(</span><span class="nv">ins</span>
</span></span><span class="line"><span class="cl">    <span class="nv">Variadic</span><span class="p">&lt;</span><span class="nv">AnyTensorOrNone</span><span class="p">&gt;:</span><span class="nv">$operands</span><span class="p">,</span>      <span class="c">// 输入参数为 操作数的数量可变的的张量
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">BoolAttr</span><span class="p">,</span> <span class="s">&#34;false&#34;</span><span class="p">&gt;:</span><span class="nv">$pipeline_parallel</span><span class="p">,</span> <span class="c">// 是否用流水线并行
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">I32Attr</span><span class="p">,</span> <span class="s">&#34;1&#34;</span><span class="p">&gt;:</span><span class="nv">$sp_stage_num</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">Tx8e_RegionAttr</span><span class="p">&gt;:</span><span class="nv">$dev_region</span><span class="p">,</span> <span class="c">// 设备的空间属性
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">UI32Attr</span><span class="p">&gt;:</span><span class="nv">$spm_alloc_size</span><span class="p">,</span>   <span class="c">// group占用的spm大小
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">I32Attr</span><span class="p">&gt;:</span><span class="nv">$group_tag</span><span class="p">,</span>         <span class="c">// 0: 正常切分, 1: split nht, 2: 不切分 (reshape)
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">DenseI32ArrayAttr</span><span class="p">&gt;:</span><span class="nv">$stream_online_check</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">DenseI32ArrayAttr</span><span class="p">&gt;:</span><span class="nv">$stream_offline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">BoolAttr</span><span class="p">,</span> <span class="s">&#34;true&#34;</span><span class="p">&gt;:</span><span class="nv">$need_barrier</span><span class="p">,</span>   <span class="c">// 是否需要tile同步
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">SI32Attr</span><span class="p">,</span> <span class="s">&#34;-1&#34;</span><span class="p">&gt;:</span><span class="nv">$group_id</span><span class="p">,</span>         <span class="c">// group id序号
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">SI32Attr</span><span class="p">,</span> <span class="s">&#34;-1&#34;</span><span class="p">&gt;:</span><span class="nv">$template_id</span>      <span class="c">// 复用其他group的id, 小于0为不复用
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">results</span> <span class="p">=</span> <span class="p">(</span><span class="nv">outs</span> <span class="nv">Variadic</span><span class="p">&lt;</span><span class="nv">AnyTensorOrNone</span><span class="p">&gt;:</span><span class="nv">$results</span><span class="p">);</span>
</span></span></code></pre></div><p>还有一些常用到的结构体
<code>SecsInfo</code> 记录了单个 Op在分布式策略搜索过程中的所有状态和信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">SecsInfo</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">sharding</span><span class="p">;</span>  <span class="c1">// space 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">split</span><span class="p">;</span>  <span class="c1">// time
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">splitry</span><span class="p">;</span>  <span class="c1">// 当前搜索的 sharding 的 split
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">reduceSplit</span><span class="p">;</span>  <span class="c1">// 针对需要进行规约 (Reduction) 的维度的切分策略。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int32_t</span> <span class="n">reducesplit</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 一个标记位，用于指示reduceSplit是否被使用
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ******************** 以下变量为factorSpace使用部分 ********************
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="n">sfinish</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>  <span class="c1">// 标记 split/reduceSplit 相关的策略是否已确定。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 枚举类型，定义了当前算子所处的切分模式，特别关注需要通信的Reduce维度。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="cm">/* SHARDING_MODE 的可能值解释：
</span></span></span><span class="line"><span class="cl"><span class="cm">   * SHARDING_INIT: 初始状态，尚未确定模式。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 0: 不切分规约 (reduce) 维度。意味着数据在每个设备上是完整的，无需通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 1: 单边切分规约维度。例如，只切分权重，不切分输入，数据在不同tile上需要通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 2: 两边都切分规约维度。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 3: 对权重(weight)的输出通道(output channel)维度进行切分，但不属于张量并行(TP)，可能需要fn/oc通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">  */</span>
</span></span><span class="line"><span class="cl">  <span class="n">SHARDING_MODE</span> <span class="n">shardingMode</span><span class="p">{</span><span class="n">SHARDING_INIT</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="n">rfinish</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>   <span class="c1">// 标记 reduceSplit 相关的策略是否已完成处理。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">nfirst</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 标记搜索方向。1: search from dim0 -&gt; dim n-1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">finish</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 表示该算子的策略搜索是否已全部完成。整个搜索流程: sharding -&gt; shardingmode -&gt; split -&gt; reduceSplit
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span> <span class="n">sliceShapeMin</span><span class="p">;</span> <span class="c1">// 标记切分后的张量 (slice) 在每个维度上是否已达到某个最小尺寸限制。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ******************** 以下变量为sliceInfo使用部分 ********************
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span> <span class="n">TemporalShape</span><span class="p">;</span>  <span class="c1">// 切分后，临时的张量形状
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">reduce_sharding_space</span><span class="p">;</span>  <span class="c1">// 规约维度切分的搜索空间
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">reduce_sharding</span><span class="p">;</span>  <span class="c1">// // 最终选定的规约维度切分策略
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">sharding2_finish</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 标记第二阶段切分 (可能与规约相关) 是否完成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><h2 id="grouppatternpass">GroupPatternPass</h2>
<p><code>GroupPatternPass</code> 其核心功能是在给定的计算图 (subgraphOp) 中，通过一种高效的模式匹配算法，识别出预定义的、可优化的子图模式 (Operator Patterns)，并将匹配到的算子 (Operations) 进行分组。这种分组通常是图优化 (如算子融合、算子调度) 的第一步。</p>
<p>该 Pass 首先获取配置，决定从哪里加载模式 (一个 map，其键是模式，即一个算子序列 <code>std::vector&lt;TX8BE_OPS&gt;</code>，值是一个整数 <code>int</code>，代表模式优先级，越大优先级越高) 。然后，它调用 <code>aca.insertPatterns</code> 将这些模式&quot;编译&quot;到 Automation 引擎中。接着，调用 <code>aca.search</code> 执行匹配。最后，从 manager 中获取匹配结果 (groups) ，并对这些 groups 进行后续处理，例如创建新的逻辑分组和进行拓扑排序。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"> <span class="kt">void</span> <span class="n">GroupPatternPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">     <span class="n">TFUNC_SCOPE</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">subgraphOp</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span> <span class="c1">// Get the current operation (e.g., a function) the pass is running on.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> 
</span></span><span class="line"><span class="cl">     <span class="n">PatternManager</span> <span class="n">manager</span><span class="p">;</span> <span class="c1">// A manager to hold graph rewriting information.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">Automation</span> <span class="nf">aca</span><span class="p">(</span><span class="o">&amp;</span><span class="n">manager</span><span class="p">);</span> <span class="c1">// Custom &#39;Automation&#39; class for pattern matching logic.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> 
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">minfo</span> <span class="o">=</span> <span class="n">getModuleConfig</span><span class="p">(</span><span class="n">getModuleByOp</span><span class="p">(</span><span class="n">getOperation</span><span class="p">()));</span> 
</span></span><span class="line"><span class="cl">     <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">path</span> <span class="o">=</span> <span class="s">&#34;&#34;</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">path</span> <span class="o">!=</span> <span class="s">&#34;&#34;</span> <span class="o">?</span> <span class="n">getPatternsFromFile</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>  <span class="c1">// Load patterns from a file if path is specified.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                           <span class="o">:</span> <span class="p">(</span><span class="n">patternConfigMap</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">GroupPatternMode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">minfo</span><span class="p">.</span><span class="n">opt_group</span><span class="p">)));</span> <span class="c1">// Otherwise, load from a pre-defined map using a config key.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">TLOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;[GroupPatternPass] config id: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">minfo</span><span class="p">.</span><span class="n">opt_group</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">     <span class="n">aca</span><span class="p">.</span><span class="n">insertPatterns</span><span class="p">(</span><span class="n">temp</span><span class="p">);</span> <span class="c1">// Insert the loaded patterns into the Automation engine. This is the starting point for building the matching structure.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">TLOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;[Automation]: </span><span class="se">\n</span><span class="s">&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">printTree</span><span class="p">(</span><span class="n">aca</span><span class="p">.</span><span class="n">root</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">     <span class="n">aca</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="n">subgraphOp</span><span class="p">);</span> <span class="c1">// Execute the search for all patterns on the given subgraph. (search function code is not provided but its role is clear).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">manager</span><span class="p">.</span><span class="n">applyAll</span><span class="p">();</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">manager</span><span class="p">.</span><span class="n">getGroups</span><span class="p">();</span> <span class="c1">// Retrieve the groups of operations that were matched.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">manager</span><span class="p">.</span><span class="n">show</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">newGroups</span> <span class="o">=</span> <span class="n">createGroups</span><span class="p">(</span><span class="n">subgraphOp</span><span class="p">,</span> <span class="n">groups</span><span class="p">);</span> <span class="c1">// Create new group structures from the matched results.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">group</span> <span class="p">:</span> <span class="n">newGroups</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">         <span class="n">sortTopologically</span><span class="p">(</span><span class="n">group</span><span class="o">-&gt;</span><span class="n">getBlock</span><span class="p">());</span> <span class="c1">// Topologically sort the operations within each new group to maintain data dependencies.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span></code></pre></div><p><code>insertPatterns</code> 对于每一个模式，它首先调用 processPattern 来处理其中的 OR, WILDCARD 算子。</p>
<ul>
<li>当遇到 OR 时，它会将模式拆分。例如，A B OR C D 这样的模式会被拆解成两个独立的模式 A B 和 C D 进行处理。</li>
<li>当遇到 WILDCARD 时，它会生成多个模式。根据代码 <code>for (int i = 0; i &lt; 5; i++)</code> 和 <code>temp.push_back(*(it - 1))</code>，OP * 可能会被扩展成 OP, OP OP, OP OP OP, OP OP OP OP 等一系列重复模式。</li>
<li>它通过递归调用自身，以处理一个模式中包含多个特殊算子的情况。
最终，它返回一个由多个具体、无特殊算子的模式组成的列表。然后，它将这些扩展后的具体模式逐一传递给 <code>insertPattern</code> 函数，以构建 Trie 树。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">insertPatterns</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">patterns</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;&gt;</span> <span class="n">tempPatterns</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">patterns</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate through each pattern from the input map.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">processPattern</span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">first</span><span class="p">);</span> <span class="c1">// Pre-process the pattern. This can expand one pattern into many.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">p</span> <span class="p">:</span> <span class="n">temp</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// For each of the generated concrete patterns...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">insertPattern</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">);</span> <span class="c1">// ...insert it into the main data structure (the Trie).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>insertPattern</code> 接收一个具体的模式，并将其插入到 Trie 树中。Trie 树是实现高效前缀匹配的关键。从root节点开始 遍历模式中的每个 op. 如果当前节点没有指向op的子节点，就创建一个然后移动到该子节点。当模式遍历完成后，在最终的节点上存储完整模式本身 (<code>node-&gt;pattern</code>) 和它的 ID (<code>node-&gt;output</code>) 。这表明一个有效的模式在此结束。</p>
<details class="custom-details">
    <summary class="custom-summary">insertPattern Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">TrieNode</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">TrieNode</span><span class="p">(</span><span class="n">TX8BE_OPS</span> <span class="n">id</span><span class="p">)</span> <span class="o">:</span> <span class="n">id</span><span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="p">{}</span> <span class="c1">// Constructor to initialize the node with an operation ID.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">TX8BE_OPS</span> <span class="n">id</span><span class="p">;</span> <span class="c1">// The operation (Op) type this node represents. This is the &#39;character&#39; in our sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">output</span><span class="p">;</span> <span class="c1">// Stores the integer IDs of the patterns that end at this node. A non-empty vector indicates a valid pattern match.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// Stores the complete operator sequence for the pattern that ends here.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="p">,</span> <span class="n">NodePtr</span><span class="o">&gt;</span> <span class="n">children</span><span class="p">;</span> <span class="c1">// A map from an operation type to the next node in the trie. `NodePtr` is likely a shared_ptr or unique_ptr to another TrieNode.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">insertPattern</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span> <span class="n">pattern</span><span class="p">,</span> <span class="kt">int</span> <span class="n">index</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">patterns_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">pattern</span><span class="p">);</span> <span class="c1">// Store the raw pattern vector.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">node</span> <span class="o">=</span> <span class="n">root</span><span class="p">;</span> <span class="c1">// Start from the root of the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">pattern</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate through each operation in the pattern sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">op</span><span class="p">)</span> <span class="o">==</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If a path for this operation does not exist...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">TrieNode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// ...create a new node in the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">node</span> <span class="o">=</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">op</span><span class="p">];</span> <span class="c1">// Move to the next node in the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">node</span><span class="o">-&gt;</span><span class="n">pattern</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// At the end of the pattern, mark this node as a terminal node by storing the full pattern.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">node</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">index</span><span class="p">);</span> <span class="c1">// Store the original pattern index/ID at this terminal node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>searchOp</code> 函数的功能是：给定一个起始 Trie 节点 (parentNode) 和一个MLIR算子 (op)，它会尝试将 op 与parentNode 的子节点进行匹配，并在匹配成功后，递归地对其所有后继算子 (users) 进行 DFS 模式匹配，最终返回这条路径上所能找到的“最佳”匹配模式的末端Trie节点。</p>
<p>这里的“最佳”通常指最长的匹配模式，或者在有多个同样长度的模式时，选择优先级最高的那个 (根据节点中的 <code>output.front()</code>) 大小比较来判断。</p>
<details class="custom-details">
    <summary class="custom-summary">searchOp Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">NodePtr</span> <span class="n">Automation</span><span class="o">::</span><span class="n">searchOp</span><span class="p">(</span><span class="n">NodePtr</span> <span class="n">parentNode</span><span class="p">,</span> <span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">opId</span> <span class="o">=</span> <span class="n">getOpId</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Get the enumerated ID (e.g., TX8BE_OPS::CONV) for the current MLIR operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isRealOp</span><span class="p">(</span><span class="n">op</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">opId</span><span class="p">)</span> <span class="o">==</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// If the current op is a &#34;real&#34; operation (not a terminator, etc.) but cannot be found in the children of the parent Trie node, it&#39;s a mismatch.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// This &#39;if&#39; block seems to be an early exit for a specific case, possibly redundant with the final return.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">opId</span><span class="p">)</span> <span class="o">!=</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If a path exists in the Trie for the current operation `opId`. This is a potential match.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// If the current op matches, continue downwards
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">currentNode</span> <span class="o">=</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">opId</span><span class="p">];</span> <span class="c1">// Move to the matched Trie node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">tempNode</span> <span class="o">=</span> <span class="n">currentNode</span><span class="p">;</span> <span class="c1">// `tempNode` will store the longest match found so far starting from this path.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// --- Query Operation Attributes and Users ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">queryInterface</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e_mlir</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Get a specific interface from the operation for querying attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">needStore</span> <span class="o">=</span> <span class="n">queryInterface</span><span class="p">.</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needStore</span><span class="p">;</span> <span class="c1">// Check an attribute, e.g., if the op&#39;s result needs to be stored.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">llvm</span><span class="o">::</span><span class="n">SmallSet</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="mi">1</span><span class="o">&gt;</span> <span class="n">users</span><span class="p">;</span> <span class="c1">// Find all direct users of the current operation&#39;s result.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">sortedUsers</span> <span class="o">=</span> <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">sortOps</span><span class="p">(</span><span class="n">users</span><span class="p">);</span> <span class="c1">// Sort the users, likely topologically or based on some priority.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// --- Recursively Search Through Users ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">sortedUsers</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">isRealOp</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip non-essential ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">            <span class="k">auto</span> <span class="n">interface</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e_mlir</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="k">auto</span> <span class="n">needLoad</span> <span class="o">=</span> <span class="n">interface</span><span class="p">.</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needLoad</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">needStore</span> <span class="o">&amp;&amp;</span> <span class="n">needLoad</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip paths with certain attribute mismatches (e.g., store-load dependency).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Recursively call searchOp for the user operation, starting from the current Trie node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">auto</span> <span class="n">terminalNode</span> <span class="o">=</span> <span class="n">searchOp</span><span class="p">(</span><span class="n">currentNode</span><span class="p">,</span> <span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">// --- Update Best Match ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">tempNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If both the previous best match (`tempNode`) and the new match (`terminalNode`) are valid patterns...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="c1">// Compare priority, take the one with the highest priority as the current node pattern)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="k">if</span> <span class="p">(</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">tempNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">())</span> <span class="p">{</span> <span class="o">/</span> <span class="p">...</span><span class="n">update</span> <span class="err">`</span><span class="n">tempNode</span><span class="err">`</span> <span class="n">to</span> <span class="n">the</span> <span class="k">new</span> <span class="n">one</span> <span class="k">if</span> <span class="n">it</span> <span class="n">has</span> <span class="n">a</span> <span class="n">higher</span> <span class="n">priority</span> <span class="p">(</span><span class="n">assuming</span> <span class="n">the</span> <span class="kt">int</span> <span class="n">ID</span> <span class="n">represents</span> <span class="n">priority</span><span class="p">).</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tempNode</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If `tempNode` was not a valid pattern end, but `terminalNode` is, update it.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="n">tempNode</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// TFOOTER(TRACE)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">return</span> <span class="n">tempNode</span><span class="p">;</span> <span class="c1">// Return the node corresponding to the longest/best pattern found from this point.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Indicates parent node cannot match current op, return parent node)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">parentNode</span><span class="p">;</span> <span class="c1">// If no match was found for `opId` in the Trie, return the original `parentNode`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>search</code>遍历计算子图 (subgraph) 中的每一个算子，并以该算子为起点，尝试进行模式匹配。</p>
<ol>
<li>预处理阶段 (第一个 walk)
在正式开始匹配之前，函数会先遍历一次整个子图，目的是收集和注册一些元数据：</li>
</ol>
<ul>
<li><code>manager_-&gt;opOrder_</code>: 一个 vector 记录图中所有算子的出现顺序。</li>
<li><code>manager_-&gt;opIndexMap_</code>: 为每个算子分配一个唯一的整数索引。
这些信息对于后续的管理和可能的图变换 (如拓扑排序) 非常重要。</li>
</ul>
<ol start="2">
<li>逐点匹配阶段 (第二个 walk)它再次遍历子图中的每一个算子 op 每次都是从 Trie 树的根节点 root 开始 <code>searchOp(root, op)</code> 函数。意味着尝试从零开始匹配所有已知的模式。 searchOp 会返回从 op 开始能找到的最长/最优的匹配模式的末端节点 (terminalNode).</li>
</ol>
<ul>
<li>如果其 output 列表不为空，说明 searchOp 成功地找到了一条完整的匹配路径。函数就会将这个匹配结果记录下来：在 manager 中更新 Pattern 对象，并在本地的 result map 中建立从起始算子 op到模式ID的映射。</li>
<li>反之说明从 op 开始无法匹配任何完整的模式，于是就什么也不做，继续检查下一个算子。</li>
</ul>
<details class="custom-details">
    <summary class="custom-summary">search Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">search</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span> <span class="n">subgraph</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// k: the starting operation of a matched pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// v: the type/ID of the matched pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">initDefsMap</span><span class="p">(</span><span class="n">subgraph</span><span class="p">);</span> <span class="c1">// Initialize manager with definition information from the subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">subgraph</span><span class="o">-&gt;</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// First pass: walk through the subgraph to gather metadata.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">opOrder_</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Record the sequential order of all operations.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">opIndexMap_</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span><span class="o">++</span><span class="p">;</span> <span class="c1">// Assign a unique index to each operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Second pass: walk through the subgraph again to perform the actual pattern matching.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">subgraph</span><span class="o">-&gt;</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Skip the return operation of the subgraph as it&#39;s not part of a computational pattern.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphReturnOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">WalkResult</span><span class="o">::</span><span class="n">skip</span><span class="p">();</span> <span class="c1">// In newer MLIR, this might be `return;`. Skips processing this op&#39;s children.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">pattern</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Pattern</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Create a Pattern object, representing a potential match starting at `op`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">patterns_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">pattern</span><span class="p">);</span> <span class="c1">// Add this potential pattern to the manager&#39;s list.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">patternMap_</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// Map the operation `op` to its corresponding Pattern object.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// terminalNode 就是最后匹配到的一个Node (terminalNode is the final matched Node)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// This is the main call to the recursive search function, starting from the Trie root for each `op`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">terminalNode</span> <span class="o">=</span> <span class="n">searchOp</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// If the Node has an output, it means a match was found. If multiple matches exist, they are replaced based on priority during the search phase
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// The final result is a match for the highest-priority pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Check if the search returned a valid pattern-terminating node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// If a match was found, update the Pattern object with the results from the terminal node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">pattern</span><span class="o">-&gt;</span><span class="n">setPattern</span><span class="p">(</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">(),</span> <span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">pattern</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Record the result: map the starting operation `op` to the matched pattern&#39;s ID.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">WalkResult</span><span class="o">::</span><span class="n">advance</span><span class="p">();</span> <span class="c1">// Proceed to the next operation in the walk.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupoptimizationpass">GroupOptimizationPass</h2>
<p>会遍历一个计算 subGraph 中的所有 OP. 对于每一个通过筛选的普通计算操作，会调用 <code>createSingleGroup</code> 函数来为其创建一个专属的 GroupOp.
<code>createSingleGroup</code> 会检查原始 OP 的所有输入。如果输入来自另一个计算操作，那么这个输入就会成为新 GroupOp 的输入。如果输入是 LoadConstOp，则被视为这个分组的内部依赖，而不是外部输入。原始 op 的所有输出会直接成为新 GroupOp 的输出。</p>
<p>新的 GroupOp 拥有上一步定义的输入和输出。原始的操作 op 和它的常量依赖 (dependencies) 被移动到这个新创建的 GroupOp 内部。最后，修改原始操作 OP 的连接关系，使其在分组内部能够正确地接收输入并产生输出。伪代码如下</p>
<pre tabindex="0"><code>for op in subGraph.ops:

  // 检查操作的类型
  if op == (GroupOp || ReturnOp || LoadConstOp || NoneOp):
    continue

  createSingleGroup(op)

------------------------------------
createSingleGroup(op):
  for pre_op in op.inputsOp:
    // 判断前置操作是否为“加载常量”或“空操作”
    if pre_op == (LoadConstOp || NoneOp):
      // 如果是，则将其添加到依赖项 (dependencies) 集合中
      dependencies.add(pre_op)
    else:
      // 如果是其他普通操作，则将其结果添加到新分组的输入 (groupInput) 中
      groupInput.add(pre_op.result)

  for result in op.results:  // 遍历当前操作的所有输出结果
    // 将这些结果添加到新分组的输出 (groupOutput) 中
    groupOutput.add(result)

  // 使用收集好的输入和输出创建一个新的 GroupOp (分组操作) 
  create GroupOp(groupInput, groupOutput)

  // 将依赖项 (如常量) 移动到新分组的末尾 (或内部) 
  move dependencies to group end

  // 将原始操作 op 本身也移动到新分组的末尾 (或内部) 
  move op to group end

  // 修改原始操作 op 的输入和输出，使其在新分组内部正确连接
  change op input and output
</code></pre><p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBca62e625dd418d0b51deb2e46c83f873?method=download&amp;shareKey=3b9dddfeca5108a0665fce242dd1019d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBca62e625dd418d0b51deb2e46c83f873?method=download&amp;shareKey=3b9dddfeca5108a0665fce242dd1019d" alt="GroupOptimizationPass">
    </a><figcaption>GroupOptimizationPass</figcaption></figure></p>
<h2 id="groupldstpass">GroupLdStPass</h2>
<p><code>GroupLdStPass</code> 作用用是处理 GroupOp 的输入和输出，通过显式插入 Load 和 Store 操作，来“固化”和“隔离”GroupOp 的边界。</p>
<p>Load 插入流程</p>
<ol>
<li>识别 Load 需求: 函数遍历 GroupOp 的每一个输入参数v。然后，它查找所有在 GroupOp 外部使用 v 的算子 (userOp) 。通过检查这些userOp的属性 (needLoad) ，它判断哪些 userOp 需要一个显式的 Load 操作来获取 v 的值。</li>
<li>处理特殊布局: 代码中有一段特殊的逻辑 (<code>if(isa&lt;...&gt;)</code>) ，用于处理 Add、Sub 等二元算子。它检查输入的layout 如果存在不匹配的情况 (例如一个NCx布局和一个Tensor布局) ，它可能会强制layout统一，以确保硬件能够正确计算。</li>
<li>插入 LoadVarOp: 在确定了所有需要 Load 的外部用户后，如果这样的用户存在 (<code>usersLoad.size() != 0</code>)，它会在GroupOp的入口处创建一个tx8e::LoadVarOp操作。</li>
<li>重定向数据流: 将所有外部用户对原始输入 v 的连接 (SSA use-def chain) ，全部断开，并重新连接到新创建的LoadVarOp的输出上 (replaceUsesOfWith).</li>
</ol>
<p>Store 插入流程</p>
<ol>
<li>识别存储需求: 函数找到 GroupOp 内部的 return 操作，并遍历它的每一个操作数 (即 GroupOp 的输出值). 通过检查产生这些输出值的内部算子 (pre_op) 的needStore属性，来判断哪些输出需要被显式地Store，以便外部世界能够访问。</li>
<li>插入 StoreVarOp: 如果一个输出值需要被存储，函数会在 GroupOp 的末尾、return 操作之前，创建一个tx8e::StoreVarOp 接收 GroupOp 的内部计算结果。</li>
<li>更新返回结果: StoreVarOp本身也有一个输出。函数会更新 GroupOp 的 return 操作，使其返回 StoreVarOp 的输出，而不是原始的内部计算结果。</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">GroupLdStPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">GroupLdStPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">subgraph</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span> <span class="n">g_op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//  For each group&#39;s input, insert a load. If used by multiple ops, multiple loads are inserted
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">v</span> <span class="p">:</span> <span class="n">g_op</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">().</span><span class="n">getArguments</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Iterate over each input argument of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">Operation</span><span class="o">*</span> <span class="n">pre_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">v</span><span class="p">);</span> <span class="c1">// Find the operation that produces this input.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">usersLoad</span><span class="p">;</span> <span class="c1">// A map to store users that need to load this input.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">userOp</span> <span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Find all users of this input argument.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Check if the user needs a &#39;load&#39; based on its attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">((</span><span class="o">!</span><span class="n">opAttr</span><span class="p">.</span><span class="n">needLoad</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">arg_idx</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// If a load is needed, record the user and its argument index.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">usersLoad</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_pair</span><span class="p">(</span><span class="n">userOp</span><span class="p">,</span> <span class="n">arg_idx</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// This block handles complex layout logic for Add/Sub/Mul/Div ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// It seems to ensure that if one input to &#39;add&#39; is rank1 tensor, the other is also handled correctly,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// potentially by forcing a specific layout (`LayoutMode::Cx`).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">AddOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">SubOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">DivOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">MulOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">userOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [复杂布局逻辑]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="p">}</span>
</span></span><span class="line"><span class="cl">          
</span></span><span class="line"><span class="cl">          <span class="k">if</span> <span class="p">(</span><span class="n">usersLoad</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// there are users that require a load operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">NamedAttribute</span><span class="o">&gt;</span> <span class="n">tmp_attrs</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [构建LoadVarOp的属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="c1">// Create the Load operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="k">auto</span> <span class="n">ld</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">LoadVarOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">g_op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">v</span><span class="p">.</span><span class="n">getType</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">tmp_attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [设置动态shape属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              
</span></span><span class="line"><span class="cl">              <span class="c1">// For each user that needs the load...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">userOp</span> <span class="p">:</span> <span class="n">usersLoad</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// ...replace its use of the original input `v` with the result of the new `Load` operation `ld`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">userOp</span><span class="p">.</span><span class="n">first</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ld</span><span class="p">.</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// For each group&#39;s output, insert a store
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">builder</span><span class="p">.</span><span class="n">setInsertionPointToEnd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">block</span><span class="p">);</span> <span class="c1">// Set the insertion point to the end of the group&#39;s body.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">Operation</span> <span class="o">*</span><span class="n">g_return</span> <span class="o">=</span> <span class="n">g_op</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">().</span><span class="n">getTerminator</span><span class="p">();</span> <span class="c1">// Get the return operation of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">getNumOperands</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate over each output of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">value</span> <span class="o">=</span> <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">          <span class="k">auto</span> <span class="n">pre_op</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="n">getDefiningOp</span><span class="p">();</span> <span class="c1">// Find the operation inside the group that produces this output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// Check if this output value needs to be stored for external users.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">pre_op</span><span class="p">)).</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needStore</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">i</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// ... [构建StoreVarOp的属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// Create the Store operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">st</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">StoreVarOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">g_op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">value</span><span class="p">.</span><span class="n">getType</span><span class="p">(),</span> <span class="n">value</span><span class="p">,</span> <span class="n">st_attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">          <span class="c1">// ... [设置动态shape属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">          <span class="c1">// Update the group&#39;s return instruction to return the result of the store op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">setOperand</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">st</span><span class="p">.</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">moveBefore</span><span class="p">(</span><span class="n">gBlock</span><span class="p">,</span> <span class="n">block</span><span class="p">.</span><span class="n">end</span><span class="p">());</span> <span class="c1">// Move the return instruction (not standard MLIR, might be custom logic).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">updateIR</span><span class="p">(</span><span class="n">g_op</span><span class="p">);</span> <span class="c1">// Update the IR of the group op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupmappingpass">GroupMappingPass</h2>
<p><code>GroupMappingPass</code> 作用是将顶层模块 (Module) 中定义的全局维度信息 (x_dim 和 y_dim) 设置到每一个 GroupOp 或 GroupOp 的调用点上。</p>
<details class="custom-details">
    <summary class="custom-summary">GroupMappingPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Defines a function to perform a simple mapping of groups.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">simpleGroupMapping</span><span class="p">(</span><span class="n">ModuleOp</span> <span class="n">module</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get x and y dimension from the module&#39;s attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// These attributes are likely defined globally for the entire compilation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">uint32_t</span> <span class="n">x_dim</span> <span class="o">=</span> <span class="n">module</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">TileDx</span><span class="p">).</span><span class="n">getInt</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="kt">uint32_t</span> <span class="n">y_dim</span> <span class="o">=</span> <span class="n">module</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">TileDy</span><span class="p">).</span><span class="n">getInt</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Create an OpBuilder instance, which is a helper to create/modify MLIR operations.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">OpBuilder</span> <span class="n">builder</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get the &#39;main&#39; function from the module.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">main</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">getMainFuncOp</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get the first block (entry block) of the main function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">main_block</span> <span class="o">=</span> <span class="n">main</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="nl">inner</span> <span class="p">:</span> <span class="n">main_block</span><span class="p">.</span><span class="n">getOperations</span><span class="p">())</span> <span class="p">{</span>  <span class="c1">// Iterate over all operations within the main function&#39;s body
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">CallOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// The module&#39;s main function contains CallOps. This implies an indirect call to a subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Find the subgraph definition (&#39;SubraphOp&#39;) using the symbol name from the CallOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span> <span class="n">sg</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">lookupSymbol</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">CallOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">).</span><span class="n">getCallee</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">      <span class="c1">// Walk through the operations inside the called subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// We are looking for the &#39;GroupOp&#39; which is the actual unit of computation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">sg</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span> <span class="n">gop</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Set a &#39;dev_region&#39; attribute on the located GroupOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">setDevRegionAttr</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">gop</span><span class="p">.</span><span class="n">getOperation</span><span class="p">(),</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">});</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// The module&#39;s main function directly contains GroupOps.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Directly set the &#39;dev_region&#39; attribute on the GroupOp found in the main function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">setDevRegionAttr</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                       <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">).</span><span class="n">getOperation</span><span class="p">(),</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">GroupMappingPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>  <span class="c1">// It will operate on the entire ModuleOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="n">simpleGroupMapping</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupcostpass">GroupCostPass</h2>
<p><code>GroupCostPass</code> 作用是为一个 GroupOp 在所有可能的切分策略中，通过 Cost Model 搜索并应用最优的一个。算法流程如下。</p>
<p>准备阶段 (Preparation):</p>
<ol>
<li>Bailout Condition: <code>if (gop-&gt;hasAttr(&quot;group_tag&quot;) &amp;&amp; ... == 2) return;</code> 如果 GroupOp的 <code>group_tag==2</code>，那么这个 Pass 就无需为它搜索切分策略了，直接返回。</li>
<li>拷贝编译选项: <code>costoption_lg.dynCompile = compileOption_-&gt;dynCompile;</code> 从一个全局的<code>compileOption_</code> 中拷贝了一系列编译参数到局部的 costoption_lg 中. 表明 Pass 的行为可以被外部配置所影响。</li>
<li>创建搜索空间: <code>auto space = std::make_shared&lt;SliceSpace&gt;();</code> 创建了一个名为 space 的对象，这个 SliceSpace 类封装了该 GroupOp 的完整搜索空间。它包含了所有可能的张量切分方式。</li>
<li>模板机制: <code>if (useTemplate) { ... }</code> 检查 <code>compileOption_-&gt;sliceHelpMap</code> 的全局映射。如果之前已经为相似的 GroupOp (由 GroupKey 标识) 计算过最优策略，它就会直接从缓存中读取结果 (sliceHelp) ，从而避免昂贵的重复搜索。如果找到了模板，它会直接应用并提前返回。</li>
</ol>
<p>搜迭代搜索循环 (The Core: Iterative Search Loop)</p>
<ol>
<li><code>while (1)</code> 循环: 这个无限循环是搜索算法的主体。</li>
<li>探索策略: 在循环内部，space对象会生成一个候选的切分策略。这通过 <code>space-&gt;shardingLevel</code> 和<code>space-&gt;factorSpace_</code> 来控制，它们共同定义了当前正在尝试的切分维度和方式。</li>
<li>判断搜索是否完成: <code>if (space-&gt;shardingLevel.isSpaceFinish() &amp;&amp; ...)</code>. 在每次迭代开始时，它会检查是否已经遍历了所有的切分可能性。如果搜索空间已耗尽，循环就会终止。</li>
<li>成本估算: 如果找到一个有效的候选策略，接下来就是估算这个策略的成本。动态构建Pass流水线:</li>
</ol>
<ul>
<li><code>auto pm = std::make_unique&lt;LgPassManager&gt;(...);</code> 添加一系列估算Pass:
<ul>
<li><code>pm-&gt;add_pass(createDataSplitNewPass(space));</code> // 根据策略进行数据切分</li>
<li><code>pm-&gt;add_pass(createTimeStepNewPass(space));</code> // 划分时间步</li>
<li><code>pm-&gt;add_pass(createSPMAllocPass(space));</code>    // 模拟SPM (片上内存) 分配</li>
<li><code>pm-&gt;add_pass(createEstimatePass(space));</code>    // 估算性能/成本</li>
</ul>
</li>
<li>运行估算流程: <code>pm-&gt;run(gop);</code></li>
</ul>
<ol start="5">
<li>比较和选择最优解: 估算完成后，<code>space-&gt;status</code> 会被更新 (SSTATUS_OK 表示估算成功，SSTATUS_SA_MemAlloc 表示内存分配失败) . 如果估算成功，它会获取成本 t，并与已知的 bestCost 进行比较。如果当前策略更优，就更新 bestCost 和 bestStrategy。</li>
</ol>
<p>应用最优策略 (Applying the Best Strategy)</p>
<ol>
<li>应用策略: <code>sliceHelp.strategy = space-&gt;strategy;</code> 和后续的 <code>compileOption_-&gt;IRHelp.ops[gop] = space-&gt;stageOps;</code> 等赋值操作，就是将搜索到的最优策略结果 (包括每个操作的切分方式、循环信息等) 保存到 compileOption_中，供后续的 Pass 使用。</li>
<li>具体计算: <code>gop-&gt;walk(...)</code> 它遍历 GroupOp 内部的操作 (如GemmOp) ，并根据策略 (lSharding, rSharding) 计算出具体的循环边界 (ls, rs) 和分片长度 (pLen) ，这些信息会被存入 gls (<code>GroupLoopSpace</code>) 对象中。</li>
</ol>
<h3 id="datasplitnewpass">DataSplitNewPass</h3>
<p>其中也包括好几个 pass
<code>DS_SpaceInitPass</code> 作用是初始化分布式策略的搜索空间。对 groupOp 中的每一个算子，它会调用 <code>space_-&gt;shardinglevel.init</code> 这个函数会根据算子自身的特性、全局约束 (如 max_sharding) 以及用户配置 (如 opt_search) ，生成该算子所有可能的切分方式。</p>
<p><code>init</code> 函数首先获取了算子的维度 dim 和目标切分路数 maxSharding，然后调用 getShardings 找出一个张量在所有维度上进行整数倍切分、且总切分路数恰好等于 maxSharding 的所有组合来填充 shardings 列表。随后，将这些组合 (并额外加上了不切分的方案) 包装成带有性能评估因子的 ShardingSpace 对象，并存入一个有序集合 <code>std::set&lt;ShardingSpace&gt; spaces</code> 中。ShardingSpace 重载了小于操作符用于对切分策略排序。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ShardingSpace</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">list</span><span class="o">&lt;</span><span class="n">ShardingInfo</span><span class="o">&gt;</span> <span class="n">shardings</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 预估的性能参数，即空间上能用到pow(2,x)个tile
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">uint8_t</span> <span class="n">factor</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 关键点：重载小于操作符，定义排序规则
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="k">operator</span><span class="o">&lt;</span><span class="p">(</span><span class="k">const</span> <span class="n">ShardingSpace</span> <span class="o">&amp;</span><span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 性能高的在前面
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="n">factor</span> <span class="o">&gt;</span> <span class="n">other</span><span class="p">.</span><span class="n">factor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="k">operator</span><span class="o">==</span><span class="p">(</span><span class="k">const</span> <span class="n">ShardingSpace</span> <span class="o">&amp;</span><span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">factor</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">factor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ShardingLevel</span><span class="o">::</span><span class="n">init</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">nFirst</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">opt_search</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ... 清理和准备工作 ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  
</span></span><span class="line"><span class="cl">  <span class="c1">// 1. 获取算子输出Tensor的维度数量 (Rank) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int32_t</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getRank</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 2. 准备容器
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&gt;</span> <span class="n">shardings</span><span class="p">;</span> <span class="c1">// 用于接收所有合法的sharding方案
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">tempSharding</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>   <span class="c1">// 一个临时的、大小为dim的向量，用于递归
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 3. 调用核心递归函数，启动搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - curDim=0: 从第0维开始搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - allDim=dim: 总共有dim个维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - curSharded=1: 当前已累乘的切分系数为1 (乘法单位元) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - maxSharding: 最大切分数目，即为每个 chip 的 tile 数目 (16)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">getShardings</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">tempSharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 4. 手动添加“不切分”的方案
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    递归函数只会寻找乘积等于maxSharding的组合，但[1, 1, ..., 1] (不切分)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    是一个非常重要的基础方案，这里手动添加进去。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">shardings</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// ... 后续处理 ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">sharding</span> <span class="p">:</span> <span class="n">shardings</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSpace</span> <span class="n">newShardingSpace</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isValid</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 1. 为每个sharding方案计算性能因子
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">newShardingSpace</span><span class="p">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">getFactor</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">sharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// ... (省略部分逻辑) ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    
</span></span><span class="line"><span class="cl">    <span class="c1">// 2. 将包含factor的ShardingSpace对象插入set中
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">spaces</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">newShardingSpace</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>getShardings</code> 函数采用的是递归算法，目标是找到所有整数向量 <code>s = {s_0, s_1, ..., s_{dim-1}}</code>，使得 <code>s_0 * s_1 * ... * s_{dim-1} == maxSharding</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ShardingLevel</span><span class="o">::</span><span class="n">getShardings</span><span class="p">(</span><span class="kt">int32_t</span> <span class="n">curDim</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">allDim</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">curSharded</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">maxSharding</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&amp;</span> <span class="n">sharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 1. 递归终止条件 (Base Case) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">curDim</span> <span class="o">==</span> <span class="n">allDim</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 已经处理完所有维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">curSharded</span> <span class="o">==</span> <span class="n">maxSharding</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 并且累乘结果正好等于目标
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// // succeeded
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">shardings</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">sharding</span><span class="p">);</span> <span class="c1">// 找到了一个合法解，存入结果列表
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span><span class="p">;</span> <span class="c1">// 回溯
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 2. 递归主体：遍历当前维度的所有可能切分系数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">maxSharding</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 尝试将当前维度(curDim)的切分系数设为 i
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">sharding</span><span class="p">[</span><span class="n">curDim</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 更新已累乘的切分系数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">curSharded</span> <span class="o">*=</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 3. 剪枝优化 (Pruning) ：这是算法效率的关键！
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 如果当前累乘的结果已经超过了目标，那么无论后续维度如何取值，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 最终结果必然大于 maxSharding，所以没有必要继续递归下去了。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">curSharded</span> <span class="o">&lt;=</span> <span class="n">maxSharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 如果还有希望，则对下一个维度进行递归搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">getShardings</span><span class="p">(</span><span class="n">curDim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">allDim</span><span class="p">,</span> <span class="n">curSharded</span><span class="p">,</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">sharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 4. 回溯 (Backtracking) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 无论上面的递归是否成功，当它返回后，我们需要“撤销”当前的选择，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 以便在 for 循环的下一次迭代中尝试新的值。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">curSharded</span> <span class="o">/=</span> <span class="n">i</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>getFactor</code> 遍历每个维度，基于内存对齐等硬件限制，计算出该维度上最大合理的切分数量 maxShardingDim.
将用户提议的切分数量 <code>sharding[i]</code> 与 maxShardingDim 取最小值，得到该维度上的有效切分数量。将所有维度上的有效切分数量相乘，得到总的有效并行度 tileNum. 对 tileNum 取以2为底的对数并向上取整后返回。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">uint8_t</span> <span class="n">ShadingLevel</span><span class="o">::</span><span class="n">getFactor</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span> <span class="n">sharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">tileNum</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">rank</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// a. 判断是否需要对齐：这里只对最后一个维度特殊处理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">align</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// b. 获取对齐基数 (alignBase)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    如果需要对齐，则调用 GetAlignBase 获取一个对齐值，否则为1 (相当于不对齐) 。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这个 alignBase 很可能代表硬件一次最优处理的最小数据块大小。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint32_t</span> <span class="n">alignBase</span> <span class="o">=</span> <span class="n">align</span> <span class="o">?</span> <span class="n">GetAlignBase</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span> <span class="o">:</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// c. 计算当前维度的最大合理切分路数 (maxShardingDim)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    一个维度能被切成多少份，不仅取决于它的总大小，还取决于对齐要求。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    例如，一个维度大小为100，但硬件要求必须按16对齐处理，那么最多只能切成 ceil(100/16) = 7 份。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">maxShardingDim</span> <span class="o">=</span> <span class="n">CEIL</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alignBase</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// d. 计算“有效”的切分路数并累乘
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这是关键！它在“提议的切分路数(sharding[i])”和“最大合理切分路数(maxShardingDim)”之间取最小值。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这意味着，即使你提议将一个维度切16份，但如果硬件限制最多只能切7份，那也只能算7份的贡献。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这可以防止对一个维度进行“无效的过度切分”。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">tileNum</span> <span class="o">*=</span> <span class="n">MIN</span><span class="p">(</span><span class="n">maxShardingDim</span><span class="p">,</span> <span class="n">sharding</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">ceil</span><span class="p">(</span><span class="n">log2</span><span class="p">(</span><span class="n">tileNum</span><span class="p">)));</span> <span class="c1">// 向上取整
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>一个例子如下</p>
<pre tabindex="0"><code>storeOp outShape[3, 4, 128, 4096]
level0: [1, 1, 1, 16], [1, 1, 2, 8], [1, 1, 4, 4]...   factor=16
level1:[1, 8, 1, 2], [1, 8, 2, 1]....                  factor=8
level2:[1, 16, 1, 1]                                   factor=4 
level3:[16, 1, 1, 1]                                   factor=2
</code></pre><p><code>DS_TileShardingPass</code> 按顺序遍历 groupOp 中的算子，并像一个状态机一样检查和更新各算子的分布式策略状态。其在每次执行时，仅为当前的待定算子，从其预先生成并排好序的候选策略列表中，选出下一个最优的切分方案并进行更新，然后立即终止当次运行。整个图的最终切分方案是通过反复执行此 Pass，将决策从图的入口逐步传播到出口而最终确定的。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBda0206ff3ade37b3cb94b73f3a564489?method=download&amp;shareKey=bd6f962093568ee599a982e7b7b1a300" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBda0206ff3ade37b3cb94b73f3a564489?method=download&amp;shareKey=bd6f962093568ee599a982e7b7b1a300" alt="An Example of Sharding">
    </a><figcaption>An Example of Sharding</figcaption></figure></p>
<p><code>DS_TileSplitPass</code> 首先检查算子是否需要 <code>reduceSplit</code> (例如 GeMM 切分 k 维度). 如果 reduce 维度切分状态为 <code>s.srfinish = true</code> 才会进行后续的 split 方案。</p>
<ol>
<li>从后向前 (或根据 nFirst 标志决定方向) 检查算子的各个维度，找到第一个“还可以再切分”的维度。判断依据是该维度切分后的大小是否已达到系统设定的最小值 (s.sliceShapeMin) .</li>
<li>一旦找到目标维度 updateDim，它会调用一个名为 <code>getNextSplit</code> 的函数。它会根据当前维度的切分值 <code>s.splitTry[updateDim]</code> 计算出下一个可能的切分值。例如，如果当前是 2，getNextSplit 可能会返回 4.</li>
<li>更新与记录：它将这个新的切分值更新到尝试性方案 <code>s.splitTry</code> 中，并记录下这次更新<code>space_-&gt;splitRecord.update(...)</code>.</li>
<li>在对当前算子的循环结束时，它会将探索出的 <code>s.splitTry</code> 赋值给最终方案 <code>s.split</code>.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBab1aedb258273670b60e2b54295f1f6c?method=download&amp;shareKey=afd6b38561485627a11fd118670b8431" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBab1aedb258273670b60e2b54295f1f6c?method=download&amp;shareKey=afd6b38561485627a11fd118670b8431" alt="An Example of Split try of above Sharding">
    </a><figcaption>An Example of Split try of above Sharding</figcaption></figure></p>
<p><code>DS_SlicePropagatePass</code> 后序遍历 (即从 groupOp 的输出到输入) 的方式反向传播切分决策，其逻辑是：对于每一个算子 (消费者)，它会调用该算子实现的 <code>ShardingInterface</code> 接口中的 <code>tileShardingSplit</code> 方法，来精确计算出其上游算子 (生产者) 应该如何切分数据以满足消费者的需求。这如果自动接口推导失败，它会回退去读取算子上预设的 <code>tile_parallel</code> 属性作为人工指令。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBb14ede2d32d997490222f36c1f0acc21?method=download&amp;shareKey=a244cf2ec9b5f31b5f3ef0ff2aa370a0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBb14ede2d32d997490222f36c1f0acc21?method=download&amp;shareKey=a244cf2ec9b5f31b5f3ef0ff2aa370a0" alt="An Example of Propagation">
    </a><figcaption>An Example of Propagation</figcaption></figure></p>
<p><code>DS_UpdateSliceIRPass</code> 核心策略是通过分析图中 reduceOp 来反向推断和划分流水线阶段。通过检查每个 reduceOp 自身的并行复杂度 (例如，tpSplit &gt; 1) 来判断其上游的计算类型，从而为不同的流水线打上诸如 STAGEIC2OC (模型并行规约段) 或 STAGEOC2NH (模式切换段) 的标签。在完成对所有算子的阶段划分后，它会最终计算每个阶段的流水线深度，并整理输出一份包含并行循环类型、算子分组和流水线阶段信息的完整执行。</p>
<ol>
<li>
<p>首先从 reduceOps 栈中取出一个关卡算子。然后，它利用 <code>getNEOPTPSlice</code> 等辅助函数，分析这个算子自身的切分策略，判断它具体采用了哪种张量并行方式。</p>
</li>
<li>
<p>确定连接到当前这个 reduceOp 的上一段流水线是什么类型</p>
</li>
</ol>
<ul>
<li><code>if (tpSplit &gt; 1)</code>: 如果这个关卡算子本身是一个张量并行度大于 1 的算子，代码就判断出：通往这个算子的路径，是一段需要最终进行集合通信 (C) 的路径。因此，它将这段路径的类型标记为 <code>STAGEIC2OC</code>.</li>
<li><code>else if (s.reduceSplit &gt; 1)</code>：如果不是上面那种情况，代码会检查另一种模型并行模式。如果一个算子的规约维度被切分了，同样意味着后续需要一个 AllReduce 集合通信。因此，它把这段路径标记为 <code>STAGEIC2IC</code>.</li>
<li>如果两个条件都不满足，意味着这可能是一个不同并行模式之间的切换，例如从模型并行切换回数据并行，此时会使用默认的 <code>STAGEOC2NH</code> 标记.</li>
</ul>
<ol start="3">
<li>通过 <code>updateLoopStage</code> 函数，将两个 reduceOp 算子之间的所有普通算子，都归类到刚刚在第 2 步中决策出的 lastRuduceLoopStage.</li>
<li>处理完所有的 reduceOp 后遍历所有算子，根据 LoopStageMap_ 中的记录，将算子放入对应的“篮子”里。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBa90c52245265edade98cd9f5b15d59bb?method=download&amp;shareKey=ccf9d74acc3642b1eac881133e98c6b9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBa90c52245265edade98cd9f5b15d59bb?method=download&amp;shareKey=ccf9d74acc3642b1eac881133e98c6b9" alt="DS_UpdateSliceIRPass">
    </a><figcaption>DS_UpdateSliceIRPass</figcaption></figure></p>
<h3 id="ts_swpipelinepass">TS_SwPipelinePass</h3>
<p>TS_SwPipelinePass 核心是调用 getPipeline 函数。其内部通过顺序执行以下三个关键步骤，。</p>
<p><code>getInitPipelineOps</code></p>
<ol>
<li>为每个流水线阶段 (如 STAGENH2OC, STAGEOC2IC等) 创建一个独立的 pipeline 列表。</li>
<li>按 IC -&gt; OC -&gt; NH 顺序来拼接这些列表。在拼接时，它会检查每个阶段的循环次数。如果循环次数大于1：它并不会简单地将操作列表复制多次，而是创建一个特殊的、类型为 PipelineOpsBase 的 <strong>Repeat 节点</strong>。这个节点内部包含需要重复的子流水线 (<code>repeatBase.repeat</code>) 和重复次数 (<code>repeatBase.repeatTimes</code>) . 然后，它将这个Repeat 节点作为一个单一的、原子性的元素，插入到下一个阶段的流水线中。这是一种高效表示嵌套循环的方法。
如果循环次数不为 1：它就直接使用 splice 操作，将当前阶段的算子列表完整地移动并拼接到下一个阶段的尾部。</li>
</ol>
<p>经过层层拼接和嵌套，该函数最终返回一个名为 groupPipeline 的 std::list。这个列表就是一份完整的、线性的逻辑执行剧本，其中所有的嵌套循环都被抽象成了 Repeat 节点。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB58fc5fb9b7b1fb4880eb020bce68afd5?method=download&amp;shareKey=f4ae610fb730689a22fe38a7226ee6e5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB58fc5fb9b7b1fb4880eb020bce68afd5?method=download&amp;shareKey=f4ae610fb730689a22fe38a7226ee6e5" alt="getInitPipelineOps">
    </a><figcaption>getInitPipelineOps</figcaption></figure></p>
<p><code>pipeline </code></p>
<p>主要工作是处理上一阶段生成的 Repeat 节点，并对流水线的衔接处进行深度优化，以减少气泡 (硬件空闲周期) 。</p>
<ol>
<li>
<p>当它在流水线中遇到一个 Repeat 节点时，它会对该节点内部的子流水线再次调用pipeline函数 (<code>auto inner = pipeline((it).repeat, ...)</code>). 通过这种方式展开任意层级的嵌套循环。</p>
</li>
<li>
<p>在处理循环的边界时，它调用 getRetract 和 doRetract 这对复杂的优化工具。</p>
</li>
</ol>
<ul>
<li><code>getRetract</code>: 在连接两个循环迭代 (或不同的流水线段) 时，通过 canParallel 函数检查后一个迭代的“头部指令”是否可以和前一个迭代的“尾部指令”并行执行，从而计算出最大可以“回缩” (即提前执行) 的指令数量。</li>
<li><code>doRetract</code>: 在 getRetract 探明了可回缩的数量后，doRetract 负责物理地修改流水线。它通过 splice 操作，将后一个迭代头部的指令，合并到前一个迭代尾部的指令列表中，从而填补了潜在的执行空隙。</li>
</ul>
<p><code>getEnginsPipeline</code> 将优化后的操作序列，翻译成具体的、分配到不同硬件引擎的指令。</p>
<ol>
<li>
<p>函数遍历输入的 pipelineOps 列表。列表中的每个元素 opsBase 代表一个流水线周期 (一“帧”) 内需要共同执行的一组MLIR操作。</p>
</li>
<li>
<p>对于每个周期，它创建一个 enginsBase 对象。这个对象是一个结构体，包含了分别对应不同硬件引擎 (如 <code>ld</code> for Load, <code>st</code> for Store, <code>ne</code> for Neural Engine, <code>tdma</code> for DMA) 的成员变量。</p>
</li>
<li>
<p>遍历当前周期的所有 op，通过查询每个 op 的 engine 属性 <code>queryOpAttr().engine</code>，得知这个操作预定由哪个硬件引擎来执行。接着，它将这个 op 的指针存放到 enginsBase 对象中对应的引擎 slot 里。例如，一个 <code>NPU_ENGINE_LOAD</code> 类型的操作会被放入 <code>enginsBase.ld</code> 列表。</p>
</li>
</ol>
<p>函数最终返回一个 <code>std::list&lt;PipelineBase&gt;</code> 描述了在同一个时钟周期内，加载、存储、计算等多个硬件单元应该同时执行**哪些不同的操作。</p>
<h3 id="spmallocpass">SPMAllocPass</h3>
<p>SPMAllocPass 包括三个 pass，下面依次介绍，首先介绍用到的数据结构</p>
<p><code>BufferLabel</code> 作为缓冲区的唯一标识符，将其链接到程序中的特定 <code>mlir::Value</code> ，并注意它是否为 Imm.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @struct BufferLabel
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @brief A unique identifier for a memory buffer.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * This struct links a buffer to a specific MLIR Value and tracks whether it&#39;s
</span></span></span><span class="line"><span class="cl"><span class="cm"> * a special &#34;immediate&#34; buffer. It&#39;s used as a key in maps to associate
</span></span></span><span class="line"><span class="cl"><span class="cm"> * MLIR Values with their buffer metadata.
</span></span></span><span class="line"><span class="cl"><span class="cm"> */</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">BufferLabel</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// The MLIR Value that this buffer represents, typically a tensor produced
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// by an operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mlir</span><span class="o">::</span><span class="n">Value</span> <span class="n">v</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// A flag indicating if this buffer holds a special &#34;immediate&#34; value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Immediate values might be treated differently during allocation (e.g.,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// small constants or internal scratchpads for an op).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">isImm</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @brief Equality operator to compare two labels.
</span></span></span><span class="line"><span class="cl"><span class="cm">     *
</span></span></span><span class="line"><span class="cl"><span class="cm">     * Two labels are considered equal if they refer to the same MLIR Value
</span></span></span><span class="line"><span class="cl"><span class="cm">     * and have the same &#39;isImm&#39; status. This is necessary for using
</span></span></span><span class="line"><span class="cl"><span class="cm">     * BufferLabel as a key in std::map or std::unordered_map.
</span></span></span><span class="line"><span class="cl"><span class="cm">     */</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="k">operator</span><span class="o">==</span><span class="p">(</span><span class="k">const</span> <span class="n">BufferLabel</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">v</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">isImm</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">isImm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p><code>ValueBuffer</code> 包含单个缓冲区所需的所有元数据，包括其标识、生存期、大小和最终内存位置。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @struct ValueBuffer
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @brief Represents the metadata for a single memory buffer, including its
</span></span></span><span class="line"><span class="cl"><span class="cm"> * lifetime, size, and allocation information.
</span></span></span><span class="line"><span class="cl"><span class="cm"> */</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ValueBuffer</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// The unique identifier for this buffer.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">BufferLabel</span> <span class="n">label</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Represents the starting point of the buffer&#39;s lifetime (inclusive),
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// measured in pipeline cycles. After memory allocation, this field may be
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// repurposed to store the starting memory address.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">start</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Represents the ending point of the buffer&#39;s lifetime (inclusive),
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// measured in pipeline cycles. After memory allocation, this field may be
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// repurposed to store the ending memory address.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">end</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// The total size of this buffer in bytes, as required by its tensor shape.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">allSize</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Size of an intermediate/temporary buffer that an operator might need
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// internally. This is often allocated contiguously with the main output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// buffer. For example, the final output address would be &#39;offset + immSize&#39;.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">immSize</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// The final memory offset assigned to this buffer in the scratchpad memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// This value is determined by the final memory allocation pass.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">offset</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @brief Less-than operator, used for sorting ValueBuffer objects.
</span></span></span><span class="line"><span class="cl"><span class="cm">     *
</span></span></span><span class="line"><span class="cl"><span class="cm">     * The active implementation sorts buffers primarily by their lifetime start
</span></span></span><span class="line"><span class="cl"><span class="cm">     * time. This is a common strategy for greedy &#34;first fit&#34; style memory
</span></span></span><span class="line"><span class="cl"><span class="cm">     * allocation algorithms. The commented-out code shows an alternative
</span></span></span><span class="line"><span class="cl"><span class="cm">     * strategy of sorting by buffer size.
</span></span></span><span class="line"><span class="cl"><span class="cm">     */</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="k">operator</span><span class="o">&lt;</span><span class="p">(</span><span class="k">const</span> <span class="n">ValueBuffer</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// return this-&gt;allSize &lt; other.allSize; // Alternative sorting by size
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">return</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">start</span> <span class="o">&lt;=</span> <span class="n">other</span><span class="p">.</span><span class="n">start</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p><code>SA_BufferLifePass</code>的核心功能是分析并确定每一个需要存放在 ScratchPad Memory 中的数据块 (Buffer，即mlir::Value对应的张量) 的生命周期。</p>
<ol>
<li>构建“定义-使用”时间表。Pass 的输入是 <code>TS_SwPipelinePass</code> 生成的最终流水线执行序列 pipelineReal. 这个序列的每一项都代表一个流水线周期，以及该周期内各个硬件引擎执行的操作。代码遍历这个流水线序列，逐个周期 (由timeStepNum计数) 地分析。它会构建两个核心的映射表：</li>
</ol>
<ul>
<li><code>opIsTemp</code>: 记录在哪一个时间步 (timeStepNum) ，有哪些值 (mlir::Value) 被定义或产出。例如，ld (加载) 和 ne (计算) 操作的输出都会被记录。</li>
<li><code>consumerOps</code>: 记录在哪一个时间步，有哪些值被作为输入消费掉了。</li>
</ul>
<p>产出：这个步骤完成后，Pass就拥有了一份完整的、按时间步索引的“谁在何时被创建”和“谁在何时被使用”的清单。</p>
<ol start="2">
<li>确定每个Buffer的生命周期。Pass会遍历所有算子和它们的输入 (operands) ，为每一个作为输入的 Value (即inValue) 计算其生命周期。</li>
</ol>
<ul>
<li>确定生命周期终点 (end)：一个 Value 的生命周期，在其被作为输入 (被消费) 时达到一个终点。因此，当代码在时间步 curTs 处理一个消费者算子时，其输入 inValue 的 <code>buf.end</code> 就被设置为 curTs.</li>
<li>确定生命周期起点 (start)：为了找到inValue何时被创建，代码会调用一个 <code>getNearestProducer</code> 的函数。这个函数会拿着当前的消费时间 curTs 和 inValue，去第一步生成的 opIsTemp (定义时间表) 中反向查找，找到离 curTs 最近的、inValue 被定义的那个时间步 <code>buf.start</code>.</li>
</ul>
<p>计算出的 start 和 end，连同 Value 的标识 (BufferLabel) ，被封装在 ValueBuffer 结构体中，并存入一个全局的数据结构 <code>space_-&gt;vBuffer</code> 里。</p>
<ol start="3">
<li>特殊情况处理</li>
</ol>
<ul>
<li><code>In-place</code>: 对于输入和输出复用同一块内存的 in-place 操作，其生命周期计算必须追溯到最初提供这块内存的那个非in-place算子。代码通过 <code>getInplaceIndex</code> 递归地回溯in-place链，以确保生命周期的 start 时间是正确的、最开始的那个定义时间。</li>
<li>中间值 (imm) 与累加值 (Psum): 代码会识别一些特殊的、可能在多个时间步中存在的中间值或累加值 (由getImmSize 或 isPsumValue 识别) . 对于这些值，它们可能会有多个离散的生存区间。Pass 中可能包含一些后处理逻辑，将这些离散的区间合并成一个从“最早的start”到“最晚的end”的连续大区间，以简化后续的内存分配。</li>
</ul>
<p><code>SA_BufferMergePass</code> 的任务就是清理这些冗余或复杂的生命周期记录，具体来说，就是合并那些存在时间上重叠或包含关系的生命周期区间，为后续的内存分配器提供一个最簡洁、无冗余的区间列表。</p>
<p>遍历由上一个 Pass 生成的 space_-&gt;vBuffer 这个map. 其中的每一项，key 是缓冲区的唯一标识 BufferLabel，value是该缓冲区所有生命周期区间的列表 <code>std::vector&lt;ValueBuffer&gt;</code>. 对于每一个value的生命周期列表，它都调用 <code>mergeOverlap</code> 来进行处理。最后，它用函数返回的、经过清理和合并的新的列表，来替换掉 map 中旧的列表。该函数流程如下</p>
<ol>
<li>根据 ValueBuffer 重载的 <code>operator&lt;</code> (即按 start 时间升序) ，将所有生命周期区间进行排序。</li>
<li>遍历已排序的列表，将 start 时间相同的连续区间收集到一个临时的 buf 向量中。遇到一个不同 start 时间的区间时，它会按照结束时间 end 排序之前收集的 buf，然后将处理后的结果 (除了最后一个元素) 重新放回 valueBuf.</li>
<li>合并被完全包含的子区间。它维护着当前最大的生命周期区间 (<code>[usedTSStart, usedTSEnd]</code>). 遍历列表中的每一个区间 <code>*it</code>. 根据 <code>bool isSub = ((*it).start &gt;= usedTSStart) &amp;&amp; ((*it).end &lt;= usedTSEnd);</code> 判断区间是否在时间上被上一个“激活”的区间完全覆盖。</li>
</ol>
<ul>
<li>如果 isSub 为 true，意味着 *it 是一个冗余的子区间。因为只要为那个更大的激活区间分配了内存，这个子区间的内存需求自然也就满足了。因此，代码通过 valueBuf.erase(it); 将这个冗余的子区间直接删除。</li>
<li>如果 isSub 为 false，说明遇到了一个新的、没有被覆盖的生命周期，于是它将成为新的“激活”区间，用于和后续的区间进行比较。</li>
</ul>
<h1 id="compile-option-1-opt_barrier">Compile Option 1: opt_barrier</h1>
<p>由 <code>groupDAGPass</code> 实现。通过 group 间的依赖关系来给 group 定层级，同一层级的 group 只有最后一个 group 需要 barrier.</p>
<ol>
<li>
<p>初始化所有 group 的 need_barrier 属性为 false。</p>
</li>
<li>
<p>从后往前遍历 group，若 group 的结果无 user 或要 return，设置 layerNum 为 0，否则设置为 userOp 的 layerNum + 1. 同时维护两个 vector: firstOpInLayers 和lastOpInLayers 来记录每一层级的第一个 op 和最后一个 op. 遍历结束把 lastOpInLayers中的 group 的 need_barrier 属性设为 true.</p>
</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8e3541f411fb9039713f3992b450f4b9?method=download&amp;shareKey=4f5dfad0d8b5a4bd8cf49ce94d5ad7c9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8e3541f411fb9039713f3992b450f4b9?method=download&amp;shareKey=4f5dfad0d8b5a4bd8cf49ce94d5ad7c9" alt="opt_barrier">
    </a><figcaption>opt_barrier</figcaption></figure></p>
<h1 id="compile-option-1-opt_ddr">Compile Option 1: opt_ddr</h1>
<p>由 <code>ddrConstReorderPass</code> 和 <code>ddrVarReorderPass</code> 实现。通过改变 const 和 var 在 ddr 中的排布，使其对齐 DDR_BANK(4096Bytes)，实现加速读取。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd3b8e4e14ab8e2c23f22a625b1d03ddd?method=download&amp;shareKey=948a94db71e0adcddb5f6107ad94a6d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd3b8e4e14ab8e2c23f22a625b1d03ddd?method=download&amp;shareKey=948a94db71e0adcddb5f6107ad94a6d0" alt="opt_ddr">
    </a><figcaption>opt_ddr</figcaption></figure></p>
]]></content:encoded>
    </item>
    <item>
      <title>InternVideo2.5</title>
      <link>http://localhost:1313/blogs/internvideo2.5/</link>
      <pubDate>Thu, 10 Jul 2025 08:40:52 +0800</pubDate>
      <guid>http://localhost:1313/blogs/internvideo2.5/</guid>
      <description>Technical report reading of InternVideo2.5</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>这篇文章的核心目标是提升多模态大语言模型（MLLM）处理视频的能力，特别是处理长且信息丰富的视频上下文（Long and Rich Context, LRC）的能力 。目前，主流的 MLLM 在处理长视频时往往会遇到困难，要么因为计算资源不堪重负而内存溢出，要么在长时序中丢失关键的细节信息，导致理解和推理能力下降。这篇论文的工作旨在增强模型的处理长视频 (length) 的能力和捕捉精细细节 (fineness) 的能力。</p>
<p>文章提出了两大核心技术：Hierarchical Token Compression (HiCo) &amp; Task Preference Optimization (TPO).</p>
<p>HiCo 主要解决处理长视频的问题。视频中存在大量的冗余信息，比如相邻帧之间背景变化很小，或者在一个长镜头中语义信息是相似的。HiCo 剔除这些冗余，保留核心信息。它通过一个三步走的非学习性过程来实现：</p>
<ol>
<li>自适应时间采样：根据视频的长短和内容特性，动态调整采样频率。短视频（如动作片段）需要密集采样来捕捉细节，而长视频（如电影）则稀疏采样以把握事件脉络。</li>
<li>Spatiotemporal Token Merging: 它使用了一种名为ToMe (Token Merging) 的技术，该技术通过计算 token 之间的语义相似度，将相似的进行合并。把视频中意思相近的画面信息捏在一起，而不是像传统方法那样粗暴地丢弃或平均。论文特别指出，与需要大量额外参数和复杂训练的Q-Former 等压缩方法相比，ToMe 是即插即用的，效率极高。</li>
<li>Multimodal Token Dropout：在模型的深层，根据注意力权重动态丢弃那些与当前任务不太相关的视觉通证，进一步精简信息流，让模型能更专注于核心内容 。</li>
</ol>
<p>通过 HiCo，模型可以在不牺牲过多性能的前提下，处理更长的视频序列。实验结果极具说服力：在大海捞针（Needle-in-a-Haystack）测试中，基础模型 InternVL2.5 在处理 500 帧视频时就已经很吃力，超过1000 帧便会内存溢出 。而应用了 HiCo 的 InternVideo2.5，不仅能轻松处理超过 5000 帧的视频，还能在 3000 帧的长度内保持极高的信息检索准确率。可以说记住比原来长 6 倍以上的视频并非虚言。</p>
<p>TPO 主要解决信息丰富的问题，也就是提升模型对精细视觉细节的感知能力。其核心思想是让专家来教通才。通用的 MLLM 虽然能力全面，但在特定视觉任务上（如物体分割、时间定位）往往不如那些专门训练的专家模型。TPO通过 Direct Preference Optimization (DPO )技术，将这些专家模型对特定任务的偏好（即更准确的输出）注入到 MLLM 中。</p>
<p>具体来说，它为 MLLM 增加了专门的“任务头”（Task Head），比如用于时间定位的</p>
<p>Temporal Head和用于实例分割的Mask Head 。在训练时，不仅优化MLLM的基础对话能力，还利用特定任务的数据集（如分割、定位数据集）来优化这些任务头的表现。这样一来，MLLM就好像学会了在需要的时候“调用”这些专家能力。</p>
]]></content:encoded>
    </item>
    <item>
      <title>ServingLLMsOnHuaweiCloudMatrix384</title>
      <link>http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/</link>
      <pubDate>Tue, 17 Jun 2025 21:50:16 +0800</pubDate>
      <guid>http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/</guid>
      <description>Paper Reading of Serving Large Language Models on Huawei CloudMatrix384</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<ol>
<li>
<p><strong>CloudMatrix384硬件架构</strong>:</p>
<ul>
<li>论文提出了一种 <strong>peer-to-peer</strong> 的硬件设计，包含 384 个 <strong>Ascend 910C NPU</strong> 和 192 个 <strong>Kunpeng CPU</strong>，通过 <strong>Unified Bus (UB) Network</strong> 互联。UB 网络支持高带宽 (392 GB/s单向带宽)和低延迟 (1.9 µs)的全局通信，解决了传统AI集群中跨节点通信的瓶颈问题。</li>
<li>架构特点包括:
<ul>
<li><strong>资源解耦和池化</strong>: 计算、存储和网络资源可以动态分配，支持灵活的并行策略 (如专家并行EP、数据并行DP).</li>
<li><strong>三层网络平面</strong>: UB平面 (超节点内通信)、RDMA平面 (跨超节点通信)和VPC平面 (数据中心网络接入)。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CloudMatrix-Infer软件优化</strong>:</p>
<ul>
<li><strong>预填充-解码-缓存 (PDC)解耦架构</strong>: 将LLM推理拆分为Prefill, Decode &amp; Caching 三个子系统，通过 UB网络实现高效协同。</li>
<li><strong>large-scale expert parallelism (EP) strategy</strong>: 支持高达<strong>EP320</strong>的专家并行度，每个NPU芯片承载一个专家，减少MoE模型中的通信开销。</li>
<li><strong>UB驱动的分布式缓存</strong>: 利用 <strong>弹性内存服务 (EMS)</strong> 构建全局缓存池，支持KV缓存和模型权重的快速访问。</li>
</ul>
</li>
<li>
<p><strong>性能优化技术</strong>:</p>
<ul>
<li>** micro-batch 流水线 (Microbatch Pipeline)**: 重叠计算和通信，提升资源利用率。</li>
<li><strong>INT8量化</strong>: 在Ascend 910C 上实现高效的8位推理，保持模型精度。</li>
</ul>
</li>
</ol>
<p>论文使用<strong>DeepSeek-R1</strong> (671B参数MoE模型)验证了 CloudMatrix-Infer 的性能:</p>
<ul>
<li><strong>预填充吞吐量</strong>: 6,688 tokens/s/NPU (4.45 tokens/s/TFLOPS)，优于 NVIDIA H100 的 SGLang (3.75 tokens/s/TFLOPS).</li>
<li><strong>解码吞吐量</strong>: 1,943 tokens/s/NPU (1.29 tokens/s/TFLOPS)，在TPOT &lt;50 ms约束下仍能保持高吞吐。</li>
<li><strong>缓存命中率提升</strong>: 上下文缓存 (Context Caching)在 90% 重用率时，预填充时间减少 59%.</li>
</ul>
<h1 id="1-introduction">1. Introduction</h1>
<p>LLM 的发展趋势有以下几点:</p>
<ol>
<li>参数规模的指数级增长。DeepSeek-R1, LLaMA-4 和 Qwen-3 通常扩展到数千亿甚至数万亿参数.</li>
<li>专家混合 (MoE)架构的广泛采用。MoE 通过每个 token 选择性激活少数专家，引入了结构稀疏性，实现了更大模型下的效率提升，但同时在专家路由和同步方面带来了新的系统级挑战。</li>
<li>上下文长度的大幅提高。上下文窗口从数万 token 扩展到超过一百万 token，对注意力计算和 KV cache 存储施加了巨大压力。 KV Cache 大小随着并发用户数量线性增长，这对其的分布、放置和访问方式提出了重大限制。</li>
</ol>
<p>LLM 服务系统必须适应可变长度的用户输入、跨 token 的不平衡专家激活以及高度突发的用户查询，同时保持严格的延迟和吞吐量目标。满足这些需求不仅仅是简单地扩展硬件资源，而是需要全面的软硬件协同设计，包括紧密集成的计算、内存和网络硬件资源，辅以智能任务调度、自适应运行时编排以及弹性资源管理策略，能够动态响应不断演变的模型结构和波动的工作负载。</p>
<p>为了应对这些挑战，论文引入了 <strong>华为CloudMatrix</strong>，一个旨在重塑 AI 基础设施基础的下一代 AI 数据中心架构。其首个生产级实现是 <strong>CloudMatrix384</strong>，它具备以下核心特点:</p>
<ul>
<li><strong>硬件构成</strong>: 它是一个集成了384个<strong>昇腾 (Ascend)910C NPU</strong>、192个 <strong>鲲鹏 (Kunpeng)CPU</strong> 的AI超级节点。</li>
<li><strong>核心网络</strong>: 所有组件通过一个超高带宽、低延迟的 UB 网络互联，实现了硬件上的 **完全点对点 (peer-to-peer)**通信。</li>
<li><strong>架构优势</strong>: 与传统层级式设计不同，UB网络允许计算、内存等资源被动态池化、统一访问和独立扩展，特别适合处理MoE专家并行和分布式KV缓存访问这类通信密集型操作。</li>
</ul>
<p>基于CloudMatrix384硬件，论文提出了一个名为 <strong>CloudMatrix-Infer</strong> 的综合性LLM服务解决方案，其包含了三大核心创新:</p>
<ol>
<li>
<p><strong>点对点服务架构</strong>: 该架构将推理系统解耦为 <strong>预填充 (prefill)</strong>, <strong>解码 (decode)</strong> 和 <strong>缓存 (caching)</strong> 三个可独立扩展的资源池。借助UB网络，所有 NPU 都能统一访问共享的缓存数据，从而摆脱了传统架构中因数据局部性限制而导致的调度复杂性和效率低下问题。</p>
</li>
<li>
<p><strong>大规模专家并行 (LEP)策略</strong>: 该策略专为MoE模型优化，利用UB网络高效地进行 token 分发和专家输出合并。它支持极高的专家并行度 (如 <strong>EP320</strong> )，允许每个 NPU Die 只承载一个专家，从而显著降低解码延迟。</p>
</li>
<li>
<p><strong>硬件感知优化</strong>: 包括为昇腾芯片高度优化的算子、基于 micro-batch 的流水线技术 (以重叠计算和通信)以及INT8量化 (以提升计算效率和减少内存消耗)。</p>
</li>
</ol>
<p>在 DeepSeek-R1 模型上的测试表明，CloudMatrix-Infer 实现了业界领先的性能和效率:</p>
<ul>
<li><strong>预填充性能</strong>: 每个NPU的吞吐量为 <strong>6,688 tokens/s</strong>，计算效率为 <strong>4.45 tokens/s/TFLOPS</strong>.</li>
<li><strong>解码性能</strong>: 在低于50ms的单Token输出延迟 (TPOT)下，每个NPU的吞吐量为<strong>1,943 tokens/s</strong>，计算效率为 <strong>1.29 tokens/s/TFLOPS</strong>.</li>
<li><strong>性能对比</strong>: 预填充和解码的计算效率均<strong>超过了在NVIDIA H100上运行SGLang和在NVIDIA H800上运行DeepSeek的公开数据</strong>.</li>
<li><strong>准确性</strong>: 在昇腾910C上进行的 INT8 量化，其模型准确率与官方 DeepSeek-R1 API 在 16 个基准测试中相当。</li>
</ul>
<h1 id="2-llm-trends-and-their-challenges-for-datacenter-infrastructure">2. LLM Trends and Their Challenges for Datacenter Infrastructure</h1>
<h1 id="21-llm-trends">2.1. LLM Trends</h1>
<p><strong>Ever-Larger Parameter Counts.</strong> scaling law 表明，增加 LLM 的参数数量可以提升其在各种任务上的表现。这一趋势的典型代表包括:</p>
<ul>
<li><strong>Meta 的 Llama 4 Behemoth</strong>: 拥有近 2 万亿参数。</li>
<li><strong>DeepSeek-V3</strong>: 包含 6710 亿参数。</li>
<li><strong>Google 的 PaLM</strong>: 包含 5400 亿参数。</li>
<li><strong>xAI 的 Grok-1</strong>: 拥有 3140 亿参数。</li>
</ul>
<p><strong>Sparsity through MoE.</strong> 为了控制不断攀升的训练和推理成本，现代 LLM 越来越多地采用稀疏激活的 MoE 架构。这种架构将模型的总大小与处理每个 token 所需的计算量解耦:</p>
<ul>
<li><strong>Mixtral 8x7B</strong>: 总参数量为 467 亿，但每个 token 只激活 129 亿参数。</li>
<li><strong>Databricks 的 DBRX</strong>: 总参数量为 1320 亿，每个 token 激活 360 亿参数。</li>
<li><strong>Meta 的 Llama 4 系列</strong>: Llama 4 Maverick 使用 128 个专家，而 Llama 4 Scout 使用 16 个专家。</li>
<li><strong>DeepSeek-V3</strong>: 将每层的专家数量从 160 个增加到 256 个，从而在不显著增加计算负载的情况下提升了模型容量。</li>
<li><strong>阿里巴巴的 Qwen3-235B</strong>: 集成了 128 个专家，每个 token 激活 220 亿参数。</li>
<li><strong>华为的盘古 Ultra MoE</strong>: 总参数量达 7180 亿，每个 token 激活 390 亿参数。</li>
</ul>
<p>这些模型共同凸显了 LLM 扩展策略的范式转变，即更强调通过架构稀疏性而非单纯的参数数量来提升性能和效率。</p>
<p><strong>Extension of Context Windows.</strong> LLM 上下文窗口的扩大使其能够处理更长的序列，这对于需要扩展推理和连贯性的任务至关重要。近期的进展包括:</p>
<ul>
<li><strong>OpenAI 的 GPT-4.5</strong>: 支持 128,000 个 token 的上下文窗口。</li>
<li><strong>Google 的 Gemini 2.5 Pro</strong>: 提供高达 100 万个 token 的上下文窗口。</li>
</ul>
<p>然而处理长文本会显著增加计算成本和推理延迟。为了缓解这一问题，生产系统普遍采用<strong>上下文缓存 (context caching)</strong> 技术。该技术通过存储和复用由先前提示片段生成的 KV block，来消除对提示的冗余注意力计算，从而降低延迟并提高效率。</p>
<h2 id="22-challenges-for-datacenter-infrastructure">2.2. Challenges for Datacenter Infrastructure</h2>
<p>上述 LLM 的发展趋势对底层的数据中心基础设施提出了严峻的新要求。随着模型能力的扩展，它们催生了如强化学习、交互式媒体生成和自主 AI 代理等日益复杂的工作负载。这些应用不仅需要海量的计算和内存资源，还需要对基础设施进行根本性的重新架构，以支持高带宽通信和低延迟存储，从而在动态、异构的真实世界条件下满足严格的服务水平目标。</p>
<p><strong>Scaling Communication-Intensive Parallelism.</strong>. 随着模型规模的增长，单个计算节点已无法容纳最先进的 AI 模型，必须使用多节点并行策略。尽管现有的 AI 集群通过 RDMA 网络支持跨节点通信，但其带宽和拓扑结构通常只为数据并行 (DP) 或流水线并行 (PP) 等通信量较小的场景优化。然而，张量并行 (Tensor Parallelism, TP) 和 专家并行 (Expert Parallelism, EP) 需要频繁、细粒度且低延迟的通信，这种通信模式难以在节点之间高效扩展。这迫使许多部署方案将 TP 和 EP 限制在单个计算节点内，从而限制了可扩展性。</p>
<p><strong>Maintaining High Utilization under Heterogeneous AI Workloads.</strong> 现代 AI 工作负载表现出高度多样化和动态的资源需求:</p>
<ul>
<li>训练任务通常是计算密集型的。</li>
<li>LLM 推理的解码阶段往往受限内存带宽。</li>
<li>自动驾驶模型训练等任务涉及大量的 CPU 端数据预处理。</li>
</ul>
<p>固定的节点配置无法高效地适应这种多样性，常常导致资源过配或利用不足。为了最大化效率和适应性，现代 AI 基础设施必须能够根据每种工作负载的特定需求，动态、细粒度地组合异构资源 (如 NPU, CPU 和内存).</p>
<p><strong>Enabling Converged Execution of AI and Data-Intensive Workloads.</strong> AI 工作流与传统数据密集型操作 (如数据摄取、预处理、检索、分析和模拟) 的交叉越来越频繁。同时，数据库、大数据和高性能计算 (HPC) 等通用工作负载本身也在不断集成 AI 功能。这种融合执行模式要求高吞吐、低延迟的通信和灵活的资源编排。然而，主要为传统通用工作负载优化的老旧数据中心基础设施难以满足这些苛刻的要求。</p>
<p><strong>Delivering Memory-class Storage Performance.</strong> 现代 AI 流水线操作的数据规模已远超传统存储系统的能力。诸如摄取 PB 级数据集、管理 TB 级模型检查点以及支持延迟敏感的推理 (特别是在使用大型 KV cache 和检索增强生成 RAG 模块时) 等任务，都需要存储子系统具备<strong>内存级的带宽、延迟和 IOPS (Input/Output Operations Per Second)</strong>. 围绕磁盘访问模式设计的传统存储层次结构频繁地成为性能瓶颈，因数据供给不足而导致 NPU 利用率低下。</p>
<h1 id="3-huawei-cloudmatrix">3 Huawei CloudMatrix</h1>
<h2 id="31-vision-for-huawei-cloudmatrix">3.1. Vision for Huawei CloudMatrix</h2>
<p>如下图所示，CloudMatrix 超越了传统以 CPU 为中心的分层设计，实现了无需 CPU 介质下包括 NPU、CPU、DRAM、SSD、NIC 及领域专用加速器在内的所有异构系统组件之间的直接高性能通信。该架构的核心是超高带宽、低延迟的 UB 网络。</p>
<p><strong>Scalable Communication for TP/EP.</strong> UB 网络可以为张量并行 (TP) 和专家并行 (EP) 提供强大的通信支持，使其能够轻松扩展到单节点边界之外。</p>
<p><strong>Flexible Resource Composition for Heterogeneous Workloads.</strong> 将 CPU、NPU、内存等资源解耦成独立的资源池，允许根据工作负载按需、细粒度地进行组合。</p>
<p><strong>Unified Infrastructure for Converged Workloads.</strong> 在单一架构内同时支持 AI 和数据密集型工作负载的融合执行。</p>
<p><strong>Memory-class Storage via Disaggregated Memory Pool.</strong> 通过聚合集群中所有 CPU 挂载的 DRAM，形成一个可通过 UB 访问的共享高性能内存池，为 KV cache 复用、参数加载等提供加速。</p>
<h2 id="32-cloudmatrix384-overview-a-fully-peer-to-peer-hardware-architecture">3.2. CloudMatrix384 Overview: A Fully Peer-to-Peer Hardware Architecture</h2>
<p>CloudMatrix384 是一个集成了 <strong>384 个昇腾 (Ascend) 910C NPU</strong> 和 <strong>192 个鲲鹏 (Kunpeng) CPU</strong> 的 AI 超级节点。其最显著的特点是，通过 UB 网络实现了跨节点通信性能与节点内性能的高度一致 (带宽下降 &lt; 3%，延迟增加 &lt; 1µs). CloudMatrix384 包含三个互补的网络平面:</p>
<ol>
<li><strong>UB 平面</strong>: 超级节点内的主要<strong>纵向扩展 (Scale-Up)</strong> 网络，以全互联、无阻塞的方式连接所有 384 个 NPU 和 192 个 CPU. 每个昇腾 910C NPU 为该平面贡献超过 392 GB/s 的单向带宽。</li>
<li><strong>RDMA 平面</strong>: 用于超级节点间的<strong>横向扩展 (Scale-Out)</strong> 通信，采用 RoCE 协议，确保与现有 RDMA 生态的兼容性。每个 NPU 为该平面贡献高达 400 Gbps 的单向带宽。</li>
<li><strong>VPC 平面</strong>: 通过擎天卡 (Qingtian Card) 将超级节点接入更广泛的数据中心网络，用于管理、控制和访问持久化存储等操作。</li>
</ol>
<h2 id="33-hardware-components">3.3. Hardware Components</h2>
<ul>
<li><strong>昇腾 910C 芯片</strong>: 作为系统的核心，它采用双晶粒 (dual-die) 封装。每个芯片包提供 752 TFLOPS 的 BF16/FP16 算力，支持 INT8 数据类型。它集成 128 GB 片上内存，总带宽高达 3.2 TB/s.</li>
<li><strong>昇腾 910C 节点</strong>: 每个计算节点集成 8 个昇腾 910C NPU 和 4 个鲲鹏 CPU.</li>
<li><strong>UB 交换系统</strong>: 采用两级 (L1/L2) 无阻塞交换网络拓扑，将所有节点紧密连接成一个统一的超级节点。</li>
</ul>
<h2 id="34-software-stack">3.4. Software Stack</h2>
<ul>
<li><strong>CANN (神经网络计算架构)</strong>: 华为为昇腾 NPU 开发的完整软件生态系统，类似于 NVIDIA 的 CUDA. 它包含驱动层、运行时层 (Runtime) 和库层 (如用于分布式通信的 HCCL)，并通过图引擎 (Graph Engine, GE) 对上层框架 (如 PyTorch, TensorFlow) 的计算图进行编译和优化。</li>
<li><strong>云部署基础设施软件</strong>: 包括 MatrixResource、MatrixLink、MatrixCompute 和 MatrixContainer 等一系列软件，用于在云环境中对 CloudMatrix 集群进行资源管理、网络配置和容器化部署。上层的 <strong>ModelArts</strong> 平台则提供端到端的 AI 开发和 MLOps 服务。</li>
</ul>
<h2 id="35-suitability-analysis-for-deepseek-models">3.5. Suitability Analysis for DeepSeek Models</h2>
<details class="custom-details">
    <summary class="custom-summary">DeepSeek Models and Their Deployment on NVIDIA H800</summary>
    <div><p>DeepSeek 在由 NVIDIA H800 GPU 组成的集群上部署其 V3 和 R1 模型，每个 GPU 内存 80 GB，节点内通过 NVLink 连接，节点间通过 400 Gbps InfiniBand 连接。该部署采用了分离式预填充-解码架构。在预填充阶段，DeepSeek 将四个 H800 节点（共 32 个 GPU）组织成一个部署单元。在每个单元内，256 个路由专家被策略性地分布在 GPU 上，每个 GPU 负责 9 个路由专家和 1 个共享专家。该配置标为 DP32+EP32，利用 32 个 GPU 之间的 EP，同时共享专家和 MLA 机制通过 DP 在同一组 GPU 上复制。在解码阶段，DeepSeek 进一步扩展并行度至 DP144+EP144，将 18 个节点组合成总计 144 个 GPU。在这一更大规模的部署中，每个 GPU 管理两个路由专家和一个共享专家，保持系统范围内 32 个路由专家副本的冗余。</p>
<p>DeepSeek 采用了 DualPipe 策略用于重叠计算和 All-to-all 通信。当一个 micro-batch 正在进行 MoE 相关的 dispatch 和 combine 时，下一个 micro-batch 则同时进行局部注意力或 MLP 计算。</p>
<p>每个 H800 GPU 在 prefill 阶段达到最高 9,213 token/s (6.3% 的上下文缓存命中率). 剔除缓存命中后有效吞吐量为 4,026 token/s. Decode 阶段，每个 GPU 维持平均 1,850 token/s 的吞吐量。</p>
</div>
</details><br>
<p>本节分析了 CloudMatrix384 的架构特性为何与大规模 MoE 模型 (以 DeepSeek-R1 为例) 的需求高度协同。</p>
<ul>
<li><strong>MoE 通信协同</strong>: 高带宽、低延迟的 UB 网络非常适合 MoE 模型中通信开销巨大的 token dispatch 和专家输出 combine 阶段。</li>
<li><strong>内存容量与管理</strong>: 整个超级节点提供高达 49.2 TB 的 NPU 内存，足以容纳像 DeepSeek-R1 (671B 参数) 这样的巨型模型及其庞大的 KV cache.</li>
<li><strong>上下文缓存复用</strong>: UB 网络使 NPU 能够以内存级的速度直接访问由 CPU DRAM 构成的解耦内存池，极大地加速了历史 KV cache 的读取，从而降低了首 token 生成延迟 (TTFT).</li>
<li><strong>量化支持</strong>: 昇腾 910C 对 INT8 计算的原生支持，为通过量化来降低模型内存占用、减少计算开销和提升推理性能提供了宝贵的机会。</li>
</ul>
<h1 id="4-deepseek-serving-on-huawei-cloudmatrix384">4. DeepSeek Serving on Huawei CloudMatrix384</h1>
<h2 id="41-overview-a-peer-to-peer-serving-architecture-with-pdc-disaggregation">4.1. Overview: A Peer-to-Peer Serving Architecture with PDC Disaggregation</h2>
<p>如下图所示，文中提出了一种独特的点对点服务架构，将系统划分为三个功能子系统，prefill, decode 和 caching (PDC)，每个子系统独立运行，并通过显式的 KV cache 传输接口进行通信。</p>
<p><strong>KVCache-centric vs. Peer-to-Peer Serving Architectures:</strong> 现有的 LLM 服务系统如 NVIDIA Dynamo（NVIDIA Corporation，2025）和 Mooncake（Qin 等，2025）采用 KVCache 为中心的设计，其中请求调度与 KV cache 的局部性紧密耦合。在这些系统中，请求通常被路由到已经持有对应 KV cache 的特定计算节点。此类缓存感知调度对于缓解远程内存访问带来的显著性能损失至关重要，因为节点内存访问（例如通过 PCIe，约 256 GB/s）远远快于节点间带宽（通常约 25 GB/s 或 200 Gbps）。因此，远程 KV cache 加载通常会带来较大延迟。然而，这种设计引入了复杂的调度难题，并且在动态工作负载下可能导致负载均衡恶化。此外，该设计限制了全局资源效率，因为解码节点上的 DRAM 通常处于孤立状态且利用率低，无法有效贡献于共享缓存容量。</p>
<p>CloudMatrix-Infer 中的点对点服务架构充分利用了 CloudMatrix384 的超高带宽 UB 互联。这使得基于解耦内存池构建的分布式缓存集群 (4.4) 能够实现统一访问。无论 NPU 是执行预填充任务还是解码任务，都可以直接访问共享解耦内存池。这种完全的点对点设计有效地扁平化了内存层次结构，弥补了本地访问和远程访问延迟之间的传统差距。这种将请求调度与 KV 缓存放置解耦带来了几个关键优势。</p>
<ol>
<li>使推理请求可以调度到任何可用的 NPU 实例，而不受数据局部性的限制。显著提升了系统范围内的负载均衡和 NPU 利用率。</li>
<li>消除了对复杂的亲和性调度机制的需求，从而降低了架构复杂性，简化了系统维护。</li>
<li>通过在预填充和解码节点之间共享 DRAM 资源，系统形成了一个统一的弹性缓存底层，提高了内存利用率，增加了缓存命中率，并在负载失衡或突发情况下提供了更强的弹性。</li>
</ol>
<p>每个预填充实例在 CloudMatrix384 上配备 16 个 Ascend 910C NPU（32 个芯片），并以 32 路专家并行（EP32）运行。每个 rank 上放置 10 个专家: 1 个共享专家、8个路由专家和 1 个冗余路由专家，以支持专家并行负载均衡（EPLB）。为了进一步提高效率，对 MLA 计算采用混合并行策略，并应用基于 micro-batch 的流水线以重叠通信开销（4.3）。</p>
<p>每个解码实例分配了 160 个 Ascend 910C NPU（320 个芯片）应于 MoE 层的 320 路专家并行（EP320）. 每个 rank 承载一个专家，整体配置包括 32 个共享专家、256 个独立路由专家和 32 个冗余路由专家，以支持 EPLB。为了进一步加速解码，引入了优化的 Ascend 原生算子、流水线解码策略以及 MTP (4.2).</p>
<h2 id="42-tightly-coupled-decode-with-large-scale-expert-parallelism">4.2. Tightly-Coupled Decode with Large-scale Expert Parallelism</h2>
<h3 id="421-fused-communication-operators-for-lep">4.2.1 Fused Communication Operators for LEP</h3>
<p>MoE 计算流程为: 在 gate network 为每个 token 选择 Top-K 专家后，进入 FFN 阶段前需要两次 all-to-all 通信。第一次 all-to-all 操作交换路由信息 (如 token 到专家的分配信息). 第二次 all-to-all 操作交换实际的 token 数据。该数据最初以 BF16 格式存储，为减少通信和计算开销每个 NPU 进行量化为 INT8 格式，然后由分配的 FFN 处理。计算完成后，第三次 all-to-all 通信将专家输出发送回其原先的 rank，每个 NPU 执行最终的 token 合并以重构输出。该流程存在三个低效问题:</p>
<ol>
<li>通信开销大: 三次 all-to-all 通信引加上通信横跨几百个 NPU 导致延迟很大。</li>
<li>动态形状: 因为每次解码迭代中分配给每个专家的 token 数量不同，导致 all2ll 通信中数据形状不固定。需要动态内存分配和频繁的 CPU-NPU 同步，降低了执行效率。</li>
<li>顺序依赖:MoE 计算的顺序执行特性导致步骤之间存在依赖关系，降低了资源利用率和吞吐量。</li>
</ol>
<p>为了解决这些问题，本文开发了 FusedDispatch 和 FusedCombine 两个融合算子，将通信和计算集成在一起，专门设计用于在 CloudMatrix384 上实现最佳的解码性能。</p>
<p><strong>AIV-Direct Communication across NPUs</strong>: AIV-Direct 使 AI vector 核心能够通过 UB 互连直接将数据写入远程 NPU 的内存，完全绕过了易产生延迟的 Serial Direct Memory Access (SDMA) 路径 (下图蓝线).</p>
<p><strong>Early Quantization</strong>: Dispatch 的时候不再发送 BF16 数据，而是传输 INT8 量化后的数据及其缩放因子。INT8 表示每个 token (7,168 维) 需要 7 KB。 缩放因子占用 4 字节（INT32），但为了对齐分配 512 B 给缩放因子。因此，每个 token 的传输消息大小为 7.5 KB.</p>
<p><strong>Static Execution via Shared‑Memory Pre‑allocation</strong>: 在每个 NPU rank 中静态预分配共享内存缓冲区。</p>
$$
buffer_size = rank_num × max_tokens × msg_size
$$<p>
其中 $max_tokens = local_batch × \min(topK, experts_per_die)$. msg_size 是每个 token 的消息长度（INT8 量化后，dispatch 为 7.5 KB，combine 为 14 KB）</p>
<p>由于 FusedDispatch 和 FusedCombine 是连续执行的，共用一个缓冲区会产生竞争，因此采用双缓冲机制来避免写入覆盖。本文设置中每个芯片处理最多 local_batch=96，并最多放置 2 专家，产生 max_tokens=96×min⁡(8,1)=96. 在包含 320 个设备的通信域中，分发缓冲区占用 320×96×7.5⁢KB≈225⁢MB ，合并缓冲区占用 320×96×14⁢KB≈420⁢MB.</p>
<p><strong>Data-Sending Pipeline</strong>: 远程数据写入需要计算目标 NPU 预分配缓冲区的偏移量，但顺序执行此计算和传输会导致执行阻塞。因此文中将执行分成三阶段流水线</p>
<ol>
<li>将下一个 micro-batch 复制到本地 UBuffer.</li>
<li>计算远程缓冲区偏移量，并进行 INT8 量化。</li>
<li>向对应 NPU 的内存发起 AIV-Direct 写入。</li>
</ol>
<p>完整的 FusedDispatch 如下:</p>
<ol>
<li>每个设备会检查自己有哪些 token 要发给其他设备的专家。 AIV 核心从内存里把 token 读到本地的 UBuffer. 把数据量化成 INT8 格式，同时记录缩放因子。给每个 token 加上标签，包括：源 rank ID，token 属于哪个批次（batch-slot ID）以及 token 在数据里的位置（key offset）.通过 AIV-direct 把打包好的数据写到目标节点的预分配内存里。</li>
<li>等所有 token 数据都通过 AIV-direct 发完后，系统会设置一个 barrier，确保每个设备的数据都写完了。设备计算每个专家收到了多少 token 后会互相同步，确保计数没出错。最后设备通过 AIV-direct 再发一个完成标志和设备上每个专家的 token 计数 (每个专家发了几个 token).</li>
<li>每个设备会一直检查别人发来的完成标志，等着所有标志都变成 1. 收到所有标志后，设备会读取每个专家的 token 计数，算出数据在内存里的偏移。然后，设备里的 AIV 核心会并行工作，把收到的数据从共享内存里取出来，整理成一个连续的的输出缓冲区。</li>
</ol>
<p>完整的 FusedCombine 如下:</p>
<ol>
<li>结合 AIV 核心遍历其负责的 peer 设备，根据接收计数，从内存中提取 FFN 结果数据，存入本地 UBuffer。核心利用 token 元数据计算原始设备的接收地址，通过 AIV-direct 通道将数据传输至原始节点的预分配缓冲区。
标志更新</li>
<li>核心根据 token 元数据推算目标设备的标志地址，通过 AIV-direct 发出原子加操作，在对目标节点上的标志增量计数。</li>
<li>每个节点的核心会一直检查自己的标志，等待所有标志都变成 1. 随后从共享内存收集 FFN 输出，提取对应缩放因子，进行逐元素缩放并求和。将合并结果加到共享的 FFN 输出中，生成最终的 token 输出。</li>
</ol>
<h3 id="422-mla-optimization">4.2.2 MLA Optimization</h3>
]]></content:encoded>
    </item>
    <item>
      <title>InternLM3 8B Instruct</title>
      <link>http://localhost:1313/blogs/internlm3-8b-instruct/</link>
      <pubDate>Tue, 17 Jun 2025 10:08:03 +0800</pubDate>
      <guid>http://localhost:1313/blogs/internlm3-8b-instruct/</guid>
      <description>&lt;h1 id=&#34;model--config&#34;&gt;model &amp;amp; config&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/InternLM/lmdeploy/blob/7ca466599f01e5ef93e8951771c62163136e21b2/lmdeploy/pytorch/models/internlm3.py#L304&#34;&gt;Network Definition&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-JSON&#34; data-lang=&#34;JSON&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;architectures&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;InternLM3ForCausalLM&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;attention_dropout&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;auto_map&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;AutoConfig&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;configuration_internlm3.InternLM3Config&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;AutoModel&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;modeling_internlm3.InternLM3Model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;AutoModelForCausalLM&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;modeling_internlm3.InternLM3ForCausalLM&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;bias&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;bos_token_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;eos_token_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;head_dim&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;hidden_act&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;silu&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;hidden_size&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4096&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;initializer_range&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.02&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;intermediate_size&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10240&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;max_position_embeddings&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32768&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;model_type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;internlm3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;num_attention_heads&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;num_hidden_layers&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;48&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;num_key_value_heads&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;pad_token_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;qkv_bias&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;rms_norm_eps&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1e-05&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;rope_scaling&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;factor&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;6.0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;rope_type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;dynamic&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;rope_theta&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;50000000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;tie_word_embeddings&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;torch_dtype&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;bfloat16&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;transformers_version&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;4.47.1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;use_cache&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;vocab_size&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;128512&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;strong&gt;模块 (Module)&lt;/strong&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;strong&gt;子模块 (Sub-module)&lt;/strong&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;strong&gt;功能描述&lt;/strong&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;strong&gt;配置参数&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;InternLM3ForCausalLM&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;model: InternLM3Model&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;模型主干，包含词嵌入和解码器层。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;lm_head: Linear&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;线性输出层，将隐藏状态映射到词汇表大小，生成预测 logits。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;hidden_size: 4096&lt;/code&gt;, &lt;code&gt;vocab_size: 128512&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;InternLM3Model&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;embed_tokens: Embedding&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将输入的 token IDs 转换为稠密向量（Embeddings）。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;vocab_size: 128512&lt;/code&gt;, &lt;code&gt;hidden_size: 4096&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;layers: ModuleList[InternLM3DecoderLayer]&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;包含多个（48个）Transformer解码器层。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;num_hidden_layers: 48&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;norm: RMSNorm&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;在所有解码器层之后，对最终的隐藏状态进行归一化。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;hidden_size: 4096&lt;/code&gt;, &lt;code&gt;rms_norm_eps: 1e-05&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;rotary_emb: RotaryEmbedding&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;计算旋转位置编码（RoPE），用于在注意力机制中融入位置信息。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;head_dim: 128&lt;/code&gt;, &lt;code&gt;max_position_embeddings: 32768&lt;/code&gt;, &lt;code&gt;rope_theta: 50000000&lt;/code&gt;, &lt;code&gt;rope_scaling: {&amp;quot;factor&amp;quot;: 6.0, &amp;quot;rope_type&amp;quot;: &amp;quot;dynamic&amp;quot;}&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;InternLM3DecoderLayer&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;self_attn: InternLM3Attention&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;多头自注意力模块，用于捕捉输入序列中的依赖关系。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;mlp: InternLM3MLP&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;前馈神经网络，用于对注意力输出进行非线性变换。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;input_layernorm: RMSNorm&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;在自注意力模块之前对输入进行层归一化。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;hidden_size: 4096&lt;/code&gt;, &lt;code&gt;rms_norm_eps: 1e-05&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;post_attention_layernorm: RMSNorm&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;在自注意力模块之后、MLP模块之前进行层归一化。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;hidden_size: 4096&lt;/code&gt;, &lt;code&gt;rms_norm_eps: 1e-05&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;InternLM3Attention&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;qkv_proj: Linear&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将输入的隐藏状态线性变换为查询（Q）、键（K）、值（V）。采用分组查询注意力（GQA）。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;hidden_size: 4096&lt;/code&gt;, &lt;code&gt;num_attention_heads: 32&lt;/code&gt; (Q), &lt;code&gt;num_key_value_heads: 2&lt;/code&gt; (K, V)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;o_proj: Linear&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将注意力模块的输出线性变换回隐藏状态的维度。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;hidden_size: 4096&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;apply_rotary_pos_emb&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将旋转位置编码（RoPE）应用于Q和K。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;-&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&amp;mdash;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;InternLM3MLP&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;gate_up_proj: Linear&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;两个并行的线性层（gate 和 up），将隐藏状态映射到中间维度。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;hidden_size: 4096&lt;/code&gt;, &lt;code&gt;intermediate_size: 10240&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;act_fn: SiluAndMul&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;使用 SiLU (Swish) 激活函数并进行逐元素相乘。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;hidden_act: &amp;quot;silu&amp;quot;&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;down_proj: Linear&lt;/code&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;将激活后的中间状态映射回隐藏状态的维度。&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;code&gt;intermediate_size: 10240&lt;/code&gt;, &lt;code&gt;hidden_size: 4096&lt;/code&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;internlm3decoderlayer-运算流程&#34;&gt;InternLM3DecoderLayer 运算流程&lt;/h3&gt;
&lt;p&gt;假设输入为 &lt;code&gt;hidden_states&lt;/code&gt; 和 &lt;code&gt;residual&lt;/code&gt; (在前一层计算得出，第一层时 &lt;code&gt;residual&lt;/code&gt; 等于 &lt;code&gt;hidden_states&lt;/code&gt;)。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h1 id="model--config">model &amp; config</h1>
<p><a href="https://github.com/InternLM/lmdeploy/blob/7ca466599f01e5ef93e8951771c62163136e21b2/lmdeploy/pytorch/models/internlm3.py#L304">Network Definition</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;architectures&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;InternLM3ForCausalLM&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;attention_dropout&#34;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;auto_map&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoConfig&#34;</span><span class="p">:</span> <span class="s2">&#34;configuration_internlm3.InternLM3Config&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoModel&#34;</span><span class="p">:</span> <span class="s2">&#34;modeling_internlm3.InternLM3Model&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;AutoModelForCausalLM&#34;</span><span class="p">:</span> <span class="s2">&#34;modeling_internlm3.InternLM3ForCausalLM&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;bias&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;bos_token_id&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;eos_token_id&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;head_dim&#34;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;hidden_act&#34;</span><span class="p">:</span> <span class="s2">&#34;silu&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;hidden_size&#34;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;initializer_range&#34;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;intermediate_size&#34;</span><span class="p">:</span> <span class="mi">10240</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;max_position_embeddings&#34;</span><span class="p">:</span> <span class="mi">32768</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;model_type&#34;</span><span class="p">:</span> <span class="s2">&#34;internlm3&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_attention_heads&#34;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_hidden_layers&#34;</span><span class="p">:</span> <span class="mi">48</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;num_key_value_heads&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;pad_token_id&#34;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;qkv_bias&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rms_norm_eps&#34;</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rope_scaling&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;factor&#34;</span><span class="p">:</span> <span class="mf">6.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;rope_type&#34;</span><span class="p">:</span> <span class="s2">&#34;dynamic&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;rope_theta&#34;</span><span class="p">:</span> <span class="mi">50000000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;tie_word_embeddings&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;torch_dtype&#34;</span><span class="p">:</span> <span class="s2">&#34;bfloat16&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;transformers_version&#34;</span><span class="p">:</span> <span class="s2">&#34;4.47.1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;use_cache&#34;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;vocab_size&#34;</span><span class="p">:</span> <span class="mi">128512</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>模块 (Module)</strong></th>
          <th style="text-align: left"><strong>子模块 (Sub-module)</strong></th>
          <th style="text-align: left"><strong>功能描述</strong></th>
          <th style="text-align: left"><strong>配置参数</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><code>InternLM3ForCausalLM</code></td>
          <td style="text-align: left"><code>model: InternLM3Model</code></td>
          <td style="text-align: left">模型主干，包含词嵌入和解码器层。</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>lm_head: Linear</code></td>
          <td style="text-align: left">线性输出层，将隐藏状态映射到词汇表大小，生成预测 logits。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>vocab_size: 128512</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3Model</code></td>
          <td style="text-align: left"><code>embed_tokens: Embedding</code></td>
          <td style="text-align: left">将输入的 token IDs 转换为稠密向量（Embeddings）。</td>
          <td style="text-align: left"><code>vocab_size: 128512</code>, <code>hidden_size: 4096</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>layers: ModuleList[InternLM3DecoderLayer]</code></td>
          <td style="text-align: left">包含多个（48个）Transformer解码器层。</td>
          <td style="text-align: left"><code>num_hidden_layers: 48</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>norm: RMSNorm</code></td>
          <td style="text-align: left">在所有解码器层之后，对最终的隐藏状态进行归一化。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>rotary_emb: RotaryEmbedding</code></td>
          <td style="text-align: left">计算旋转位置编码（RoPE），用于在注意力机制中融入位置信息。</td>
          <td style="text-align: left"><code>head_dim: 128</code>, <code>max_position_embeddings: 32768</code>, <code>rope_theta: 50000000</code>, <code>rope_scaling: {&quot;factor&quot;: 6.0, &quot;rope_type&quot;: &quot;dynamic&quot;}</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3DecoderLayer</code></td>
          <td style="text-align: left"><code>self_attn: InternLM3Attention</code></td>
          <td style="text-align: left">多头自注意力模块，用于捕捉输入序列中的依赖关系。</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>mlp: InternLM3MLP</code></td>
          <td style="text-align: left">前馈神经网络，用于对注意力输出进行非线性变换。</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>input_layernorm: RMSNorm</code></td>
          <td style="text-align: left">在自注意力模块之前对输入进行层归一化。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>post_attention_layernorm: RMSNorm</code></td>
          <td style="text-align: left">在自注意力模块之后、MLP模块之前进行层归一化。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>rms_norm_eps: 1e-05</code></td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3Attention</code></td>
          <td style="text-align: left"><code>qkv_proj: Linear</code></td>
          <td style="text-align: left">将输入的隐藏状态线性变换为查询（Q）、键（K）、值（V）。采用分组查询注意力（GQA）。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>num_attention_heads: 32</code> (Q), <code>num_key_value_heads: 2</code> (K, V)</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>o_proj: Linear</code></td>
          <td style="text-align: left">将注意力模块的输出线性变换回隐藏状态的维度。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>apply_rotary_pos_emb</code></td>
          <td style="text-align: left">将旋转位置编码（RoPE）应用于Q和K。</td>
          <td style="text-align: left">-</td>
      </tr>
      <tr>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
          <td style="text-align: left">&mdash;</td>
      </tr>
      <tr>
          <td style="text-align: left"><code>InternLM3MLP</code></td>
          <td style="text-align: left"><code>gate_up_proj: Linear</code></td>
          <td style="text-align: left">两个并行的线性层（gate 和 up），将隐藏状态映射到中间维度。</td>
          <td style="text-align: left"><code>hidden_size: 4096</code>, <code>intermediate_size: 10240</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>act_fn: SiluAndMul</code></td>
          <td style="text-align: left">使用 SiLU (Swish) 激活函数并进行逐元素相乘。</td>
          <td style="text-align: left"><code>hidden_act: &quot;silu&quot;</code></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"><code>down_proj: Linear</code></td>
          <td style="text-align: left">将激活后的中间状态映射回隐藏状态的维度。</td>
          <td style="text-align: left"><code>intermediate_size: 10240</code>, <code>hidden_size: 4096</code></td>
      </tr>
  </tbody>
</table>
<h3 id="internlm3decoderlayer-运算流程">InternLM3DecoderLayer 运算流程</h3>
<p>假设输入为 <code>hidden_states</code> 和 <code>residual</code> (在前一层计算得出，第一层时 <code>residual</code> 等于 <code>hidden_states</code>)。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"><strong>步骤</strong></th>
          <th style="text-align: left"><strong>模块/操作</strong></th>
          <th style="text-align: left"><strong>输入</strong></th>
          <th style="text-align: left"><strong>运算描述</strong></th>
          <th style="text-align: left"><strong>输出</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>1. 输入归一化</strong></td>
          <td style="text-align: left"><code>input_layernorm</code> (RMSNorm)</td>
          <td style="text-align: left"><code>hidden_states</code>, <code>residual</code></td>
          <td style="text-align: left">对 <code>hidden_states</code> 进行 RMS 归一化。同时，将 <code>hidden_states</code> 加上上一层的残差 <code>residual</code>，为后续的残差连接做准备。</td>
          <td style="text-align: left"><code>norm_hidden_states</code>, <code>new_residual</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>2. 自注意力 (Self-Attention)</strong></td>
          <td style="text-align: left"><code>self_attn</code> (InternLM3Attention)</td>
          <td style="text-align: left"><code>norm_hidden_states</code></td>
          <td style="text-align: left"><strong>这是最复杂的部分，内含多个子步骤：</strong><br>a. <strong>QKV 投射</strong>: <code>qkv_proj</code> 将 <code>norm_hidden_states</code> 线性变换，生成查询 <code>Q</code>、键 <code>K</code> 和值 <code>V</code>。<br>b. <strong>位置编码</strong>: <code>apply_rotary_pos_emb</code> 将旋转位置编码 (RoPE) 应用于 <code>Q</code> 和 <code>K</code>，注入位置信息。<br>c. <strong>注意力计算</strong>: <code>attn_fwd</code> 根据 <code>Q</code>、<code>K</code>、<code>V</code> 计算注意力分数，并生成加权和。<br>d. <strong>输出投射</strong>: <code>o_proj</code> 将注意力计算结果线性变换回 <code>hidden_size</code> 维度。</td>
          <td style="text-align: left"><code>attn_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>3. 第一次残差连接</strong></td>
          <td style="text-align: left"><code>+</code></td>
          <td style="text-align: left"><code>attn_output</code>, <code>new_residual</code></td>
          <td style="text-align: left">将步骤 2 的 <code>attn_output</code> 与步骤 1 的 <code>new_residual</code> 逐元素相加。</td>
          <td style="text-align: left"><code>attn_residual_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>4. 注意力后归一化</strong></td>
          <td style="text-align: left"><code>post_attention_layernorm</code> (RMSNorm)</td>
          <td style="text-align: left"><code>attn_residual_output</code></td>
          <td style="text-align: left">对残差连接后的结果进行第二次 RMS 归一化。</td>
          <td style="text-align: left"><code>norm_attn_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>5. MLP (前馈网络)</strong></td>
          <td style="text-align: left"><code>mlp</code> (InternLM3MLP)</td>
          <td style="text-align: left"><code>norm_attn_output</code></td>
          <td style="text-align: left"><strong>包含三个子步骤：</strong><br>a. <strong>Gate &amp; Up 投射</strong>: <code>gate_up_proj</code> 同时将 <code>norm_attn_output</code> 线性变换到 <code>intermediate_size</code>，得到 <code>gate</code> 和 <code>up</code> 两个张量。<br>b. <strong>激活</strong>: <code>act_fn</code> (SiLU and Multiply) 对 <code>gate</code> 应用 SiLU 激活函数，然后与 <code>up</code> 逐元素相乘。<br>c. <strong>Down 投射</strong>: <code>down_proj</code> 将激活后的结果从 <code>intermediate_size</code> 线性变换回 <code>hidden_size</code>。</td>
          <td style="text-align: left"><code>mlp_output</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>6. 第二次残差连接</strong></td>
          <td style="text-align: left"><code>+</code></td>
          <td style="text-align: left"><code>mlp_output</code>, <code>norm_attn_output</code></td>
          <td style="text-align: left">将步骤 5 的 <code>mlp_output</code> 与步骤 4 的 <code>norm_attn_output</code> 逐元素相加。</td>
          <td style="text-align: left"><code>final_hidden_states</code></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>7. 输出</strong></td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">-</td>
          <td style="text-align: left">这一层的最终输出 <code>hidden_states</code> 将作为下一层的输入，而 <code>final_residual</code> 将作为下一层的残差输入。</td>
          <td style="text-align: left"><code>hidden_states</code> (用于下一层), <code>residual</code> (用于下一层)</td>
      </tr>
  </tbody>
</table>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">graph TD
    subgraph &#34;InternLM3DecoderLayer 内部流程&#34;
    
    direction LR
    
    %% 定义输入
    Input[Input: hidden_states&lt;br&gt;Input: residual_in] --&gt; Norm1
    
    %% 第一个模块：注意力
    subgraph &#34;模块1: 注意力 (Pre-Norm)&#34;
        Norm1(RMSNorm:&lt;br&gt;input_layernorm) --&gt; Attention[Self-Attention]
        Input -- residual --o Add1
        Attention -- attn_output --o Add1
    end
    
    %% 第二个模块：MLP
    subgraph &#34;模块2: MLP (Pre-Norm)&#34;
        Add1(第一次&lt;br&gt;残差连接 +) --&gt; Norm2(RMSNorm:&lt;br&gt;post_attention_layernorm)
        Norm2 --&gt; MLP[MLP Block]
        Add1 -- residual --o Add2
        MLP -- mlp_output --o Add2
    end

    %% 定义输出
    Add2(第二次&lt;br&gt;残差连接 +) --&gt; Output[Output: hidden_states&lt;br&gt;Output: residual_out]

    end

    %% 样式定义
    classDef default fill:#f9f9f9,stroke:#333,stroke-width:2px;
    classDef subgraph_style fill:#eef,stroke:#333,stroke-width:2px,color:#333;
    class Input,Output,Add1,Add2,Norm1,Norm2,Attention,MLP subgraph_style
</code></pre><h1 id="group-pattern">Group Pattern</h1>
<p>在 <code>include/tx8be_mlir/OpHelper.h</code> 中添加你自己定义的 pattern 到</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">enum</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">GROUP_NAME</span> <span class="o">=</span> <span class="n">id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">GroupPatternMode</span><span class="p">;</span>
</span></span></code></pre></div><p>在 <code>include/tx8be_mlir/Transforms/LayerGroup/GroupPattern.h</code> 定义好你自己的</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">PATTERN_NAME</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><p>然后添加进 <code>patternConfigMap</code> 中</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;&gt;</span> <span class="n">patternConfigMap</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="n">GROUP_NAME</span><span class="p">,</span> <span class="n">PATTERN_NAME</span><span class="p">},</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>最后在 <code>lib/Support/OpHelper.cpp</code> 的 <code>getGroupPatternMode</code> 函数里添加</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">lowerOption</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s">&#34;opt_group_name&#34;</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">mode</span> <span class="o">=</span> <span class="n">GROUP_NAME</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>opt_group_name</code> 由 <code>run_codegen_layer</code> 命令的 <code>--opt_group=opt_group_name</code> 参数指定</p>
]]></content:encoded>
    </item>
    <item>
      <title>PipeFusion</title>
      <link>http://localhost:1313/blogs/pipefusion/</link>
      <pubDate>Fri, 13 Jun 2025 11:39:32 +0800</pubDate>
      <guid>http://localhost:1313/blogs/pipefusion/</guid>
      <description>Paper Reading of PipeFusion</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>PipeFusion 是一种利用多 GPU 并行来进行 DiT 模型推理的方法。</p>
<ul>
<li>将图像分割成 patch，并将 Transformer Blocks 分布在多个设备上。</li>
<li>通过重用上一步 (<em>one-step stale</em>) 的特征图作为当前步的上下文，消除了流水线中的等待时间。</li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>由于 Attention 的计算特性，计算时间与序列长度的平方成正比，使得 DiT 模型生成高分辨率图形 (长视觉序列) 的推理延迟非常高。
<a href="https://arxiv.org/abs/2402.19481">DistriFusion</a> 观察到在相邻的扩散时间步中输入和激活存在高度相似性，我们将这种现象称为输入时间冗余 (<em>input temporal redundancy</em>). 它保留所有层 KV 的完整形状。内存开销不会随着计算设备数量的增加而减少，在可扩展性方面表现不佳。</p>
<p>如下图所示，DistriFusion 在 2 个设备上都保存一份 DiT 参数。它将图像分成 2 个小块，并对每一层的激活使用异步 allgather. PipeFusion 将 DiT 参数切分到 2 个设备上，将图像分成 4 个 patch ，两个设备之间采用异步 P2P 通信来传输激活。它只在每个设备上传输初始层的输入激活和最终层的输出激活</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB0b7d56775c290d039aba1c7aab220319?method=download&amp;shareKey=6330f70503af01609e9fe13a35826aab" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB0b7d56775c290d039aba1c7aab220319?method=download&amp;shareKey=6330f70503af01609e9fe13a35826aab" alt="Comparsion Between DistriFusion &amp; PipeFusion">
    </a><figcaption>Comparsion Between DistriFusion &amp; PipeFusion</figcaption></figure></p>
<h1 id="background--related-works">Background &amp; Related Works</h1>
<p>扩散模型通常使用 DNN 预测噪声。给定有噪声的图像 xt，模型 ϵθ 将 xt、去噪时间步 t 和附加条件 c (例如文本、图像) 作为输入，以预测 xt 中相应的噪声ϵt.</p>
<p>扩散模型具有较长的序列长度和较小的模型大小，但在推理过程中通信开销仍然很大。DistriFusion 为 U-Net 为主干的扩散模型引入了位移 patch 并行(displacement patch parallelism)，将模型的输入划分为多个 patch，便于激活的异步通信并且使得通信与计算重叠。然而，当将该方法应用于 DiT 时，内存缓冲区的开销将导致巨大的内存开销。</p>
<h1 id="methods">Methods</h1>
<p>不同并行策略下 DiT 单步扩散过程的比较如下表所示。</p>
<ul>
<li>p: 生成的序列长度 (即隐空间下的像素数量).</li>
<li>hs: 模型的隐藏通道大小。</li>
<li>N: 设备数量。</li>
<li>M: 图像切分的 patch 数量。</li>
<li>L: Transformer Blocks 层数。</li>
<li>P: 模型总参数量。</li>
<li>A: Attention 的过程中的激活大小 (Q, K, V, O 大小一样)</li>
</ul>
<p>名称后面的 * 表示采用异步通信，通信开销可以通过计算隐藏。</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>attn-KV</th>
          <th>communication cost</th>
          <th>param</th>
          <th>QO Activations</th>
          <th>KV Activations</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Tensor Parallel</td>
          <td>fresh</td>
          <td>4O(p × hs)L</td>
          <td>P/N</td>
          <td>(2/N) A = (1/N) QO</td>
          <td>(2/N) A = (1/N) KV</td>
      </tr>
      <tr>
          <td>DistriFusion*</td>
          <td>stale</td>
          <td>2O(p × hs)L</td>
          <td>P</td>
          <td>A</td>
          <td>2AL = (KV)L</td>
      </tr>
      <tr>
          <td>Ring Seq Parallel*</td>
          <td>fresh</td>
          <td>NA</td>
          <td>P</td>
          <td>A</td>
          <td>A</td>
      </tr>
      <tr>
          <td>Ulysses Seq Parallel</td>
          <td>fresh</td>
          <td>4O(p × hs)L</td>
          <td>P</td>
          <td>(2/N) A = (1/N) QO</td>
          <td>(2/N) A = (1/N) KV</td>
      </tr>
      <tr>
          <td>PipeFusion*</td>
          <td>stale-</td>
          <td>2O(p × hs)</td>
          <td>P/N</td>
          <td>(2/M) A = (1/M) QO</td>
          <td>(2L/N) A = (1/N) (KV)L</td>
      </tr>
  </tbody>
</table>
<h2 id="sequence-parallelism--tensor-parallelism">Sequence Parallelism &amp; Tensor Parallelism</h2>
<p>针对 LLM 提出的张量并行 (tensor parallelism, TP) 和序列并行 (sequence parallelism, SP) 可以应用于 DiT 推理。因为他们的主干都是 Transformer.
在 TP<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 中，权重矩阵按列被切分为 <em>N</em> 份，这样矩阵乘法后激活值也被切分成 <em>N</em> 份，使得每个设备的参数量和激活量均为原来的 1/N. 在 attention 计算和 FFN 层之后都需要进行两次同步 all-reduce 操作，因此每一层通信量为 4O(p × hs).</p>
<p>在 SP 中，可以将输入图像分割成 patch，DiT 中的多头注意模块可以采用 Ring-Attention<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，DeepSpeed-Ulysses<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，或者两者的组合。Ulysses SP 并行需要 4 次 all-to-all 操作，因此每一层通信量为 4O(p × hs), 和 TP 相同。</p>
<blockquote>
<p>TP 和 SP 可以在 DiT 推理中一起使用。</p></blockquote>
<h2 id="displaced-patch-parallelism">Displaced Patch Parallelism</h2>
<p>输入时间冗余意味着给定层中激活 patch 的计算并不完全取决于其他 patch 的最新激活。在前一个扩散步骤中加入稍微过时的激活是可行的。该方法将输入图像划分为 N 个patch，每个设备计算其各自 patch 的输出结果。 如下图所示 attention 模块需要具有完整形状的 KV 激活。它采用异步 all-gather 收集上一步扩散步骤的 KV 激活，并用其进行当前步的 attention 计算。</p>
<p>DistriFusion<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> 可以看作是异步 SQ 的一种形式。它通过正向计算扩散步骤来隐藏 KV 通信，但代价是消耗更多内存。DistriFusion 利用 N-1/N 的 T+1 步的 KV 激活和 T 步的 1/N 的局部 KV 激活相结合。与 Ring-Attention 相比，DistriFusion 可以更有效地隐藏通信开销，因为它允许 KV 通信与扩散步骤的整个前向计算重叠，而 Ring-Attention 只允许通信在注意模块内部重叠。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB9a64239185e8b3604db9a46098203d05?method=download&amp;shareKey=eab8f5ec3cff754ab7711e87333e8797" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB9a64239185e8b3604db9a46098203d05?method=download&amp;shareKey=eab8f5ec3cff754ab7711e87333e8797" alt="DistriFusion vs. Ring-Attention SQ for an Attention Module">
    </a><figcaption>DistriFusion vs. Ring-Attention SQ for an Attention Module</figcaption></figure></p>
<p>在 Ring-Attention中，其通信缓冲区 c × hs 可由图中块大小 c 控制，其值小于 p/N. DistriFusion要求每个计算设备始终保持 KV 的完整形状的通信缓冲区，因此通信开销总共是 AL.</p>
<h2 id="displaced-patch-pipeline-parallelism">Displaced Patch Pipeline Parallelism</h2>
<p>PipeFusion 相比于 DistriFusion 有着更高的内存效率和更低的通信成本。它将输入图像划分为 M 个不重叠的 patch，DiT Blocks 被划分为 N 个阶段，每个阶段按顺序分配给 N 个计算设备。每个设备在其被分配的阶段以流水线方式处理一个 patch 的计算任务。</p>
<blockquote>
<p>DiT 模型中因有许多相同的 transformer block，很容易将去噪网络的工作负载均匀地划分为 N 个部分。然而，U-Net 扩散模型没有这种重复结构。</p></blockquote>
<p>一个 M=4, N=4 的 PipeFusion 例子如下图所示，利用输入时间冗余，设备不需要等待接收到当前步骤的完整形状激活，利用上一步的激活就可以开始自己所处阶段的计算。考虑流水线气泡，流水线的有效计算比为 MS/MS+N−1，其中 S 为扩散步长数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB157240ab4733f1ca4cca87a2389a7b08?method=download&amp;shareKey=b0593903312b6da2796d081015a30baa" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB157240ab4733f1ca4cca87a2389a7b08?method=download&amp;shareKey=b0593903312b6da2796d081015a30baa" alt="The Workflow of Displaced Patch Pipeline Parallelism">
    </a><figcaption>The Workflow of Displaced Patch Pipeline Parallelism</figcaption></figure></p>
<p>PipeFusion 在计算设备之间仅传输属于一个阶段的 (连续 transformerl blocks) 的输入和输出的激活，因此通信开销为 2O(p × hs). PipeFusion 通过异步 P2P 传输前一步 Patch 数据和接收后一步骤 Patch 数据来与当前 Patch 计算重叠，从而将通信隐藏在计算中。PipeFusion 中的每个设备仅存储与其特定阶段相关的 1/N 份参数。由于使用陈旧 KV 进行注意力计算，要求每个设备保持其阶段对应的 L/N 层的完整 KV.</p>
<p>PipeDiffusion 理论上优于 DistriFusion，因为它利用了更多的新激活。如图所示，在单个扩散步骤内，PipeDiffusion 中最新激活的占比随着流水线执行而增加。而 DistriFusion 中最新激活的占比一直都是 1/N.</p>
<blockquote>
<p>尽管 DiT 没有采用 GroupNorm 层，但在 PipeFusion 中，U-Net 中 DistriFusion 对 GroupNorm 层的精度保持设计，特别是校正异步群归一化 (Corrected Asynchronous GroupNorm)，可以无缝地应用于 PipeFusion.</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf19f26f13cfdf79d2e13e8b012b2954b?method=download&amp;shareKey=9983249909c804bca818835ce2f953ce" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf19f26f13cfdf79d2e13e8b012b2954b?method=download&amp;shareKey=9983249909c804bca818835ce2f953ce" alt="The Fresh Part of Activations">
    </a><figcaption>The Fresh Part of Activations</figcaption></figure></p>
<p>由于使用输入时间冗余需要一个预热期，DistriFusion 使用了几次同步 path 并行的预热步骤作为预备阶段。为了优化预热开销，可以将预热步骤与其余步骤分开，并将其分配给不同的计算资源。</p>
<h1 id="experiments">Experiments</h1>
<p>我们在 Pixart-α 上进行实验 (0.6B)，它支持分辨率 1024px 的高分辨率图像合成，采用标准的 DiT，并结合交叉注意模块注入文本条件。使用了三个 GPU 集群，包括 4xGPU A100 80GB (PCIe) 集群，8xGPU A100 80GB (NVLink) 集群和 8xGPU L20 40GB (PCIe) 集群。测试的 GPU P2P 带宽分别为23 GB/s、268 GB/s 和 26 GB/s. 切分的 patch 数目 M 从 2,4,8,16,32 中搜索来确定最佳延迟性能。</p>
<ul>
<li>TP: 参考 Megatron-LM实 现了一个 TP DiT.</li>
<li>SP: 采用了两种不同的序列并行，DeepSpeed-Ulysses 和 Ring-Attention.</li>
<li>DistriFusion: 将 U-Net 扩散模型中的官方 DistriFusion 应用于DiT.</li>
<li>Original:在单个 GPU 上的串行实现。</li>
</ul>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在 VAE 中由于卷积算子的临时内存使用会产生内存峰值，因此 VAE 比 DiT 层需要更多的内存。为了缓解这个问题，我们将卷积层的输入图像分成几个块，将单个卷积操作转换为按顺序执行的多个操作的序列。</p></div>

<h2 id="quality-results">Quality Results</h2>
<p>使用 20 步 DPM-Solver，预热步骤为 4 步。当 patch 数为 1 时，PipeFusion 的精度与 DistriFusion 相当。当 patch 数超过 1 时，其精度在理论上比 PipeFusion 更接近原始版本。PipeFusion 在 FID 方面略优于 DistriFusion.</p>
<h2 id="latency-and-memory">Latency and Memory</h2>
<p>20 步 DPM-Solver，预热步骤为 1 步。</p>
<ul>
<li>4xA100 (PCIe)集群上: 对于 8192px 的情况，在，DistriFusion 和 SQ 都会遇到内存不足 (OOM) 问题。</li>
<li>8xL20 (PCIe)集群上: 生成 4096px 分辨率的图像时，DistriFusion 和 SQ 都会遇到 OOM 问题。</li>
<li>8xA100 (NVLink) 集群上: 使用异步通信的 SQ (Ulysses) 的延迟与异步 DistriFusion 非常相似，并且优于 Ring 版本。此外，PixArt-α 在跨 8 个设备部署时面临限制，因为28个DiT层不能在均分，从而导致额外的开销。</li>
</ul>
<h3 id="4x-a100-pcie-集群">4x A100 (PCIe) 集群</h3>
<table>
  <thead>
      <tr>
          <th><strong>Latency</strong></th>
          <th><strong>PipeFusion</strong></th>
          <th><strong>Tensor Parallel</strong></th>
          <th><strong>DistriFusion</strong></th>
          <th><strong>Seq Parallel (Ulysses)</strong></th>
          <th><strong>Seq Parallel (Ring)</strong></th>
          <th>Single-GPU</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1024px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>2.41x</td>
          <td>2.69x</td>
          <td>2.01x</td>
          <td>3.04x</td>
          <td>2.4x</td>
      </tr>
      <tr>
          <td><strong>2048px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>3.02x</td>
          <td>1.79x</td>
          <td>1.48x</td>
          <td>2.06x</td>
          <td>3.02x</td>
      </tr>
      <tr>
          <td><strong>4096px</strong></td>
          <td>1.02x</td>
          <td>1.77x</td>
          <td>1.16x</td>
          <td><strong>1.00x</strong></td>
          <td>1.12x</td>
          <td>3.05x</td>
      </tr>
      <tr>
          <td><strong>8192px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>1.10x</td>
          <td>OOM</td>
          <td>OOM</td>
          <td>OOM</td>
          <td>3.1x</td>
      </tr>
  </tbody>
</table>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7384a580336eb8b92972343922c549b6?method=download&amp;shareKey=a203914b94020ad72b87708703fd829f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7384a580336eb8b92972343922c549b6?method=download&amp;shareKey=a203914b94020ad72b87708703fd829f" alt="Overall Latency on a 4×A100-80GB (PCIe)">
    </a><figcaption>Overall Latency on a 4×A100-80GB (PCIe)</figcaption></figure></p>
<p>内存效率方面，PipeFusion优于除了张量并行的其他方法。虽然张量并行的内存占用最低，但与其他并行化策略相比，由于通信量大会导致更高的延迟。</p>
<table>
  <thead>
      <tr>
          <th><strong>Max Memory</strong></th>
          <th><strong>PipeFusion (Baseline)</strong></th>
          <th><strong>Original</strong></th>
          <th><strong>Tensor Parallel</strong></th>
          <th><strong>DistriFusion</strong></th>
          <th><strong>Seq Parallel (Ulysses)</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1024px</strong></td>
          <td>1.00x</td>
          <td>1.04x</td>
          <td>0.98x</td>
          <td>1.21x</td>
          <td>1.21x</td>
      </tr>
      <tr>
          <td><strong>2048px</strong></td>
          <td>1.00x</td>
          <td>0.98x</td>
          <td>0.90x</td>
          <td>1.54x</td>
          <td>1.33x</td>
      </tr>
      <tr>
          <td><strong>4096px</strong></td>
          <td>1.00x</td>
          <td>1.18x</td>
          <td>0.69x</td>
          <td>2.35x</td>
          <td>1.63x</td>
      </tr>
      <tr>
          <td><strong>8192px</strong></td>
          <td>1.00x</td>
          <td>1.41x</td>
          <td>0.71x</td>
          <td>2.34x</td>
          <td>OOM</td>
      </tr>
  </tbody>
</table>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB27159a6d3fb6cafa4dfc7fbc5883a211?method=download&amp;shareKey=90098eb080c078296e3b0fa0fd260ee6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB27159a6d3fb6cafa4dfc7fbc5883a211?method=download&amp;shareKey=90098eb080c078296e3b0fa0fd260ee6" alt="Overall GPU Memory on a 4×A100-80GB (PCIe)">
    </a><figcaption>Overall GPU Memory on a 4×A100-80GB (PCIe)</figcaption></figure></p>
<h3 id="8x-l20-pcie-集群">8x L20 (PCIe) 集群</h3>
<table>
  <thead>
      <tr>
          <th><strong>Latency</strong></th>
          <th><strong>PipeFusion</strong></th>
          <th><strong>Tensor Parallel</strong></th>
          <th><strong>DistriFusion</strong></th>
          <th><strong>Seq Parallel (Ulysses)</strong></th>
          <th><strong>Seq Parallel (Ring)</strong></th>
          <th>Single-GPU</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1024px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>2.46x</td>
          <td>3.26x</td>
          <td>1.48x</td>
          <td>4.42x</td>
          <td>2.46x</td>
      </tr>
      <tr>
          <td><strong>2048px</strong></td>
          <td>0.99x</td>
          <td>2.26x</td>
          <td><strong>1.00x</strong></td>
          <td>1.58x</td>
          <td>1.09x</td>
          <td>4.16x</td>
      </tr>
      <tr>
          <td><strong>4096px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>1.16x</td>
          <td>OOM</td>
          <td>1.31x</td>
          <td>4.40x</td>
          <td>4.30x</td>
      </tr>
  </tbody>
</table>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB883bda408bc38824e7bda7425ae4fb51?method=download&amp;shareKey=403004516bb9ff8d9271e0f8ef88a693" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB883bda408bc38824e7bda7425ae4fb51?method=download&amp;shareKey=403004516bb9ff8d9271e0f8ef88a693" alt="Overall latency on a 8×L20 (PCIe)">
    </a><figcaption>Overall latency on a 8×L20 (PCIe)</figcaption></figure></p>
<h3 id="8x-a100-nvlink-集群">8x A100 (NVLink) 集群</h3>
<table>
  <thead>
      <tr>
          <th><strong>Latency</strong></th>
          <th><strong>PipeFusion</strong></th>
          <th><strong>Tensor Parallel</strong></th>
          <th><strong>DistriFusion</strong></th>
          <th><strong>Seq Parallel (Ulysses)</strong></th>
          <th><strong>Seq Parallel (Ring)</strong></th>
          <th>Single-GPU</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1024px</strong></td>
          <td>1.26x</td>
          <td>1.59x</td>
          <td><strong>1.00x</strong></td>
          <td>1.79x</td>
          <td>3.38x</td>
          <td>2.15x</td>
      </tr>
      <tr>
          <td><strong>2048px</strong></td>
          <td>1.64x</td>
          <td>2.85x</td>
          <td><strong>1.00x</strong></td>
          <td><strong>1.00x</strong></td>
          <td>1.43x</td>
          <td>3.99x</td>
      </tr>
      <tr>
          <td><strong>4096px</strong></td>
          <td>1.08x</td>
          <td>1.56x</td>
          <td><strong>1.00x</strong></td>
          <td>1.18x</td>
          <td>1.93x</td>
          <td>7.28x</td>
      </tr>
      <tr>
          <td><strong>8192px</strong></td>
          <td>1.35x</td>
          <td><strong>1.00x</strong></td>
          <td>OOM</td>
          <td>OOM</td>
          <td>OOM</td>
          <td>5.98x</td>
      </tr>
  </tbody>
</table>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf294c0c56206b35fb2bd495b65caa1f8?method=download&amp;shareKey=a6780d54a1429bba9fa05174f1ab44e8" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf294c0c56206b35fb2bd495b65caa1f8?method=download&amp;shareKey=a6780d54a1429bba9fa05174f1ab44e8" alt="Overall latency on a 8×A100 (NVLink)">
    </a><figcaption>Overall latency on a 8×A100 (NVLink)</figcaption></figure></p>
<h3 id="scalability">Scalability</h3>
<p>PipeFusion 在 NVLink 和 PCIe 上的时延相似，PCIe 甚至在表现出了轻微的优势。在 PCIe 集群上，对于相同的任务，PipeFusion 总是比 DistriFusion 快。说明 PipeFusion 的通信带宽要求非常低，因此不需要使用 NVLink 等高带宽网络。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbd3f234475f8a4dd0831cb0de02c3023?method=download&amp;shareKey=186d86f87e9eaadc6df309ff516b2b1c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbd3f234475f8a4dd0831cb0de02c3023?method=download&amp;shareKey=186d86f87e9eaadc6df309ff516b2b1c" alt="Scalability of PipeFusion and DistriFusion on A100 PCIe vs. NVLink cluster">
    </a><figcaption>Scalability of PipeFusion and DistriFusion on A100 PCIe vs. NVLink cluster</figcaption></figure></p>
<h2 id="ablation-studies">Ablation Studies</h2>
<p>随着 patch 数目 M 的增加，内存消耗减少，并且对通信没有影响。但在实践中，M 不应该设置得太高。生成 1024px 和 2048px 图像时，当 M 超过一定阈值时，整体延迟增加。然而，这种现象很少出现在高分辨率图像 4K×4K 的情况下。这是因为过于细粒度的计算分区会导致 GPU 的理论吞吐量下降。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBeff3ad6b39db3ce27ce5019245deecbc?method=download&amp;shareKey=65604fe4530f3c7236a57f0497d163c4" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBeff3ad6b39db3ce27ce5019245deecbc?method=download&amp;shareKey=65604fe4530f3c7236a57f0497d163c4" alt="Latency of PipeFusion with various patch numbers M">
    </a><figcaption>Latency of PipeFusion with various patch numbers M</figcaption></figure></p>
<p>绝大多数差异可以忽略不计或接近零，即扩散过程中连续步骤输入之间的高度相似性。</p>
<p>有一些方法可以减轻由预热步骤引起的性能损失: 增加采样步骤，在单独的设备上执行，利用序列或张量并行。</p>
<h1 id="summary">Summary</h1>
<p>我们的方法是先用 Pipeline Parallel 将模型的 transformer block 切分成多个 stage, 再用 Tensor Parallel (Megatron: 切分前一个权重的列，后一个权重的行, Two-dimenson: 切分输入的列，切分权重的行和列)，每一层的 KV 结果需要进行 all-reduce 或者 all-gather + reduce-scatter. 不同 stage 之间是 P2P 通信.</p>
<p>PipeFusion 行为更像单纯的 Pipeline Parallel，利用上一步的 KV 完成当前步的计算，P2P 通信的是自己所处 stage 的激活 (与切分的 patch 数成反比)，与 transformer block 的层数无关。</p>
<p><a href="https://darkenstar.github.io/2024/09/27/xDiT/#Construct-Parallel-Groups">xDiT的分析中</a>提到过将并行维度从小到大可以分为 TP-SP-PP-CFG-DP，其中 CFG 和 DP 实际上是对 数据的 batchsize 维度进行切分，PP 的大小取决于划分的 patch 数，每个 stage 的 transformer block 计算的时候可以进一步再进行 SP 和 TP.</p>
<h1 id="references">References</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://darkenstar.github.io/blogs/MegatronLM/">https://darkenstar.github.io/blogs/MegatronLM/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://darkenstar.github.io/blogs/ringattention/">https://darkenstar.github.io/blogs/ringattention/</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://darkenstar.github.io/blogs/deepspeedulysses/">https://darkenstar.github.io/blogs/deepspeedulysses/</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://darkenstar.github.io/blogs/distrifusion/">https://darkenstar.github.io/blogs/distrifusion/</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Fast-dLLM</title>
      <link>http://localhost:1313/blogs/fast-dllm/</link>
      <pubDate>Thu, 12 Jun 2025 23:01:49 +0800</pubDate>
      <guid>http://localhost:1313/blogs/fast-dllm/</guid>
      <description>Paper Reading of Fast-dLLM</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Diffusion LLMs 被视为下一代文本生成技术的有力竞争者，其核心优势在于理论上可以并行生成多个 token，从而有望实现比自回归模型快几个数量级的推理速度。谷歌的 Gemini Diffusion 和 Inception Labs 的Mercury等模型已经展示了其惊人的潜力，宣称能达到每秒上千 token 的生成速度。</p>
<p>当前开源的扩散LLM (LLaDA、Dream) 在实际应用中的速度远远达不到预期，甚至比优化良好的自回归模型还要慢。这篇论文的工作，就是要拆掉阻碍扩散 LLM 起飞的两座大山。</p>
<ol>
<li>无法使用 KV Cache</li>
</ol>
<p>扩散LLM的注意力机制是双向的，即一个 token 的生成不仅依赖于它前面的内容，也依赖于它后面的内容 (尽管后面可能是未知的 MASK token ) 。这种特性使得过去的信息和未来的信息相互纠缠，无法像自回归模型那样简单地缓存和复用过去的信息。导致扩散LLM在每一步推理中都需要进行大量的重复计算，严重拖慢了速度。</p>
<p>Fast-dLLM 的第一个核心贡献，就是提出了一种分块近似 (block-wise approximate) KV Cache 机制。</p>
<blockquote class="quote"><p>While the bidirectional nature of attention in Diffusion LLMs precludes a fully equivalent KV Cache, our approximation closely resembles an ideal cache in practice.</p></blockquote>
<p>它将待生成的文本序列分成若干个块. 在生成某一个块 (比如Block 1) 时，它会提前计算并缓存其他所有块 (比如 Prompt 和 Block 0) 的 KV. 在这个块的内部生成过程中，这些缓存被反复利用。当这个块生成完毕后，再整体更新一次所有块的KV缓存 。</p>
<p>这个方法的近似在于，在一个块的生成过程中，缓存是固定的，而实际上随着块内 token 的不断去噪和清晰化，这些缓存理论上也应该随之微调。但论文通过可视化实验 (图3) 有力地证明，在相邻的推理步骤中，KV 激活值的 余弦相似度非常高，几乎接近于1. 这说明使用固定的近似缓存带来的误差微乎其微，完全可以用极小的精度损失换取巨大的速度提升。</p>
<p>论文还进一步提出了双缓存 (DualCache) 版本，不仅缓存了前面的“前缀” (prefix) ，还缓存了后面的“后缀” (suffix，通常是 MASK  token )  ，从而进一步压榨了计算优化的空间，实现了更快的速度。</p>
<ol start="2">
<li>并行解码带来的质量下降</li>
</ol>
<p>扩散LLM的另一大理论优势是 并行解码 (Parallel Decoding)，即一次性预测和生成多个 token  。然而，实践再次证明，当并行解码的 token 数量增多时，生成文本的质量会急剧下降 。</p>
<p>论文深刻地剖析了其根源：条件独立性假设 (conditional independence assumption) 的破坏 。在并行解码时，模型是独立地为每个待生成的 MASK 位置预测一个概率分布，然后从中采样。但实际上，一句话中的 token 之间存在着强烈的依赖关系。论文举了一个例子:</p>
<blockquote class="quote"><p>Consider an example from [30]: The list of poker hands that consist of two English words are: The subsequent two words could be, for instance, &ldquo;high card,&rdquo; &ldquo;two pair,&rdquo; &ldquo;full house,&rdquo; or &ldquo;straight flush.&rdquo; [&hellip;] However, the multi-token prediction procedure in MDMs first generates a probability distribution for each token and then samples from these distributions independently. This independent sampling can lead to undesirable combinations, such as &ldquo;high house.&rdquo;</p></blockquote>
<p>模型可能会独立地预测出 &ldquo;high&rdquo; 和 &ldquo;house&quot;这两个词，但把它们组合在一起就成了毫无意义的 high house. 这是因为模型在并行预测时忽略了 token 间的联合概率，而错误地直接使用了边缘概率的乘积。</p>
<p>为了解决这个问题，Fast-dLLM提出了第二个核心贡献：置信度感知并行解码 (Confidence-Aware Parallel Decoding) 策略 。这个想法非常直观且有效：我们只对那些模型非常有把握的 token 进行并行解码。</p>
<p>具体来说，在每一步解码时，模型会为每个待生成的 MASK 位置计算一个 置信度分数 (比如softmax概率的最大值). 然后，设定一个全局的置信度阈值 τ，只有那些置信度超过这个阈值的 token 才会被揭开，而置信度不足的 token 则继续保持 MASK 状态，留到下一步再做决策。为了避免无限循环，如果没有任何 token 的置信度达标，模型会强制解码置信度最高的那一个。</p>
<p>这个策略的精妙之处在于，它在理论上是站得住脚的。论文通过定理一从数学上证明了：当模型对一组 token 的预测置信度足够高时 (即 p&gt;1−ϵ，且 ϵ 足够小)，基于独立边缘概率的“贪心并行解码”与基于真实联合概率的“贪心串行解码”会得到完全相同的结果。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&amp;shareKey=c9e48ddb1e1f0600394ce8baa1d84426" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&amp;shareKey=c9e48ddb1e1f0600394ce8baa1d84426" alt="Effectiveness of Components of Fast-dLLM across Different Approaches">
    </a><figcaption>Effectiveness of Components of Fast-dLLM across Different Approaches</figcaption></figure></p>
<p>Fast-dLLM 的创新性体现在它是一种 training-free 的加速框架。它没有修改模型结构，也不需要重新训练，而是通过两项即插即用的推理策略——“分块近似KV缓存”和“置信度感知并行解码”，分别从减少重复计算和提升并行效率两个维度，精准地解决了当前开源扩散 LLM 面临的核心瓶颈。 实验结果在 LLaDA 和 Dream 等模型上，结合两种策略，实现了高达 27.6 倍的端到端吞吐量提升，同时在多个基准测试上几乎没有精度损失。</p>
<h1 id="2-preliminary">2. Preliminary</h1>
<h3 id="21-masked-diffusion-model">2.1. Masked Diffusion Model</h3>
<p>针对离散数据的扩散模型最早在 Argmax Flows and Multinomial Diffusion 和 Deep Unsupervised Learning using
Nonequilibrium Thermodynamics 中被探提出。随后 D3PM 提出了一个更通用的框架，通过特定的转移矩阵 $Q_{t}$ 定义了前向加噪过程的离散状态马尔可夫链，并通过最大化 ELBO 来学习反向过程的参数化模型 $p_{\theta}(x_{0}|x_{t})$. CTMC 进一步将 D3PM 扩展到连续时间，将其形式化为一个连续时间马尔可夫链 (CTMC) 框架。在另一种不同的方法中，SEDD 通过参数化似然比 $\frac{p_{t}(y)}{p_{t}(x)}$ 来学习反向过程，并采用去噪分数熵来训练该比率。</p>
<p>在各种离散扩散的噪声处理方式中，<strong>Masked Diffusion Models, MDMs</strong>，也被称为吸收状态离散扩散模型，获得了相当大的关注。MDMs 采用一种前向加噪过程，其中 token 被逐步替换为一个特殊的 MASK  token  。这个过程由以下转移概率定义：</p>
$$
q_{t|0}(x_{t}|x_{0})=\prod_{i=1}^{n}q_{t|0}(x_{t}^{i}|x_{0}^{i})=\prod_{i=1}^{n}Cat(x_{t}^{i};(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}) \tag{1}
$$<ul>
<li>$q_{t|0}(x_t|x_0)$: 表示给定原始序列 $x_0$，得到噪声序列 $x_t$ 的概率 。</li>
<li>$\prod_{i=1}^{n}$: 连乘符号，表示整个序列的噪声过程是序列中每个 token  (token) 独立进行噪声过程的概率乘积 。</li>
<li>$Cat(\cdot)$: 代表<strong>类别分布 (Categorical Distribution)</strong> 。</li>
<li>$t \in [0,1]$: 表示<strong>扩散时间</strong>或<strong>掩码级别</strong>。当 $t=0$ 时，序列完全是原始的；当 $t=1$ 时，序列被完全替换为 <code>[MASK]</code>  token 。</li>
<li>$(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}$: 在时间 <code>t</code>，第 <code>i</code> 个 token 有 $1-t$ 的概率保持其原始身份 $x_0^i$，有 $t$ 的概率变成 <code>[MASK]</code>  token 。<code>$\delta$</code> 是克罗内克函数，用于指定概率。</li>
</ul>
<p>最近，MDLM 和 RADD 的工作表明，对于 MDMs 不同的参数化是等价的。此外，他们证明了 MDMs 的训练目标可以被简化或直接从数据似然中推导出来 。这导出了以下目标函数，即 $log~p_{\theta}(x)$ 的一个 ELBO:</p>
<details class="custom-details">
    <summary class="custom-summary">Reparameterized Absorbing Discrete Diffusion, RADD</summary>
    <div><p><strong>定理 1（Theorem 1）</strong>
</p>
$$
\frac{p_t(\hat{x}_t)}{p_t(x_t)} = \underbrace{\frac{e^{-\bar{\sigma}(t)}}{1-e^{-\bar{\sigma}(t)}}}_{\text{时间相关的标量}} \cdot \underbrace{p_0(\hat{x}_t^i | x_t^{UM})}_{\text{干净数据的条件概率}}
$$<ul>
<li>$p_t(x_t)$ 是时间步 $t$ 的数据分布</li>
<li>$x_t$ 是带噪声（被掩码）的序列</li>
<li>$x_t^{UM}$ 是其中未被掩码的部分</li>
<li>$\hat{x}_t$ 是在 $x_t$ 的一个掩码位置上填入一个新 token 后的序列</li>
<li>$p_0$ 是原始干净数据的分布，$\bar{\sigma}(t)$ 是一个与噪声水平相关的函数。</li>
</ul>
<p>这个公式表明，模型需要学习的目标可以分解。其中一部分是一个可以精确计算的、只与时间 $t$ 有关的标量，而另一部分则是一个<strong>与时间无关</strong>的、在给定其他可见 token 的条件下，预测被掩码 token 的条件概率。正是LLM 所做的事情。这个看似简单的改动带来了巨大的实际优势：</p>
<ol>
<li><strong>架构简化</strong>：移除了时间编码和相关的自适应归一化层，使得模型参数更少，结构更简洁 。</li>
<li><strong>采样加速</strong>：由于模型输出不再依赖于时间 $t$，当输入序列 $x_t$ 在某个采样区间内没有发生变化时，可以直接缓存上一步的计算结果，而无需再次调用网络。这极大地减少了<strong>函数评估次数（Number of Function Evaluations, NFEs）</strong>。论文给出了在特定采样策略下，期望函数评估次数（E-NFEs）的解析公式 ：</li>
</ol>
$$
E\text{-}NFEs(n) = n \left( 1 - \left( 1 - \frac{1}{n} \right)^l \right)
$$<p><strong>定理 2（Theorem 2）</strong></p>
<p>证明了吸收态扩散模型的训练目标（具体来说是 DSE 损失）在数学上等价于**任意阶自回归模型（Any-Order Autoregressive Models, AO-ARMs）**的训练目标 。</p>
<p>AO-ARMs 是一类特殊的生成模型，它们不像标准自回归模型那样固定从左到右的生成顺序，而是学习在所有可能的 $d!$（$d$ 为序列长度）种生成顺序下对数据进行建模。论文通过一系列精巧的数学推导，建立了四种不同损失函数之间的等价关系链 ：</p>
<p>$\mathcal{L}_{DSE} \iff \mathcal{L}_{t-DCE} \iff \mathcal{L}_{\lambda-DCE} \iff \mathcal{L}_{AO}$</p>
<p>它表明吸收态扩散模型本质上是在学习一个集成了所有可能生成顺序的自回归模型的期望 。这可能解释了为什么它们在某些任务上表现得非常稳健。</p>
</div>
</details><br>
$$
-log~p_{\theta}(x)\le\int_{0}^{1}\frac{1}{t}\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})]dt:=\mathcal{L}_{MDM}. \tag{2}
$$<ul>
<li>$-log~p_{\theta}(x)$: 模型的目标是最大化生成真实数据 $x$ 的对数似然，这等价于最小化它的负对数似然。这个公式给出了负对数似然的一个* ELBO.</li>
<li>$\int_{0}^{1}...dt$: 对所有可能的噪声级别 <code>t</code> (从0到1) 进行积分，意味着模型需要学会在任何噪声水平下都能很好地复原数据 。</li>
<li>$\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[...]$: 表示对所有可能的噪声样本求期望。在训练时，我们根据公式(1)随机生成一个带 <code>[MASK]</code> 的噪声序列 $x_t$.</li>
<li>$\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})$:
<ul>
<li>$\sum_{i:x_{t}^{i}=[MASK]}$: 对所有被 <code>[MASK]</code> 的位置 <code>i</code> 进行求和 。</li>
<li>$-log~p_{\theta}(x_{0}^{i}|x_{t})$: 这是交叉熵损失。它的意思是，给定带有 <code>[MASK]</code> 的序列 $x_t$，模型 $p_{\theta}$ 需要预测在位置 i 上的原始 token  $x_0^i$ 应该是什么。模型预测得越准，这个损失值就越小。</li>
</ul>
</li>
</ul>
<h3 id="22-mdms-的生成过程">2.2. MDMs 的生成过程</h3>
<p>对于公式1中定义的前向过程，其解析上的逆过程在生成时计算效率低下，因为它通常每步只修改一个 token 。一个常见的加速策略是采用 $\tau$-leaping 近似法来处理反向过程。在 MDMs 的背景下，这允许一个迭代式的生成过程，其中多个被掩码的 token 可以从一个噪声水平 t 近似地单步恢复到一个更早的水平 s &lt; t.</p>
$$
q_{s|t}(x_s|x_t)=\prod_{i=0}^{n-1}q_{s|t}(x_{s}^{i}|x_{t})
$$<p>其中</p>
$$
q_{s|t}(x_{s}^{i}|x_{t})=\begin{cases}1, & \text{if } x_{t}^{i}\ne[MASK], x_{s}^{i}=x_{t}^{i} \\ \frac{s}{t}, & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}=[MASK] \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}\ne[MASK]\end{cases} \tag{3}
$$<ul>
<li>$q_{s|t}(x_{s}^{i}|x_{t})$: 表示从 <code>t</code> 时刻的 token  $x_t^i$ 变为 <code>s</code> 时刻的 token  $x_s^i$ 的概率 。</li>
<li><strong>Case 1</strong>: 如果一个 token 在 <code>t</code> 时刻就不是 <code>[MASK]</code>，那么它在更早的 <code>s</code> 时刻也保持不变 。</li>
<li><strong>Case 2</strong>: 一个在 t 时刻是 <code>[MASK]</code> 的 token ，在更早的 s 时刻仍然是 <code>[MASK]</code>.</li>
<li><strong>Case 3</strong>: 这是关键的去噪步骤。如果一个 token 在 <code>t</code> 时刻是 <code>[MASK]</code>，模型会尝试在 s 时刻预测出一个具体的 token.
<ul>
<li>$\frac{t-s}{t}$: 代表一个在 <code>t</code> 时刻被掩码的 token，在 <code>s</code> 时刻被“揭示”出来的概率 。</li>
<li>$q_{0|t}(x_{s}^{i}|x_{t})$: 这是由神经网络模型给出的预测分布。模型会观察整个带有 <code>[MASK]</code> 的上下文 $x_t$，然后为当前位置预测一个最有可能的原始 token ，并给出一个在整个词汇表上的概率分布 。</li>
</ul>
</li>
</ul>
<p>在涉及条件数据的场景中，例如根据一个 propmt p 生成一个回应 $x_{0}$，MDM 的反向过程 (公式3所定义) 需要进行调整。具体来说，模型用于揭示一个 token  $x_{s}^{i}$ 的预测分布 $q_{0|t}(x_{s}^{i}|x_{t})$ 现在也需要以 prompt p 为条件，即 $q_{0|t}(x_{s}^{i}|x_{t},p)$ 。</p>
<p><strong>并行解码的诅咒</strong>
直接逆转公式1的前向过程来进行生成是缓慢的，通常每步只改变一个 token. 一个常见的加速策略是采用 $\tau$-leaping 近似法来处理反向过程。对于 MDMs，这意味着多个被掩码的 token 将在一个步骤中并行生成。然而，由于条件独立性假设，多 token 预测中出现了一个重大挑战。考虑一个例子：由两个英文单词组成的扑克手牌列表是：随后的两个词可能是，例如，high card，two pair，full house，或 straight flush. 值得注意的是，这两个词之间存在着关联。然而，MDMs 中的多 token 预测过程首先为每个 token 生成一个概率分布，然后独立地从这些分布中进行采样。这种独立采样可能导致不希望的组合，例如 high house.</p>
<p>为了将其形式化，考虑揭示两个 token 位置 i 和 j. 由于条件独立性假设，MDMs 从 $p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t})$ 中采样这些 token. 然而，真实的联合概率需要考虑它们之间的依赖关系：</p>
$$
p(x_{s}^{i},x_{s}^{j}|x_{t})=p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t},x_{s}^{i})
$$<p>或者对称地，通过将 i 依赖于条件 j. 这种假设的独立生成与真实的依赖性数据分布之间的差异，会降低生成序列的质量和连贯性。当在单一步骤中同时揭示大量 token 时，这个问题会变得更加严重。</p>
<h1 id="3-methodology">3. Methodology</h1>
<h2 id="31-pipeline-overview">3.1. Pipeline Overview</h2>
<p><strong>Fast-dLLM</strong>，建立在 MDM 架构之上，以实现高效和高质量的序列生成。为了加速推理，整体流水线融合了两大关键策略：通过 KV Cache 实现的高效注意力计算，以及一个由预测置信度引导的 并行解码方案。具体来说，我们采用了分块解码设计的 KV Cache，它允许在不同步骤间复用注意力激活值，并显著减少了冗余计算。在每个块内部，进一步提出了置信度感知的并行解码，它能根据置信度分数选择性地更新 token ，从而在保持输出质量的同时提高效率。通过结合这些策略，Fast-dLLM 在对生成性能影响最小的情况下，显著加快了 MDM 的推理速度。整体流程在算法 1 中进行了总结。</p>
<h2 id="32-key-value-cache-for-block-wise-decoding">3.2. Key-Value Cache for Block-Wise Decoding</h2>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&amp;shareKey=8952caa17d664bd8bcc33b9ebcec321e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&amp;shareKey=8952caa17d664bd8bcc33b9ebcec321e" alt="Illustration of our Key-Value Cache for Block-Wise Decoding">
    </a><figcaption>Illustration of our Key-Value Cache for Block-Wise Decoding</figcaption></figure></p>
<p>如上图所示，我们采用了一种分块解码的策略来支持 KV Cache 的使用。一开始计算并存储 prompt 的 KV 缓存，这个缓存将在整个块 0的解码过程中被复用。在每个块的内部，相同的缓存会被多个解码步骤复用。<strong>在完成一个块的解码之后，更新所有 token (不仅仅是新生成的 token) 的缓存</strong>。这个缓存更新可以与解码步骤联合执行，因此与不使用缓存相比，没有额外的计算开销。由于掩码扩散模型中使用的是完全注意力机制，这种方法导致了一个近似的解码过程。</p>
<p>我们的近似 KV 缓存方法的有效性，源于我们观察到 KV 激活值在相邻的推理步骤中表现出高度的相似性，如下图所示。图 a 中红色方框区域突显了块内的相似性分数，这些分数始终接近于 1. 表明在分块解码期间，前缀 (prefix) 的键和值的差异可以忽略不计，使我们能够安全地复用缓存而不会有显著的准确率损失。 此外，我们实现了一个我们 KV 缓存机制的双向版本，名为 <strong>DualCache</strong>，它不仅缓存前缀 token ，还缓存后缀 (suffix)  token ，在我们的分块解码方案中，后缀完全由掩码 token 组成。如表3所示，DualCache 带来了进一步的加速。图 b 中的红色方框区域进一步证明，在分块解码期间，后缀的键和值的差异也可以忽略不计。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&amp;shareKey=6a5005c556aaa11edb4006a48b755b4a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&amp;shareKey=6a5005c556aaa11edb4006a48b755b4a" alt="Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA">
    </a><figcaption>Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA</figcaption></figure></p>
<h2 id="33-confidence-aware-parallel-decoding">3.3. Confidence-Aware Parallel Decoding</h2>
<p>尽管存在一些方法，例如使用辅助模型来显式地捕捉不同位置 token 之间的依赖关系，但它们通常会增加整个流水线的复杂性。与这些方法相反，我们提出了一个简单而有效的<strong>置信度感知解码算法</strong>，旨在缓解这种条件独立性问题。</p>
<p>在每次迭代中，我们不是冒然地使用它们独立的边缘概率来揭示所有被掩码的 token ，而是为每个 token 计算一个置信度分数 (例如最大的 softmax 概率). 只有那些置信度超过一个阈值的 token 才会在当前步骤被揭示；其余的则保持掩码状态，并在未来的步骤中重新考虑。如果没有 token 的置信度超过阈值，就揭示置信度最高的那一个，以确保过程能够进行并防止无限循环。这个策略在加速生成的同时，减少了由不确定或模糊预测引起的错误。</p>
<p>一个关键问题是</p>
<blockquote class="quote"><p><em>When is it theoretically justifiable to decode tokens in parallel using independent marginals, despite the true joint distribution potentially containing dependencies?</em></p></blockquote>
<p>以下结果来回答了在高置信度情况下，greedy parallel 解码等同于 greedy sequential 解码的条件，并量化了两种分布之间的差异。在给出定理之前，我们将定义其表述中使用的数学符号。</p>
<p>设 $p_{\theta}(\cdot|E)$ 表示一个 MDM 在给定 E (包括 prompt $p_{0}$ 和先前生成的 token) 的条件下给出的 PMF. 假设模型要为不在 E 中的位置 $i_{1},...,i_{n}$ 预测 n 个 token.</p>
<p>令 $X=(X_{i_{1}},...,X_{i_{n}})$ 是 n 个 token 的向量，其中每个 $X_{i_{j}}$ 在词汇表 V 中取值。设 $p(X|E)\equiv p_{\theta}(X_{i_{1}},...,X_{i_{n}}|E)$ 是模型给出的联合条件 PMF。设 $p_{j}(X_{i_{j}}|E)\equiv p_{\theta}(X_{i_{j}}|E)$ 是位置 $i_{j}$ 的边缘条件 PMF。并行解码使用边缘概率的乘积来生成 token ：$q(X|E)=\tilde{\prod}_{j=1}^{n}p_{j}(X_{i_{j}}|E)$。定理1的证明及相关讨论见附录A。</p>
<p><strong>定理 1 (高置信度下的并行解码).</strong> 假设存在一个特定的 token 序列 $x^{*}=(x_{i_{1}},...,x_{i_{n}})$，使得对于每个 $j\in\{1,...,n\}$，模型对 $x_{i_{j}}$ 都有很高的置信度：$p_{j}(X_{i_{j}}=x_{i_{j}}|E)>1-\epsilon$，对于某个很小的 $\epsilon>0$. 那么，以下结论成立：</p>
<ol>
<li><em>Equivalence of Greedy Decoding</em>：如果 $(n+1)\epsilon\le1$ (即 $\epsilon\le\frac{1}{n+1}$) ，那么
$$
\text{argmax}_{z} p(z|E) = \text{argmax}_{z} q(z|E) = x^{*}. \tag{4}
$$</li>
</ol>
<p>这意味着 greedy parallel 解码 (选择 argmax q) 与贪婪序贯解码 (选择 argmax p) 产生相同的结果。  这个界是紧的：如果 $\epsilon > \frac{1}{n+1}$，则存在满足高置信度边缘假设的分布 $p(X|E)$，使得 argmax $p(z|E)$ ≠ argmax $q(z|E)$。</p>
<ol start="2">
<li><em>Distance and Divergence Bounds</em>：为简洁起见，将 $p(\cdot|E)$ 和 $q(\cdot|E)$ 表示为 p 和 q.</li>
</ol>
<p><strong>$L_p$ Distance ($p \ge 1$)</strong>: 对于 $n>1$，$D_{p}(p,q)<((n-1)^{p}+2n)^{1/p}\epsilon$。特别地，对于总变差距离 ($D_{TV}(p,q)=\frac{1}{2}D_{1}(p,q)$)，$D_{TV}(p,q)<\frac{3n-1}{2}\epsilon$.</p>
<p>这个公式说明，<strong>真实分布 p 和近似分布 q 之间的总变差距离有一个上限</strong>。这个上限取决于两个因素：</p>
<ol>
<li>$n$: 生成序列的长度。序列越长，这个上限就越大。这是符合直觉的，因为每增加一个 token，近似所累积的潜在误差就可能增加一点。</li>
<li>$\epsilon$: 模型在每个位置上的“不确定性”。$\epsilon$ 越小 (即模型越自信)，这个上限就越低。</li>
</ol>
<p><strong>Forward KL Divergence</strong>: 对于 $n > 1$，$D_{KL}(p||q)<(n-1)(H_{b}(\epsilon)+\epsilon~ln(|\mathcal{V}|-1))$，其中 $H_{b}(\epsilon)=-\epsilon~ln~\epsilon-(1-\epsilon)ln(1-\epsilon)$ 是二元熵函数，而 $|\mathcal{V}|$ 是词汇表的大小。</p>
<ol>
<li>$n-1$: 同样，损失会随着序列长度线性增长。</li>
<li>$H_{b}(\epsilon)$: 它衡量了一个概率为 $\epsilon$ 的事件带来的“意外程度”或不确定性。当 $\epsilon$ 很小时，$H_b(\epsilon)$ 也非常小。</li>
<li>$\epsilon~ln(|\mathcal{V}|-1)$: 这一项反映了那部分微小的 $\epsilon$ 概率被分配到词汇表 $\mathcal{V}$ 中其他所有 token 上所带来的不确定性。即使 $\epsilon$ 很小，如果词汇表非常巨大 ($|\mathcal{V}|$ 很大)，这一项也可能有影响。</li>
</ol>
<hr>
<ul>
<li>$L_p$ 距离说明在高置信度下，两种方法找到的<strong>最佳答案</strong>是相同的。</li>
<li>KL 散度说明高置信度下，不仅最佳答案相同，两种方法描绘的概率分布都非常相似。近似方法 q 不仅猜对了可能性最大的 token， 对其他可能性的估计，也和精确方法 p 的判断高度一致。</li>
</ul>
<h1 id="4-experiments">4. Experiments</h1>
<h2 id="41-experimental-setup">4.1 Experimental Setup</h2>
<ul>
<li><strong>硬件与环境</strong> 🖥️: 所有实验均在单张 <strong>NVIDIA A100 80GB GPU</strong> 上进行，batch size=1.</li>
<li><strong>评测模型</strong> 🧠: <strong>LLaDA</strong>  和 <strong>Dream</strong>.</li>
<li><strong>评测基准</strong> 📊: 采用了四个广泛使用的基准数据集：<strong>GSM8K</strong>、<strong>MATH</strong>、<strong>HumanEval</strong> 和 <strong>MBPP</strong>.</li>
<li><strong>核心指标</strong> ⏱️:
<ul>
<li><strong>准确率 (Accuracy)</strong>: 衡量模型在具体任务上的表现。</li>
<li><strong>吞吐量 (Throughput)</strong>: 以 tokens/sec 为单位，反映端到端的真实解码速度。</li>
</ul>
</li>
<li><strong>超参数</strong> ⚙️:
<ul>
<li><strong>缓存块大小</strong>: 在 4 到 32 之间进行探索。</li>
<li><strong>置信度阈值</strong>: 在 0.5 到 1.0 之间进行探索。</li>
<li>实验默认使用 <strong>PrefixCache</strong>，块大小为 <strong>32</strong>，置信度阈值为 <strong>0.9</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="42-main-results-performance-and-speed">4.2 Main Results: Performance and Speed</h2>
<p>实验结果表明，Fast-dLLM 在各种任务和设置上都取得了显著的速度提升，同时对模型准确率的影响微乎其微 。</p>
<ul>
<li>加速效果:
<ul>
<li>单独引入 KV Cache 机制，通常能带来 <strong>2x-3.6x</strong> 的速度提升。</li>
<li>当 KV Cache 和并行解码两种策略结合使用时，性能提升更为显著。在 LLaDA 模型上，最 高可达 <strong>11.0x</strong> 的吞吐量提升；在 Dream 模型上，最高可达 <strong>7.8x</strong> 的提升 。</li>
</ul>
</li>
<li>极小的精度损失: 在所有基准测试中，加速后模型的准确率与原始基线模型的差距基本保持在 <strong>1-2个百分点</strong> 以内，有时甚至略有提高。</li>
<li>对长序列更友好: 实验还发现，在处理更长的文本序列时 (例如 few-shot 场景或长代码生成)，Fast-dLLM 的加速效果更为明显。</li>
</ul>
<p>下表以 GSM8K (5-shot) 任务为例，直观展示了 Fast-dLLM (即 +Cache+Parallel) 相较于 baseline 模型的性能提升。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">模型</th>
          <th style="text-align: left">生成长度</th>
          <th style="text-align: left">配置</th>
          <th style="text-align: left">准确率 (%)</th>
          <th style="text-align: left">吞吐量 (tok/s)</th>
          <th style="text-align: left">相对加速</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>LLaDA</strong></td>
          <td style="text-align: left">256</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">79.3</td>
          <td style="text-align: left">6.7</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>78.5</strong></td>
          <td style="text-align: left"><strong>54.4</strong></td>
          <td style="text-align: left"><strong>8.1x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">512</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">77.5</td>
          <td style="text-align: left">3.2</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>77.2</strong></td>
          <td style="text-align: left"><strong>35.3</strong></td>
          <td style="text-align: left"><strong>11.0x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Dream</strong></td>
          <td style="text-align: left">256</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">75.0</td>
          <td style="text-align: left">9.1</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>74.8</strong></td>
          <td style="text-align: left"><strong>48.2</strong></td>
          <td style="text-align: left"><strong>5.3x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">512</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">76.0</td>
          <td style="text-align: left">7.7</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>74.0</strong></td>
          <td style="text-align: left"><strong>42.9</strong></td>
          <td style="text-align: left"><strong>5.6x</strong></td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="43-ablations-and-analysis">4.3 Ablations and Analysis</h2>
<p>为了深入理解各个组件的贡献，论文进行了一系列详细的消融实验。</p>
<ul>
<li>
<p><strong>输入与生成长度的影响</strong>:</p>
<ul>
<li>实验证明，更长的上下文 (prefill，如从 5-shot 增加到 8-shot) 和更长的生成长度，都能显著放大加速效果。</li>
<li>在 8-shot 和 1024 生成长度的设置下，<strong>DualCache</strong> 实现了 <strong>27.6x</strong> 端到端加速。</li>
</ul>
</li>
<li>
<p><strong>PrefixCache vs. DualCache</strong>:</p>
<ul>
<li><strong>DualCache</strong> 通常比只缓存前缀的 <strong>PrefixCache</strong> 实现更高的加速比，尤其是在长序列生成任务中 。</li>
</ul>
</li>
<li>
<p><strong>缓存块大小的影响</strong>:</p>
<ul>
<li><strong>small block size</strong>：准确率最高，但因频繁更新缓存导致开销较大，速度提升有限 。</li>
<li><strong>small block size</strong>：速度快，但可能因上下文不匹配导致准确率下降 。</li>
<li>实验发现，块大小为 <strong>32</strong> 时在速度和精度之间取得了最佳平衡。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&amp;shareKey=1e3a007e630de1a9cbf8b3d9f318f307" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&amp;shareKey=1e3a007e630de1a9cbf8b3d9f318f307" alt="Impact of Cache Block Size on Accuracy and Throughput">
    </a><figcaption>Impact of Cache Block Size on Accuracy and Throughput</figcaption></figure></p>
<ul>
<li><strong>动态阈值 vs. 固定步数策略</strong>:
<ul>
<li>论文提出的 <strong>置信度感知并行解码</strong> 策略，在性能上持续优于每步固定解码 K 个 token 的 baseline 方法。</li>
<li>在达到相似甚至更高准确率的同时，该动态策略能实现更高的平均每步解码 token 数，从而获得更高的吞吐量。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&amp;shareKey=88d29eb3e40615a74c4846d278413e5b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&amp;shareKey=88d29eb3e40615a74c4846d278413e5b" alt="Threshold VS Fxied Step">
    </a><figcaption>Threshold VS Fxied Step</figcaption></figure></p>
<h1 id="5-related-work">5. Related Work</h1>
<p>本章节回顾了与 Fast-dLLM 相关的两个核心领域：扩散语言模型的发展，以及大语言模型的通用加速技术。</p>
<hr>
<h2 id="51-diffusion-llm">5.1. Diffusion LLM</h2>
<p>扩散模型作为一种强大的生成范式，最初在图像和音频等连续数据领域取得了巨大成功，随后其影响力扩展到了 NLP. 特别是离散扩散模型的最新进展为大语言模型提供了一种替代自回归 (AR) 范式的可行方案 。</p>
<ul>
<li>
<p><strong>理论基础的发展</strong>:</p>
<ul>
<li>离散数据的扩散模型最早由 [29, 11] 探索 。</li>
<li><strong>D3PM</strong> 提出了一个更通用的框架，将前向加噪过程建模为离散状态马尔可夫链，并通过最大 ELBO 来学习反向过程。</li>
<li><strong>CTMC</strong> 将 D3PM 扩展到连续时间设定 。</li>
<li><strong>SEDD</strong> 采用了不同的方法，通过参数化边际似然比来学习反向过程 。</li>
<li><strong>MDMs</strong> 近期受到了广泛关注，其中 <strong>MDLM</strong> 和 <strong>RADD</strong> 的研究表明，MDMs 的不同参数化方法是等价的，并且其训练目标可以被简化 。</li>
</ul>
</li>
<li>
<p><strong>与预训练语言模型的结合</strong>: 一个关键的突破是将离散扩散与现有的大语言模型架构相结合 。</p>
<ul>
<li><strong>Diffusion-NAT</strong> [40] 将离散扩散的去噪过程与 BART 的非自回归解码相结合，通过迭代式地优化被掩码的 token ，实现了比同类自回归 Transformer 快20倍的生成速度 。</li>
<li><strong>LLaDA</strong> [21]、<strong>DiffuLLaMA</strong> [7] 和 <strong>Dream</strong> [36] 等框架将扩散模型扩展到了 7B 参数的规模，通过在扩散时间步上进行递归式的 token 预测，展现了与 LLaMA3 等主流自回归模型相匹敌的性能 。</li>
</ul>
</li>
</ul>
<h2 id="52-llm-acceleration">5.2. LLM Acceleration</h2>
<ul>
<li>KV Cache</li>
</ul>
<p>由于 LLaDA 等扩散语言模型采用的是 <strong>full attention</strong>，将 KV 缓存直接应用于这类模型并非易事。 一篇相关的研究 <strong>Block diffusion</strong>  通过<strong>分块生成 (block-by-block)</strong> 的方式，克服了先前扩散语言模型的局限，使得缓存和复用先前已解码块的键和值成为可能 。</p>
<ul>
<li>Non-Autoregressive Generation</li>
</ul>
<p>非自回归 (NAR) 生成标志着一种根本性的转变，它通过同时生成多个 token 来显著加速推理过程。NAR 方法最初被用于神经机器翻译，现已扩展到语法纠错、文本摘要和对话系统等多种任务
。
尽管 NAR 在速度上优势巨大，但它通常以牺牲一定的生成质量为代价。扩散语言模型是 NAR 领域一个新兴的范式；然而，先前的工作 (如 LLaDA) 在实践中难以实现预期的加速，因为并行生成会导致输出质量显著下降。</p>
<h1 id="weakness">Weakness</h1>
<p>近似缓存的误差累积效应：论文证明了在相邻步骤中，KV激活值的差异很小 。但随着生成块的增多，这种“近似”带来的微小误差是否会累积，并在生成非常长的文本 (如数万 token 的小说) 时导致语义漂移或一致性下降？论文的最长测试序列为1024 ，对于更长的序列，其鲁棒性有待进一步验证。</p>
<p>对模型能力的依赖：“置信度感知解码”策略的有效性，隐式地依赖于模型本身具有良好的“校准度” (calibration) ，即模型的置信度能够真实反映其预测的正确性。如果模型本身“过于自信”或“不够自信”，可能会导致该策略效果不佳。论文没有对所用模型的校准度进行分析。
定理一的理论与实践差距：论文坦诚地指出了定理一的局限性</p>
<blockquote>
<p>In practice, while MDM may not strictly satisfy this property, its behavior typically offers a close approximation.</p></blockquote>
<p>理论证明假设了一个“理想的”联合概率分布，而真实模型是否以及在多大程度上符合这个理想假设，是一个需要进一步探究的问题。理论和实践之间的差距可能在某些刁钻的 (adversarial) 或分布外 (Out-of-Distribution) 的场景下被放大。
超参数的敏感性与调优成本：尽管论文分析了块大小和阈值的影响，但并未提供一套系统性的方法来为新模型或新任务选择最佳超参数。在实际应用中，这可能意味着需要为每个特定用例进行成本不菲的网格搜索 (grid search) ，增加了方法的应用门槛。
评估维度的局限性：论文主要使用了基于准确率的基准测试。但在开放式生成、对话等任务中，评估指标 (如流畅度、一致性、多样性) 更为复杂。Fast-dLLM是否会在这些“软”指标上引入不易察觉的负面影响，需要更全面的评估。</p>
<h1 id="source-code">Source Code</h1>
<ol>
<li>
<p><strong>初始化</strong>:</p>
<ul>
<li>函数首先创建一个张量 <code>x</code>，其长度为“提示词长度 + 待生成长度”。</li>
<li>提示词 (<code>prompt</code>) 部分被填充到 <code>x</code> 的开头，而所有待生成的位置则被初始化为特殊的掩码标记 <code>[MASK]</code> (<code>mask_id</code>) 。</li>
<li>将总生成任务分解为多个块 (<code>num_blocks</code>) ，并为每个块分配固定的解码步数 (<code>steps</code>)</li>
</ul>
</li>
<li>
<p><strong>分块生成 (外层循环)</strong>:</p>
<ul>
<li>代码以块为单位进行循环，依次生成每个文本块。</li>
</ul>
</li>
<li>
<p><strong>处理单个块 (内层循环与缓存机制)</strong>:</p>
<ul>
<li>
<p><strong>步骤 A: 全局缓存初始化 (第一次模型调用)</strong></p>
<ul>
<li>在处理一个新块的开始，它首先将<strong>整个序列 <code>x</code></strong> (包含提示词、已生成的块和所有未来待生成的<code>[MASK]</code>块) 完整地输入模型。</li>
<li>这次调用的主要目的是计算并存储整个序列的键值对缓存 (<code>past_key_values</code>). 这是一个全局缓存。</li>
<li>然后，模型根据输出的 <code>logits</code>，使用 <code>get_transfer_index</code> 函数决定在<strong>当前块</strong>中，哪些 <code>[MASK]</code> 标记应该被优先替换掉 (例如，基于最高置信度的预测) ，并用预测出的 token  (token) 进行填充。这个过程只发生一次。</li>
</ul>
</li>
<li>
<p><strong>步骤 B: 块内迭代优化 (第二次及后续模型调用)</strong></p>
<ul>
<li>接下来，进入一个 <code>while</code> 循环，对当前块进行迭代式地“精炼”，直到这个块中所有的 <code>[MASK]</code> 标记都被填满。</li>
<li><strong>核心优化点</strong>：在这次及后续的模型调用中，<strong>不再需要输入整个序列</strong>。它只将<strong>当前块的张量</strong> (<code>x[:, current_block_start:current_block_end]</code>) 作为输入，并<strong>重用步骤 A 中生成的全局缓存 <code>past_key_values</code></strong>。</li>
<li>这就是 dual cache: 一个为上下文 (提示词+之前块) 准备的、基本不变的静态缓存，和一个为当前块服务的、动态更新的缓存。这避免了对上下文部分的重复计算，极大地提升了效率。</li>
<li>模型会为当前块中剩余的 <code>[MASK]</code> 位置生成新的预测，并根据策略继续填充。</li>
<li>这个迭代过程会持续进行，直到当前块不再有 <code>[MASK]</code> 标记为止。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>完成与返回</strong>:</p>
<ul>
<li>当所有块都处理完毕后，函数返回最终生成的完整序列 <code>x</code> 和总的模型前向传播次数 <code>nfe</code> (一个衡量计算成本的指标) 。</li>
</ul>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@torch.no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_with_dual_cache</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">gen_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">block_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">remasking</span><span class="o">=</span><span class="s1">&#39;low_confidence&#39;</span><span class="p">,</span> <span class="n">mask_id</span><span class="o">=</span><span class="mi">126336</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Generates text using a non-autoregressive, block-wise decoding strategy with a dual-cache mechanism.
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">        model: The mask predictor model.
</span></span></span><span class="line"><span class="cl"><span class="s1">        prompt: A tensor of shape (1, L) representing the input prompt.
</span></span></span><span class="line"><span class="cl"><span class="s1">        steps: Total number of sampling/refinement steps for the entire generation.
</span></span></span><span class="line"><span class="cl"><span class="s1">        gen_length: The desired length of the generated text.
</span></span></span><span class="line"><span class="cl"><span class="s1">        block_length: The size of each block to be generated in parallel. gen_length must be divisible by this.
</span></span></span><span class="line"><span class="cl"><span class="s1">        temperature: Sampling temperature for token selection. 0 means greedy decoding.
</span></span></span><span class="line"><span class="cl"><span class="s1">        remasking: The strategy for choosing which masks to fill (&#39;low_confidence&#39; or &#39;random&#39;).
</span></span></span><span class="line"><span class="cl"><span class="s1">        mask_id: The token ID for the [MASK] token.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create the full tensor &#39;x&#39; with the prompt and space for generation, initialized with the mask token.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">gen_length</span><span class="p">),</span> <span class="n">mask_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Copy the prompt into the beginning of the tensor &#39;x&#39;.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Ensure that the generation length can be evenly divided into blocks.</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">gen_length</span> <span class="o">%</span> <span class="n">block_length</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">gen_length</span> <span class="o">//</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Distribute the total steps among the blocks.</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">steps</span> <span class="o">%</span> <span class="n">num_blocks</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">steps_per_block</span> <span class="o">=</span> <span class="n">steps</span> <span class="o">//</span> <span class="n">num_blocks</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># nfe: Number of Forward-pass Evaluations. A counter for computational cost.</span>
</span></span><span class="line"><span class="cl">    <span class="n">nfe</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Outer loop: iterate through each block to be generated.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">num_block</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Define the start and end positions of the current block within the full tensor &#39;x&#39;.</span>
</span></span><span class="line"><span class="cl">        <span class="n">current_block_start</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num_block</span> <span class="o">*</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">        <span class="n">current_block_end</span> <span class="o">=</span> <span class="n">current_block_start</span> <span class="o">+</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Find the indices of mask tokens within the current block.</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Determine the number of tokens to fill at each refinement step for this block.</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_transfer_tokens</span> <span class="o">=</span> <span class="n">get_num_transfer_tokens</span><span class="p">(</span><span class="n">block_mask_index</span><span class="p">,</span> <span class="n">steps_per_block</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- First Model Call: Initialize Global Cache ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># A single forward pass on the ENTIRE sequence (prompt + all masked blocks) to pre-calculate</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># the Key-Value cache for all tokens. This is the &#34;global&#34; cache.</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">past_key_values</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Identify all mask tokens up to the end of the current block.</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Ignore masks that are in future blocks for this step&#39;s prediction.</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask_index</span><span class="p">[:,</span> <span class="n">current_block_end</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Select which tokens to predict and fill in this initial step for the current block.</span>
</span></span><span class="line"><span class="cl">        <span class="n">x0</span><span class="p">,</span> <span class="n">transfer_index</span> <span class="o">=</span> <span class="n">get_transfer_index</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">remasking</span><span class="p">,</span> <span class="n">mask_index</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_transfer_tokens</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Update the tensor &#39;x&#39; by filling the selected mask positions with the predicted tokens.</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">nfe</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Increment the forward-pass counter.</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Counter for refinement steps within the block.</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># A boolean mask indicating the position of the current block, used to update the cache efficiently.</span>
</span></span><span class="line"><span class="cl">        <span class="n">replace_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">replace_position</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Inner Loop: Iterative Refinement of the Current Block ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This loop continues until all masks in the current block are filled.</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">nfe</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Increment the forward-pass counter for each refinement step.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Find the remaining masks ONLY within the current block.</span>
</span></span><span class="line"><span class="cl">            <span class="n">mask_index_block</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># --- Efficient Model Call using Dual Cache ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Instead of passing the whole sequence, only pass the CURRENT BLOCK&#39;s tokens.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Reuse the &#39;past_key_values&#39; (global cache) computed earlier. The model internally</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># uses &#39;replace_position&#39; to update the cache only at the current block&#39;s location.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># This is the &#34;dual cache&#34; trick, avoiding re-computation for the prompt and previous blocks.</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">],</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">replace_position</span><span class="o">=</span><span class="n">replace_position</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># Select which of the remaining masks to fill in this refinement step.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0</span><span class="p">,</span> <span class="n">transfer_index</span> <span class="o">=</span> <span class="n">get_transfer_index</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">remasking</span><span class="p">,</span> <span class="n">mask_index_block</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                            <span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">],</span> <span class="n">num_transfer_tokens</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Update the current block with the newly predicted tokens.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">][</span><span class="n">transfer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># If there are no more masks in the current block, exit the refinement loop.</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Move to the next refinement step.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Return the fully generated sequence and the total number of model evaluations.</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">nfe</span>
</span></span></code></pre></div>]]></content:encoded>
    </item>
    <item>
      <title>LLaDA</title>
      <link>http://localhost:1313/blogs/llada/</link>
      <pubDate>Thu, 12 Jun 2025 13:43:16 +0800</pubDate>
      <guid>http://localhost:1313/blogs/llada/</guid>
      <description>Paper Reading of LLaDA</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>LLM 主要的思想是 <em>generative modeling</em> 的思想是通过最大似然估计来优化模型的分布 $\log p_\theta(\cdot)$ 来逼近数据的分布 $\log p_{\text{data}}(\cdot)$
</p>
$$
\underbrace{\max_\theta\mathbb{E}_{p_{\text{data}}(x)}\log p_\theta(x)\Leftrightarrow\min_\theta\operatorname{KL}(p_{\text{data}}(x)||p_\theta(x)).}_{\text{Generative modeling principles}} \tag{1}
$$<p>当前，几乎所有我们熟知的大语言模型，从GPT系列到LLaMA系列，都基于<em>autoregressice modeling</em> 来实现。这种范式的核心是 <strong>next-token prediction</strong> ，即根据已经生成的文本序列，逐 toekn 地预测下一个最有可能出现的 token.</p>
$$
\underbrace{p_\theta(x)=p_\theta(x^1)\prod_{i=2}^Lp_\theta(x^i\mid x^1,\ldots,x^{i-1})}_{\text{Autoregressive formulation}} \tag{2}
$$<p>这种单向、顺序的生成方式在处理需要双向推理的任务时表现不佳，一个典型的例子就是 <strong>Reversal Curse</strong> ——模型知道 A is B，却往往无法推断出 B is A.</p>
<p>LLM 能力的核心基石是生成式建模原理本身，即通过最大似然估计让模型学习真实世界的数据分布 ，而非自回归这一具体的实现形式。</p>
<blockquote class="quote"><p><strong>It is the generative modeling principles (i.e., Eq. (1)), rather than the autoregressive formulation (i.e., Eq. (2)) itself, that fundamentally underpin the essential properties of LLMs.</strong></p></blockquote>
<ol>
<li>
<p>大语言模型的可扩展性 (scalability) ——即模型越大、数据越多、效果越好的特性——并非自回归模型所独有 。相反，这种可扩展性来源于更底层的生成式建模原理，而这些原理恰好保证了<em>fisher consistency</em><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</li>
<li>
<p><em>instruction-following</em> 和 <em>in-context learning</em><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> 并非自回归模型所独有，而是所有设计得当的条件生成模型 (conditional generative models) 在处理结构化语言任务时都应具备的内在属性 。</p>
</li>
</ol>
<p>因此作者提出了<strong>LLaDA</strong> (<strong>L</strong>arge <strong>L</strong>anguage <strong>D</strong>iffusion with m<strong>A</strong>sking)，一个从零开始训练的、参数量达到 8B 的扩散语言模型。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&amp;shareKey=94170299ede39d5102cf1cf6e397c5c7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&amp;shareKey=94170299ede39d5102cf1cf6e397c5c7" alt="Zero&amp;Few-Shot Benchmarks">
    </a><figcaption>Zero&amp;Few-Shot Benchmarks</figcaption></figure></p>
<p>LLaDA 使用了 Masked Diffusion Model (MDM)，该方法结合了离散随机掩蔽过程，并训练了一个掩码预测器来近似其反向过程。</p>
<h1 id="2-approach">2 Approach</h1>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&amp;shareKey=0293b80db53bfd7b8a9ba03f15a6f802" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&amp;shareKey=0293b80db53bfd7b8a9ba03f15a6f802" alt="A Conceptual Overview of LLaDA">
    </a><figcaption>A Conceptual Overview of LLaDA</figcaption></figure></p>
<h2 id="21-probabilistic-formulation">2.1 Probabilistic Formulation</h2>
<p>与公式(2)中的自回归模型不同，LLaDA通过<strong>前向过程 (forward process)</strong> 和 <strong>反向过程 (reverse process)</strong> 来定义模型分布 $p_{\theta}(x_{0})$。</p>
<h3 id="forward-process">Forward Process</h3>
<p>逐步地、独立地 mask $x_{0}$ 中的 token，直到在 $t=1$ 时序列被完全 mask.</p>
<p>给定 $x_{0}$ 时 $x_{t}$ 的条件分布可以被分解为：</p>
$$
q_{t|0}(x_{t}|x_{0}) = \prod_{i=1}^{L} q_{t|0}(x_{t}^{i}|x_{0}^{i})
$$<p>对于 $t \in (0,1)$，序列 $x_{t}$ 是部分被掩码的，其中每个 token 有 $t$ 的概率被mask，或有 $1-t$ 的概率保持不变。</p>
$$
q_{t|0}(x_{t}^{i}|x_{0}^{i}) = \begin{cases} 1-t, & x_{t}^{i} = x_{0}^{i} \\ t, & x_{t}^{i} = M \end{cases}
$$<p>其中 M 表示掩码 token. 直观上，每个 token 要么保持不变，要么被掩码，<strong>被掩码的概率随着 t 从 0 到 1 线性增加</strong>。在 $t=1$ 时，所有 token 都被 mask. 线性变化的被掩码概率和原先扩散模型的加噪流程不一样，是基于文本信息和 token 长度成正比的假设。</p>
<h2 id="reverse-process">Reverse Process</h2>
<p>反向过程则通过在 $t=1\rightarrow 0$ 从完全被掩码的序列中生成新数据。</p>
<p>对于 $0 \le s < t \le 1$，反向过程的条件分布分解为：</p>
$$
q_{s|t}(x_{s}|x_{t}) = \prod_{i=1}^{L} q_{s|t}(x_{s}^{i}|x_{t})
$$<p>其中每个 token 的条件分布为：</p>
$$
q_{s|t}(x_{s}^{i}|x_{t}^{i}) = \begin{cases} 1, & x_{t}^{i} \ne M, x_{s}^{i} = x_{t}^{i} \\ \frac{s}{t}, & x_{t}^{i} = M, x_{s}^{i} = M \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & x_{t}^{i} = M, x_{s}^{i} \ne M \\ 0, & \text{otherwise} \end{cases}
$$<p>需要估计的关键函数是条件分布 $q_{0|t}(x_{s}^{i}|x_{t})$，它在输入 $x_{t}$ 中对应位置被掩码的情况下，预测出原始的 token. 类似于连续扩散模型中的数据预测形式。如 (Ou et al., 2024) 所证明，可以推导出一个等价但无时间依赖的参数化形式</p>
$$
q_{0|t}(x_s^i|x_t)=p_{\text{data}}(x_0^i|x_t^\text{UM}),\quad\forall i\text{ such that }x_t^i=\mathbf{M}
$$<p>其中 $x_{t}^{\text{UM}}$ 表示 $x_{t}$ 中未被掩码 token 的集合，它与原始数据 $x_{0}$ 中对应的 token 相同，因为未掩码的 token 仅由 $x_{0}$ 决定且与时间 t 无关 。直观上，这意味着估计数据预测函数等同于估计在干净数据上的条件分布，而后者是时不变的。因此，时间 t 不需要作为输入提供给参数化模型 。</p>
<p>尽管 MDM 的推导过程不简单，但其实现是直接的。我们首先引入<strong>掩码预测器</strong>，一个参数化模型 $p_{\theta}(\cdot|x_{t})$ (例如一个没有因果掩码的 Transformer)，它将任意 t 时刻的 $x_{t}$ 作为输入，并同时预测所有被 mask 的 token. 然后，我们如下定义模型分布 $p_{\theta}(x_{0})$：从一个被完全 mask 序列的 $x_{1}$ 开始，从 $t=1$ 到 0 模拟一个由 $p_{\theta}(\cdot|x_{t})$ 参数化的近似反向过程。在 $t=0$ 时刻推导出的边缘分布即代表了模型分布 $p_{\theta}(x_{0})$ 。</p>
<p>掩码预测器将 $x_{t}$ 作为输入并同时预测所有被掩码的 token (表示为 M). 它通过一个仅在被掩码 token 上计算的交叉熵损失进行训练：</p>
$$
\mathcal{L}(\theta)\triangleq-\mathbb{E}_{t,x_{0},x_{t}}[\frac{1}{t}\sum_{i=1}^{L}I[x_{t}^{i}=M]log~p_{\theta}(x_{0}^{i}|x_{t})], \tag{3}
$$<p>其中，$x_{0}$ 从训练数据中采样，$t$ 从<code>[0, 1]</code>中均匀采样<span class="sidenote-number"><small class="sidenote">Notably, LLaDA employs a masking ratio that <em>varies randomly</em> between 0 and 1 while masked language models (Devlin, 2018) use a fixed ratio.</small></span>
，$x_{t}$ 从前向过程中采样。指示函数 $I[\cdot]$ 确保损失仅针对被掩码的 token 计算。一旦训练完成，便可以模拟一个由该掩码预测器参数化的反向过程（详见2.4节），并将模型分布 $p_{\theta}(x_{0})$ 定义为该过程的边缘分布。</p>
<p>公式(3)已被证明是模型分布负对数似然的上界</p>
$$
-\mathbb{E}_{p_{\text{data}}(x_{0})}\left[\log p_{\theta}(x_{0})\right]\leq\mathcal{L}(\theta) \tag{4}
$$<p>该方法通过在正向过程中逐步屏蔽 token 并在反向过程中学习恢复数据分布来训练生成模型，所有这些都在（近似）最大似然估计框架下。</p>
<h2 id="pretraining">Pretraining</h2>
<ul>
<li>
<p>LLaDA 8B 模型在一个包含 2.3T tokens 的高质量、多源数据集上从零开始进行预训练。该数据集覆盖了通用文本、代码、数学和多语言内容 。</p>
</li>
<li>
<p>训练总共消耗了 0.13M H800 GPU hours. 训练序列长度固定为4096. 其核心训练步骤是：对每个序列随机采样一个掩码率 t，并独立地以该概率掩码每个 token，然后让模型去预测被掩码的部分 。</p>
</li>
<li>
<p><strong>架构调整</strong> 相较于LLaMA3 8B，LLaDA 8B在架构上做了一些必要调整，如使用标准的 MHA 而非 GQA，并相应地调整了 FFN 的维度以保持模型总参数量相当 。</p>
</li>
<li>
<p><strong>优化器与学习率</strong> 训练使用了 AdamW 优化器和一个特殊的 Warmup-Stable-Decay 学习率调度策略。整个8B模型的训练实验只执行了一次，没有进行任何超参数调优。</p>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center">Our ARM Baseline 1B</th>
          <th style="text-align: center">LLaDA IB</th>
          <th style="text-align: center">Our ARM Baseline 7B</th>
          <th style="text-align: center">LLaDA 8B</th>
          <th style="text-align: center">LLaMA3 8B</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>Layers</strong></td>
          <td style="text-align: center">22</td>
          <td style="text-align: center">22</td>
          <td style="text-align: center">28</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Model dimension</strong></td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">4096</td>
          <td style="text-align: center">4096</td>
          <td style="text-align: center">4096</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Attention heads</strong></td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Vocabulary size</strong></td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126.464</td>
          <td style="text-align: center">128,000</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>FFN dimension</strong></td>
          <td style="text-align: center">5634</td>
          <td style="text-align: center">5634</td>
          <td style="text-align: center">13.440</td>
          <td style="text-align: center">12,288</td>
          <td style="text-align: center">14,336</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Key/Value heads</strong></td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">8</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">8</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Total parameters</strong></td>
          <td style="text-align: center">1.49 B</td>
          <td style="text-align: center">1.49 B</td>
          <td style="text-align: center">6.83 B</td>
          <td style="text-align: center">8.02 B</td>
          <td style="text-align: center">8.03 B</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Non-embedding parameters</strong></td>
          <td style="text-align: center">0.97 B</td>
          <td style="text-align: center">0.97 B</td>
          <td style="text-align: center">5.80 B</td>
          <td style="text-align: center">6.98 B</td>
          <td style="text-align: center">6.98 B</td>
      </tr>
  </tbody>
</table>
<h2 id="supervised-fine-tuning">Supervised Fine-Tuning</h2>
<p>我们通过使用配对数据 $(p_{0}, r_{0})$ 进行 <strong>监督微调 (SFT)</strong> 来增强LLaDA遵循指令的能力，其中 $p_{0}$ 是 prompt，$r_{0}$ 表示响应(response). 这是针对LLM最简单、最基础的 post-training 方法。从技术上讲，这要求模型对条件分布 $p_{\theta}(r_{0}|p_{0})$，而非预训练中的 $p_{\theta}(x_{0})$ 进行建模。</p>
<p>其实现方式与预训练类似。如图2(b)所示，保持 prompt 部分不变，并像处理 $x_{0}$ 一样，独立地 mask response 中的 token. 然后，将提示和被掩码的响应 $r_{t}$ 一同送入预训练好的掩码预测器，以计算用于 SFT 的损失</p>
$$
-\mathbb{E}_{t,p_{0},r_{0},r_{t}}[\frac{1}{t}\sum_{i=1}^{L^{\prime}}I[r_{t}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{t})] \tag{5}
$$<p>其中，$L^{\prime}$ 表示稍后指定的动态长度。这种方法与预训练是完全兼容的。本质上，将 $p_{0}$ 和 $r_{0}$ 拼接起来可以被视为干净的预训练数据 $x_{0} $，而将 $p_{0}$ 和 $r_{t}$ 拼接起来则可作为其被掩码后的版本 $x_{t}$. 这个过程与预训练完全相同，唯一的区别在于所有被掩码的 token 恰好都出现在 $r_{0}$ 部分。</p>
<p>LLaDA 8B 模型在一个包含 4.5M 对样本的数据集上进行了 SFT. 与预训练过程一致，数据准备和训练都遵循了现有LLM (Chu et al., 2024; Yang et al., 2024) 中使用的 SFT 协议，没有引入任何额外的技术来优化 LLaDA 的性能。该数据集涵盖了多个领域，包括代码、数学、指令遵循和结构化数据理解。我们在每个 mini-batch 中的短样本对末尾附加 EOS token，以确保所有数据长度相等。在训练期间将 EOS视为一个普通 token ，并在采样时将其移除，使得LLaDA能够自动控制响应的长度。</p>
<p>我们在SFT数据上训练了 3 个 epoch，其调度策略与预训练阶段相似。学习率在最初 50 次迭代中从 0 线性增加到 $2.5 \times 10^{-5}$，然后保持不变。在最后 10% 的迭代中，学习率性降低到 $2.5 \times 10^{-6}$. 此外，我们将权重衰减设置为 0.1，全局 batch size 设置为 256，每个 GPU 的本地 batch size 设置为 2. SFT实验只执行了一次，没有进行任何超参数调优。</p>
<h2 id="inference">Inference</h2>
<p>作为一个生成式模型，LLaDA既能 <strong>采样 (sampling)</strong> 新文本，也能 <strong>评估 (evaluating)</strong> 候选文本的似然。</p>
<p>先从采样说起。如图 2(c) 所示，给定一个 prompt $p_{0}$，我们通过离散化反向过程来从模型分布 $p_{\theta}(r_{0}|p_{0})$ 中进行采样，这个过程从一个被完全掩码的 response 开始。</p>
<p><strong>总的采样步数是一个超参数</strong>，为 LLaDA 提供了一个在效率和样本质量之间的权衡（详见3.3节分析）。我们默认使用均匀分布的时间步。
此外，<strong>生成长度也被视为超参数</strong>，它指定了采样过程开始时完全被掩码句子的长度。如附录B.4所述，由于预训练和SFT都是在可变长度的数据集上进行的，最终结果对这个长度超参数不敏感。</p>
<p>在一个从时间 $t \in (0, 1]$ 到 $s \in [0, t)$的中间步骤中，我们将 $p_{0}$ 和 $r_{t}$ 同时送入掩码预测器，并一次性预测所有被掩码的 token. 随后 <em>remask</em> $\frac{s}{t}$ 比例的已预测 token 得到$r_{s}$，从而确保反向过程的转换与前向过程保持一致，以实现准确采样。</p>
<p>受 LLM 采样中退火技巧的启发，我们探索了两种确定性但有效的重掩码策略。</p>
<ul>
<li><strong>low-confidence remasking</strong>: remask 那些基于预测置信度最低的 $\frac{s}{t}$ 比例的 token.</li>
<li><strong>semi-autoregressive remasking</strong>: 对于经过 SFT 的 LLaDA 模型，将序列分成几个块，并从左到右地生成. 在每个块内部，采用反向过程进行采样。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&amp;shareKey=838350c5b31c7e78112324263cdf5621" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&amp;shareKey=838350c5b31c7e78112324263cdf5621" alt="A Conceptual Overview of the Semi-autoregressive Sampling">
    </a><figcaption>A Conceptual Overview of the Semi-autoregressive Sampling</figcaption></figure></p>
<p>对于条件似然评估，我们自然可以利用公式(5)中的上界。然而，我们发现下面这个等价形式（公式6）表现出更低的方差，在评估时更为稳定：</p>
$$
-\mathbb{E}_{l,r_{0},r_{l}}[\frac{L}{l}\sum_{i=1}^{L}I[r_{l}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{l})] \tag{6}
$$<p>其中，$l$ 从 ${1, 2, ..., L}$ 中均匀采样，$r_{l}$ 是通过从 $r_{0}$ 中不放回地均匀采样 $l$ 个没被 mask 的 token 得到的。此外，我们还采用了 unsupervised classifier-free guidance.</p>
<p><strong>虽然这两个形式的期望值相同，但它们的方差不同</strong>。直观上，在公式 (5) 中，我们期望 $x_{t}=[p_0,r_t]$ 有 $t$ 比例的 token 被掩码。然而，前向过程的随机性常常会导致偏差，尤其当 $x_{t}$ 包含的 token 很少时。相比之下，在公式 (6) 中，$r_{l}$ 中被掩码 token 的比例 $\frac{l}{L}$ 是确定的。</p>
<p>虽然理论分析取决于数据分布，但经验结果表明，公式 (5) 需要超过 1000 次蒙特卡洛估计才能得到稳定结果，而公式 (6) 仅需 128 次估计即可达到稳定。</p>
<p>Any-order autoregressive models (AO-ARM)  通过对 L 个变量所有可能的排列顺序进行自回归来描述联合分布。为了学习这样的分布，AO-ARM 利用一个权重共享的神经网络来为所有单变量条件概率建模，并使用掩码 token 来表示缺失的变量。在训练期间，模型会最小化在所有顺序的均匀分布 $U_{\pi}$ 上的期望负对数似然：</p>
$$
-\mathbb{E}_{x_{0},\pi \sim U_{\pi}}[\sum_{i=1}^{L}log~p_{\theta}(x_{0}^{\pi(i)}|x_{0}^{\pi(<i)}; \pi)]
$$<p> (15)</p>
<p>直观上，$x_{0}^{\pi(<i)}$ 可以被理解为一个被掩码的 token 序列 $x_{t}$，其中索引在 $\pi(\ge i)$ 的 token 被掩码 。可以进一步证明，公式 (15) 等价于公式 (12) 。这种联系解释了 LLaDA 的双向推理能力，即使它在推理过程中从未被显式使用 。</p>
<p>Nie et al. (2024) 引入了无监督的无分类器指导，这是一种即插即用的技术，可以平衡与提示的对齐度和文本多样性 。具体来说，无监督的无分类器指导在推理时采用以下修改过的掩码预测器 ：</p>
$$
\tilde{p}_{\theta}(r_{0}|p_{0},r_{t}) \propto \frac{p_{\theta}(r_{0}|p_{0},r_{t})^{1+w}}{p_{\theta}(r_{0}|m,r_{t})^{w}}
$$<p> (16)</p>
<p>其中，$m$ 是一个与 $p_{0}$ 长度相同的掩码序列，$w$ 是一个控制 $p_{0}$ 强度的超参数 。我们在下游任务中采用了无监督的无分类器指导，详见附录 B.5 。</p>
<h1 id="3-experiment">3 Experiment</h1>
<p>实验主要围绕以下三个核心方面展开：</p>
<ol>
<li>
<p>可扩展性 (Scalability)：研究 LLaDA 的性能是否随着计算资源和模型规模的增加而稳定提升。通过与自建的自回归模型 (ARM) 基线在相同数据上进行对比，结果显示 LLaDA 表现出强大的可扩展性，其性能增长趋势与 ARM 相当，甚至在 MMLU 和 GSM8K 等任务上更具优势。</p>
</li>
<li>
<p>基准测试结果 (Benchmark Results)：将 8B 规模的 LLaDA 与 LLaMA3 8B、LLaMA2 7B 等主流模型在涵盖通用，数学，代码和中文四大类的 15 个标准基准上进行对比。</p>
<ul>
<li>
<p>预训练模型：LLaDA 8B Base 模型的性能全面超越 LLaMA2 7B，并与 LLaMA3 8B 整体上具有竞争力，尤其在数学和中文任务上表现突出。</p>
</li>
<li>
<p>微调模型：仅经过 SFT 的 LLaDA 8B Instruct 模型，在未进行强化学习对齐的情况下，其性能在多数任务上得到提升 ，并展现出令人印象深刻的 Instruction Follow 能力。</p>
</li>
</ul>
</li>
<li>
<p>反向推理 (Reversal Reasoning)：为了量化模型克服“反转诅咒”的能力，实验在一个中文古诗补全任务上进行了测试。结果表明，LLaDA 在正向和反向任务上表现均衡，一致性强，而 GPT-4o 等模型则在反向任务上表现出显著的性能下降。</p>
</li>
</ol>
<h1 id="generation-code">Generation code</h1>
<ol>
<li>
<p><strong>初始化 (The Canvas) 🎨</strong></p>
<p>函数首先会创建一个如下所示的序列：
<code>[&lt;start_token&gt;, &lt;prompt_tokens&gt;, [MASK], [MASK], ..., [MASK]]</code>
generate 的目标就是用一个连贯的答案来替换掉所有的 <code>[MASK]</code> 标记。</p>
</li>
<li>
<p><strong>分块 (Semi-Autoregressive) 🧱</strong></p>
<p>算法并不会一次性填充所有 <code>gen_length</code> 个掩码，而是将整个过程分解为 <code>num_blocks</code> 个块。它会先完全填满第一个 <code>block_length</code> 长度的掩码，然后再开始处理下一个块。这种方式在宏观层面引入了从左到右的生成顺序。</p>
</li>
<li>
<p><strong>迭代式精炼 (核心循环) 🔄</strong></p>
<p>对于每一个块，代码都会进入一个内部循环，该循环运行 <code>steps_per_block</code> 次。循环的每一步中：</p>
<ul>
<li>
<p><strong>A. 预测：</strong> 将当前的 <code>x</code> 包含其中剩余的掩码 输入到 <code>LLaDA</code> 模型中。模型会为序列中的<em>每一个</em>位置预测最可能的 token，即使是那些没有被掩码的位置也会进行预测。</p>
</li>
<li>
<p><strong>B. 生成候选 token：</strong> 算法通过对模型的输出 <code>logits</code> 执行 <code>argmax</code> 操作，为每个位置确定一个候选 token. 在这里可以加入 Gumbel 噪声来引入随机性，其作用类似于自回归采样中的 <code>temperature</code>。这样我们就得到了一个完整的候选序列 <code>x0</code>。</p>
</li>
<li>
<p><strong>C. 置信度评分：</strong> 算法需为每个 <code>[MASK]</code> 位置上预测出的 token 计算一个<strong>置信度分数</strong>。<code>low_confidence</code> 策略（尽管其在代码逻辑中的命名可能有点误导）使用预测 token 的 softmax 概率作为其置信度。概率越高，代表模型越自信。</p>
</li>
<li>
<p><strong>D. token 选择：</strong> 基于置信度分数，算法会保留<strong>置信度最高的 K 个</strong>预测结果。每一步要保留的 token 数量 (K) 由 <code>get_num_transfer_tokens</code> 函数预先计算好，以确保线性的 unksk 速率。</p>
</li>
<li>
<p><strong>E. 状态更新：</strong> 在那些被选中的高置信度位置，<code>[MASK]</code>  token 会被替换成 <code>x0</code> 中对应的预测 token. 而其他的 <code>[MASK]</code> 位置则保持不变，留待下一次迭代。</p>
</li>
</ul>
</li>
<li>
<p><strong>重复与推进 ➡️</strong>
内部循环不断重复。在下一次迭代中，模型会看到更新后的 <code>x</code>，其中包含了更多上下文信息和更少的掩码。这使得模型在后续步骤中能做出更好的预测。这个精炼过程会一直持续，直到当前块中所有的 <code>[MASK]</code> 都被去除。</p>
</li>
<li>
<p><strong>下一区块与完成 ✅</strong>
当一个块完成后，外部循环会移动到下一个 <code>[MASK]</code> 块，并重复整个迭代式精炼过程，直到生成了完整的 <code>gen_length</code> 长度。最后，返回最终被完全填充的序列。</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">add_gumbel_noise</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    The Gumbel max is a method for sampling categorical distributions.
</span></span></span><span class="line"><span class="cl"><span class="s1">    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Thus, we use float64.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">logits</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">gumbel_noise</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">noise</span><span class="p">))</span> <span class="o">**</span> <span class="n">temperature</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">/</span> <span class="n">gumbel_noise</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_num_transfer_tokens</span><span class="p">(</span><span class="n">mask_index</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),
</span></span></span><span class="line"><span class="cl"><span class="s1">    the expected number of tokens transitioned at each step should be consistent.
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    This function is designed to precompute the number of tokens that need to be transitioned at each step.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mask_num</span> <span class="o">=</span> <span class="n">mask_index</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">base</span> <span class="o">=</span> <span class="n">mask_num</span> <span class="o">//</span> <span class="n">steps</span>
</span></span><span class="line"><span class="cl">    <span class="n">remainder</span> <span class="o">=</span> <span class="n">mask_num</span> <span class="o">%</span> <span class="n">steps</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">num_transfer_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mask_num</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">steps</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">mask_index</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">+</span> <span class="n">base</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mask_num</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_transfer_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">remainder</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">num_transfer_tokens</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">gen_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">block_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">cfg_scale</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">remasking</span><span class="o">=</span><span class="s1">&#39;low_confidence&#39;</span><span class="p">,</span> <span class="n">mask_id</span><span class="o">=</span><span class="mi">126336</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">        model: Mask predictor.
</span></span></span><span class="line"><span class="cl"><span class="s1">        prompt: A tensor of shape (1, L).
</span></span></span><span class="line"><span class="cl"><span class="s1">        steps: Sampling steps, less than or equal to gen_length.
</span></span></span><span class="line"><span class="cl"><span class="s1">        gen_length: Generated answer length.
</span></span></span><span class="line"><span class="cl"><span class="s1">        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
</span></span></span><span class="line"><span class="cl"><span class="s1">        temperature: Categorical distribution sampling temperature.
</span></span></span><span class="line"><span class="cl"><span class="s1">        cfg_scale: Unsupervised classifier-free guidance scale.
</span></span></span><span class="line"><span class="cl"><span class="s1">        remasking: Remasking strategy. &#39;low_confidence&#39; or &#39;random&#39;.
</span></span></span><span class="line"><span class="cl"><span class="s1">        mask_id: The toke id of [MASK] is 126336.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1. Initialization: Create the full sequence tensor &#39;x&#39;.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># It starts with the prompt, followed by `gen_length` [MASK] tokens.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># This is the &#34;canvas&#34; that will be filled in.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">gen_length</span><span class="p">),</span> <span class="n">mask_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Keep track of where the original prompt is, so we don&#39;t modify it.</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 2. Semi-Autoregressive Setup (Blocking)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># The generation is split into &#39;num_blocks&#39; chunks. This handles long generation</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># by generating `block_length` tokens at a time before moving to the next chunk.</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">gen_length</span> <span class="o">%</span> <span class="n">block_length</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">gen_length</span> <span class="o">//</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># The total number of refinement steps is distributed among the blocks.</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">steps</span> <span class="o">%</span> <span class="n">num_blocks</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">steps_per_block</span> <span class="o">=</span> <span class="n">steps</span> <span class="o">//</span> <span class="n">num_blocks</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 3. Outer Loop: Process each block sequentially.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">num_block</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Define the current working area (the block to be filled).</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Note: The original code has a small typo `...:]`, corrected here for clarity.</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_pos</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num_block</span> <span class="o">*</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_pos</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_block</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Calculate how many tokens to &#34;unmask&#34; or &#34;confirm&#34; in each refinement step for this block.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This ensures a steady, linear progression of unmasking.</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_transfer_tokens</span> <span class="o">=</span> <span class="n">get_num_transfer_tokens</span><span class="p">(</span><span class="n">block_mask_index</span><span class="p">,</span> <span class="n">steps_per_block</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. Inner Loop: Iteratively refine the current block.</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps_per_block</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Get the indices of all currently masked tokens in the entire sequence.</span>
</span></span><span class="line"><span class="cl">            <span class="n">mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4a. Prediction with optional Classifier-Free Guidance (CFG) ---</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">cfg_scale</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Create an unconditional version of the input by masking the prompt.</span>
</span></span><span class="line"><span class="cl">                <span class="n">un_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">un_x</span><span class="p">[</span><span class="n">prompt_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_id</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="c1"># Run the model on both the conditional (x) and unconditional (un_x) inputs.</span>
</span></span><span class="line"><span class="cl">                <span class="n">x_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">un_x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">logits_cat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">                <span class="n">logits</span><span class="p">,</span> <span class="n">un_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">logits_cat</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="c1"># Combine logits to steer the generation towards the prompt.</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># The formula is: unconditional + scale * (conditional - unconditional)</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># An algebraic simplification gives the line below.</span>
</span></span><span class="line"><span class="cl">                <span class="n">logits</span> <span class="o">=</span> <span class="n">un_logits</span> <span class="o">+</span> <span class="p">(</span><span class="n">cfg_scale</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">un_logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># If no CFG, just do a single forward pass.</span>
</span></span><span class="line"><span class="cl">                <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4b. Candidate Generation ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Add Gumbel noise for stochastic sampling. If temperature is 0, this is a simple argmax.</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits_with_noise</span> <span class="o">=</span> <span class="n">add_gumbel_noise</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Get the most likely token prediction for every position in the sequence.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits_with_noise</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4c. Confidence Scoring for Remasking ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Determine which of the new predictions to keep for the next step.</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">remasking</span> <span class="o">==</span> <span class="s1">&#39;low_confidence&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Calculate the softmax probabilities of the predicted tokens.</span>
</span></span><span class="line"><span class="cl">                <span class="n">p</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Get the probability of the chosen token `x0` at each position. This is the &#34;confidence&#34;.</span>
</span></span><span class="line"><span class="cl">                <span class="n">x0_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">elif</span> <span class="n">remasking</span> <span class="o">==</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Use random scores as confidence for random unmasking.</span>
</span></span><span class="line"><span class="cl">                <span class="n">x0_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">x0</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">remasking</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4d. Selecting Tokens to Update ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># We are only interested in updating tokens inside the current block.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Set confidence outside the current active generation area to -infinity to ignore them.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0_p</span><span class="p">[:,</span> <span class="n">end_pos</span><span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Only consider predictions for positions that are currently masked.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Original tokens (prompt and previously confirmed tokens) should have -infinity confidence.</span>
</span></span><span class="line"><span class="cl">            <span class="n">confidence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask_index</span><span class="p">,</span> <span class="n">x0_p</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># Replace the content of `x` at masked positions with the new predictions (`x0`).</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask_index</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># This will hold the indices of the tokens we decide to &#34;confirm&#34; in this step.</span>
</span></span><span class="line"><span class="cl">            <span class="n">transfer_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x0</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># For each item in the batch (here, just 1)...</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">confidence</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># ...select the `k` tokens with the HIGHEST confidence scores among the masked positions.</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># `k` is determined by `num_transfer_tokens` for the current step `i`.</span>
</span></span><span class="line"><span class="cl">                <span class="n">_</span><span class="p">,</span> <span class="n">select_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">confidence</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="n">num_transfer_tokens</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Mark these high-confidence positions to be updated.</span>
</span></span><span class="line"><span class="cl">                <span class="n">transfer_index</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">select_index</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4e. State Update ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Update the main tensor &#39;x&#39; by replacing [MASK] tokens with the selected high-confidence predictions.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># The other [MASK]s remain for the next refinement iteration.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 5. Return Final Generation</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Once all blocks and steps are complete, return the generated part of the sequence.</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></div><h1 id="reference">Reference</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>简单来说就是拥有无限数据、一个足够大的网络和最优训练的理想条件下，模型有能力恢复出真实的数据分布。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>在不更新其自身参数的情况下，仅通过在 Prompt 中提供少量示例 (few-shot) 或任务描述 (zero-shot)，就能当场学会并执行一个新任务的能力。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Tx8read</title>
      <link>http://localhost:1313/blogs/tx8read/</link>
      <pubDate>Wed, 11 Jun 2025 10:21:42 +0800</pubDate>
      <guid>http://localhost:1313/blogs/tx8read/</guid>
      <description>tx8 regression</description>
      <content:encoded><![CDATA[<h1 id="testgraphcompute">TestGraphCompute</h1>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">**</span><span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 1. 初始化与命令行参数处理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">Timer</span> <span class="n">timer</span><span class="p">(</span><span class="s">&#34;TestGraphCompute&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">registerAsmPrinterCLOptions</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">registerMLIRContextCLOptions</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">registerPassManagerCLOptions</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 解析命令行参数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cl</span><span class="o">::</span><span class="n">ParseCommandLineOptions</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">,</span> <span class="s">&#34;tx8be compiler</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 2. 初始化 MLIR 模块和上下文
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mlir</span><span class="o">::</span><span class="n">OwningOpRef</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ModuleOp</span><span class="o">&gt;</span> <span class="n">module</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="n">context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 定义一个正则表达式，用于从命令行选项中提取 codegen_path 参数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">regex</span> <span class="n">pattern</span><span class="p">(</span><span class="s">&#34;codegen_path=([a-zA-Z0-9_]+)&#34;</span><span class="p">);</span>  
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">smatch</span> <span class="n">matches</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">cachePath</span> <span class="o">=</span> <span class="s">&#34;codegen&#34;</span><span class="p">;</span>  <span class="c1">// 默认文件夹名字
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">regex_search</span><span class="p">(</span><span class="n">optionstr</span><span class="p">,</span> <span class="n">matches</span><span class="p">,</span> <span class="n">pattern</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// 寻找命令行选项中是否指定 codegen_path
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">cachePath</span> <span class="o">=</span> <span class="n">matches</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 3. 加载 MLIR 模块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 如果 cache 为 2 或 4，则从缓存路径加载模块；否则，使用默认的 gModelFil
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">gModelFile</span> <span class="o">=</span> <span class="p">(</span><span class="n">cache</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">||</span> <span class="n">cache</span> <span class="o">==</span> <span class="mi">4</span><span class="p">)</span> <span class="o">?</span> <span class="n">cachePath</span> <span class="o">+</span> <span class="s">&#34;/cache.mlir&#34;</span> <span class="o">:</span> <span class="n">gModelFile</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="kt">int</span> <span class="n">error</span> <span class="o">=</span> <span class="n">getMLIRFromFile</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">gModelFile</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">error</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 4. 配置模块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">mconfig</span> <span class="o">=</span> <span class="n">getModuleConfig</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">optionstr</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">mconfig</span><span class="p">.</span><span class="n">option</span> <span class="o">+=</span> <span class="n">optionstr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">mconfig</span><span class="p">.</span><span class="n">constCache</span> <span class="o">=</span> <span class="n">cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">updateModuleConfig</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">mconfig</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">mconfig</span> <span class="o">=</span> <span class="n">getModuleConfig</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">showModuleConfig</span><span class="p">(</span><span class="n">mconfig</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 5. 处理多卡信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">json_info_multi_card_t</span> <span class="o">*</span><span class="n">multi_card_jinfo</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">multi_card_jinfo</span> <span class="o">=</span> <span class="p">(</span><span class="n">cache</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">||</span> <span class="n">cache</span> <span class="o">==</span> <span class="mi">4</span><span class="p">)</span> <span class="o">?</span> <span class="n">get_multi_card_info_from_file</span><span class="p">(</span><span class="n">cachePath</span> <span class="o">+</span> <span class="s">&#34;/model_info.json&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                                <span class="o">:</span> <span class="n">parseMultiCardModuleInfo</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">dumpIR</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 6. 读取参考路径
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">fast_codegen</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// NOT fast_codegen
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">in_files</span> <span class="o">=</span> <span class="n">parseStringArgs</span><span class="p">(</span><span class="n">gInputBin</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&#34;,&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">out_files</span> <span class="o">=</span> <span class="n">parseStringArgs</span><span class="p">(</span><span class="n">gOutputBin</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&#34;,&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 定义一个 lambda 函数，用于从文件中读取参考文件路径。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">getRefFiles</span> <span class="o">=</span> <span class="p">[]</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">gFile</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">files</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">gFile</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">llvm</span><span class="o">::</span><span class="n">sys</span><span class="o">::</span><span class="n">fs</span><span class="o">::</span><span class="n">exists</span><span class="p">(</span><span class="n">gFile</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">ifstream</span> <span class="n">gf</span><span class="p">(</span><span class="n">gFile</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">text</span><span class="p">((</span><span class="n">std</span><span class="o">::</span><span class="n">istreambuf_iterator</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">(</span><span class="n">gf</span><span class="p">)),</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">istreambuf_iterator</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">()));</span>
</span></span><span class="line"><span class="cl">            <span class="n">files</span> <span class="o">=</span> <span class="n">parseStringArgs</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&#34;,&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">};</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">((</span><span class="n">gInputBin</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">gInputFile</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">getRefFiles</span><span class="p">(</span><span class="n">gInputFile</span><span class="p">,</span> <span class="n">in_files</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="n">getRefFiles</span><span class="p">(</span><span class="n">gOutputFile</span><span class="p">,</span> <span class="n">out_files</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// 7. computeGolden
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">cache</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">mconfig</span><span class="p">.</span><span class="n">tile</span><span class="p">.</span><span class="n">chip_num</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// 遍历芯片数量，创建对应的目录结构
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 构造并创建创建目 codegen/node_0_0/chip0/agent/data 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">path</span> <span class="o">=</span> <span class="s">&#34;codegen/node_0_0/chip&#34;</span><span class="p">;</span>  
</span></span><span class="line"><span class="cl">        <span class="n">path</span> <span class="o">+=</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;/agent/data&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">createDir</span><span class="p">(</span><span class="n">path</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 调用 computeGolden 函数，计算参考输出保存到 codegenPath
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">computeGolden</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">multi_card_jinfo</span><span class="p">,</span> <span class="n">in_files</span><span class="p">,</span> <span class="n">out_files</span><span class="p">,</span> <span class="n">mconfig</span><span class="p">.</span><span class="n">codegenPath</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 调用 moduleCompileCodegen 函数，对 MLIR 模块进行编译和代码生成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">moduleCompileCodegen</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">ret</span> <span class="o">==</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// 9. 获取内存大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 从模块中获取立即数 (Immediate) 和常量参数的 DDR 大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint64_t</span> <span class="n">imm_size</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ImmDdrSize</span><span class="p">)</span> <span class="o">?</span>
</span></span><span class="line"><span class="cl">    <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ImmDdrSize</span><span class="p">).</span><span class="n">getInt</span><span class="p">()</span> <span class="o">:</span> <span class="mi">2147483648</span><span class="p">;</span>  
</span></span><span class="line"><span class="cl">    <span class="kt">uint64_t</span> <span class="n">params_size</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ConstDdrSize</span><span class="p">)</span> <span class="o">?</span>
</span></span><span class="line"><span class="cl">        <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ConstDdrSize</span><span class="p">).</span><span class="n">getInt</span><span class="p">()</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// 10. 更新每个芯片的内存大小信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">mconfig</span><span class="p">.</span><span class="n">tile</span><span class="p">.</span><span class="n">chip_num</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_infos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">imm_size</span> <span class="o">=</span> <span class="n">imm_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_infos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">params_size</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">params_size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 11. 保存多卡模型信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">chipIds</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ChipIds</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// 获取芯片 ID 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">mlir</span><span class="o">::</span><span class="n">ArrayAttr</span> <span class="n">chipIdsAttr</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ArrayAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ChipIds</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">chipIdsAttr</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">chipIds</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">chipIdsAttr</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getInt</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 多卡模型文件保存到 codegenPath 路径下
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">saveMultiCardModelJson</span><span class="p">(</span><span class="n">multi_card_jinfo</span><span class="p">,</span> <span class="n">mconfig</span><span class="p">.</span><span class="n">codegenPath</span><span class="p">,</span> <span class="n">chipIds</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// uint64_t ddrSize = getModelDDRSize(multi_card_jinfo);
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="computegolden">computeGolden</h2>
<p>输入参数的来源：</p>
<ul>
<li><code>module</code>：在 main 文件中，通过 <code>getMLIRFromFile</code> 函数从文件中加载 MLIR 模块</li>
<li><code>multi_card_jinfo</code>：在 main 文件中，通过 <code>get_multi_card_info_from_file</code> 或 <code>parseMultiCardModuleInfo</code> 从 JSON 文件或 MLIR 模块中提取多卡信息。</li>
<li><code>in_files</code> 和 <code>out_files</code>：在 main 文件中，通过 <code>parseStringArgs</code> 或 <code>getRefFiles</code> 解析输入和输出文件路径。</li>
<li><code>mconfig.codegenPath</code>：在 main 文件中，通过命令行选项或默认值设置代码生成路径，并传递给 computeGolden。</li>
</ul>
<p>computeGolden 函数生成的数据（输入和输出的二进制文件）将保存到指定路径 mconfig.codegenPath.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">computeGolden</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">OwningOpRef</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ModuleOp</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">module</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">json_info_multi_card_t</span> <span class="o">*</span><span class="n">multi_card_jinfo</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">inFiles</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">outFiles</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">file_path</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 定义形状类型，用于存储多维张量的形状信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">using</span> <span class="n">ShapeType</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 用于存储多芯片的输入和输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span> <span class="o">*&gt;&gt;</span> <span class="n">multiInputDdata</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span> <span class="o">*&gt;&gt;</span> <span class="n">multiOutputData</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="o">&gt;</span> <span class="n">threads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">chip_num</span> <span class="o">=</span> <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_num</span><span class="p">;</span>  <span class="c1">// 获取芯片数量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">tile_info</span> <span class="o">=</span> <span class="n">get_tileinfo</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>  <span class="c1">// 从 MLIR 模块中提取 tile 信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ShapeType</span><span class="o">&gt;</span> <span class="n">outShapes</span><span class="p">(</span><span class="n">chip_num</span><span class="p">);</span>  <span class="c1">// 存储每个芯片的输出形状信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">chip_num</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">chip_info_t</span> <span class="o">*</span><span class="n">chip_info</span> <span class="o">=</span> <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_infos</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 分配当前芯片的输入和输出数据指针数组
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span> <span class="o">*&gt;</span> <span class="n">input_data</span><span class="p">(</span><span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input_num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span> <span class="o">*&gt;</span> <span class="n">output_data</span><span class="p">(</span><span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output_num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 用于 OneDNN 计算的输入和输出缓冲区
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span> <span class="o">*&gt;</span> <span class="n">computeInputs</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span> <span class="o">*&gt;</span> <span class="n">computeOutputs</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 当前芯片的输入和输出文件路径
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">chipInFiles</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">chipOutFiles</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">parseInOutfile</span><span class="p">(</span><span class="n">inFiles</span><span class="p">,</span> <span class="n">chipInFiles</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">chip_num</span><span class="p">,</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input_num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">parseInOutfile</span><span class="p">(</span><span class="n">outFiles</span><span class="p">,</span> <span class="n">chipOutFiles</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">chip_num</span><span class="p">,</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output_num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 生成当前芯片的输入输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">genInputs4SingleChip</span><span class="p">(</span><span class="n">computeInputs</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">chip_info</span><span class="p">,</span> <span class="n">chipInFiles</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">genOutputs4SingleChip</span><span class="p">(</span><span class="n">computeOutputs</span><span class="p">,</span> <span class="n">output_data</span><span class="p">,</span> <span class="n">chip_info</span><span class="p">,</span> <span class="n">chipOutFiles</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 如果输入文件为空，则生成随机输入数据并校正
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">chipInFiles</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">updateSpecialInputData</span><span class="p">(</span><span class="n">computeModuleRef</span><span class="p">,</span> <span class="n">computeInputs</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">chip_info</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">multiInputDdata</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">input_data</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">multiOutputData</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">output_data</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">outFiles</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">threads</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">moduleComputeInterface</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ref</span><span class="p">(</span><span class="n">computeModuleRef</span><span class="p">),</span> <span class="n">std</span><span class="o">::</span><span class="n">ref</span><span class="p">(</span><span class="n">outShapes</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">computeInputs</span><span class="p">,</span> <span class="n">computeOutputs</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 等待所有线程完成计算
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="kr">thread</span> <span class="o">:</span> <span class="n">threads</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kr">thread</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 遍历每个芯片，保存输入和输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">chip_num</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">chip_info_t</span> <span class="o">*</span><span class="n">chip_info</span> <span class="o">=</span> <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_infos</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kt">uint32_t</span> <span class="n">node_id</span> <span class="o">=</span> <span class="n">get_node_id</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tile_info</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int32_t</span> <span class="n">relative_chip_id</span> <span class="o">=</span> <span class="n">get_relative_chip_id</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tile_info</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 构造当前芯片的数据保存路径  file_path/node_x_y/chip_z/agent/data
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">data_path</span> <span class="o">=</span> <span class="n">file_path</span> <span class="o">+</span> <span class="s">&#34;/node_&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">node_id</span> <span class="o">/</span> <span class="n">tile_info</span><span class="p">.</span><span class="n">node_y</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;_&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">node_id</span> <span class="o">%</span> <span class="n">tile_info</span><span class="p">.</span><span class="n">node_y</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;/chip&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">relative_chip_id</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;/agent/data&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">createDir</span><span class="p">(</span><span class="n">data_path</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// bin格式保存当前芯片的输入数据  
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input_num</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">inout_tensor_info_t</span> <span class="o">*</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="n">saveInOutTensor</span><span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dim</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">layout</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">data_path</span> <span class="o">+</span> <span class="s">&#34;/in&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;.bin&#34;</span><span class="p">,</span> <span class="n">multiInputDdata</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// bin格式保存当前芯片的输出数据  out_j_ref.bin
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output_num</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">inout_tensor_info_t</span> <span class="o">*</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int32_t</span> <span class="n">tensorShape</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">// 根据 outShapes 或原始形状计算输出张量的形状
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dim</span><span class="p">;</span> <span class="o">++</span><span class="n">idx</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">outShapes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tensorShape</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">outShapes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tensorShape</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">// 保存输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">saveInOutTensor</span><span class="p">(</span><span class="n">tensorShape</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dim</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">layout</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">data_path</span> <span class="o">+</span> <span class="s">&#34;/out&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;_ref.bin&#34;</span><span class="p">,</span> <span class="n">multiOutputData</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 释放当前芯片的输入和输出数据内存
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input_num</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">free</span><span class="p">(</span><span class="n">multiInputDdata</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output_num</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">free</span><span class="p">(</span><span class="n">multiOutputData</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="run_code_gen_layer">run_code_gen_layer</h1>
<p>主要用于运行代码生成 (codegen) 相关的任务，以下是函数的详细功能解释：</p>
<ol>
<li>解析参数：</li>
</ol>
<ul>
<li>接受至少两个参数：$1 是可执行文件的名称，$2 是种子文件 (seed file)</li>
<li>如果有更多参数 ($# &gt; 2)，则将额外参数存储为配置参数 (config_params)</li>
<li>从 config_params 中提取 <code>codegen_path</code> (代码生成输出路径) ，如果未指定则使用默认值 &ldquo;codegen&rdquo;</li>
</ul>
<ol start="2">
<li>切换工作目录：</li>
</ol>
<ul>
<li>切换到 <code>${BEMLIR_PROJECT_ROOT}/build/bin</code> 目录。</li>
<li>删除旧的 <code>codegen_path</code> 目录，确保环境干净。</li>
</ul>
<ol start="3">
<li>执行可执行文件：</li>
</ol>
<ul>
<li>使用 <code>${layer_cmd}</code> (即 ./$1) 运行指定的可执行文件，传入种子文件和配置参数。</li>
<li>检查返回值，如果失败 <code>(ret != 0)</code>，则恢复目录并返回错误。</li>
</ul>
<ol start="4">
<li>处理生成的代码：</li>
</ol>
<ul>
<li>根据参数中的 <code>chip_num</code> 或 <code>static_shape</code> 判断 <code>host_type</code>.</li>
<li>调用 <code>get_codegen_file</code> 处理生成的代码文件。</li>
</ul>
<ol start="5">
<li>运行 cmodel 测试:</li>
</ol>
<ul>
<li>根据参数中的 <code>fast_codegen</code> 或 <code>not_run</code> 设置 cmp_flag.</li>
<li>调用 <code>run_on_cmodel</code> 在 cmodel 上运行生成的代码。</li>
<li>检查返回值，失败则返回错误。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 定义 run_codegen_layer 函数，用于运行代码生成层测试流程</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> run_codegen_layer<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1. 打印开始时间，用于调试和性能追踪</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> -n <span class="s2">&#34;time==&gt;&gt;run_codegen_layer-start   &#34;</span><span class="p">;</span> date<span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 2. 函数参数说明</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># $1: 可执行文件名称</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># $2: 种子文件 (seed file）</span>
</span></span><span class="line"><span class="cl">    <span class="nv">layer_cmd</span><span class="o">=</span><span class="s2">&#34;./</span><span class="nv">$1</span><span class="s2">&#34;</span>  <span class="c1"># 在当前目录下执行的可执行文件路径</span>
</span></span><span class="line"><span class="cl">    <span class="nv">seed_file</span><span class="o">=</span><span class="nv">$2</span>      <span class="c1"># 种子文件或配置文件</span>
</span></span><span class="line"><span class="cl">    <span class="nv">config_params</span><span class="o">=</span><span class="s2">&#34;&#34;</span>  <span class="c1"># 配置参数，默认为空</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 默认代码生成输出路径</span>
</span></span><span class="line"><span class="cl">    <span class="nv">codegen_path</span><span class="o">=</span><span class="s2">&#34;codegen&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 3. 检查是否有超过2个参数</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> <span class="nv">$#</span> -gt <span class="m">2</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 提取除前两个参数外的所有参数作为配置参数</span>
</span></span><span class="line"><span class="cl">        <span class="nv">config_params</span><span class="o">=</span><span class="nv">$*</span>
</span></span><span class="line"><span class="cl">        <span class="nv">config_params</span><span class="o">=</span><span class="si">${</span><span class="nv">config_params</span><span class="p">#*</span><span class="si">}</span>  <span class="c1"># 移除第一个参数 (可执行文件）</span>
</span></span><span class="line"><span class="cl">        <span class="nv">config_params</span><span class="o">=</span><span class="si">${</span><span class="nv">config_params</span><span class="p">#*</span><span class="si">}</span>  <span class="c1"># 移除第二个参数 (种子文件）</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 4. 如果配置参数中包含 --codegen_path，提取其值作为代码生成路径</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">config_params</span><span class="si">}</span> <span class="o">==</span> *<span class="s2">&#34;--codegen_path=&#34;</span>* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 提取 --codegen_path= 后面的值</span>
</span></span><span class="line"><span class="cl">        <span class="nv">codegen_path</span><span class="o">=</span><span class="si">${</span><span class="nv">config_params</span><span class="p">#*codegen_path=</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 移除可能存在的引号或其他字符</span>
</span></span><span class="line"><span class="cl">        <span class="nv">codegen_path</span><span class="o">=</span><span class="si">${</span><span class="nv">codegen_path</span><span class="p">%%</span><span class="se">\&#34;</span><span class="p">*</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="nv">codegen_path</span><span class="o">=</span><span class="si">${</span><span class="nv">codegen_path</span><span class="p">-*</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="nv">codegen_path</span><span class="o">=</span><span class="si">${</span><span class="nv">codegen_path</span><span class="p">*</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 5. 切换到 build/bin 目录执行命令</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span>/build/bin
</span></span><span class="line"><span class="cl">        <span class="c1"># 删除旧的 codegen_path 目录，确保环境干净</span>
</span></span><span class="line"><span class="cl">        rm -rf <span class="si">${</span><span class="nv">codegen_path</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 执行层命令，传入种子文件和配置参数</span>
</span></span><span class="line"><span class="cl">        <span class="si">${</span><span class="nv">layer_cmd</span><span class="si">}</span> <span class="si">${</span><span class="nv">seed_file</span><span class="si">}</span> <span class="si">${</span><span class="nv">config_params</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 捕获命令的返回值</span>
</span></span><span class="line"><span class="cl">        <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果命令执行失败 (返回码非0），恢复目录并返回错误</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">            <span class="nb">echo</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">fi</span>
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>  <span class="c1"># 恢复原始目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 6. 打印代码生成完成的时间</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> -n <span class="s2">&#34;time==&gt;&gt;run_codegen_layer-codegen=== &#34;</span><span class="p">;</span> date<span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 7. 根据参数判断主机类型</span>
</span></span><span class="line"><span class="cl">    <span class="nv">host_type</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果参数中包含 chip_num 或 static_shape，则将 host_type 设为 1</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$*</span> <span class="o">==</span> *<span class="s2">&#34;chip_num&#34;</span>* <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="nv">$*</span> <span class="o">==</span> *<span class="s2">&#34;static_shape&#34;</span>* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nv">host_type</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 8. 调用 get_codegen_file 处理生成的代码文件</span>
</span></span><span class="line"><span class="cl">    get_codegen_file <span class="si">${</span><span class="nv">codegen_path</span><span class="si">}</span> <span class="si">${</span><span class="nv">host_type</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 捕获返回值</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果处理失败，恢复目录并返回错误</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 9. 初始化比较标志</span>
</span></span><span class="line"><span class="cl">    <span class="nv">cmp_flag</span><span class="o">=</span><span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果参数中包含 fast_codegen 或 not_run，则设置 cmp_flag 为 &#34;not_run&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$*</span> <span class="o">==</span> *<span class="s2">&#34;fast_codegen&#34;</span>* <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="nv">$*</span> <span class="o">==</span> *<span class="s2">&#34;not_run&#34;</span>* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nv">cmp_flag</span><span class="o">=</span><span class="s2">&#34;not_run&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 10. 在 cmodel 上运行生成的代码，传入比较标志</span>
</span></span><span class="line"><span class="cl">    run_on_cmodel <span class="si">${</span><span class="nv">codegen_path</span><span class="si">}</span> <span class="si">${</span><span class="nv">cmp_flag</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 捕获返回值</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果运行失败，恢复目录并返回错误</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 11. 打印结束时间</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> -n <span class="s2">&#34;time==&gt;&gt;run codegen layer-end===   &#34;</span><span class="p">;</span> date<span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="get_codegen_file">get_codegen_file</h2>
<p><code>get_codegen_file</code> 用于整理代码生成的结果 (位于 <code>${BEMLIR_PROJECT_ROOT}/build/bin/${codegen_case}</code>)，为每个节点生成版本信息 (version.txt)，并将生成的文件复制到测试目录 (<code>${BEMLIR_PROJECT_ROOT}/external/tx8be-oplib/tests/test_codegen</code>)，最后调用 <code>get_codegen_host</code> 完成主机相关处理。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Function to process and organize generated codegen files</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> get_codegen_file<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Print all input arguments for debugging</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="nv">$*</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign first argument as the codegen case name or path</span>
</span></span><span class="line"><span class="cl">    <span class="nv">codegen_case</span><span class="o">=</span><span class="nv">$1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Second argument: 0 for host thread mode, 1 for host stream mode</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Note: $2 is passed to get_codegen_host</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Change to the codegen case directory under build/bin</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span><span class="s2">/build/bin/</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Find node directories matching node_[0-9]+_[0-9] pattern (e.g., node_123_4)</span>
</span></span><span class="line"><span class="cl">        <span class="nv">node_dirs</span><span class="o">=</span><span class="k">$(</span>find . -maxdepth <span class="m">1</span> -type d -regex <span class="s1">&#39;.*/node_[0-9]+_[0-9]&#39;</span> -exec basename <span class="o">{}</span> <span class="se">\;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Iterate through each node directory</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> dir in <span class="nv">$node_dirs</span><span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Check if libTX8MLIRTransforms.a exists to determine version type</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="o">[</span> ! -e <span class="s2">&#34;</span><span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span><span class="s2">/lib/libTX8MLIRTransforms.a&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Write &#39;tx8be-mlir&#39; to version.txt if library is absent</span>
</span></span><span class="line"><span class="cl">                <span class="nb">echo</span> -e <span class="s2">&#34;tx8be-mlir&#34;</span> &gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Write &#39;tx8be-mlir-sdk&#39; to version.txt if library is present</span>
</span></span><span class="line"><span class="cl">                <span class="nb">echo</span> -e <span class="s2">&#34;tx8be-mlir-sdk&#34;</span> &gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="k">fi</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Append git status to version.txt to record repository state</span>
</span></span><span class="line"><span class="cl">            git status --porcelain &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Append last two git commits to version.txt for version history</span>
</span></span><span class="line"><span class="cl">            git log -2 &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">done</span>
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Change to the test_codegen directory</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span><span class="s2">/external/tx8be-oplib/tests/test_codegen&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Remove existing codegen_case directory to ensure a clean state</span>
</span></span><span class="line"><span class="cl">        rm -rf <span class="s2">&#34;</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Copy the codegen_case directory from build/bin</span>
</span></span><span class="line"><span class="cl">        cp -r <span class="s2">&#34;</span><span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span><span class="s2">/build/bin/</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span> .
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Call get_codegen_host to process host-related tasks</span>
</span></span><span class="line"><span class="cl">    get_codegen_host <span class="s2">&#34;</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span> <span class="s2">&#34;</span><span class="nv">$2</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="get_codegen_host">get_codegen_host</h2>
<p><code>get_codegen_host </code> 用于为 host 环境准备代码生成用例的测试文件。它在指定的测试用例目录中处理 node &amp; chip 相关的文件，复制必要的配置文件、源代码和构建脚本，并根据 host_type 选择不同的主机实现文件 host_thread.cpp 或 host_stream.cpp.</p>
<ol>
<li>函数输入参数：</li>
</ol>
<ul>
<li><code>$1 (codegen_case)</code>: 代码生成用例的名称或路径，通常是一个目录 (例如 codegen0 或 codegen1) ，表示测试用例的根目录。</li>
<li><code>$2 (host_type)</code>: 主机执行模式，0: host_thread.cpp，1: host_stream.cpp.</li>
</ul>
<ol start="2">
<li>切换到 <code>${{OPLIB_PROJECT_ROOT}}/tests/test_codegen/${codegen_case}</code> 目录:</li>
</ol>
<ul>
<li>使用 find 命令查找符合 node_[0-9]+_[0-9] 模式 (例如 node_123_4) 的子目录，表示代码生成中的节点。</li>
<li>对每个 node_dir 追加版本信息和复制相关文件。</li>
</ul>
<ol start="3">
<li>处理 chip 目录:</li>
</ol>
<ul>
<li>在每个节点目录下，查找符合 <code> chip[0-9]+</code> 模式 (例如 chip0, chip1) 的子目录</li>
<li>为每个 dir 复制 Makefile_tile 到 <code>./${node_dir}/${dir}/Makefile</code>. 在 <code>./${node_dir}/${dir}/</code> 下创建 16 个子目录 (tiles0 - tiles15)，并为每个子目录复制 Makefile_main 到 t <code>iles$i/Makefile</code></li>
</ul>
<ol start="4">
<li>根据 <code>host_type</code> 复制 host_thread.cpp 或 host_stream.cpp 到当前目录的 host.cpp.</li>
<li>复制 CMakeLists.txt 和 Makefile 到当前目录。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Function to prepare host-related files for a codegen test case</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> get_codegen_host<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Print all input arguments for debugging</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="nv">$*</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign first argument as the codegen case name or path</span>
</span></span><span class="line"><span class="cl">    <span class="nv">codegen_case</span><span class="o">=</span><span class="nv">$1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign second argument as host type (0: thread mode, 1: stream mode)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">host_type</span><span class="o">=</span><span class="nv">$2</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Define relative path for test_codegen directory</span>
</span></span><span class="line"><span class="cl">    <span class="nv">oplib_path</span><span class="o">=</span><span class="s2">&#34;tests/test_codegen&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Change to the test_codegen directory for the codegen case</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tests/test_codegen/</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Find node directories matching node_[0-9]+_[0-9] pattern (e.g., node_123_4)</span>
</span></span><span class="line"><span class="cl">        <span class="nv">node_dirs</span><span class="o">=</span><span class="k">$(</span>find . -maxdepth <span class="m">1</span> -type d -regex <span class="s1">&#39;.*/node_[0-9]+_[0-9]&#39;</span> -exec basename <span class="o">{}</span> <span class="se">\;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> node_dir in <span class="nv">$node_dirs</span><span class="p">;</span> <span class="k">do</span>  <span class="c1"># # Iterate through each node directory</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Append oplib version info to version.txt</span>
</span></span><span class="line"><span class="cl">            <span class="nb">echo</span> -e <span class="s2">&#34;\n\ntx8be-oplib:&#34;</span> &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Append git status to version.txt to record repository state</span>
</span></span><span class="line"><span class="cl">            git status --porcelain &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Write last two git commits to version.txt for version history</span>
</span></span><span class="line"><span class="cl">            git log -2 &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Copy all stream-related files to node directory</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/stream*&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Copy CMakeLists_chip.txt as CMakeLists.txt for node</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/CMakeLists_chip.txt&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/CMakeLists.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Copy main_kcore.c to node directory</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/main_kcore.c&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Copy Makefile_chip as Makefile for node</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/Makefile_chip&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/Makefile&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Find chip directories matching chip[0-9]+ pattern (e.g., chip0, chip1)</span>
</span></span><span class="line"><span class="cl">            <span class="nv">chip_dirs</span><span class="o">=</span><span class="k">$(</span>find <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">&#34;</span> -maxdepth <span class="m">1</span> -type d -regex <span class="s1">&#39;.*/chip[0-9]+&#39;</span> -exec basename <span class="o">{}</span> <span class="se">\;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Iterate through each chip directory</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> dir in <span class="nv">$chip_dirs</span><span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Copy Makefile_tile as Makefile for chip</span>
</span></span><span class="line"><span class="cl">                cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/Makefile_tile&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/Makefile&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Create Makefiles for 16 tiles (tiles0 to tiles15)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="o">((</span><span class="nv">i</span><span class="o">=</span>0<span class="p">;</span> i&lt;16<span class="p">;</span> i++<span class="o">))</span><span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">                    <span class="nv">dst_file</span><span class="o">=</span><span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/tiles</span><span class="si">${</span><span class="nv">i</span><span class="si">}</span><span class="s2">/Makefile&#34;</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># Copy Makefile_main to each tile&#39;s Makefile</span>
</span></span><span class="line"><span class="cl">                    cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/Makefile_main&#34;</span> <span class="s2">&#34;</span><span class="nv">$dst_file</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="k">done</span>
</span></span><span class="line"><span class="cl">            <span class="k">done</span>
</span></span><span class="line"><span class="cl">        <span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Copy host implementation based on host_type</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="o">[</span> <span class="nv">$host_type</span> -eq <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Use thread-based host implementation</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/host_thread.cpp&#34;</span> <span class="s2">&#34;host.cpp&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Use stream-based host implementation</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Note: Fixed typo &#39;$t{OPLIB_PROJECT_ROOT}&#39; to &#39;${{OPLIB_PROJECT_ROOT}}&#39;</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/host_stream.cpp&#34;</span> <span class="s2">&#34;host.cpp&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Copy top-level CMakeLists.txt for test case</span>
</span></span><span class="line"><span class="cl">        cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/CMakeLists.txt&#34;</span> .
</span></span><span class="line"><span class="cl">        <span class="c1"># Copy top-level Makefile for test case</span>
</span></span><span class="line"><span class="cl">        cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/Makefile&#34;</span> .
</span></span><span class="line"><span class="cl">    <span class="c1"># Restore original directory</span>
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="run_on_cmodel">run_on_cmodel</h2>
<p><code>run_on_cmodel</code> 用于在指定的测试用例目录中运行 cmodel 仿真任务。函数的主要功能包括环境设置、构建、执行仿真脚本或程序，并处理错误。以下是详细的功能说明：</p>
<ol>
<li>函数输入参数：</li>
</ol>
<ul>
<li>$1 (case_name): 来自 <code>run_codegen_layer</code> 的 <code>codegen_path</code>，可能附加 <code>host_type</code>.</li>
<li>$2 (run_flag): 运行标志，来自 <code>run_codegen_layer</code> 的 <code>cmp_flag</code> 用于控制仿真执行的方式 (例如是否运行或运行模式) 。</li>
</ul>
<ol start="2">
<li>切换工作目录并执行:</li>
</ol>
<ul>
<li>切换到测试用例目录 <code>${{OPLIB_PROJECT_ROOT}}/tests/test_codegen/${case_name}</code></li>
<li>运行 <code>cmake .. -DUSING_RISCV=OFF</code>，配置构建系统，禁用 RISCV 支持。</li>
<li>运行 <code>make -j</code> 并动态设置并行任务数 (基于 CPU 核心数，<code>cat /proc/stat | grep cpu[0-9] -c</code>)</li>
</ul>
<ol start="3">
<li>仿真执行: 根据参数运行仿真脚本 (host_sim.sh) 或 host_sim.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Function to run a cmodel simulation for a given test case</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> run_on_cmodel<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign first argument as the test case name</span>
</span></span><span class="line"><span class="cl">    <span class="nv">case_name</span><span class="o">=</span><span class="nv">$1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign second argument as the run flag (controls execution mode)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">run_flag</span><span class="o">=</span><span class="nv">$2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Check if case_name is empty</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$case_name</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;Error: case_name is empty&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Check if the test case directory exists</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> ! -d <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tests/test_codegen/</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;Can not find </span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Change to the test case directory</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tests/test_codegen/</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">&#34;</span>  <span class="c1"># FIXED DIR</span>
</span></span><span class="line"><span class="cl">        rm -rf build
</span></span><span class="line"><span class="cl">        mkdir build
</span></span><span class="line"><span class="cl">        <span class="nb">cd</span> build
</span></span><span class="line"><span class="cl">        <span class="c1"># Run cmake to configure the build, disabling RISCV support</span>
</span></span><span class="line"><span class="cl">        cmake .. -DUSING_RISCV<span class="o">=</span>OFF
</span></span><span class="line"><span class="cl">        <span class="c1"># Run make with parallel jobs based on CPU core count</span>
</span></span><span class="line"><span class="cl">        make -j<span class="k">$(</span>cat /proc/stat <span class="p">|</span> grep cpu<span class="o">[</span>0-9<span class="o">]</span> -c<span class="k">)</span>
</span></span><span class="line"><span class="cl">        <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  <span class="c1"># Capture the return code</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># If make fails, restore directory, print error, and exit</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="o">[[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">            <span class="nb">echo</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">        <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Check if run_flag is empty</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$run_flag</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="o">[</span> -e ../host_sim.sh <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># Check if host_sim.sh exists in the parent directory</span>
</span></span><span class="line"><span class="cl">                cp ../host_sim.sh .
</span></span><span class="line"><span class="cl">                sh ./host_sim.sh
</span></span><span class="line"><span class="cl">                <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  <span class="c1"># Capture the return code</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># If script fails, restore directory, print error, and exit</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="o">[[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>  
</span></span><span class="line"><span class="cl">                    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">echo</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                    <span class="k">return</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                <span class="k">fi</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span>
</span></span><span class="line"><span class="cl">                ./host_sim ../  <span class="c1"># Run host_sim with parent directory as argument</span>
</span></span><span class="line"><span class="cl">                <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  <span class="c1"># Capture the return code</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># If host_sim fails, restore directory, print error, and exit</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="o">[[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">echo</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                    <span class="k">return</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                <span class="k">fi</span>
</span></span><span class="line"><span class="cl">            <span class="k">fi</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Check if run_flag is &#34;0&#34; or &#34;1&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="o">[[</span> <span class="nv">$run_flag</span> <span class="o">==</span> <span class="s2">&#34;0&#34;</span> <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="nv">$run_flag</span> <span class="o">==</span> <span class="s2">&#34;1&#34;</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Run host_sim with parent directory and run_flag</span>
</span></span><span class="line"><span class="cl">            ./host_sim ../ <span class="s2">&#34;</span><span class="nv">$run_flag</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  <span class="c1"># Capture the return code</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># If host_sim fails, restore directory, print error, and exit</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="o">[[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">                <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">                <span class="nb">echo</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">            <span class="k">fi</span>
</span></span><span class="line"><span class="cl">        <span class="k">fi</span>
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> </span><span class="nv">$*</span><span class="s2"> passed&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h1 id="run_codegen_case_soc_rtt">run_codegen_case_soc_rtt</h1>
<p>run_codegen_case_soc_rtt 位于 <code>tx8-oplib/scripts/regression.sh</code>，函数用于在 SOC 环境下运行 RTT (Real-Time Transfer) 测试。其主要流程如下：</p>
<ol>
<li>初始化和参数获取：</li>
</ol>
<ul>
<li>函数从命令行参数中获取 <code>case_name</code>, <code>copy_option</code>, 和 <code>multi_graph_enable</code>.</li>
<li>检查 <code>case_name</code> 是否为空，如果为空则输出错误信息并返回 1.</li>
</ul>
<ol start="2">
<li>环境设置和目录导航：</li>
</ol>
<ul>
<li>将工作目录切换到 <code>${OPLIB_PROJECT_ROOT}/tests/test_codegen/${case_name}</code>. 如果目录不存在，则输出错误信息并返回 1。</li>
</ul>
<ol start="3">
<li>构建和配置：</li>
</ol>
<ul>
<li>执行 <code>rm -rf ${case_name}_build</code> 清理之前的构建文件。</li>
<li>根据 <code>multi_graph_enable</code> 设置 <code>CONFIG_ARGS</code>，如果启用多图则设置为 &ldquo;-DMULTI_GRAPH=1&rdquo;，否则为空。</li>
<li>调用 cmake 命令生成构建文件，指定构建目录为 <code>${case_name}_build</code>，并根据 <code>copy_option</code> 设置 <code>COPY_RTT_FLAG</code>.</li>
<li>执行 make 命令进行实际构建，目标包括 all 和 chip_out.</li>
</ul>
<ol start="4">
<li>错误处理和退出：</li>
</ol>
<ul>
<li>每次关键步骤执行后，检查返回状态 <code>$ret</code>，如果非 0，则弹出目录并返回错误码。</li>
<li>构建成功后输出 <code>${FUNCNAME[0]} &quot;passed&quot;</code> 表示通过。</li>
</ul>
<ol start="5">
<li>清理和返回: 函数结束时弹出目录，恢复原始工作目录。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> run_codegen_case_soc_rtt<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> &#39;start&#39;&#34;</span>  <span class="c1"># 输出函数名和&#34;start&#34;表示开始</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">case_name</span><span class="o">=</span><span class="nv">$1</span>                  <span class="c1"># 获取用例名称</span>
</span></span><span class="line"><span class="cl">    <span class="nv">copy_option</span><span class="o">=</span><span class="nv">$2</span>                 <span class="c1"># 获取复制选项</span>
</span></span><span class="line"><span class="cl">    <span class="nv">multi_graph_enable</span><span class="o">=</span><span class="nv">$3</span>          <span class="c1"># 获取多图启用标志</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$case_name</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>   <span class="c1"># 如果用例名称为空</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;case_name(</span><span class="nv">$case_name</span><span class="s2">) not found &#34;</span>  <span class="c1"># 输出错误信息</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="m">1</span>                   <span class="c1"># 返回错误码 1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">case_dir</span><span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/tests/test_codegen/<span class="si">${</span><span class="nv">case_name</span><span class="si">}</span>  <span class="c1"># 设置用例目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">case_dir</span><span class="si">}</span>              <span class="c1"># 切换到用例目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    rm -rf <span class="si">${</span><span class="nv">case_name</span><span class="si">}</span>_build      <span class="c1"># 清理之前的构建文件</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 检查清理是否成功</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$multi_graph_enable</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># 如果多图启用标志为空</span>
</span></span><span class="line"><span class="cl">        <span class="nv">CONFIG_ARGS</span><span class="o">=</span><span class="s2">&#34;&#34;</span>                 <span class="c1"># 配置参数为空</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span>                                 <span class="c1"># 否则</span>
</span></span><span class="line"><span class="cl">        <span class="nv">CONFIG_ARGS</span><span class="o">=</span><span class="s2">&#34;-DMULTI_GRAPH=1&#34;</span>  <span class="c1"># 设置多图配置参数</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    cmake -B <span class="s2">&#34;</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">_build&#34;</span> -DUSING_RISCV<span class="o">=</span>ON -TX8FW_BASE<span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/release/riscv/tx8-yoc-rt-thread-smp <span class="si">${</span><span class="nv">CONFIG_ARGS</span><span class="si">}</span> <span class="p">;</span> <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 生成构建文件</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    make -j -C <span class="s2">&#34;</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">_build&#34;</span> --target all chip_out <span class="p">;</span> <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 执行构建</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>                          <span class="c1"># 恢复到原始目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> &#39;passed&#39;&#34;</span> <span class="c1"># 输出函数名和&#34;passed&#34;表示通过</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="export_tx8fw_to_env">export_tx8fw_to_env</h2>
<p><code>export_tx8fw_to_env</code> 函数的主要目的是设置与 TX8FW 相关的环境变量，以便后续构建或运行时使用。以下是其流程：</p>
<ol>
<li>设置 SDK 路径：</li>
</ol>
<ul>
<li>定义 TX8FW 的 SDK 路径 <code>soc_sdk_path</code> 为 <code>${OPLIB_PROJECT_ROOT}/3rd_party/tx8-yoc-rt-thread-smp</code>.</li>
</ul>
<ol start="2">
<li>检查路径是否存在:</li>
</ol>
<ul>
<li>检查路径 <code>${soc_sdk_path}/tool/tx8fw-xuantie-sdk</code> 是否存在。如果不存在，打印错误信息并退出，状态码为 1.</li>
</ul>
<ol start="3">
<li>导出环境变量: 打印并设置以下环境变量</li>
</ol>
<ul>
<li>TX8FW_SDK_INSTALL_DIR：指向 ${soc_sdk_path}/tool/tx8fw-xuantie-sdk。</li>
<li>TX8FW_TOOLCHAIN_VARIANT：设置为 cross-compile。</li>
</ul>
<ol start="4">
<li>清理目录: 使用 popd 命令恢复到之前的目录.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> export_tx8fw_to_env<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nv">soc_sdk_path</span><span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/3rd_party/tx8-yoc-rt-thread-smp  <span class="c1"># 设置 TX8FW SDK 路径</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">soc_sdk_path</span><span class="si">}</span>/tool/tx8fw-xuantie-sdk  <span class="c1"># 切换到 TX8FW SDK 工具目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> ! -d <span class="s2">&#34;xuantie-900-gcc-elf-newlib-x86_64-V2.8.0&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># 检查指定 SDK 目录是否存在</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">soc_sdk_path</span><span class="si">}</span><span class="s2">/tool/tx8fw-xuantie-sdk didn&#39;t exist&#34;</span>  <span class="c1"># 如果不存在，打印错误信息</span>
</span></span><span class="line"><span class="cl">        <span class="nb">exit</span> <span class="m">1</span>  <span class="c1"># 退出并返回状态码 1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;export TX8FW_SDK_INSTALL_DIR=</span><span class="si">${</span><span class="nv">soc_sdk_path</span><span class="si">}</span><span class="s2">/tool/tx8fw-xuantie-sdk&#34;</span>  <span class="c1"># 打印并设置 TX8FW SDK 安装目录环境变量</span>
</span></span><span class="line"><span class="cl">    <span class="nb">export</span> <span class="nv">TX8FW_SDK_INSTALL_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">soc_sdk_path</span><span class="si">}</span>/tool/tx8fw-xuantie-sdk  <span class="c1"># 导出 TX8FW SDK 安装目录环境变量</span>
</span></span><span class="line"><span class="cl">    <span class="nb">export</span> <span class="nv">TX8FW_TOOLCHAIN_VARIANT</span><span class="o">=</span>cross-compile  <span class="c1"># 导出工具链变体为 cross-compile</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>  <span class="c1"># 恢复到之前的目录</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="build_oplib_with_soc-函数">build_oplib_with_soc 函数</h2>
<p><code>build_oplib_with_soc</code> 函数用于构建 OPLib 并结合特定 SoC 配置。以下是其流程：</p>
<p>打印项目根目录：
打印 OPLIB_PROJECT_ROOT 环境变量，用于调试或日志记录。</p>
<ol>
<li>切换目录和初始化：</li>
</ol>
<ul>
<li>使用 pushd 切换到 <code>OPLIB_PROJECT_ROOT</code> 目录。</li>
<li>定义变量 <code>rm=rf build</code>, <code>mkdir=build</code> 和 <code>cd=build</code>，这些变量实际上是模拟命令（rm -rf build、mkdir build 和 cd build）。</li>
</ul>
<ol start="2">
<li>设置复制标志：</li>
</ol>
<ul>
<li>检查 <code>$1</code> (即 <code>copy_option</code>) 是否为 &ldquo;NOT_COPY&rdquo;，如果是，则设置 <code>COPY_RTT_FLAG</code> 为 <code>--DRTT_HOST_COPY=OFF</code>，否则为空。</li>
</ul>
<ol start="3">
<li>导出环境变量并构建：</li>
</ol>
<ul>
<li>调用 <code>export_tx8fw_to_env</code> 函数设置 TX8FW 相关环境变量。</li>
<li>运行 cmake 命令，生成构建文件，指定构建选项 <code>-DUSING_RISCV=ON</code> 和 <code>TX8FW_BASE</code>，并根据 <code>COPY_RTT_FLAG</code> 添加额外参数。</li>
<li>使用 make 命令执行构建，目标包括 grep epilog 和 c</li>
</ul>
<ol start="4">
<li>清理目录: 使用 popd 命令恢复到之前的目录.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> build_oplib_with_soc<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>  <span class="c1"># 打印 OPLib 项目根目录路径</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>  <span class="c1"># 切换到 OPLib 项目根目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">rm</span><span class="o">=</span>rf build  <span class="c1"># 定义清理构建目录的命令</span>
</span></span><span class="line"><span class="cl">    <span class="nv">mkdir</span><span class="o">=</span>build  <span class="c1"># 定义创建构建目录的命令</span>
</span></span><span class="line"><span class="cl">    <span class="nv">cd</span><span class="o">=</span>build     <span class="c1"># 定义切换到构建目录的命令</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">COPY_RTT_FLAG</span><span class="o">=</span><span class="s2">&#34;&#34;</span>  <span class="c1"># 初始化 RTT 复制标志</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> <span class="s2">&#34;</span><span class="nv">$1</span><span class="s2">&#34;</span> <span class="o">==</span> <span class="s2">&#34;NOT_COPY&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># 如果传入的复制选项为 NOT_COPY</span>
</span></span><span class="line"><span class="cl">        <span class="nv">COPY_RTT_FLAG</span><span class="o">=</span><span class="s2">&#34;--DRTT_HOST_COPY=OFF&#34;</span>  <span class="c1"># 设置 RTT 复制标志为关闭</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    export_tx8fw_to_env  <span class="c1"># 调用函数导出 TX8FW 相关环境变量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    cmake .. -DUSING_RISCV<span class="o">=</span>ON -TX8FW_BASE<span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/release/riscv/tx8-yoc-rt-thread-smp <span class="si">${</span><span class="nv">COPY_RTT_FLAG</span><span class="si">}</span>  <span class="c1"># 生成构建文件，指定 RISCV 和 TX8FW 路径</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 检查 cmake 是否成功，失败则返回</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    make -j cat /proc/stat <span class="p">|</span> grep epilog -c  <span class="c1"># 执行构建并检查 epilog 相关信息</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 检查 make 是否成功，失败则返回</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>  <span class="c1"># 恢复到之前的目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span> <span class="s2">&#34;passed&#34;</span>  <span class="c1"># 输出函数名和&#34;passed&#34;表示构建成功</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="run_on_soc_rtt">run_on_soc_rtt</h2>
<p><code>run_on_soc_rtt</code>，用于在特定 SoC 和 RTT 环境下运行测试用例。以下是其主要流程：</p>
<ol>
<li>初始化和参数获取:</li>
</ol>
<ul>
<li>函数从命令行参数中获取 <code>case_name</code>, <code>rtt_option</code> 和 <code>multi_graph_enable</code>.</li>
<li>检查 <code>case_name</code> 是否为空，如果为空则输出错误信息并返回 1。</li>
</ul>
<ol start="2">
<li>目录切换和清理:</li>
</ol>
<ul>
<li>将工作目录切换到 <code>${OPLIB_PROJECT_ROOT}/tests/test_codegen/${case_name}</code>.</li>
<li>执行 <code>rm -rf ${case_name}_build</code> 清理之前的构建文件。</li>
</ul>
<ol start="3">
<li>配置设置: 根据 <code>multi_graph_enable</code> 设置 CONFIG_ARGS，如果启用多图则设置为 &ldquo;-DMULTI_GRAPH=1&rdquo;，否则为空。</li>
<li>构建和运行：</li>
</ol>
<ul>
<li>使用 cmake 生成构建文件，指定构建目录为 <code>${case_name}_build</code>，并设置 <code>-DUSING_RISCV=ON</code> 和 <code>-TX8FW_BASE</code> 路径。</li>
<li>使用 make 命令执行构建，目标包括 all 和 chip_out.</li>
</ul>
<ol start="5">
<li>清理和返回: 使用 popd 恢复到原始目录。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> run_on_soc_rtt<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> &#39;start&#39;&#34;</span>  <span class="c1"># 输出函数名和&#34;start&#34;表示开始</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">case_name</span><span class="o">=</span><span class="nv">$1</span>                  <span class="c1"># 获取用例名称</span>
</span></span><span class="line"><span class="cl">    <span class="nv">rtt_option</span><span class="o">=</span><span class="nv">$2</span>                 <span class="c1"># 获取 RTT 选项</span>
</span></span><span class="line"><span class="cl">    <span class="nv">multi_graph_enable</span><span class="o">=</span><span class="nv">$3</span>         <span class="c1"># 获取多图启用标志</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$case_name</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>   <span class="c1"># 如果用例名称为空</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;case_name(</span><span class="nv">$case_name</span><span class="s2">) not found &#34;</span>  <span class="c1"># 输出错误信息</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="m">1</span>                   <span class="c1"># 返回错误码 1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">case_dir</span><span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/tests/test_codegen/<span class="si">${</span><span class="nv">case_name</span><span class="si">}</span>  <span class="c1"># 设置用例目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">case_dir</span><span class="si">}</span>              <span class="c1"># 切换到用例目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    rm -rf <span class="si">${</span><span class="nv">case_name</span><span class="si">}</span>_build      <span class="c1"># 清理之前的构建文件</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 检查清理是否成功，失败则返回</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$multi_graph_enable</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># 如果多图启用标志为空</span>
</span></span><span class="line"><span class="cl">        <span class="nv">CONFIG_ARGS</span><span class="o">=</span><span class="s2">&#34;&#34;</span>                 <span class="c1"># 配置参数为空</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span>                                 <span class="c1"># 否则</span>
</span></span><span class="line"><span class="cl">        <span class="nv">CONFIG_ARGS</span><span class="o">=</span><span class="s2">&#34;-DMULTI_GRAPH=1&#34;</span>  <span class="c1"># 设置多图配置参数</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    cmake -B <span class="s2">&#34;</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">_build&#34;</span> -DUSING_RISCV<span class="o">=</span>ON -TX8FW_BASE<span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/release/riscv/tx8-yoc-rt-thread-smp <span class="si">${</span><span class="nv">CONFIG_ARGS</span><span class="si">}</span> <span class="p">;</span> <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 生成构建文件，指定 RISCV 和 TX8FW 路径</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    make -C <span class="s2">&#34;</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">_build&#34;</span> --target all chip_out <span class="p">;</span> <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 执行构建，目标为 all 和 chip_out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>                          <span class="c1"># 恢复到原始目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> &#39;passed&#39;&#34;</span> <span class="c1"># 输出函数名和&#34;passed&#34;表示通过</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div>]]></content:encoded>
    </item>
    <item>
      <title>astra-Sim</title>
      <link>http://localhost:1313/blogs/astra-sim/</link>
      <pubDate>Mon, 09 Jun 2025 13:34:39 +0800</pubDate>
      <guid>http://localhost:1313/blogs/astra-sim/</guid>
      <description>source code reading of astra-sim</description>
      <content:encoded><![CDATA[<h1 id="build-analytical-backend">Build Analytical Backend</h1>
<p><code>build.sh</code> 脚本是构建过程的高级控制器。其核心职责是解析用户意图，执行预构建步骤，并以正确的参数调用底层的 CMake 工具链。</p>
<ol>
<li>
<p><strong>选项解析</strong>: 脚本通过 <code>getopts</code> 处理以下命令行标志：</p>
<ul>
<li><code>-t &lt;target&gt;</code>: 指定编译目标。有效值为 <code>all</code>, <code>congestion_unaware</code>, <code>congestion_aware</code>。此值将作为变量传递给 CMake。</li>
<li><code>-l</code>: 触发清理 (<code>cleanup</code>) 流程，删除所有构建产物并终止脚本。</li>
<li><code>-d</code>: 启用调试 (<code>Debug</code>) 模式进行编译。</li>
</ul>
</li>
<li>
<p><strong>环境准备 (<code>setup</code>, <code>compile_chakra_et</code>)</strong>:</p>
<ul>
<li><code>setup</code> 函数负责创建用于存放中间文件和最终产物的 <code>build</code> 目录，确保源码树的清洁。同时，它会根据系统核心数设置一个上限为 16 的并发编译线程数，以优化编译效率。</li>
<li><code>compile_chakra_et</code> 函数负责处理 <code>et_def.proto</code> 这一 Protobuf 依赖。它检查目标文件是否存在，若不存在，则调用 <code>protoc</code> 编译器生成相应的 C++ 和 Python 源码。</li>
</ul>
</li>
<li>
<p><strong>构建执行 (<code>compile_astrasim_analytical</code>, <code>compile_astrasim_analytical_as_debug</code>)</strong>:</p>
<ul>
<li>这两个函数是脚本与 CMake 交互的核心。它们根据用户是否指定 <code>-d</code> 标志，决定是执行标准 <code>Release</code> 构建还是 <code>Debug</code> 构建。关键在于它们会将用户指定的 <code>build_target</code> 作为 <code>-DBUILDTARGET</code> 参数传递给 CMake。</li>
</ul>
</li>
<li>
<p><strong>后处理 (<code>create_symlink_*</code>)</strong>:</p>
<ul>
<li>编译完成后，<code>create_symlink_congestion_unaware</code> 和 <code>create_symlink_congestion_aware</code> 等函数会为生成的二进制文件创建符号链接。此举旨在维持对旧文件路径的向后兼容性。</li>
</ul>
</li>
</ol>
<hr>
<p><code>CMakeLists.txt</code> 文件是项目的构建蓝图，它向 CMake 阐述了项目的结构、依赖关系以及编译规则。</p>
<ol>
<li>
<p><strong>编译环境设定</strong>:</p>
<ul>
<li><code>cmake_minimum_required(VERSION 3.15)</code>: 规定了运行此配置所需的最低 CMake 版本。</li>
<li><code>set(CMAKE_CXX_STANDARD 17)</code> 和 <code>set(CMAKE_CXX_STANDARD_REQUIRED ON)</code>: 强制项目必须在支持 C++17 标准的编译环境中构建。</li>
</ul>
</li>
<li>
<p><strong>编译标志 (Compiler Flags)</strong>:</p>
<ul>
<li>此文件为不同的构建类型（<code>CMAKE_BUILD_TYPE</code>）定义了不同的编译器标志。</li>
<li><strong><code>Release</code></strong> (默认模式): <code>set(CMAKE_CXX_FLAGS_RELEASE &quot;-O3&quot;)</code> 指示编译器进行高等级优化，以追求最大化程序性能。</li>
<li><strong><code>Debug</code></strong>: <code>set(CMAKE_CXX_FLAGS_DEBUG &quot;...&quot;)</code> 包含一系列用于调试的标志：
<ul>
<li><code>-O0</code>: 关闭所有优化，确保编译后的代码与源码行为一致。</li>
<li><code>-g</code>: 在可执行文件中包含调试符号，这是 GDB 等调试器工作的前提。</li>
<li><code>-fsanitize=address,undefined,leak</code>: 启用 AddressSanitizer、UndefinedBehaviorSanitizer 和 LeakSanitizer。这些是强大的运行时诊断工具，用于捕获内存访问错误、未定义行为及内存泄漏。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>项目结构与依赖</strong>:</p>
<ul>
<li><code>project(AstraSim_Analytical)</code>: 声明项目名称。</li>
<li><code>add_subdirectory(...)</code>: 此指令是组织项目的关键。它将 <code>AstraSim</code> 核心库、<code>Analytical</code> 网络后端和 <code>AstraSim_Analytical</code> 前端等多个子模块纳入构建过程。</li>
</ul>
</li>
<li>
<p><strong>用户自定义选项</strong>:</p>
<ul>
<li><code>set(BUILDTARGET &quot;all&quot; CACHE STRING ...)</code>: 此行定义了一个名为 <code>BUILDTARGET</code> 的可缓存变量。这使得用户可以通过 <code>cmake -D</code> 命令从外部注入该变量的值。此变量随后会被子目录中的 <code>CMakeLists.txt</code> 文件用来实现条件编译。</li>
</ul>
</li>
</ol>
<h1 id="build-ns-3-backend">Build ns-3 Backend</h1>
<p>构建命令为 <code>./build/astra_ns3/build.sh -c</code>，他会执行该脚本里的 compile 函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> compile <span class="o">{</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">NS3_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">./ns3 configure --enable-mpi
</span></span><span class="line"><span class="cl">./ns3 build AstraSimNetwork -j <span class="m">12</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="ns3-configure---enable-mpi"><code>./ns3 configure --enable-mpi</code></h2>
<ol>
<li>参数解析 (<code>parse_args</code>): 脚本的 <code>argparse</code> 模块会识别出 <code>configure</code> 子命令和 <code>--enable-mpi</code> 选项。<code>--enable-mpi</code> 是一个预定义的&quot;On-Off&quot;选项，用于控制 MPI (Message Passing Interface) 分布式仿真功能的支持。</li>
<li>进入配置步骤 (<code>configuration_step</code>): 由于检测到 configure 命令，脚本会调用 <code>configuration_step</code> 函数。</li>
<li>调用 CMake (<code>configure_cmake</code>): <code>configuration_step</code> 函数内部会调用 <code>configure_cmake</code>. 这个函数是会动态地构建一个 cmake 命令。
<ul>
<li>它会检测到 <code>--enable-mpi</code> 选项，并通过 <code>on_off_condition</code> 函数将其转换为 CMake 变量 <code>-DNS3_MPI=ON</code>.</li>
<li>最终组装出的命令为为 <code>cmake -S . -B cmake-cache -G &quot;Unix Makefiles&quot; -DCMAKE_BUILD_TYPE=default -DNS3_ASSERT=ON -DNS3_LOG=ON -DNS3_WARNINGS_AS_ERRORS=OFF -DNS3_MPI=ON --warn-uninitialized</code></li>
</ul>
</li>
<li>执行配置: 脚本通过 <code>subprocess.run()</code> 执行这条 cmake 命令</li>
</ol>
<h2 id="ns3-build-astrasimnetwork--j-12"><code>./ns3 build AstraSimNetwork -j 12</code></h2>
<ol>
<li>参数解析 (<code>parse_args</code>): 脚本识别出 <code>build</code> 子命令，目标 <code>AstraSimNetwork</code>，以及并行任务数 <code>-j 12</code>. 前者会被存入 <code>args.build</code> 列表，后者会被存入 <code>args.jobs</code>.</li>
<li>进入构建步骤 (<code>build_step</code>): 脚本检测到 <code>build</code> 命令，并调用 <code>build_step</code> 函数。</li>
<li>调用 CMake 构建 (<code>cmake_build</code>): <code>build_step</code> 函数会遍历 <code>args.build</code> 列表中的所有目标。在这里，它会为 <code>AstraSimNetwork</code> 这个目标调用 <code>cmake_build</code> 函数。
<ul>
<li>cmake_build 函数会组装出一条 <code>cmake --build</code> 命令。</li>
<li>将目标 AstraSimNetwork 转换为 <code>--target AstraSimNetwork</code>.</li>
<li>将并行任务数 12 转换为 <code>-j 12</code>.</li>
<li>最终组装出的命令为 <code>cmake --build cmake-cache --target AstraSimNetwork -j 12</code>.</li>
</ul>
</li>
</ol>
<h1 id="error-when-building-ns-3">Error When Building ns-3</h1>
<h2 id="call-of-overloaded-format-is-ambiguous-">call of overloaded ‘format(&hellip;)’ is ambiguous ❌</h2>
<h3 id="问题诊断-">问题诊断 🩺</h3>
<p>错误信息 <code>call of overloaded ‘format(...)’ is ambiguous</code> 的意思是，编译器在你的代码中遇到了一个名为 <code>format</code> 的函数调用，但它找到了多个同名的、并且参数类型都能匹配的 <code>format</code> 函数定义，导致编译器不知道该选择哪一个，因此产生了“歧义”（ambiguous）。</p>
<p><strong>这个歧义的来源是：</strong></p>
<ol>
<li><strong><code>std::format</code> (来自 C++20 标准库)</strong>: 你的项目很可能正在使用支持 C++20 或更高版本的现代编译器（如 GCC 11+）。C++20 标准库引入了一个新的格式化函数 <code>std::format</code>。</li>
<li><strong><code>fmt::format</code> (来自 {fmt} 库)</strong>: <code>spdlog</code> 这个日志库是基于一个非常流行的第三方格式化库 <code>{fmt}</code> 构建的。这个库也提供了一个功能几乎完全相同的 <code>fmt::format</code> 函数。在 <code>spdlog</code> 的上下文中，它通常可以直接以 <code>format</code> 的形式被调用。</li>
</ol>
<p>当你的代码（这里是 <code>spdlog_setup</code> 的一部分）简单地调用 <code>format(...)</code> 时，如果 C++20 的 <code>&lt;format&gt;</code> 头文件被包含，编译器就会同时看到 <code>std::format</code> 和 <code>spdlog</code> 内部的 <code>fmt::format</code>。由于两者都能处理字符串字面量 (<code>const char[]</code>) 和 <code>std::string</code>，编译器无法决定用哪个，从而报错。</p>
<hr>
<h3 id="关于-using-fmtformat-为何仍然无效的解释">关于 <code>using fmt::format;</code> 为何仍然无效的解释</h3>
<p>原因是，除了常规的命名空间查找规则，C++ 还有一个更强大的规则叫做<strong>参数依赖查找（Argument-Dependent Lookup, ADL）</strong>，有时也被称为 Koenig 查找。</p>
<hr>
<p>我们来梳理一下编译器在看到 <code>format(...)</code> 这行代码时的“思考过程”：</p>
<ol>
<li>
<p><strong>在当前作用域查找</strong></p>
<p>编译器看到了你的 <code>using fmt::format;</code> 声明。很好，它在当前作用域里找到了一个叫做 <code>format</code> 的函数（也就是 <code>fmt::format</code>）。这成为了<strong>候选者 A</strong>。</p>
</li>
<li>
<p><strong>参数依赖查找 (ADL) —— 问题的根源</strong></p>
<p>接下来，编译器会检查 <code>format(...)</code> 函数的所有参数类型。在你的错误日志里，我们看到了 <code>const std::string&amp;</code> 这样的参数。</p>
<ul>
<li>ADL 规则规定：如果一个函数的参数是某个命名空间 <code>N</code> 下的类型（比如 <code>std::string</code> 是 <code>std</code> 命名空间下的），那么编译器<strong>也必须</strong>去那个命名空间 <code>N</code> (这里是 <code>std</code>) 里面去查找同名的函数。</li>
<li>由于 <code>std::string</code> 是 <code>std</code> 命名空间的成员，ADL 规则被触发，编译器自动地去 <code>std</code> 命名空间里寻找名为 <code>format</code> 的函数。</li>
<li>因为你使用了 C++20 编译器，它在 <code>std</code> 命名空间里成功找到了 <code>std::format</code>。这成为了<strong>候选者 B</strong>。</li>
</ul>
</li>
<li>
<p><strong>产生歧义</strong></p>
<p>现在编译器陷入了困境。它手头有两个同样匹配的候选函数：</p>
<ul>
<li><strong>候选者 A</strong>: <code>fmt::format</code> (通过 <code>using</code> 声明找到)</li>
<li><strong>候选者 B</strong>: <code>std::format</code> (通过 ADL 在参数的命名空间里找到)</li>
</ul>
<p><code>using</code> 声明只是将一个名字引入当前作用域，它并**没有足够的“特权”**去压制一个通过 ADL 找到的同样优秀的候选者。因为两个函数都能完美处理你传入的参数，编译器无法做出选择，所以它只能放弃并报告“调用是模糊的 (ambiguous)”。</p>
</li>
</ol>
<h3 id="结论与最终解决方案-">结论与最终解决方案 ✅</h3>
<p>这个 C++ 的特性意味着，只要你的函数参数中包含了 <code>std</code> 命名空间里的类型（如 <code>std::string</code>, <code>std::vector</code> 等），ADL 就有可能被触发，从而把 <code>std</code> 里的函数（如 <code>std::format</code>, <code>std::to_string</code> 等）也拉入候选列表，造成意想不到的冲突。</p>
<p>因此，唯一能 100% 消除歧义、让编译器别无选择的方法，就是使用<strong>显式的命名空间限定</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// 这样做，是在直接告诉编译器：“别去猜了，我就是要调用 fmt 命名空间里的这个 format！”
</span></span></span><span class="line"><span class="cl"><span class="c1">// 这会完全绕过 ADL 和其他查找规则，直达目标。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(...);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="runing-arguments">Runing Arguments</h1>
<p>执行仿真需要传递一些参数，命令模板如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">{</span>ASTRA_SIM_BIN<span class="o">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --workload-configuration<span class="o">=</span><span class="si">${</span><span class="nv">WORKLOAD_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --system-configuration<span class="o">=</span><span class="si">${</span><span class="nv">SYSTEM_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --network-configuration<span class="o">=</span><span class="si">${</span><span class="nv">NETWORK_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --remote-memory-configuration<span class="o">=</span><span class="si">${</span><span class="nv">REMOTE_MEMORY_CONFIG</span><span class="si">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="workload_config">WORKLOAD_CONFIG</h2>
<p>astra-sim 使用的是 Chakra (Execution Trace) 作为 workload 层的输入。将 chakra 作为 python package 安装后有几个命令通过 pyproject.toml 对应到 python函数。</p>
<details class="custom-details">
    <summary class="custom-summary">Explanation of toml file</summary>
    <div><p><code>pyproject.toml</code> 是一个标准化的配置文件，用于定义 Python 项目的元数据、依赖关系以及构建和开发工具的配置。</p>
<hr>
<ol>
<li><code>[build-system]</code> 构建系统配置，这部分定义了如何构建你的 Python 包。</li>
</ol>
<ul>
<li><code>**requires**</code>: 列出了构建项目本身所必需的包。这些是构建环境的依赖，而不是你代码运行时的依赖。
<ul>
<li><code>setuptools</code>, <code>setuptools-grpc</code>: 表明此项目使用 <code>setuptools</code> 作为其构建工具，并需要 <code>setuptools-grpc</code> 插件。</li>
</ul>
</li>
<li><code>**build-backend**</code>: 指定了构建工具中实际执行构建过程的 Python 对象（入口点）。
<ul>
<li><code>setuptools.build_meta</code>: 这是 <code>setuptools</code> 提供的标准构建后端。</li>
</ul>
</li>
</ul>
<hr>
<ol start="2">
<li><code>[project]</code>：这部分包含了项目的基本信息，这些信息会展示在 PyPI (Python Package Index) 上。</li>
</ol>
<ul>
<li><code>**name**</code>: 包的名称，即 <code>pip install chakra</code> 中的 <code>chakra</code>。</li>
<li><code>**requires-python**</code>: 运行此包所需的最低 Python 版本，这里是 <code>3.7</code> 或更高。</li>
<li><code>**version**</code>: 当前包的版本号。</li>
<li><code>**readme**</code>: 指向一个文件，该文件的内容将作为项目在 PyPI 上的详细描述。</li>
<li><code>**license**</code>: 指向包含许可证信息的文件。</li>
<li><code>**authors**</code>：项目的作者信息。</li>
<li><code>**dependencies**</code>: <strong>项目运行时的依赖项</strong>。当用户 <code>pip install chakra</code> 时，这些包也会被一并安装。
<ul>
<li><code>protobuf==5.*</code>: 需要版本为 5.x 的 <code>protobuf</code> 库。</li>
<li><code>graphviz</code>, <code>networkx</code>, <code>pydot</code>: 其他标准的第三方库依赖。</li>
<li><code>HolisticTraceAnalysis @ git+...</code>: 这是一个特殊的依赖。它直接从 GitHub 仓库的一个<strong>特定 commit</strong> (<code>d731cc...</code>) 来安装。这确保了项目依赖于一个稳定且不会意外变动的版本。</li>
</ul>
</li>
</ul>
<hr>
<ol start="3">
<li><code>[project.urls]</code>：项目相关链接，这些链接会显示在 PyPI 页面的侧边栏，为用户提供更多信息的入口。</li>
</ol>
<ul>
<li><code>**Homepage**</code>, <code>**Documentation**</code>, <code>**Repository**</code>: 分别指向项目主页、文档和代码仓库的 URL。</li>
</ul>
<hr>
<ol start="4">
<li><code>[tool.setuptools]</code>：这部分是针对构建工具 <code>setuptools</code> 的详细配置。</li>
</ol>
<ul>
<li><code>**package-dir**</code>: 定义了 Python 包名与实际源代码目录之间的映射关系。
<ul>
<li>例如，<code>&quot;chakra.src.converter&quot; = &quot;src/converter&quot;</code> 表示当用户 <code>import chakra.src.converter</code> 时，Python 会从 <code>src/converter/</code> 目录下寻找代码。这使得项目可以使用 <code>src</code> 布局。</li>
</ul>
</li>
<li><code>**package-data**</code>: 指定需要包含在最终发布包中的非 Python 文件。
<ul>
<li><code>&quot;chakra.schema.protobuf&quot; = [&quot;et_def.proto&quot;]</code>: 表示需要将 <code>et_def.proto</code> 这个文件打包到 <code>chakra.schema.protobuf</code> 这个包里。</li>
</ul>
</li>
</ul>
<hr>
<ol start="5">
<li><code>[project.scripts]</code>：这部分定义了在安装包时应创建的命令行工具。</li>
</ol>
<ul>
<li><code>**chakra_converter = &quot;chakra.src.converter.converter:main&quot;**</code>: 这行配置意味着，当用户安装此包后，他们可以在终端中直接运行 <code>chakra_converter</code> 命令。执行此命令时，系统会调用 <code>chakra.src.converter.converter</code> 模块中的 <code>main</code> 函数。</li>
</ul>
<hr>
<ol start="6">
<li><code>[tool.ruff]</code>：这部分是用于配置 <code>Ruff</code> 高性能代码检查（Linter）和格式化（Formatter）工具。</li>
</ol>
<ul>
<li><code>**target-version**</code>, <code>**line-length**</code>, <code>**exclude**</code>: 基本配置，如目标 Python 版本、每行最大长度和要排除检查的文件。</li>
<li><code>**[tool.ruff.lint]**</code>: Linter 的具体配置。
<ul>
<li><code>**select**</code>: 启用一系列代码规则集（例如 <code>D</code> 代表文档字符串 <code>pydocstyle</code>，<code>I</code> 代表导入排序 <code>isort</code>）。</li>
<li><code>**ignore**</code>: 全局禁用的特定规则。注释中解释了忽略它们的原因（例如，规则冲突或待办事项）。</li>
<li><code>**per-file-ignores**</code>: 针对特定文件或目录禁用规则。例如，<code>&quot;**/tests/*&quot; = [&quot;D&quot;]</code> 表示在所有测试文件中都禁用文档字符串检查。</li>
</ul>
</li>
<li><code>**[tool.ruff.format]**</code>: 格式化器的配置，如使用空格作为缩进风格。</li>
</ul>
<hr>
<ol start="7">
<li><code>[tool.pyright]</code>：这部分配置了 <code>Pyright</code>，一个由微软开发的静态类型检查工具。</li>
</ol>
<ul>
<li><code>**typeCheckingMode**</code>: 类型检查的严格程度，这里是 <code>basic</code>（基础模式）。</li>
<li><code>**exclude**</code>：在进行类型检查时要忽略的文件和目录。</li>
<li><code>**report...**</code>：关闭特定的错误或警告报告。</li>
</ul>
<hr>
<ol start="8">
<li><code>[tool.vulture]</code>：这部分配置了 <code>Vulture</code>，一个用于发现项目中未使用（&ldquo;死&rdquo;）代码的工具。</li>
</ol>
<ul>
<li><code>**ignore_names**</code>: 让 Vulture 忽略某些特定的变量名或函数名，即使它们看起来未使用。</li>
<li><code>**min_confidence**</code>: 设置报告问题的最低置信度阈值。<code>100</code> 表示只有在 Vulture 100% 确定代码是无用的时候才会报告，这可以有效减少误报。</li>
</ul></div>
</details><br>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-toml" data-lang="toml"><span class="line"><span class="cl"><span class="p">[</span><span class="nx">project</span><span class="p">.</span><span class="nx">scripts</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_converter</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.converter.converter:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_generator</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.generator.generator:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_jsonizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.jsonizer.jsonizer:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_timeline_visualizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.timeline_visualizer.timeline_visualizer:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_trace_link</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.trace_link.trace_link:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_visualizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.visualizer.visualizer:main&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="generate-execution-trace">Generate Execution Trace</h3>
<p>ASTRA-sim 的 ET 命名格式为 <code>{path prefix/trace name}.{npu_id}.et</code>. Chakra ET 的获取流程如下图所示<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<ol>
<li>Collect ET from PyTorch
<ul>
<li>PyTorch ET 负责 CPU 算子，并明确表示它们之间的依赖关系。</li>
<li>Kineto Trace 编码 GPU 算子及其开始和结束时间。</li>
</ul>
</li>
<li>Merge Trace by <code>chkra_trace_link</code>：将它们合并为一个 PyTorch ET+. 该格式本质上遵循 PyTorch ET 的模式，但同时也编码了 GPU 操作符及其依赖关系。</li>
<li>Convert to Chakra ET by <code>chakra_converter</code>

<figure class="post-figure">
    <a href="https://private-user-images.githubusercontent.com/7621438/294028976-67228699-cec5-4a4d-b03e-e76647a80ce8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1NDQxNDUsIm5iZiI6MTc0OTU0Mzg0NSwicGF0aCI6Ii83NjIxNDM4LzI5NDAyODk3Ni02NzIyODY5OS1jZWM1LTRhNGQtYjAzZS1lNzY2NDdhODBjZTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MTBUMDgyNDA1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWE4NzAyMGQ0NWQ0MDA2MzIzMmY1MmNhYWU4YWUzNTJiNjI3OTAzZDk2ZDU3NDIwMWJhZTFlMjNjZDhjN2JmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.-DDH2mackHVASqoCbmyvN2xl8vZemaa73OiLmBER1o0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://private-user-images.githubusercontent.com/7621438/294028976-67228699-cec5-4a4d-b03e-e76647a80ce8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1NDQxNDUsIm5iZiI6MTc0OTU0Mzg0NSwicGF0aCI6Ii83NjIxNDM4LzI5NDAyODk3Ni02NzIyODY5OS1jZWM1LTRhNGQtYjAzZS1lNzY2NDdhODBjZTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MTBUMDgyNDA1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWE4NzAyMGQ0NWQ0MDA2MzIzMmY1MmNhYWU4YWUzNTJiNjI3OTAzZDk2ZDU3NDIwMWJhZTFlMjNjZDhjN2JmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.-DDH2mackHVASqoCbmyvN2xl8vZemaa73OiLmBER1o0" alt="Overview of Trace Collection">
    </a><figcaption>Overview of Trace Collection</figcaption></figure></li>
</ol>
<p>具体的教程和例子可以在 <a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#3-from-raw-traces-to-chakra-a-step-by-step-conversion-guide">Conversion Guide</a> 和 <a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#3-from-raw-traces-to-chakra-a-step-by-step-conversion-guide">Practical Example</a> 找到。</p>
<h3 id="using-et-converter">Using ET Converter</h3>
<p>可以将 astra-sim 1.0 的文本输入转换成 Chakra ET.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> ./extern/graph_frontend/chakra/
</span></span><span class="line"><span class="cl">pip3 install .
</span></span><span class="line"><span class="cl">chakra_converter Text <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --input ../../../examples/text_converter/text_workloads/Resnet50_DataParallel.txt <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --output ../../../examples/text_converter/text_workloads/Resnet50_DataParallel <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --num-npus <span class="m">8</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --num-passes <span class="m">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>workload 文本格式要求如下，其中通信大小单位是字节，计算时间以周期数表示。</p>
<ul>
<li>第一行：(DATA/HYBRID_TRANSFORMER/HYBRID_DLRM)
<ul>
<li>该行指定训练循环的并行化类型。DATA 表示纯数据并行方法，HYBRID_TRANSFORMER 表示专为 Transformer DNN 网络设计的混合并行方法，而 HYBRID_DLRM 表示专为 DLRM DNN 网络优化的混合并行方法。</li>
</ul>
</li>
<li>第二行：(int)
<ul>
<li>该行表示 DNN 的层数。</li>
</ul>
</li>
<li>后续行：每行描述一层。层的描述格式如下：
<ul>
<li>{(string: 层名称)</li>
<li>(int: 保留变量)</li>
<li>(int: 前向计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 前向通信类型)</li>
<li>(int: 前向通信大小)</li>
<li>(int: 输入梯度计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 输入梯度通信类型)</li>
<li>(int: 输入梯度通信大小)</li>
<li>(int: 权重梯度计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 权重梯度通信类型)</li>
<li>(int: 权重梯度通信大小)</li>
<li>(集合通信完成后，权重/输入/输出更新的延迟)}`</li>
</ul>
</li>
</ul>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>每一层的参数写要在同一行！！！</p></div>

<h3 id="enable-communicator-groups">Enable Communicator Groups</h3>
<p>astra-sim 2.0 支持<a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html">通信组</a>。可以通过指定 <code>--comm-group-configuration</code> JSON 文件来指定，默认只有一个通信组。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// The first/second communicator group, with ID 0/1, includes GPU IDs from 0-3/4-7. 
</span></span></span><span class="line"><span class="cl"><span class="c1">//   &#34;0&#34;: [0, 1, 2, 3],
</span></span></span><span class="line"><span class="cl"><span class="c1">//   &#34;1&#34;: [4, 5, 6, 7]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="nt">&#34;&lt;communicator_group_id&gt;&#34;</span> <span class="p">:</span> <span class="p">[</span><span class="err">gpu_ids</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="system_config">SYSTEM_CONFIG</h2>
<h1 id="system-layer">System Layer</h1>
<p>Workload 层会遍历 Chakra ET 中的节点，并为每个节点所指代的操作发出相应的命令。System 层接收这些命令，并将其转换为适合网络、计算或内存后端的格式，从而正确模拟操作。根据操作的类型，系统层的行为会有所不同，具体如下：</p>
<ul>
<li>计算操作：向计算后端发出调用，以模拟操作的持续时间。</li>
<li>内存操作：  内存</li>
<li>通信操作：将集合通信分解为点对点的发送和接收消息，并向网络后端发出“发送”或“接收”调用，以模拟消息的传输过程。</li>
</ul>
<h2 id="collective-scheduler">Collective Scheduler</h2>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-sim-docs/_images/system_overview_queue.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-sim-docs/_images/system_overview_queue.svg" alt="Collective Scheduler">
    </a><figcaption>Collective Scheduler</figcaption></figure></p>
<p>每个队列都有许多 <code>StreamBaseline</code> 对象 (图中右上角)，代表了整个集合通信的流程，<code>phase_to_go</code> 是一个用于表示这些阶段的队列，<code>my_current_phase</code> 是指向当前执行阶段的指针。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">StreamBaseline</span> <span class="o">:</span> <span class="k">public</span> <span class="n">BaseStream</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">StreamBaseline</span><span class="p">(</span><span class="n">Sys</span><span class="o">*</span> <span class="n">owner</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">DataSet</span><span class="o">*</span> <span class="n">dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="kt">int</span> <span class="n">stream_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">list</span><span class="o">&lt;</span><span class="n">CollectivePhase</span><span class="o">&gt;</span> <span class="n">phases_to_go</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="kt">int</span> <span class="n">priority</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// my_current_phase[CollectivePhase] is defined in BaseStream
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">void</span> <span class="nf">init</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">call</span><span class="p">(</span><span class="n">EventType</span> <span class="n">event</span><span class="p">,</span> <span class="n">CallData</span><span class="o">*</span> <span class="n">data</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">consume</span><span class="p">(</span><span class="n">RecvPacketEventHandlerData</span><span class="o">*</span> <span class="n">message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于每个 stream <code>proceed_to_next_vnet_baseline</code> (astra-sim/system/Sys.cc) 用于推进通信阶段并且负责在队列之间移动 stream 对象。以下几种情况会调用该函数：</p>
<ol>
<li>stream 第一次被移动出 ready_list 并且将被插入到 <code>active_streams</code>.</li>
<li>stream 完成了一个通信阶段并且等待下一个阶段。</li>
<li>stream 完成了所有的通信阶段。</li>
</ol>
<p>(2-1) 到 (2-5) 描述了该函数的行为</p>
<ol>
<li>
<p>查看当前持有 stream 的队列: 从队列中删除 <code>StreamBaseline</code> 对象 (流的完成顺序可能与它们开始执行的顺序不同)。</p>
</li>
<li>
<p>修改 <code>StreamBaseline</code> 对象: 已完成的集合通信阶段从 <code>phases_to_go</code> 中弹出，<code>my_current_phase</code> 现在指向下一个待执行的阶段。</p>
</li>
<li>
<p>使用 <code>insert_stream</code> 将 <code>StreamBaseline</code> 对象插入到下一个队列中。</p>
</li>
<li>
<p>调用函数 <code>notify_stream_removed</code> 函数查看前一个队列的头部。 <code>stream_pointer</code> 指向队列中第一个未运行的 stream (标记为蓝色)。该函数通过调用 <code>StreamBaseline::init()</code> 来启动 stream 的下一个阶段的执行。</p>
</li>
<li>
<p>使用 <code>notify_stream_added</code> 触发新队列头部 stream 的通信阶段执行。</p>
</li>
</ol>
<p>在其他情况下，<code>proceed_to_next_vnet_baseline</code> 会执行上述步骤的一部分。具体如下：</p>
<ol>
<li>
<p>刚从 <code>ready_list</code> 中移除：<br>
<code>proceed_to_next..</code> 会初始化 stream (1-2)，将其插入到第一个队列中 (1-3)，并触发该队列头部的流执行。</p>
</li>
<li>
<p>stream 完成：<br>
该函数会从之前的队列中删除 stream (3-1)，并触发之前队列头部的 stream 执行。此外，<code>StreamBaseline</code> 对象会被删除，并调用 <code>notify_stream_finished</code>，以通知 <code>Sys</code> 对象 stream 已经结束 (3-6)</p>
</li>
</ol>
<h2 id="collective-implementation">Collective Implementation</h2>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-sim-docs/_images/coll_implementation.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-sim-docs/_images/coll_implementation.svg" alt="Overview of Collective Implementation">
    </a><figcaption>Overview of Collective Implementation</figcaption></figure>
模拟器将集体通信分解为发送和接收消息的方式有两种。目前最常用的方法是模拟器实现一组预定义的常见算法 (例如 Ring、DoubleBinary、HalvingDoubling 等)。这种“原生”实现逻辑位于模拟器的代码库中，允许用户快速探索一组预定义的算法。</p>
<p>自 2024 年 8 月以来，ASTRA-sim 支持了一种新的集合通信算法表示方式。System 层通过暴露一个集体 API，可以接收任意集体算法的定义。</p>
<p>这两种方法都是对 <code>CollectivePhase::Algorithm</code> 对象的实现，该对象是 System 层中的调度单元. <a href="https://github.com/astra-sim/astra-sim/blob/15a4334ade00fe1040fd00495cd13fd1ea5177e4/astra-sim/system/Sys.cc#L1037">generate_collective_phase</a> 会根据不同的算法在创建 <a href="https://github.com/astra-sim/astra-sim/blob/15a4334ade00fe1040fd00495cd13fd1ea5177e4/astra-sim/system/CollectivePhase.hh#L17">CollectivePhase</a> 的时候传入对应的 Algorithm.</p>
<h3 id="astra-sim-native-implementation">ASTRA-Sim Native Implementation</h3>
<p>相关的实现都位于<a href="https://github.com/astra-sim/astra-sim/tree/master/astra-sim/system/collective">该文件夹</a>下, naive 实现的限制是当需要模拟一个新的集合通信算法时算法，必须实现整个集合？随着不规则集合通信 (如 TACOS(Topology Aware CollectiveS), MSCCLang(基于 DSL)) 中工作的增加，快速模拟和迭代各种算法的需求变得越来越多。</p>
<h3 id="chakra-based-arbitrary-definition-through-collective-api">Chakra Based Arbitrary Definition Through Collective API</h3>
<p>因此一个新的 AP来接受任何集合通信算法的定义，而不局限于预定义的规则通信模式。对于通信表示，使用 Chakra ET 模式作为单独的图。将集合通信算法表示为Chakra ET 模式中 COMM_SEND，COMM_RECV 节点的图。也就是说，System 层不是将集合通信分解为发送和接收消息，而是简单地遵循 Chakra 图中已经表示的分解。由于已经使用 Chakra ET 来表示 workload，使用 Chakra ET 来额外定义集合通信算法提供了一种轻松简单的方式来遍历整个图。</p>
<p>如上图所示当 workload 层发出 AllReduce 集体操作时，System 层不会运行模拟器代码库中已有的原生实现逻辑，而是会遍历通过 API 提供的 Chakra ET，该 ET 表示集合通信算法。需要注意 workload Chakra 图和集合通信算法的 Chakra 图是解耦的，并通过不同的输入点提供。最终，asytra-sim 模拟器会将通信节点替换为集体实现。</p>
<h2 id="input-files-for-collective-api">Input Files for Collective API</h2>
<h3 id="astra-sim-native">ASTRA-sim Native</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="s2">&#34;active-chunks-per-dimension&#34;</span><span class="err">:</span> <span class="mi">1</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-reduce-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-gather-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">,</span> <span class="s2">&#34;doubleBinaryTree&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-to-all-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">,</span> <span class="s2">&#34;doubleBinaryTree&#34;</span><span class="p">,</span> <span class="s2">&#34;halvingDoubling&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span></code></pre></div><p><code>all-*-implementation</code> 指定了模拟器将如何将给定的集合通信分解为发送和接收消息。All-Gather 操作列表中的两个条目表示模拟器将按两个维度分解 ——第一个维度使用 Ring 算法，第二个维度使用 doubleBinaryTree 算法。</p>
<blockquote class="quote"><p>Native Implementation Requires That the Dimensions for Collective Algorithms Are Same Across All Collectives.</p></blockquote>
<div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p><strong>Native 实现要求所有集体操作的维度必须相同</strong>。换句话说，如果一个集合通信算法被定义为二维的，那么其他集合通信算法也必须是二维操作。上述只是一个例子。</p></div>

<h3 id="collective-api">Collective API</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="s2">&#34;active-chunks-per-dimension&#34;</span><span class="err">:</span> <span class="mi">1</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;all-reduce-implementation-chakra&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;/app/hoti2024/demo5/inputs/custom_ring&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span></code></pre></div><p>需要注意这里要使用 <code>all-*-implementation-chakra</code>，而不是 <code>all-*-implementation</code>. 另外  Chakra ET 文件与传递给 workload 层的文件是不同的，每一项的值是 Chakra ET 文件的绝对路径，不包括最后的 <code>{rank}.et</code> 字符串 (类似于 Workload 层输入)。此外，即使有许多维度，列表也只接受一个值。这是因为跨维度通信的概念已经包含在 ET 中。</p>
<div class="github">
    <div class="github_bar">
        <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" viewBox="0 0 50 50"><path d="M17.791,46.836C18.502,46.53,19,45.823,19,45v-5.4c0-0.197,0.016-0.402,0.041-0.61C19.027,38.994,19.014,38.997,19,39 c0,0-3,0-3.6,0c-1.5,0-2.8-0.6-3.4-1.8c-0.7-1.3-1-3.5-2.8-4.7C8.9,32.3,9.1,32,9.7,32c0.6,0.1,1.9,0.9,2.7,2c0.9,1.1,1.8,2,3.4,2 c2.487,0,3.82-0.125,4.622-0.555C21.356,34.056,22.649,33,24,33v-0.025c-5.668-0.182-9.289-2.066-10.975-4.975 c-3.665,0.042-6.856,0.405-8.677,0.707c-0.058-0.327-0.108-0.656-0.151-0.987c1.797-0.296,4.843-0.647,8.345-0.714 c-0.112-0.276-0.209-0.559-0.291-0.849c-3.511-0.178-6.541-0.039-8.187,0.097c-0.02-0.332-0.047-0.663-0.051-0.999 c1.649-0.135,4.597-0.27,8.018-0.111c-0.079-0.5-0.13-1.011-0.13-1.543c0-1.7,0.6-3.5,1.7-5c-0.5-1.7-1.2-5.3,0.2-6.6 c2.7,0,4.6,1.3,5.5,2.1C21,13.4,22.9,13,25,13s4,0.4,5.6,1.1c0.9-0.8,2.8-2.1,5.5-2.1c1.5,1.4,0.7,5,0.2,6.6c1.1,1.5,1.7,3.2,1.6,5 c0,0.484-0.045,0.951-0.11,1.409c3.499-0.172,6.527-0.034,8.204,0.102c-0.002,0.337-0.033,0.666-0.051,0.999 c-1.671-0.138-4.775-0.28-8.359-0.089c-0.089,0.336-0.197,0.663-0.325,0.98c3.546,0.046,6.665,0.389,8.548,0.689 c-0.043,0.332-0.093,0.661-0.151,0.987c-1.912-0.306-5.171-0.664-8.879-0.682C35.112,30.873,31.557,32.75,26,32.969V33 c2.6,0,5,3.9,5,6.6V45c0,0.823,0.498,1.53,1.209,1.836C41.37,43.804,48,35.164,48,25C48,12.318,37.683,2,25,2S2,12.318,2,25 C2,35.164,8.63,43.804,17.791,46.836z"></path></svg>
        <a class="github_name" href="https://github.com/astra-sim/collectiveapi" target="_blank">Collective API</a>
    </div>
    <div class="github_description">参考该仓库实现</div>
    <div class="github_language">
        
    </div>
</div>

<h1 id="network-backend">Network Backend</h1>
<h2 id="analytical-network-backend">Analytical Network Backend</h2>
<p>Analytical Network 模拟器通过数学方程模拟所有网络行为。因此，该后端最适合于大规模分布式平台的建模和仿真。目前支持两种分析模式</p>
<ul>
<li>congestion_<strong>unaware</strong> analytical network simulator</li>
<li>congestion_<strong>aware</strong> analytical network simulator</li>
</ul>
<hr>
<ul>
<li>T<strong>Topology</strong></li>
</ul>
<p>Analytical Network 支持三种拓扑结构: Ring, FullConnected, Switch. 并且可以堆叠来表示多维网络。</p>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/network-building-blocks.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/network-building-blocks.svg" alt="Basic Network Building Block">
    </a><figcaption>Basic Network Building Block</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">topology</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="l">Ring, Switch ] </span><span class="w"> </span><span class="c"># 2D topology</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">topology</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="l">Ring, Ring, Ring ] </span><span class="w"> </span><span class="c"># 3D topology</span><span class="w">
</span></span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/multidim-network-example.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/multidim-network-example.svg" alt="Example of 2D &amp; 3D Topologies">
    </a><figcaption>Example of 2D &amp; 3D Topologies</figcaption></figure></p>
<hr>
<ul>
<li><strong>NPUs Count</strong></li>
</ul>
<p>指定了每个维度上的设备数目</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 5 NPUs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 4 × 2 = 8 NPUs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 4 × 2 × 2 = 16 NPUs</span><span class="w">
</span></span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/npus-count-example.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/npus-count-example.svg" alt="NPUs Count Example">
    </a><figcaption>NPUs Count Example</figcaption></figure></p>
<hr>
<ul>
<li><strong>Bandwidth</strong> &amp; <strong>Latency</strong></li>
</ul>
<p><code>latency</code> 定义了每条单向链路的延迟 (ns).
<code>bandwidth</code> 定义了每条单向链路的带宽 (GB/s).</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>$1 GB = 2^{30} B$ and $1 s = 10^9 ns$</p></div>

<h2 id="ns3-backend">ns3 backend</h2>
<p>下面是用 ns3 后端进行方针的一个执行命令。这里使用了 <code>--network-backend</code> 和 <code>--logical-topology</code> 这两个参数。需要说明的是，Analytical Backend 中仅使用了-<code>-network-backend</code> 参数，这是因为分析型后端的逻辑拓扑与物理拓扑是相同的，而 ns3 则允许我们将逻辑拓扑与物理拓扑分离。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">   <span class="c1"># {NS3_DIR} is the directory of the ns-3 backend. That is, &#39;{ASTRA_SIM_ROOT_DIRECTORY}/extern/network_backend/ns-3&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">NS3_DIR</span><span class="si">}</span><span class="s2">/build/scratch&#34;</span>
</span></span><span class="line"><span class="cl">    ./ns3.42-AstraSimNetwork-default <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --workload-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../extern/graph_frontend/chakra/one_comm_coll_node_allgather  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --system-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/system/Switch.json  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --network-configuration<span class="o">=</span><span class="s2">&#34;../../../ns-3/scratch/config/config.txt&#34;</span>   <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --remote-memory-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/remote_memory/analytical/no_memory_expansion.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --logical-topology-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/network/ns3/sample_8nodes_1D.json   <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --comm-group-configuration<span class="o">=</span><span class="se">\&#34;</span>empty<span class="se">\&#34;</span>
</span></span></code></pre></div><hr>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#2-overview-of-trace-collection-and-simulation-methodology">Overview of Trace Collection</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Transformer Family</title>
      <link>http://localhost:1313/blogs/transformerfamily/</link>
      <pubDate>Sat, 07 Jun 2025 21:24:13 +0800</pubDate>
      <guid>http://localhost:1313/blogs/transformerfamily/</guid>
      <description>Introduction of Transformer Family</description>
      <content:encoded><![CDATA[<h1 id="origin-of-transformer">Origin of Transformer</h1>
<p>Transformer 由谷歌研于 2017 年在一篇名为 <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> 的论文中提出。与 RNN 的输入仅为一个 token 不同，Transformer 一次性可以输入一整个完整的序列。总体结构如下图所示，包含一个 Encoder 和一个 Decoder.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBd293bc1a46904e1af31ce993b83c68f1?method=download&amp;shareKey=47cf357e488e7da5483a1b98f3257ab1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBd293bc1a46904e1af31ce993b83c68f1?method=download&amp;shareKey=47cf357e488e7da5483a1b98f3257ab1" alt="Transformers Architecture">
    </a><figcaption>Transformers Architecture</figcaption></figure></p>
<h2 id="embedding">Embedding</h2>
<p>Embedding 是一种将离散的、稀疏的输入 (如词语、字符、类别标签&hellip;) 转换为连续的、密集的向量表示的技术，核心是通过一个映射函数将离散的输入符号 (如单词) 映射到一个低维向量空间中。假设我们有一个包含 V 个单词的 Vocabulary，维度为 d，那么 Embedding Matrix 将是一个大小为 V×d 的矩阵，其中每一行是一个单词的向量表示。通过嵌入层，输入的词索引 (通常是整数) 就会被映射到该矩阵的对应行，从而得到词的向量表示。常见的预训练词嵌入方法包括：</p>
<ul>
<li>Word2Vec：通过上下文预测词语的方式学习词向量。</li>
<li>GloVe：通过统计词共现信息来学习词向量。</li>
<li>FastText：考虑了子词信息的词嵌入方法，能更好地处理词形变化。</li>
</ul>
<p>在 PyTorch 和 TensorFlow 等框架中，通常有专门的 Embedding 层，Hugging Face 也有 tokenizer 将句子划分成单词并转换成对应的索引：</p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Positional Encoding 作用是为输入的序列中的每个元素提供位置信息。由于 Transformer 架构并没有使用递归或卷积结构，本身无法捕捉输入序列中元素的相对位置关系，因此需要通过位置编码来显式地引入这些位置信息。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>Transformer 的主要优势是通过 Self-Attention 并行处理序列中的每个元素，但是这也意味着它没有自带顺序感知能力，它并不会自动知道一个单词是在句子的开头还是结尾，因此需要额外的机制来编码每个元素在序列中的位置。</p>
<p>位置编码 通过将每个单词的位置信息 (即它在序列中的位置) 编码为一个向量，并将该向量添加到单词的嵌入表示中，从而让模型能够感知每个元素的相对或绝对位置。</p></div>

<p>经典的 Transformer 位置编码使用 正弦和余弦函数的组合，为每个位置生成的向量在不同维度上具有不同的周期性，这能够捕捉到不同级别的相对位置关系。假设输入的序列中有 N 个单词，每个单词的嵌入维度为 d，那么 Positional Encodin(PE) 的计算公式如下:</p>
$$
\begin{aligned}
&PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{\frac{2i}d}}\right)\\
&PE_{(pos,2i+1)}=\cos\left(\frac{pos}{10000^{\frac{2i}d}}\right)
\end{aligned}
$$<p>其中：</p>
<ul>
<li>pos 是单词在序列中的位置索引 (位置从 0 开始).</li>
<li>i 是位置编码的维度索引，表示该位置编码向量中的第 i 个元素。</li>
<li>d 是 Embedding 的维度</li>
</ul>
<p>这些位置编码与单词的词嵌入 (Word Embedding) 相加，最终形成输入模型的向量表示。</p>
<h2 id="masked-multi-head-attention">(Masked) Multi-Head Attention</h2>
<p>Multi-Head Attention (MHA) 的目的是通过并行地计算多个注意力头 (Attention Head)，从多个子空间中学习输入序列的不同表示。经过 Word Embedding 后的输入 X 形状为 Nxd. 计算步骤如下</p>
<ol>
<li>
<p>通过学习的变换矩阵将 X 映射到查询 (Q)、键 (K) 和值 (V) 空间。
</p>
$$
\begin{aligned}&Q=XW^{Q}\\&K=XW^{K}\\&V=XW^{V}\end{aligned}
$$<p>
其中 $W^{Q},W^{K}\in\mathbb{R}d_{model}\times d_{k},W^{Q},W^{V}\in\mathbb{R}d_{model}\times d_{v}$</p>
</li>
<li>
<p>根据 QKV 计算 Attention
每个查询向量会与所有键向量进行相似度计算 (一般采用 scaled inner product)，从而获得权重，然后利用这些权重对所有值向量进行加权求和。</p>
</li>
</ol>
$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p><br>在多头注意力中，为了增加模型的表达能力，通常将 Q、K 和 V 通过多个不同的线性变换矩阵进行多次计算，得到多个注意力头 (Attention Heads). 每个头的计算是独立的，但它们的结果会在最后进行拼接并经过线性变换。最终的 Multi-Head Attention 公式为：</p>
$$
\text{MultiHead}(Q,K,V)=\text{Concat}(head_1,head_2,\ldots,head_h)W^O
$$<p><br>每个头 $head_i$ 计算公式为</p>
$$
\text{MultiHead}(Q,K,V)=\text{Concat}(head_1,head_2,\ldots,head_h)W^O
$$<p><br>这里的 $W^{Q}_{i},W^{K}_{i},W^{V}_{i}$ 是为每个头学习到的不同权重矩阵，$W^O$ 是输出的线性变换矩阵。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB85e0bf86b5d9f2c649bbc3f08c03d203?method=download&amp;shareKey=b5e662d324237709f786beb08c27b774" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB85e0bf86b5d9f2c649bbc3f08c03d203?method=download&amp;shareKey=b5e662d324237709f786beb08c27b774" alt="Multi-Head Attention">
    </a><figcaption>Multi-Head Attention</figcaption></figure></p>
<p>Decoder 中的 Masked MHA 确保模型只能在解码序列的当前位置及其之前的位置上操作，而不能 “看到” 将要生成的未来信息。与标准的 MHA 相同，注意力分数 $\mathrm{Attention Scores}=\frac{QK^T}{\sqrt{d_k}}$ 是通过 Q 和 K 的点积计算得到的。计算完成后我们给其加上一个下三角元素 (包含主对角线) 为 0，上三角元素为 —∞ 的 mask，这样未来的信息经过 Softmax 后的权重为 0，被完全屏蔽。</p>
<h2 id="grouped-query-attentiongqa-multi-query-attention-mqa">Grouped Query Attention（GQA）&amp; Multi-query Attention (MQA)</h2>
<p><a href="https://arxiv.org/pdf/2305.13245">GQA</a> 将多个 Q 分成若干组，每一组共享相同的权重矩阵。这使得每组查询可以共同处理同一个 K 和 V，降低了计算量和内存需求。在 MHA 中，所有的头共享相同的输入 X，但使用不同的投影矩阵来生成 K 和 V. GQA 中 K 和 V 通常是对输入 X 进行一次性线性变换，并在所有同一分组中的 Q 共享。MQA 更为极端，所有的 Q 共享一个 K 和 V.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3c7dc003db55abf4b8a1ebeb4aabd667?method=download&amp;shareKey=f1570d975432b38d6f74742e9bb4cf6e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3c7dc003db55abf4b8a1ebeb4aabd667?method=download&amp;shareKey=f1570d975432b38d6f74742e9bb4cf6e" alt="Overview of MHA, GQA &amp; MQA">
    </a><figcaption>Overview of MHA, GQA &amp; MQA</figcaption></figure></p>
<h2 id="multi-head-cross-attention">Multi-Head Cross Attention</h2>
<p>Multi-Head Cross Attention 是 Transformer Decoder 中的一个核心组件。与 Self-Attention 不同，Cross Attention 负责将解码器的隐藏状态与编码器的输出上下文信息进行交互，允许解码器的每一个解码时间步的状态 <strong>查看整个编码器的输出</strong>。每个解码的时间步 t，Decoder 的隐藏状态作为 Q，Encoder 的输出作为 K 和 V，计算过程与 标准的 Self-Attention 相同。</p>
<h1 id="evolution-tree-of-transformer">Evolution Tree of Transformer</h1>
<p>后续的研究逐渐把 Encoder 和 Decoder 分离开来，形成 Encoder-Only 和 Decoder-Only 的模型。如下图所示</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa2db49ee75b563db2d846dab14947060?method=download&amp;shareKey=12514a3314f3bb4c5e30936c2d634650" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa2db49ee75b563db2d846dab14947060?method=download&amp;shareKey=12514a3314f3bb4c5e30936c2d634650" alt="Transformer Evolution Tree">
    </a><figcaption>Transformer Evolution Tree</figcaption></figure></p>
<h2 id="feed-forward-network">Feed Forward Network</h2>
<p>FFN 是一个两层的前馈全连接网络，中间有一个非线性激活函数。第一层全连接将 $d_model$ 映射到 $4d_model$ ，经过非线性激活函数后，第二层全连接再重新映射回 $d_model$.</p>
<h1 id="decoder-only-transformer">Decoder-Only Transformer</h1>
<p>Decoder-Only 删除了原先 Transformer Encoder 的部分以及 Encoder 和 Decoder 进行 Cross Attention 的部分。它具有三个必要的特征:</p>
<ol>
<li>在给定编码器输入作为上下文的情况下基于迄今为止生成的 token 自动回归预测下一个。</li>
<li>在评估对输入序列的 Q 时看不到未来值。这就是为什么仅解码器的模型通常被称为 Casual Language Model (CLM).</li>
<li>训练模型以在给定当前输入序列的情况下预测下一个 token. 这种训练方法与回归相结合，允许模型自回归生成任意长 (最高达输入序列的最大长度) 的序列。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa6c37075488053053efa01808163d0ba?method=download&amp;shareKey=5542015805dbda24ff7ab5dbf44a368b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa6c37075488053053efa01808163d0ba?method=download&amp;shareKey=5542015805dbda24ff7ab5dbf44a368b" alt="Decoder-only (left) and Encoder-only (right) Transformer Architectures">
    </a><figcaption>Decoder-only (left) and Encoder-only (right) Transformer Architectures</figcaption></figure></p>
<h1 id="llama-transformer-architecture">LLaMA Transformer Architecture</h1>
<p>LLaMA Transformer 结构如下，主要有以下变化</p>
<ol>
<li>使用 RoPE (Rotary Position Embedding) 替代传统的位置编码。</li>
<li>RMSNorm 替代 LayerNorm</li>
<li>引入 Gated Linear Unit (GLU)</li>
</ol>
<h2 id="rotary-position-embedding">Rotary Position Embedding</h2>
<p>传统的 Transformer 模型使用可学习的绝对位置编码 (如 sinusoidal position embedding)，但 RoPE 采用了旋转矩阵的思想，将位置编码与输入的 token 表示直接结合，而不依赖于额外的可学习参数。</p>
<p>输入向量的旋转角度为 $\theta(p,i)=p\cdot10000^{-2i/d}$. p 表示位置索引，i 表示维度索引，d 为向量的总维度。对于输入的 token 向量 x 中的每一对偶数和奇数维度 $(x_{2i},x_{2i+1})$，旋转操作可以用 2D 旋转矩阵表示为</p>
$$\begin{bmatrix}x_{2i}^{\prime}\\x_{2i+1}^{\prime}\end{bmatrix}=\begin{bmatrix}\cos(\theta)&-\sin(\theta)\\\sin(\theta)&\cos(\theta)\end{bmatrix}\cdot\begin{bmatrix}x_{2i}\\x_{2i+1}\end{bmatrix}$$<p><br>对于输入的 token 向量 $\mathbf{x}\left[x_{0},x_{1},x_{2},x_{3},\cdots,x_{d-1}\right]$, RoPE 将其两两一组配对，每一组都会与位置相关的旋转角度 θ 对应地应用旋转操作。这个过程的本质是对输入 token 的表示做了旋转变换，使得这些特征不仅依赖于输入的特征，还隐含了该 token 在序列中的位置。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf24aca24d7ff8bc2901ca4983cbf6c47?method=download&amp;shareKey=9ac054d415fe2e172bb8a719935d4793" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf24aca24d7ff8bc2901ca4983cbf6c47?method=download&amp;shareKey=9ac054d415fe2e172bb8a719935d4793" alt="RoPE">
    </a><figcaption>RoPE</figcaption></figure></p>
<h2 id="rmsnorm">RMSNorm</h2>
<p>RMSNorm 相对于 LayerNorm 去掉了均值计算，仅基于输入的均方根进行归一化 $\mathrm{RMSNorm}(\mathbf{x})=\frac{\mathbf{x}}{\mathrm{RMS}(\mathbf{x})+\epsilon}\cdot\gamma$</p>
<p>其中</p>
<ul>
<li>$\mathrm{RMS}(\mathbf{x})=\sqrt{\frac1d\sum_{i=1}^dx_i^2}$ 为输入的均方根。</li>
<li>$\gamma{:}$ 为可学习的缩放参数。</li>
<li>$\epsilon{:}$ 为防止除以 0 的小数。</li>
</ul>
<h2 id="silu">SiLU</h2>
<p>SiLU (Sigmoid Linear Unit) 是一种激活函数，也称为 Swish，其定义为输入 x 和 Sigmoid 函数输出的乘积。其定义为
</p>
$$\mathrm{SiLU}(x)=x\cdot\sigma(x)$$<p>
其中 $\sigma(x)=\frac1{1+e^{-x}}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB552a846c520bf2b5194c621e7b8e224e?method=download&amp;shareKey=519f3a1e4cce59da1895fa7bc2bcc842" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB552a846c520bf2b5194c621e7b8e224e?method=download&amp;shareKey=519f3a1e4cce59da1895fa7bc2bcc842" alt="SiLU">
    </a><figcaption>SiLU</figcaption></figure></p>
]]></content:encoded>
    </item>
    <item>
      <title>ZeRO, ZeRO-Offload, ZeRO-Infinity</title>
      <link>http://localhost:1313/blogs/zero/</link>
      <pubDate>Sat, 07 Jun 2025 21:11:32 +0800</pubDate>
      <guid>http://localhost:1313/blogs/zero/</guid>
      <description>Paper reading of ZeRO.</description>
      <content:encoded><![CDATA[<h1 id="zero">ZeRO</h1>
<p>Zero 用于优化内存，极大地提高了训练速度，同时增加了可以训练的模型大小。ZeRO 消除了数据和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度，能够以持续的高效率按设备数量等比例扩展可训练模型的大小。</p>
<h2 id="introduction">Introduction</h2>
<p>ZeRO 首先总结了下当前并行方法存在的问题</p>
<ul>
<li>Basic DP: 没有减少每个设备的内存，在 32GB 内存的 GPU 上训练超过 1.4B 参数的模型便会 OOM.</li>
<li>Model Parallelsim (MP): 切分了每一层的计算和激活到每个设备上，但引入了大量的通信 (前向和反向都需要 2xAll-Reduce)，因此扩展性差，通常只在一个节点内的高带宽连接的 GPU 中进行。在 DGX-2 节点训练 40B 参数的模型每个 V100 GPU 仅能达到硬件峰值的 5% 算力 (5T flops).</li>
</ul>
<p>模型状态通常占据了训练时的大部分内存，但 DP 在所有数据并行进程中保存一份模型状态，导致冗余内存消耗；而 MP 对这些状态进行切分以获得高内存效率，但通常会导致过于细粒度的计算和昂贵的通信，扩展效率较低。此外，这些方法都静态地维护整个训练过程所需的<strong>整个模型状态</strong>。</p>
<p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO-DP</a> 通过在数据并行过程中划分模型状态 (参数、梯度和优化器状态) 消除了数据并行过程中的内存冗余。</p>
<p>结论：如下图所示 ZeRO-DP 有三个主要的优化阶段，它们对应于优化器状态、梯度和参数的划分。
对于使用 FP16 的模型，内存占用包括参数 (FP16)、梯度 (FP16)、Adam 优化器状态 (动量 (FP32)，方差 (FP32) 以及更新后的参数 (FP32), 因此 K=12).</p>
<ol>
<li>优化器状态划分 (Pos) —— 内存减少 4 倍，需要对梯度进行 reduce-scatter，用各自的优化器状态更新梯度后进行 All-gather 使所有设备都有最新的梯度，通信量与数据并行性相同 (对 Loss 进行一次 All-reduce).</li>
<li>添加梯度划分  (Pos+g) &ndash; 内存减少 8 倍，每个设备需要将自己的梯度 scatter 到负责更新那部分参数的设备上，然后使用 Gather 将其他设备更新后的模型参数同步到自己上面，通信量与数据并行性相同。</li>
<li>添加参数划分 (Pos+g+p) &ndash; 内存减少与数据并行度 Nd 呈线性关系。通信量增加了50%，因为在前向/反向传播中需要每个设备需要额外广播自己存储的模型参数 <code>2*(N-1)/N*P</code>，反向传播时需要对发送梯度到对应的设备上 <code>(N-1)/N*P</code>.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBcfab82173b0f76eb5b3c8396e81e238a?method=download&amp;shareKey=1b8bb86256be5b15bec039beecee062b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBcfab82173b0f76eb5b3c8396e81e238a?method=download&amp;shareKey=1b8bb86256be5b15bec039beecee062b" alt="Memory Savings and Communication Volume for the 3-stage of ZeRO">
    </a><figcaption>Memory Savings and Communication Volume for the 3-stage of ZeRO</figcaption></figure></p>
<p>激活、临时缓冲区和不可用的内存片段会成为次要内存瓶颈。作者开发了 ZeRO-R 优化了这三个因素分别消耗的剩余内存。</p>
<ol>
<li>对于激活 (在前向传播中存储，反向传播中使用)，仅仅使用激活检查点是不够的。ZeRO-R 通过激活划分识别和删除现有 MP 方法中重复存储的激活，并且在适当时候将激活存储在 CPU 中。</li>
<li>ZeRO-R 定义了适当大小的临时缓冲区，以实现内存和计算效率的平衡。</li>
<li>由于不同张量的寿命存在差异，ZeRO-R 根据张量的不同生命周期主动管理内存，防止内存碎片。</li>
</ol>
<p>在某些情况下，MP 仍可以和 ZeRO 一起使用：i）当与 ZeRO-R 一起使用时，MP 可以减少超大模型的激活内存占用。ii）对于较小模型，当单独使用 DP 的 batchsize 太大而无法实现良好的收敛时，MP 也可以带来好处。</p>
<h2 id="where-did-all-the-memory-go">Where Did All the Memory Go?</h2>
<p>在模型训练期间，大部分内存被模型状态消耗 (优化器状态、梯度和参数). 除了这些模型状态之外，剩余的内存被激活、临时缓冲区和碎片内存所消耗，称之为剩余状态。</p>
<h3 id="model-states-optimizer-states-gradients-and-parameters">Model States: Optimizer States, Gradients and Parameters</h3>
<p>Adam 优化器需要存储两个优化器状态：时间平均动量和梯度方差来计算更新后的参数。此外，还需要有足够的内存来存储梯度和权重本身。</p>
<p><strong>混合精度训练 (Mixed-Precision Training)</strong> 中参数和激活以 fp16 格式存储并且在前向和反向传播中也使用 fp16 格式的权重和激活。Adam 优化器存储 fp32 格式的参数副本、动量和方差以保证更新的精度。</p>
<p>假设模型参数量为 ψ，模型参数需要占用 2ψ 字节的内存，反向传播中产生的 fp16 梯度需要占用 2ψ 字节的内存。Adam 优化器存储 fp32 格式的参数副本、动量和方差每个都需要占用 4ψ 字节的内存。因此训练时总共需要 16ψ 字节的内存，为存储模型参数的 8x.</p>
<h3 id="residual-memory-consumption">Residual Memory Consumption</h3>
<p>在训练过程中，<strong>激活</strong>会占用大量的内存。基于 transformer 的模型的激活内存占用与层数×隐藏维度×序列长度×批大小成正比。对于类似 GPT-2的结构，总激活约为 12×隐藏亮度×批大小×序列长度×变层数 (<code>QKV(h*3h) + O(h*h) + MLP(h*4h+4h*h)=12h*h</code>，没有考虑 mask). 激活重计算可以以 33% 的额外计算开销 (之前是一次前向，一次反向，反向因为需要对输入和参数都进行求导所以计算量是前向的两倍，现在多了一次前向) 换取接近原先激活大小平方级别的内存占用。</p>
<p>对于大型模型，用于存储中间结果的<strong>临时缓冲区</strong>会消耗大量内存。对梯度进行 All-Reduce 或梯度归一化计算等操作倾向于在操作之前将所有梯度融合到单个扁平缓冲区中，以提高吞吐量。</p>
<p><strong>碎片化内存</strong>会导致即使有足够的内存但没有足够大的连续块进行分配时的 OOM，作者观察到极端情况下在有 30% 剩余内存时也会产生 OOM.</p>
<h2 id="zero-insight-and-overview">ZeRO: Insight and Overview</h2>
<p>ZeRO有两组优化：ZeRO-DP 旨在减少模型状态的内存占用；ZeRO-R 旨在减少剩余内存消耗。</p>
<p>ZeRO-DP 基于三个关键见解：</p>
<ol>
<li>DP 比 MP 具有更好的扩展效率，因为 MP 减少了计算的粒度，同时也增加了通信开销。</li>
<li>DP 内存效率低下，因为模型状态被在所有数据并行进程中都存有一份。</li>
<li>DP 和 MP 都保留了整个训练过程中所需的所有模型状态，但并非所有状态在整个训练期间都需要。</li>
</ol>
<p>ZeRO-DP 划分模型状态，并使用动态通信调度利用模型状态的内在的暂时性，同时最小化通信量。</p>
<p>ZeRO-R 基于两个关键见解：</p>
<ol>
<li>MP 对模型状态进行切分，但通常需要重复存储激活。</li>
<li>对于GPT-2或更大的模型，算术强度 (每次迭代计算量与激活检查点数量的比值) 非常大 (≥10K)，并且随着隐藏维数的增加而线性增加，即使在带宽较低的情况下，也可以隐藏激活检查点的数据移动成本。</li>
</ol>
<p>ZeRO 通过跨 GPU 划分激活检查点来消除 MP 中的内存冗余，并根据需要使用 All-Gather 来重建；使用恒定大小的缓冲区来避免临时缓冲区随着模型大小的增加而爆炸；通过将激活检查点和梯度移动到预分配的连续内存缓冲区来执行动态内存碎片整理。</p>
<h2 id="deep-dive-into-zero-dp">Deep Dive into ZeRO-DP</h2>
<p>下表显示了逐渐切分 (1) 优化器状态、(2) 梯度和 (3) 参数冗余后的内存占用。称为ZeRO-DP的三个优化阶段：Pos， Pg和Pp，将在下面详细说明。</p>
<table border="1" cellspacing="0" cellpadding="5">
  <thead>
    <tr>
      <th rowspan="2">DP</th>
      <th colspan="3">7.5B Model (GB)</th>
      <th colspan="3">128B Model (GB)</th>
      <th colspan="3">1T Model (GB)</th>
    </tr>
    <tr>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>120</td>
      <td>120</td>
      <td>120</td>
      <td>2048</td>
      <td>2048</td>
      <td>2048</td>
      <td>16000</td>
      <td>16000</td>
      <td>16000</td>
    </tr>
    <tr>
      <td>4</td>
      <td>52.5</td>
      <td>41.3</td>
      <td><b>30</b></td>
      <td>896</td>
      <td>704</td>
      <td>512</td>
      <td>7000</td>
      <td>5500</td>
      <td>4000</td>
    </tr>
    <tr>
      <td>16</td>
      <td>35.6</td>
      <td><b>21.6</b></td>
      <td>7.5</td>
      <td>608</td>
      <td>368</td>
      <td>128</td>
      <td>4750</td>
      <td>2875</td>
      <td>1000</td>
    </tr>
    <tr>
      <td>64</td>
      <td><b>31.4</b></td>
      <td>16.6</td>
      <td>1.88</td>
      <td>536</td>
      <td>284</td>
      <td><b>32</b></td>
      <td>4187</td>
      <td>2218</td>
      <td>250</td>
    </tr>
    <tr>
      <td>256</td>
      <td>30.4</td>
      <td>15.4</td>
      <td>0.47</td>
      <td>518</td>
      <td>263</td>
      <td>8</td>
      <td>4046</td>
      <td>2054</td>
      <td>62.5</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>30.1</td>
      <td>15.1</td>
      <td>0.12</td>
      <td>513</td>
      <td>257</td>
      <td>2</td>
      <td>4011</td>
      <td>2013</td>
      <td><b>15.6</b></td>
    </tr>
  </tbody>
</table>
<h3 id="pos-optimizer-state-partitioning">Pos: Optimizer State Partitioning</h3>
<p>设 DP 并行度为 Nd, 每个数据并行进程只需要存储和更新总优化器状态的 1/Nd，然后只更新参数的 1/Nd. 在每个训练步骤结束时，在数据并行进程中执行一次 All-Gather，以获得所有数据并行过程中完全更新的参数。这使得每个设备上保存模型状态需要的内存从 4ψ+Kψ 变成 4ψ+Kψ/Nd，当使用 Adam 优化器 (K=12) 并且 Nd 很大时，内存需求可以降低接近 4x.</p>
<h3 id="pg-gradient-partitioning">Pg: Gradient Partitioning</h3>
<p>由于每个数据并行进程只用更新自己被分配的参数，因此他也只需要那部分参数 reduce 后的梯度。只在负责更新相应参数的数据并行过程中进行 reduce. 完成后它们的内存可以被释放。这使得了梯度所需的内存占用从 2Ψ 字节减少到 2Ψ/Nd. 更新后的参数再被 scatter 到其他进程。</p>
<p>通常为了效率，将需要 reduce 的梯度按照参数的分区划分成多个 buckets，每个 bucket 对应特定的一组参数，对每个 bucket 进行整体 reduce 操作，而不是对单个梯度进行操作。进一步划分梯度后，每个设备上保存模型状态需要的内存进一步减少到 2ψ+(K+2)ψ/Nd</p>
<p>蓝色箭头串起来的白色长方形代表的是 Transformer Block，蓝色的第一行代表 FP16 参数；橙色的第二行代表 FP16 梯度，反向传播时将用于更新参数；绿色的行代表优化器状态 (FP32 的梯度，动量，方差，以及更新后的参数)，其中在计算完 FP16 梯度以后不再需要保存 FP32 参数。同时也需要 buffer 来保存部分 transformer block 的输出激活。</p>
<h3 id="pp-parameter-partitioning">Pp: Parameter Partitioning</h3>
<p>更进一步我们可以将模型参数也进行划分，当设备所没有的参数需要进行向前和向后传播时，通过广播从其他的的数据并行进程接收。通过前面的分析可知这使得通信量变为原来的 1.5x， 但使得所有的模型参数都被划分到每个设备上，只需要 (4+K)/Nd 字节的内存。</p>
<h3 id="execution-steps-of-zero3">Execution Steps of ZeRO3</h3>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fd41e8b92bcd256a910ce757d4eea21?method=download&amp;shareKey=d82f49f0d59309d987c164a100966895" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fd41e8b92bcd256a910ce757d4eea21?method=download&amp;shareKey=d82f49f0d59309d987c164a100966895" alt="Overview of Memory Consumption">
    </a><figcaption>Overview of Memory Consumption</figcaption></figure></p>
<p>每个 GPU 只需要保存自己部分的 Pos+g+p. 前向传播时保存对应模型参数的 GPU 需要把参数广播到其他 GPU 中，其他 GPU 用自己部分的数据完成前向传播后就可以删除这部分参数 (最后一部分除外). <code>(N-1)/N*P</code></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB82b66a8a7b40a2fdf9512545189cc37a?method=download&amp;shareKey=37a4614d85a77d2ca00e6d5a4769e2f9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB82b66a8a7b40a2fdf9512545189cc37a?method=download&amp;shareKey=37a4614d85a77d2ca00e6d5a4769e2f9" alt="Broadcast of Model Parameters">
    </a><figcaption>Broadcast of Model Parameters</figcaption></figure></p>
<p>前向传播完成后，第一次反向传播可以利用最后一次正向传播已经广播了的模型参数，每个 GPU 计算自己部分的梯度，然后 Reduce 到存储对应模型参数的 GPU 中。之后和前向传播一样，每个 GPU 都需要广播自己的参数，然后其他 GPU 用自己的数据完成梯度计算以后 Reduce 到自己的梯度。<code>(N-1)/N*P + 1/N*G*(N-1)</code></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB575b3869e59814ae1449351cf1b18d01?method=download&amp;shareKey=10f29390f47227ca8eefbcc00f4fca6e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB575b3869e59814ae1449351cf1b18d01?method=download&amp;shareKey=10f29390f47227ca8eefbcc00f4fca6e" alt="Gradient Accumulation">
    </a><figcaption>Gradient Accumulation</figcaption></figure></p>
<p>反向传播结束以后，每个 GPU 使用优化器更新自己的 FP32 模型参数后转换成 FP16 格式。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB19cb21dfa63ab76437a2246ff52b00aa?method=download&amp;shareKey=ddb63d57f3bf976b1dce4596e77a2009" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB19cb21dfa63ab76437a2246ff52b00aa?method=download&amp;shareKey=ddb63d57f3bf976b1dce4596e77a2009" alt="Update Parameters Locally">
    </a><figcaption>Update Parameters Locally</figcaption></figure></p>
<h2 id="deep-dive-into-zero-r">Deep Dive into ZeRO-R</h2>
<h3 id="pa-partitioned-activation-checkpointing">Pa: Partitioned Activation Checkpointing</h3>
<p>一旦计算了模型的一层的前向传播，输入激活将在所有模型并行过程中进行划分，直到在反向传播期间再次需要输入激活。此时，ZeRO 使用一个 All-Gather 操作来重新实现激活的复制副本。称这个优化为 Pa. 将 Pa 与激活检查点结合，只存储分区的激活检查点，这样使得激活占用空间的减少与 MP 并行度成正比。</p>
<h3 id="cb-constant-size-buffers">CB: Constant Size Buffers</h3>
<p>通信的效率不仅仅与数据量相关，还受到固定启动开销和带宽利用率的影响。较大的输入更容易充分利用硬件的带宽和优化机制，因而能显著提高 All-Reduce 操作的效率。因此经常将需要进行通信的数据合并到一个缓冲器。然而，合并缓冲区的内存开销与模型大小成正比，模型过大时容易 OOM. 为了解决这个问题，<strong>当模型很大时，简单地使用一个性能高效的固定大小的合并缓冲区</strong>。</p>
<h3 id="md-memory-defragmentation">MD: Memory Defragmentation</h3>
<p>前向传播中只需要保存检查点的激活而丢弃其他激活会产生碎片化内存。同样的反向传播中只需要保存参数的梯度而丢弃激活的梯度也会产生碎片化内存。内存碎片导致两个问题: (1) 即使有足够的可用内存，由于缺乏连续内存导致 OOM. (2) 由于内存分配器花费大量时间搜索连续内存块以满足内存请求而导致效率低下。ZeRO 通过<strong>为激活检查点和梯度预分配连续内存块，并在它们产生时将它们复制到预分配的内存中</strong>，从而实时地进行内存碎片整理。</p>
<h2 id="communication-analysis-of-zero-dp">Communication Analysis of ZeRO-DP</h2>
<p>使用 Pos 和 Pg 时，ZeRO-DP 不会产生额外的通信，同时可以减少高达 8 倍的内存。使用 Pos+g+p 时，ZeRO-DP 最多会产生 1.5 倍的通信，同时减少内存占用为原来的 1/Nd.</p>
<p>在数据并行训练过程中，在计算下一步的更新之前，在反向传播结束时对所有数据并行进程的梯度使用 All-Reduce 进行平均，因此通信量为 2ψ. 使用 Pos+g 时每个设备需要将自己的梯度 scatter 到负责更新那部分参数的设备上，然后使用 Gather 将其他设备更新后的模型参数同步到自己上面，总通信量仍为 2ψ，与数据并行相同。使用 Pos+g+p 时负责该分区的数据并行进程将权重 brocast 给所有数据并行进程 (前向反向各一次)，最后仍需要 Gather 其他进程上更新好的参数，因此总通信量为 3ψ.</p>
<h2 id="communication-analysis-of-zero-r">Communication Analysis of ZeRO-R</h2>
<p>在使用激活检查点的 Megatron-LM 中，每个 transformer block 在前向传播中执行 2 次大小为 批大小×序列长度×隐藏维度的 All-Reduce 操作，反向传播中执行 2 次同样大小的 All-Reduce 操作，同时激活重计算也需要 2 次同样大小的 All-Reduce 操作。因此每个块的总通信量为 12×序列长度×隐藏维度。</p>
<p>当使用 ZeRO-R 划分激活检查点时，在对每个激活检查点上的反向传播进行前向重新计算之前，需要进行额外的一次 All-Gather 操作。因此，Pa的总通信开销相对于原先 MP 通信量增加了 1/12，但是使得激活内存占用减小到原来的 1/MP_degree.</p>
<p>如果使用了 Pa+cpu，则分区激活检查点将被存储到 CPU，对激活内存需求减少到几乎为零，而代价是与 Pa 相比，需要从 CPU 和内存之间的数据移动增加了 2 倍。</p>
<h1 id="zero-offload">ZeRO-Offload</h1>
<p>ZeRO-Offload 通过将数据和计算下放到 CPU 来实现大型模型训练。为了保持计算效率，它尽可能减少数据在 GPU 和 CPU 之间的移动，同时最大限度地减少 CPU 的计算时间，并最大限度地节省 GPU 上的内存。</p>
<h2 id="introduction-1">Introduction</h2>
<p>PP, MP 和 ZeRO 等并行技术都需要有足够的 GPU 设备，使得它们的内存之和能够容纳训练所需的模型状态的存储。目前基于注意力的大模型训练的主要内存瓶颈是模型状态，而不是激活。现有的异构训练在两个主要方面受到限制：(i) 几乎所有的训练都利用 CPU 内存，而不是 CPU算力。(ii) 它们主要是为单个 GPU 设计和评估的。</p>
<p>ZeRO-Offload 为了提高计算效率采取的设计原则有三条：(i) 它需要的 CPU 计算量与 GPU 相比减少了几个数量级。(ii) 它最小化了 CPU 和 GPU 之间的通信量，防止了通信成为瓶颈。(iii) 可以证明在实现最小通信量的同时最大限度地节省了 GPU 的内存。</p>
<p>ZeRO-Offload 将梯度，优化器状态和优化器计算卸载到 CPU，而将参数和前向和反向计算保留在 GPU上。这样 CPU 上的计算量为 O(M)，而 GPU 上的计算量则为 O(MB)，其中 M 和 B 分别为模型大小和 batchsize. 因为 CPU 只处理模型参数的更新，而不参与与 batch size 相关的梯度求平均的操作。在大多数情况下，batchsize 较大，因此 CPU 计算不是瓶颈。但是对于较小的 batchsize，CPU 计算可能会成为瓶颈。</p>
<h2 id="unique-optimal-offload-strategy">Unique Optimal Offload Strategy</h2>
<p>为了确定最佳的卸载策略，ZeRO-Offload 将 DL 训练建模为如下图所示的数据流，并有效地在 CPU 和 GPU 设备之间进行划分。GPU 和 CPU 之间的卸载策略可以使用该图的二分图来表示，这样一个分区中的计算节点将在拥有该分区的设备上执行，分区中的数据节点也存储在拥有该分区的设备上。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc55ea0bd058b2e603052658a6cb25aa6?method=download&amp;shareKey=e2474b8904eff6869bcdce3b09e545f6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc55ea0bd058b2e603052658a6cb25aa6?method=download&amp;shareKey=e2474b8904eff6869bcdce3b09e545f6" alt="The Dataflow of Fully Connected Neural Networks">
    </a><figcaption>The Dataflow of Fully Connected Neural Networks</figcaption></figure></p>
<p>由于 CPU 的算力远远低于 GPU，所以前向传播和反向传播 (它们的计算复杂度都是 O(MB)) 必须在 GPU上完成，而其余复杂度为 O(M) 的计算 (如归一化计算、权重更新等) 会被卸载到 CPU 上。</p>
<p>CPU 内存带宽 (100xGB) 至少比 CPU 和 GPU 之间的 PCIe 带宽 (10xGB) 快一个数量级，而 GPU 内存带宽比 CPU 内存带宽 (TB) 快另一个数量级。数据流中的每个节点都是环的一部分。因此，对该图进行任何划分都需要切割至少两条边，每条边的权值至少为 2M，从而总通信量至少 4M (通过仅卸载部分模型状态，可以进一步减少通信量). 因此，为了实现最小的通信量，所有卸载策略必须使得关于 fp32 模型状态操作的生产者和消费者相同。fp16 参数节点必须和 FWD-BWD 节点在一个子图中，因为这两个节点之间的边权值是 4M.</p>
<p>下表显示了最小化通信量情况下的所有有效分区策略所节省的内存。通过将 fp16 梯度和 Update Super 节点放到 CPU 可以实现 8x 的最大内存节省。</p>
<table>
  <thead>
      <tr>
          <th>FWD-BWD</th>
          <th>p16</th>
          <th>g16</th>
          <th>Update</th>
          <th>Memory</th>
          <th>Reduction</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>gpu</td>
          <td>gpu</td>
          <td>16M</td>
          <td>1x (baseline)</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>cpu</td>
          <td>gpu</td>
          <td>14M</td>
          <td>1.14x</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>4M</td>
          <td>4x</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>2M</td>
          <td>8x</td>
      </tr>
  </tbody>
</table>
<p>综上所述 ZeRO-Offload 在 CPU 上存储所有 fp32 模型状态以及 fp16 梯度，并且还在 CPU 上计算更新后的参数。fp16 的参数保存在 GPU 上，前向和反向计算也在GPU上完成。</p>
<h2 id="zero-offload-schedule">ZeRO-Offload Schedule</h2>
<p>在训练过程中，首先通过前向传播计算损失。由于 fp16 参数已经存放在GPU上，因此这部分计算不需要与 CPU 通信。在损失的反向传播过程中，不同设备计算不同参数的梯度。ZeRO-Offload 可以在计算完每个参数后，将这些梯度单独或分组传输到 CPU 内存中。由于梯度是逐层传输的，因此 GPU 上只需要很小的缓冲区来存放每一层的梯度。在反向传播之后，ZeRO-Offload 直接在 CPU 上更新 fp32 参数和优化器状态），并将更新后的 fp32 参数从 CPU 内存复制到 GPU 内存上的 fp16 参数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB810cb8d722c2c9e8e140b80084c47cbe?method=download&amp;shareKey=6ab97f8c087ad948e03121afde1c266a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB810cb8d722c2c9e8e140b80084c47cbe?method=download&amp;shareKey=6ab97f8c087ad948e03121afde1c266a" alt="ZeRO-Offload Training Process on a Single GPU">
    </a><figcaption>ZeRO-Offload Training Process on a Single GPU</figcaption></figure></p>
<p>在卸载之前进行如上一节所述的划分的主要好处是，对于具有超过 1 个 GPU 的系统，每个数据并行进程只负责更新参数的一个子集。所有数据并行进程的 GPU 到 CPU 的通信量总和保持不变，CPU 资源可以并行使用，共同计算单个权重更新。ZeRO-Offload 在不同 GPU 之间划分梯度和优化器状态，每个 GPU 将其拥有的部分卸载到 CPU 内存中，并在整个训练过程中将其一直保存在那里。在反向传播过程中，在 GPU上使用 reduce-scatter 计算普遍复核一遍梯度，每个 GPU 只将属于其那一部分的平均梯度卸载到 CPU 内存中。然后优化器状态将由每个数据并行进程直接在 CPU 上并行更新。更新后，参数被移回 GPU，然后在 GPU 上执行类似于 ZeRO-2 的 All-Gather 操作来获取所有更新后的参数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6e4ed2b9f7f37bf8a5f8f9326bb98971?method=download&amp;shareKey=0bf3ca60c8edb9a602a3ae93262c0967" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6e4ed2b9f7f37bf8a5f8f9326bb98971?method=download&amp;shareKey=0bf3ca60c8edb9a602a3ae93262c0967" alt="ZeRO-Offload Data Placement with Multiple GPUs">
    </a><figcaption>ZeRO-Offload Data Placement with Multiple GPUs</figcaption></figure></p>
<h2 id="optimized-cpu-execution">Optimized CPU Execution</h2>
<ol>
<li>作者使用高性能计算技术实现了一个加速版的 CPU Adam 优化器</li>
<li>开发了一个一步延迟参数更新计划，将 CPU 参数更新计算与 GPU 上的前向和反向计算重叠，隐藏了 CPU 执行时间。</li>
</ol>
<h3 id="implementing-the-cpu-optimizer">Implementing the CPU Optimizer</h3>
<p>作者使用三级并行性来提高 CPU 优化器的性能。</p>
<ol>
<li>SIMD 矢量指令，充分利用 CPU 架构的硬件并行性。</li>
<li>循环展开，一种提高指令级并行性的有效技术，能更好地利用内存带宽。</li>
<li>OMP 多线程，可以有效地并行利用 CPU 上的多个内核和线程。</li>
</ol>
<p>算法的输入为 β₁(动量系数), β₂(RMSProp 的平方梯度衰减系数), α(学习率)，以及梯度，动量，方差和 fp32 参数作为输入。我们还使用了一些特定于实现的参数，如 simd_width 和 unroll_width. Adam 优化器分别发送更新的方差、动量和参数的 fp16 和 fp32 格式到 GPU 和 CPU. 首先将数据读入矢量寄存器。然后，主循环中使用 Fused Multiplication Add 矢量操作。其他操作，如乘法、除法和平方根，也在矢量模式下运行。为了获得最佳性能，使用 AVX512 simd 指令集和基于自动调优结果的 unroll_width=8. 除了 CPU-Adam 优化器之外，还以分块的方式实现了 CPU 到 GPU 的 fp16 参数复制。通过并行化 Adam 计算并将参数复制到 GPU 来重叠 CPU 和 GPU 的执行。<strong>当在 CPU 上处理当前数据块的 Adam 计算时，将先前处理过的数据块的参数写回 GPU.</strong></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7e25392bc19670dfd7b80cc9a84a5d73?method=download&amp;shareKey=ac89809b9f51717d9a5429d1cd5d9865" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7e25392bc19670dfd7b80cc9a84a5d73?method=download&amp;shareKey=ac89809b9f51717d9a5429d1cd5d9865" alt="CPU-ADAM Optimizer">
    </a><figcaption>CPU-ADAM Optimizer</figcaption></figure></p>
<h3 id="one-step-delayed-parameter-update">One-Step Delayed Parameter Update</h3>
<p>下图展示了 Delayed Parameter Update(DPU) 的 ZeRO-Offload 训练的工作流程。</p>
<ol>
<li>前 N−1 步不使用 DPU 进行训练，避免在梯度变化迅速的早期阶段破坏训练的稳定性。</li>
<li>在第 N 步中，从 GPU 获取梯度，但跳过 CPU 优化步骤，也不更新 GPU 上的 fp16 参数。</li>
<li>在第 N+1 步中，我们使用第 N 步的梯度计算 CPU 上的参数更新，同时使用第 N-1 步更新的参数并行计算 GPU 上的前向和反向。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc56f9c814071e1766b9f33833024d829?method=download&amp;shareKey=d5be7d41c0247d8243bddb3452eba75b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc56f9c814071e1766b9f33833024d829?method=download&amp;shareKey=d5be7d41c0247d8243bddb3452eba75b" alt="Delayed Parameter Update During the Training Process">
    </a><figcaption>Delayed Parameter Update During the Training Process</figcaption></figure></p>
<h1 id="zero-infinity">ZeRO-Infinity</h1>
<p>ZeRO-Infinity 是一种新的异构系统技术，它利用 GPU, CPU 和 NVMe 内存，在有限的资源上实现前所未有的模型扩展，并且不需要模型代码重构。</p>
<p>目前大型模型训练技术中最先进的是三维并行 (3D parallelism)，它将模型（张量切片）和流水线并行与数据并行相结合。但是 GPU 内存跟不上模型大小的增长。</p>
<p>ZeRO-Infinity 的优势如下</p>
<ol>
<li>通过同时利用 CPU 和 NVMe 内存，在有限的 GPU 资源上支持大模型训练。</li>
<li>引入了一种称为 <em>memory-centric tiling</em> 的 GPU 内存优化技术，以应对 GPU 内存无法一次放下的超大 block 情况。</li>
<li>引入了一种称作 <em>bandwidth-centric partitioning</em> 的数据分区策略，用于利用所有设备上的内存带宽，并将其与重叠通信与计算的技术结合。</li>
</ol>
<h2 id="memory-requirements">MEMORY REQUIREMENTS</h2>
<p><strong>Memory for Model States:</strong> 基于 Transformer 的模型中的参数总数主要取决于隐藏维度 (hd) 和 Transformer 层数 (nl). Transformer block 中的几乎所有参数都来自四个线性层，大小分别为：QKV_Linear(nd,3nd), O_Linear(hd, hd),MLP(hd, 4hd)+(4hd, hd). 因此一个 Transformer block 的参数量约为 <strong>12 x nl x (hd)²</strong>，因此占用的内存大小为 192 x nl x (hd)² 字节。</p>
<p><strong>Memory for Residual States:</strong> 剩余状态主要由激活内存组成，它取决于模型架构、批处理大小 (bsz) 和序列长度 (seq). 存储激活检查点所需的内存估计为 <strong>2×bsz×seq×hd×nl/ci</strong>，其中 ci(checkpoint interval) 是两个激活检查点之间的 Transformer block 的数量。</p>
<p><strong>Model State Working Memory (MSWM)</strong> 是在所有模型状态被卸载到 CPU 或 NVMe 之后，在模型中最大的单个算子上执行前向或反向传播所需的最小 GPU 内存。对于基于 Transformer 的模型，最大的算子是将隐藏维度从 hd 转换为 4hd 的线性层，因此 fp32 格式下需要 <strong>4xhdx4hd</strong> 字节的内存。</p>
<p><strong>Activation Working Memory (AWM):</strong> 是在执行实际的反向传播之前重新计算激活所需的内存，即两个连续激活检查点之间的激活大小 bsz × seq × ci × (16 × hd + 2 × attn_heads × seq) 字节。</p>
<h2 id="bandwidth-requirements">BANDWIDTH REQUIREMENTS</h2>
<p>假设没有任何计算和通信重叠的工作负载执行，我们可以使用峰值计算吞吐量 (peaktp)，数据移动带宽 (bw) 及其算术强度 (ait) 来估计训练效率。需要注意 peaktp 不是理论上的硬件峰值，而是在没有任何通信瓶颈的情况下可以达到的峰值。</p>
<p>算术强度 (AIT) 是总计算量与计算所需数据量之比。它描述了每次数据移动的计算量。</p>
<ol>
<li>compute_time = total_computation / peaktp</li>
<li>ait = total_computation / total_data_movement</li>
<li>communication_time = total_data_movement / bw = total_computation / (ait × bw)</li>
<li>efficiency = compute_time / (compute_time + communication_time) = ait x bw / (ait x bw + peaktp)</li>
</ol>
<h3 id="quantifying-ait-in-dl-training">Quantifying AIT in DL training</h3>
<p>Transformer block 中一次前向传播中的计算量可以近似为输入乘以参数大小 2 × bsz × seq × params. 反向传播则为其 2 倍。如果使用激活检查点则还需要一次额外的前向传播，因此每次迭代的总计算量为 computation_per_iter = 2 × 4 × bsz × seq × parameters = 2 × 4 × 12 × bsz × seq × nl × (hd)²</p>
<p><strong>AIT w.r.t. Parameters and Gradients:</strong> 前向和反向过程中模型参数必须从存储位置位置加载到 GPU 寄存器各次。在使用激活检查点的情况下，还需要加载一次，以便在反向传播期间重新计算。此外，梯度必须从 GPU 寄存器存储到其最终位置至少一次。因此总共要移动模型参数 4 次，总计 2 x 4 x parameters 字节。因此关于参数和梯度的计算强度为 seq x bsz.</p>
<p><strong>AIT w.r.t. Optimizer States:</strong> 优化器状态必须至少读取和写入一次。所以总的数据移动是 2 × optimizer_states，总计 2 × 16 × parameters 字节。因此关于优化器状态的计算强度为 seq x bsz / 4.</p>
<p><strong>AIT w.r.t. Activation Checkpoints:</strong> 前向传播时必须将激活检查点保存到它们的最终位置，然后在反向传播期间加载激活检查点。因此总数据移动量为 4 × nl/ci × hd × seq × bsz 字节。因此关于激活检查点的计算强度为 24 × hd × ci.</p>
<h3 id="bandwidth-requirements-1">Bandwidth Requirements</h3>
<p>通过前面的分析可知模型状态的计算强度仅取决于批大小和序列长度，激活检查点的计算强度仅取决于存储间隔和模型的隐藏维度大小。下图 a 说明当传输参数和梯度的带宽超过 70 GB/s 时，即使是最小的批处理大小，也可以实现超过 50% 的效率。图 b 说明，传输优化器状态需要近 4 倍的带宽才能达到 50% 的效率。并且<strong>优化器状态更新需要等待所有前向和反向传播结束，不能与计算重叠</strong>。图 c 说明，启用激活检查点后，即使隐藏大小为2K，2 GB/s 的带宽也能够保持 50% 以上的效率。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfc095fefa5837b449dcafbd0b9441d63?method=download&amp;shareKey=42ab0064e0986bfde564af6e879f5cd6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfc095fefa5837b449dcafbd0b9441d63?method=download&amp;shareKey=42ab0064e0986bfde564af6e879f5cd6" alt="Impact of Bandwidth on Efficiency with 70 TFlops of single GPU Peak Throughput">
    </a><figcaption>Impact of Bandwidth on Efficiency with 70 TFlops of single GPU Peak Throughput</figcaption></figure></p>
<h2 id="zero-infinity-design-overview">ZERO-INFINITY DESIGN OVERVIEW</h2>
<p>GPU 集群采用异构内存存储，除了 GPU 内存还拥有 CPU 内存以及比 GPU 内存大 50x, 比 CPU 内存大近 20x 的大规模 NVMe 存储。下图为 ZeRO-Infinity 架构，描述了第一层的反向传递的通信。将划分后的参数从慢速内存移动到 GPU，然后 All-Gather 以形成完整的层。在计算梯度之后，参数被聚合和重新划分，然后卸载到慢速内存中。层用下标表示，DP rank 用上标表示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB07d94538fa97fe4b478f44d44eb19e79?method=download&amp;shareKey=82f65d6660819491e1bec3688d2715ae" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB07d94538fa97fe4b478f44d44eb19e79?method=download&amp;shareKey=82f65d6660819491e1bec3688d2715ae" alt="A Snapshot of ZeRO-Infinity Training a Model with 2 Layers on 4 DP Ranks">
    </a><figcaption>A Snapshot of ZeRO-Infinity Training a Model with 2 Layers on 4 DP Ranks</figcaption></figure></p>
<p><strong>Efficiency w.r.t Parameter and Gradients:</strong> 现有的异构解决方案 (例如 ZeRO-Offload) 要求先将参数从 CPU 移动到拥有这些参数的 GPU，然后再进行广播。这种方式需要在每个 GPU 上使用足够大的 batchsize，以确保通信能被计算掩盖。但这带来了两个问题：</p>
<ol>
<li>对于超大规模模型，激活的内存占用会过大，甚至超过 CPU 的内存容量。</li>
<li>当扩展到数百甚至上千个 GPU 时，为了实现有效的收敛，实际的 batchsize 会变得过大。</li>
</ol>
<p><strong>Efficiency w.r.t Optimizer States:</strong> 与在前向和反向传播期间参数和梯度的产生有先后顺序不同，优化器状态可以同时更新。ZeRO-Infinity 建立在 ZeRO-3 之上，因此在将优化器状态卸载到 CPU 内存时，它还可以利用所有的 GPU 和 CPU 内存带宽以及所有 CPU 算力用于优化器状态更新。然而，使用 NVMe 卸载，需要将数据从 NVMe 传入到 CPU 内存中，再从 CPU 内存返回。由于 CPU 内存有限，必须将数据分块从 NVMe 加载到 CPU 内存，进行计算后再写回 NVMe.</p>
<p><strong>Efficiency w.r.t Activations:</strong> 在一台 DGX-2 节点上，每个 GPU 可以通过 PCIe 接口以大约 3 GB/s 的速度并行读写数据到 CPU 内存。这使得在隐藏层大小为 8K 或更大时，可以将激活检查点卸载到 CPU 内存的同时保持超过 80% 的效率。</p>
<h2 id="efficiency-optimizations">EFFICIENCY OPTIMIZATIONS</h2>
<h3 id="bandwidth-centric-partitioning">Bandwidth-Centric Partitioning</h3>
<p>在 ZeRO-3 和 ZeRO-Offload 中每层的参数为单个数据并行进程拥有，在需要时将它们广播给其他进程，ZeRO-Infinity 在所有数据并行进程中划分单个参数，并在需要参数时使用 All-Gather. 相较于广播只用到了单个 PCIe 链路将参数从存储位置加载到 GPU，All-Gather 同时使用所有的 PCIe 链路，每条链路传输 1/dp 的参数。</p>
<h3 id="overlap-centric-design">Overlap Centric Design</h3>
<p>访问 NVMe 内存需要三个步骤：(i) 从 NVMe 读取数据到CPU内存 (nc-transfer). (ii) 将数据从 CPU 内存复制到 GPU 内存 (cg-transfer). (iii) 执行 All-Gather 以在所有 GPU 上获得完整参数 (gg-transfer).</p>
<p>ZeRO-Infinity 的通信重叠有两个组件</p>
<ol>
<li>一个 dynamic prefetcher，在每次迭代期间，跟踪其在算子序列中的位置，并预取未来算子所需的参数。在执行第 i 个操作符之前，prefetcher 可以分别对第 i+3，第 i+2 和第 i+1 个算子所需的参数调用 nc, cg 和 gg-transfer.</li>
<li>一个通信和卸载重叠机制，用于并行执行梯度所需的数据移动和反向计算。将第 i+1 个算子中参数梯度的 Reduce-Scatter 与第 i 个算子的计算重叠，同时将第 i+2 个算子 Reduce-Scatter 划分的梯度传输给 CPU 或 NVMe.</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>xDiT Principle</title>
      <link>http://localhost:1313/blogs/xdit/</link>
      <pubDate>Sat, 07 Jun 2025 20:44:50 +0800</pubDate>
      <guid>http://localhost:1313/blogs/xdit/</guid>
      <description>This is a brief introduction to the xDiT Principle.</description>
      <content:encoded><![CDATA[<h1 id="parse-config-arguments">Parse Config Arguments</h1>
<p>会从命令行参数中获取有关 Model, Runtime, Parallel Processing &amp; Input 有关的信息。前三者被包含在 <code>engine_config</code> 中，而最后者则被包含在 <code>input_config</code> 中。在 <code>create_config()</code> 函数中，会初始化 <code>_WORLD</code> 全局变量，它是一个 <code>GroupCoordinator</code> 实例。很明显它只有一个包含所有的设备进程组。
<details class="custom-details">
    <summary class="custom-summary">GroupCoordinator</summary>
    <div><p><code>GroupCoordinator</code> 类是一个 PyTorch 的进程组封装器，主要用于管理一组进程之间的通信。它可以根据不同的通信后端（如 NCCL、Gloo、MPI 等）来协调进程之间的操作。包含以下信息</p>
<ul>
<li><code>rank</code>: 当前进程的全局索引（全局唯一）。</li>
<li><code>ranks</code>: 组内所有进程的全局索引列表。</li>
<li><code>world_size</code>: 组的大小，即进程的数量 <code>len(ranks)</code></li>
<li><code>local_rank</code>: 当前进程在本地节点中的索引。</li>
<li><code>rank_in_group</code>: 当前进程在组内的索引。</li>
<li><code>cpu_group</code>: 用于 CPU 通信的进程组。</li>
<li><code>device_group</code>: 用于设备（如 GPU）通信的进程组。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">we</span> <span class="n">have</span> <span class="n">a</span> <span class="n">group</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span> <span class="n">across</span> <span class="n">two</span> <span class="n">nodes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">Process</span> <span class="o">|</span> <span class="n">Node</span> <span class="o">|</span> <span class="n">Rank</span> <span class="o">|</span> <span class="n">Local</span> <span class="n">Rank</span> <span class="o">|</span> <span class="n">Rank</span> <span class="ow">in</span> <span class="n">Group</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span>     <span class="o">|</span>   <span class="mi">0</span>  <span class="o">|</span>  <span class="mi">0</span>   <span class="o">|</span>     <span class="mi">0</span>      <span class="o">|</span>       <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span>     <span class="o">|</span>   <span class="mi">0</span>  <span class="o">|</span>  <span class="mi">1</span>   <span class="o">|</span>     <span class="mi">1</span>      <span class="o">|</span>       <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="mi">2</span>     <span class="o">|</span>   <span class="mi">1</span>  <span class="o">|</span>  <span class="mi">2</span>   <span class="o">|</span>     <span class="mi">0</span>      <span class="o">|</span>       <span class="mi">2</span>
</span></span><span class="line"><span class="cl">  <span class="mi">3</span>     <span class="o">|</span>   <span class="mi">1</span>  <span class="o">|</span>  <span class="mi">3</span>   <span class="o">|</span>     <span class="mi">1</span>      <span class="o">|</span>       <span class="mi">3</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>__init__</code> 方法接收以下参数：</p>
<ul>
<li><code>group_ranks</code>: 一个包含多个进程索引列表的列表，每个子列表表示一个进程组。</li>
<li><code>local_rank</code>: 当前进程的本地索引。</li>
<li><code>torch_distributed_backend</code>: 指定用于通信的后端类型 (如 &ldquo;gloo&rdquo; 或 &ldquo;nccl&rdquo;).</li>
</ul>
<p>初始化过程：</p>
<ol>
<li>使用 <code>torch.distributed.get_rank()</code> 获取当前进程的全局索引。</li>
<li>遍历传入的 <code>group_ranks</code> 列表，为每个子列表创建一个新的设备组和 CPU 组。</li>
<li>如果当前进程的索引在当前子列表中，则设置该进程的组内信息 (包括 <code>ranks</code>、<code>world_size</code> 和 <code>rank_in_group</code>).</li>
<li>确保 CPU 组和设备组都已成功创建。</li>
<li>根据是否可用 CUDA 设置当前设备为 GPU 或 CPU.</li>
</ol>
</div>
</details><br></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span> <span class="o">=</span> <span class="n">FlexibleArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&#34;xFuser Arguments&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">xFuserArgs</span><span class="o">.</span><span class="n">add_cli_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>  <span class="c1"># Add Command Line Interface (CLI) arguments</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_args</span> <span class="o">=</span> <span class="n">xFuserArgs</span><span class="o">.</span><span class="n">from_cli_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Extract CLI args and pass them to xFuserArgs Constructor</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_config</span><span class="p">,</span> <span class="n">input_config</span> <span class="o">=</span> <span class="n">engine_args</span><span class="o">.</span><span class="n">create_config</span><span class="p">()</span>  <span class="c1"># Init _WORLD. engine_config: model, run_time &amp; parallel infos, input_config: input shape, prompt &amp; sampler infos</span>
</span></span><span class="line"><span class="cl">    <span class="n">local_rank</span> <span class="o">=</span> <span class="n">get_world_group</span><span class="p">()</span><span class="o">.</span><span class="n">local_rank</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>关于可以支持的并行策略如下，包括 Data Parallel, Sequence Parallel, Pipefusion Parallel &amp; Tensor Parallel.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Parallel Processing Options:
</span></span><span class="line"><span class="cl">  --use_cfg_parallel    Use split batch in classifier_free_guidance. cfg_degree will be <span class="m">2</span> <span class="k">if</span> <span class="nb">set</span>
</span></span><span class="line"><span class="cl">  --data_parallel_degree DATA_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Data parallel degree.
</span></span><span class="line"><span class="cl">  --ulysses_degree ULYSSES_DEGREE
</span></span><span class="line"><span class="cl">                        Ulysses sequence parallel degree. Used in attention layer.
</span></span><span class="line"><span class="cl">  --ring_degree RING_DEGREE
</span></span><span class="line"><span class="cl">                        Ring sequence parallel degree. Used in attention layer.
</span></span><span class="line"><span class="cl">  --pipefusion_parallel_degree PIPEFUSION_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Pipefusion parallel degree. Indicates the number of pipeline stages.
</span></span><span class="line"><span class="cl">  --num_pipeline_patch NUM_PIPELINE_PATCH
</span></span><span class="line"><span class="cl">                        Number of patches the feature map should be segmented in pipefusion parallel.
</span></span><span class="line"><span class="cl">  --attn_layer_num_for_pp <span class="o">[</span>ATTN_LAYER_NUM_FOR_PP ...<span class="o">]</span>
</span></span><span class="line"><span class="cl">                        List representing the number of layers per stage of the pipeline in pipefusion parallel
</span></span><span class="line"><span class="cl">  --tensor_parallel_degree TENSOR_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Tensor parallel degree.
</span></span><span class="line"><span class="cl">  --split_scheme SPLIT_SCHEME
</span></span><span class="line"><span class="cl">                        Split scheme <span class="k">for</span> tensor parallel.
</span></span></code></pre></td></tr></table>
</div>
</div><p>从 CLI 解析的参数后会在 <code>create_config()</code> 中组成如下的 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/config/config.py#L185">ParallelConfig</a>.</p>
<ul>
<li><code>DataParallelConfig</code>: 总的并行度为 <code>dp_degree * cfg_degree</code>.
<ul>
<li><code>dp_degree</code>: 相当于对 batch 维度进行切分，</li>
<li><code>cfg_degree</code>: Class-free Guidance(cfg) 用于控制无条件的图片生成 (若使用相当于 <code>batchsize *= 2</code>).</li>
</ul>
</li>
<li><code>SequenceParallelConfig</code>: 总的并行度为 <code>sp_degree = ulysses_degree * ring_degree</code>
<ul>
<li><code>ulysses_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2309.14509">DeepSeed-Ulesses</a> 的序列并行度。</li>
<li><code>ring_degree</code>: 用于控制计算 Ring Attention 时对 Q K V 沿着 Sequence 维度的切分块数。</li>
</ul>
</li>
<li><code>TensorParallelConfig</code>: 总的并行度为 <code>tp_degree</code>.
<ul>
<li><code>tp_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2104.05343">2D Tensor Parallel</a> 的并行度。</li>
<li><code>split_scheme</code>: 用于控制张量切分方式.</li>
</ul>
</li>
<li><code>PipeFusionParallelConfig</code>: 总的并行度为 <code>pp_degree=num_pipeline_patch</code>.
<ul>
<li><code>pp_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2112.11446">PipeFusion</a> 中模型 Transoformer Blocks 的切分个数。</li>
<li><code>num_pipeline_patch</code>: 用于控制对 latent feature map 的切分块数.</li>
<li><code>attn_layer_num_for_pp</code>: 是一个 list，表示 <code>pp_degree</code> 里每个 stage 的 Transformer 层数。</li>
</ul>
</li>
</ul>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>关于 PipeFusion，原文说切分的 patch 数和 pipeline 大小可以不同，但这里要求 <code>len(attn_layer_num_for_pp)=pp_degree</code></p></div>

<div class="notice info" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="92 59.5 300 300">
  <path d="M292 303.25V272c0-3.516-2.734-6.25-6.25-6.25H267v-100c0-3.516-2.734-6.25-6.25-6.25h-62.5c-3.516 0-6.25 2.734-6.25 6.25V197c0 3.516 2.734 6.25 6.25 6.25H217v62.5h-18.75c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h87.5c3.516 0 6.25-2.734 6.25-6.25Zm-25-175V97c0-3.516-2.734-6.25-6.25-6.25h-37.5c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h37.5c3.516 0 6.25-2.734 6.25-6.25Zm125 81.25c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Info</p><p>设备数必须等于 <code>dp_degree * cfg_degree * sp_degree * tp_degree * num_pipeline_patch</code>，并且 <code>pp_degree</code> 必须小于等于设备数。
<code>ulysses_degree</code> 必须要大于且能被 attention 的头数整除。</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">ParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">dp_config</span><span class="o">=</span><span class="n">DataParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">dp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cfg_parallel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_cfg_parallel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">sp_config</span><span class="o">=</span><span class="n">SequenceParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">ulysses_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ulysses_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">ring_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ring_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">tp_config</span><span class="o">=</span><span class="n">TensorParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">tp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">split_scheme</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">split_scheme</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_config</span><span class="o">=</span><span class="n">PipeFusionParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">pp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pipefusion_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_pipeline_patch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_layer_num_for_pp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layer_num_for_pp</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="construct-pipeline">Construct Pipeline</h1>
<p>解析完配置参数并构建了 <code>engine_config</code> 后，下一步是构建模型的 pipeline.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">pipe</span> <span class="o">=</span> <span class="n">xFuserPixArtAlphaPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>  <span class="c1"># First construct a PixArtAlphaPipeline, then pass it and engine_config to xFuserPipelineBaseWrapper</span>
</span></span><span class="line"><span class="cl">        <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">engine_config</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">engine_config</span><span class="o">=</span><span class="n">engine_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pipe</span><span class="o">.</span><span class="n">prepare_run</span><span class="p">(</span><span class="n">input_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>xFuserPixArtAlphaPipeline 继承自 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/model_executor/pipelines/base_pipeline.py#L61">xFuserPipelineBaseWrapper</a>，_init_runtime_state 函数经过一番调用后会使用 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/core/distributed/parallel_state.py#L265">initialize_model_parallel</a> 初始化 <code>_RUNTIME</code> 有关模型参数的部分和模型并行的全局变量 <code>_DP, _CFG, _PP, _SP, _TP</code>，它是一个 DiTRuntimeState (继承 RuntimeState) 实例，记录了每个 Group 包含的设备索引，除此之外还包括 PipeFusionParallel 中有关 patch 索引的参数 (在稍后 pipeline 执行的时候计算).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xFuserPipelineBaseWrapper</span><span class="p">(</span><span class="n">xFuserBaseWrapper</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pipeline</span><span class="p">:</span> <span class="n">DiffusionPipeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">engine_config</span><span class="p">:</span> <span class="n">EngineConfig</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">:</span> <span class="n">DiffusionPipeline</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_init_runtime_state</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">engine_config</span><span class="o">=</span><span class="n">engine_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># backbone</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;transformer&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;unet&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># vae</span>
</span></span><span class="line"><span class="cl">        <span class="n">vae</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;vae&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scheduler</span>
</span></span><span class="line"><span class="cl">        <span class="n">scheduler</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;scheduler&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">transformer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_transformer_backbone</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">unet</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">unet</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_unet_backbone</span><span class="p">(</span><span class="n">unet</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_scheduler</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">pipeline</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   
</span></span><span class="line"><span class="cl">   
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_convert_transformer_backbone</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">				<span class="c1">#...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&#34;Transformer backbone found, paralleling transformer...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wrapper</span> <span class="o">=</span> <span class="o">**</span><span class="n">xFuserTransformerWrappersRegister</span><span class="o">.</span><span class="n">get_wrapper</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span><span class="o">**</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span> <span class="o">=</span> <span class="n">wrapper</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">transformer</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="initialize_model_parallel">initialize_model_parallel</h2>
<p>该函数中会初始化一个 <code>RankGenerator</code>，它接收每个并行方法的设备组大小和并行度大小顺序。其主要的方法是通过 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/core/distributed/utils.py#L4">generate_masked_orthogonal_rank_groups</a> 函数确定每个并行组由包含哪些设备，先把并行方法按照并行度从小到大排列成 <code>tp-sp-pp-cfg-dp</code>. 再根据要生成的并行组产生对应的 <code>mask</code>. 即如果要生成 <code>pp</code> 组对应的 rank，那么 <code>mask = [0, 0, 1, 0, 0]</code></p>
<p>该函数首先会生成需要生成的并行组的大小组成的 masked_shape 和不需要生成的 unmasked_shape. 首先要用 prefix_product 计算 <code>global_stride</code>，即每个并行度的设备组包含几个设备。再根据 <code>mask</code> 取出对应的 <code>mask_stride</code> 和 <code>unmaskd_stride</code>. <code>group_size = mask_stride[-1]</code> 即为最大并行度的组包含的设备数。<code>num_of_group = num_of_device / mask_stride[-1]</code> 即为要生成几个并行度最大的组。先遍历要生成的每个设备组，并用 decompose 函数确定该设备组在不需要并行维度上的索引；再遍历该组中的每个设备的 lock rank，确定该设备在需要并行维度上的索引，最后用 inner_product 确定该设备的 global rank.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_masked_orthogonal_rank_groups</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">parallel_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">prefix_product</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">init</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>  <span class="c1"># Exclusive</span>
</span></span><span class="line"><span class="cl">        <span class="n">r</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">a</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">init</span> <span class="o">=</span> <span class="n">init</span> <span class="o">*</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">            <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">r</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">inner_product</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">b</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">decompose</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># index: 第几个并行组  # shape: 并行组大小的 list</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function solve the math problem below:
</span></span></span><span class="line"><span class="cl"><span class="s2">            There is an equation: index = sum(idx[i] * stride[i])
</span></span></span><span class="line"><span class="cl"><span class="s2">            And given the value of index, stride.
</span></span></span><span class="line"><span class="cl"><span class="s2">            Return the idx.
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function will used to get the pp/dp/pp_rank from group_index and rank_in_group.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">stride</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">idx</span> <span class="o">=</span> <span class="p">[(</span><span class="n">index</span> <span class="o">//</span> <span class="n">d</span><span class="p">)</span> <span class="o">%</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">)]</span>  <span class="c1">#  计算在每个并行维度上的索引</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># stride is a prefix_product result. And the value of stride[-1]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># is not used.</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stride</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])])</span> <span class="o">==</span> <span class="n">index</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span> <span class="s2">&#34;idx </span><span class="si">{}</span><span class="s2"> with shape </span><span class="si">{}</span><span class="s2"> mismatch the return idx </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">masked_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="p">]</span>  <span class="c1"># 需要采取并行的维度</span>
</span></span><span class="line"><span class="cl">    <span class="n">unmasked_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="p">]</span>  <span class="c1"># 不需要的</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">global_stride</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">)</span>  <span class="c1"># exclusive 前缀积 表示大的并行维度包括几个设备</span>
</span></span><span class="line"><span class="cl">    <span class="n">masked_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_stride</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">unmasked_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_stride</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">group_size</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">masked_shape</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 最大的一个并行维度包括几个设备</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_of_group</span> <span class="o">=</span> <span class="n">world_size</span> <span class="o">//</span> <span class="n">group_size</span>  <span class="c1"># 分成几个大组</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>  
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">group_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_group</span><span class="p">):</span>  <span class="c1"># 遍历每个设备组</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get indices from unmaksed for group_index.</span>
</span></span><span class="line"><span class="cl">        <span class="n">decomposed_group_idx</span> <span class="o">=</span> <span class="n">decompose</span><span class="p">(</span><span class="n">group_index</span><span class="p">,</span> <span class="n">unmasked_shape</span><span class="p">)</span>  <span class="c1"># 得到在不需要采取并行的维度上的索引</span>
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rank_in_group</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">):</span>  <span class="c1"># 遍历该组中的每个设备 local rank</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># get indices from masked for rank_in_group.</span>
</span></span><span class="line"><span class="cl">            <span class="n">decomposed_rank_idx</span> <span class="o">=</span> <span class="n">decompose</span><span class="p">(</span><span class="n">rank_in_group</span><span class="p">,</span> <span class="n">masked_shape</span><span class="p">)</span>  <span class="c1"># 得到最大并行组的每个设备在采取并行的维度上的索引</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>  <span class="o">//</span> <span class="n">相加得到全局rank</span>
</span></span><span class="line"><span class="cl">                <span class="n">inner_product</span><span class="p">(</span><span class="n">decomposed_rank_idx</span><span class="p">,</span> <span class="n">masked_stride</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">                <span class="o">+</span> <span class="n">inner_product</span><span class="p">(</span><span class="n">decomposed_group_idx</span><span class="p">,</span> <span class="n">unmasked_stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">ranks</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="hybrid-parallelsim-design">Hybrid Parallelsim Design</h2>
<p>xDiT支持四种并行方式：PipeFusion、Sequence、Data 和 CFG Parallel。其中，Data 和 CFG Parallel在图像间并行相对简单，而 PipeFusion和 Sequence 在图像内部的不同 Patch 间并行则较为复杂。能</p>
<p>PipeFusion 利用 Input Tempor Redundancy特点，使用过时的 KV（Stale KV）进行 Attention 计算，这使得 PipeFusion 无法像大型语言模型那样轻松地实现并行策略的混合。使用标准的序列并行接口，如RingAttention、Ulysses或 USP，无法满足 SP 与PipeFusion混合并行的需求。</p>
<p>我们对这个问题具体说明，下图展示了pipe_degree=4，sp_degree=2的混合并行方法。设置 <code>num_pipeline_patch</code>=4，图片切分为 M=<code>num_pipeline_patch*sp_degree</code>=8 个 Patch，分别是 P0~P7.</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/hybrid_pp_scheme.png" alt="hybrid process group config"  width="60%">
</div>
<p>Standard SP Attention 的输入Q，K，V 和输出 O 都是沿着序列维度切分，且切分方式一致。如果不同 rank 的输入 patch 没有重叠，每个 micro step 计算出 fresh KV 更新的位置在不同 rank 间也没有重叠。如下图所示，standard SP 的 KV Buffer 中黄色部分是 SP0 rank=0 拥有的 fresh KV，绿色部分是 SP1 rank=1 拥有的fresh KV，二者并不相同。在这个 diffusion step 内，device=0 无法拿到 P1,3,5,7 的 fresh KV 进行计算，但是 PipeFusion 则需要在下一个 diffusion step 中，拥有上一个diffusion step 全部的 KV. standard SP 只拥有 1/sp_degree 的 fresh kv buffer，因此无法获得混合并行推理正确的结果。</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/hybrid_workflow.png" alt="hybrid parallel workflow">
</div>
<p>xDiT专门定制了序列并行的实现方式，以适应这种混合并行的需求。xDiT使用 <code>xFuserLongContextAttention</code> 把SP的中间结果存在 KV Buffer 内。效果如下图，每个 micro-step SP 执行完毕后，SP Group 内不同 rank 设备的 fresh KV是 replicate 的。这样一个 diffusion step 后，SP Group 所有设备的 KV Buffer 都更新成最新，供下一个 Diffusion Step 使用。</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/kvbuffer_hybrid.png" alt="kvbuffer in hybrid parallel">
</div>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>假设一共有 16 个 GPU，索引表示为 g0 &hellip; g15，并行方法和并行度设置如下</p>
<p><code>dp_degree (2) * cfg_degree (2) * pp_degree (2) * sp_degree (2) = 16</code>.</p>
<p>那么一共会创建 2 data parallel-groups, 8 CFG groups, 8 pipeline-parallel groups &amp; 8 sequence-parallel groups:</p>
<ul>
<li>2 data-parallel groups:
[g0, g1, g2, g3, g4, g5, g6, g7],
[g8, g9, g10, g11, g12, g13, g14, g15]</li>
<li>8 CFG-parallel groups:
[g0, g4], [g1, g5], [g2, g6], [g3, g7],
[g8, g12], [g9, g13], [g10, g14], [g11, g15]</li>
<li>8 pipeline-parallel groups:
[g0, g2], [g4, g6], [g8, g10], [g12, g14],
[g1, g3], [g5, g7], [g9, g11], [g13, g15]</li>
<li>8 sequence-parallel groups:
[g0, g1], [g2, g3], [g4, g5], [g6, g7],
[g8, g9], [g10, g11], [g12, g13], [g14, g15]</li>
</ul></div>

<h2 id="convert-model">Convert Model</h2>
<p><a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/model_executor/models/transformers/base_transformer.py#L76">_split_transformer_blocks</a> 会对 transformer block 进行分配，如果 parallel_config 指定了 attn_layer_num_for_pp，即存有每个 pipeFusion 的设备被分配的 transformer block 数量的列表，按其进行分配；否则平均分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_split_transformer_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># omit</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># transformer layer split</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_layer_num_for_pp</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># 获取每个 pipeFusion 的设备被分配的 transformer block 数量</span>
</span></span><span class="line"><span class="cl">        <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pp_config</span><span class="o">.</span><span class="n">attn_layer_num_for_pp</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_rank</span> <span class="o">=</span> <span class="n">get_pipeline_parallel_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_world_size</span> <span class="o">=</span> <span class="n">get_pipeline_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attn_layer_num_for_pp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span> <span class="p">:</span> <span class="n">attn_layer_num_for_pp</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">attn_layer_num_for_pp</span><span class="p">[:</span> <span class="n">pp_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="p">:</span> 
</span></span><span class="line"><span class="cl">                                                                            <span class="nb">sum</span><span class="p">(</span><span class="n">attn_layer_num_for_pp</span><span class="p">[:</span><span class="n">pp_rank</span><span class="p">])]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>  <span class="c1"># 没有指定则平均分</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_blocks_per_stage</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">)</span> <span class="o">+</span> <span class="n">pp_world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">pp_world_size</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">pp_rank</span> <span class="o">*</span> <span class="n">num_blocks_per_stage</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">pp_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_blocks_per_stage</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">),)</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># position embedding</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">norm_out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">transformer</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>同时也会 convert 原先的 transformer backbone 为 <a href="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/models/transformers/pixart_transformer_2d.py#L21">xFuserPixArtTransformer2DWrapper</a>，具体表现为只有 pipeline 的第一阶段进行 position embedding，最后一阶段进行 unpatchify 变为原来的图像形状。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@xFuserTransformerWrappersRegister.register</span><span class="p">(</span><span class="n">PixArtTransformer2DModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xFuserPixArtTransformer2DWrapper</span><span class="p">(</span><span class="n">xFuserTransformerBaseWrapper</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="p">:</span> <span class="n">PixArtTransformer2DModel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">submodule_classes_to_wrap</span><span class="o">=</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">PatchEmbed</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">submodule_name_to_wrap</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;attn1&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@xFuserBaseWrapper.forward_check_condition</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">added_cond_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cross_attention_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>  
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_patch_height_width</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># * only pp rank 0 needs pos_embed (patchify)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">	    <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">output</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">Transformer2DModelOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="pipeline-execution">Pipeline Execution</h1>
<p>在进行 warm up 后便会进行模型推理和采样器的去噪过程。模型推理通过调用 pipeline 的 <code>__call__</code> 方法实现。在原先 diffusers 包中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py">PixaeArtAlphaPipeline</a> 基础上做了一些修改。我们直接看修改的部分。</p>
<p><code>get_runtime_state()</code> 返回 <code>_RUNTIME</code> ，再调用 <code>set_input_parameters</code> 方法，设置输入参数和计算 PipeFusionParallel 中有关 patch 索引的参数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">set_input_parameters</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">num_inference_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该函数会计算</p>
<ul>
<li>pipeline parallel 中每个 patch 的高度，必须是 <code>patch_size * num_sp_patches</code> 的整数倍。</li>
<li>将每个流水线阶段的 patch 高度均匀地分配给 <code>num_sp_patches</code> 个序列并行设备，计算每个设备的 patch 高度和起始索引。</li>
</ul>
<p>然后会对 prompt 嵌入后的正样本和负样本在 cfg parallel 组中的设备进行分割, rank 0 负样本，rank 1 正样本。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">prompt_embeds</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">prompt_attention_mask</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_cfg_split_batch</span><span class="p">(</span><span class="n">negative_prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">negative_prompt_attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">prompt_attention_mask</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_process_cfg_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_0_negative</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_0</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_1_negative</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">get_classifier_free_guidance_world_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">concat_group_0_negative</span><span class="p">,</span> <span class="n">concat_group_0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">concat_group_1_negative</span><span class="p">,</span> <span class="n">concat_group_1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">get_classifier_free_guidance_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">concat_group_0_negative</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">concat_group_1_negative</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">get_classifier_free_guidance_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">concat_group_0</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">concat_group_1</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid classifier free guidance rank&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">concat_group_0</span><span class="p">,</span> <span class="n">concat_group_1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="async-pipeline">Async Pipeline</h1>
<h2 id="initialize-pipeline">Initialize Pipeline</h2>
<p>首先会初始化 pipeline，rank 0 会接收 warmup 阶段的 latents 然后沿着 H 维度进行分块，rank -1 也会沿着 H 维度进行分块。然后为每个 patch 创建接收的任务，注意 rank 0 第一次是从 warmup 阶段接收 latents，所以他的需要接收的 timestep 少一个。
<code>patch_latents</code> 表示当前设备正在处理的 patch 数据，它会在流水线的每一阶段进行处理和传递。<code>last_patch_latents</code> 只在流水线的最后阶段设备中使用，用来存储每个 patch 的最终计算结果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">latents</span>
</span></span><span class="line"><span class="cl"><span class="n">num_pipeline_patch</span> <span class="o">=</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span>
</span></span><span class="line"><span class="cl"><span class="n">num_pipeline_warmup_steps</span> <span class="o">=</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">warmup_steps</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_async_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_timesteps</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span><span class="o">=</span><span class="n">latents</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_pipeline_warmup_steps</span><span class="o">=</span><span class="n">num_pipeline_warmup_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">last_patch_latents</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># 每个 pipeline group 最后的设备接收所有的 patch</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pipeline_patch</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">is_pipeline_last_stage</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_init_async_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_timesteps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_pipeline_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">set_patched_mode</span><span class="p">(</span><span class="n">patch_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get latents computed in warmup stage</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ignore latents after the last timestep</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents</span> <span class="o">=</span> <span class="p">(</span><span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_recv</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                  <span class="k">if</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">                  <span class="k">else</span> <span class="n">latents</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">latents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_height</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">latents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_height</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">recv_timesteps</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_timesteps</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="k">else</span> <span class="n">num_timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># construct receive tasks for each patch</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">recv_timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">add_pipeline_recv_task</span><span class="p">(</span><span class="n">patch_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">patch_latents</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="iterate-over-timesteps">Iterate Over Timesteps</h1>
<p>对于每个 <code>timestep</code>（即每个去噪步骤），会对每个 patch 执行：</p>
<ol>
<li>如果当前设备是流水线的最后一阶段 (<code>is_pipeline_last_stage()</code>)，将当前 patch 的数据保存到 <code>last_patch_latents</code> 中。</li>
<li>如果不是第一阶段的第一个时间步 (<code>i == 0</code>)，调用 <code>recv_next()</code> 来异步接收来自上一设备的 patch 数据（非阻塞操作，通过 <code>irecv</code> 完成）。</li>
<li>对每个 patch 执行模型的前向传播 <code>_backbone_forward</code>，根据当前时间步 <code>t</code> 进行推理和计算。</li>
<li>如果当前设备是最后一阶段，调用 <code>_scheduler_step</code> 来根据噪声进行去噪，并将数据发送给下一个设备 <code>pipeline_isend</code>。</li>
<li>对于非最后阶段的设备，继续将当前 patch 的计算结果发送到下一设备。</li>
</ol>
<p><code>get_pp_group().pipeline_isend</code> 用于将当前 patch 发送到下一个设备，使用的是 torch.distributed.isend，这是非阻塞发送。
<code>get_pp_group().recv_next</code> 会准备好接收来自上一个设备的数据，recv_buffer 用来存放接收到的数据。irecv 实现非阻塞接收，可以在等待数据的同时进行其他操作。</p>
<div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>scheduler_step 只对单独的 patch 进行，原因未知。</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">first_async_recv</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">last_patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">first_async_recv</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">recv_next</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">                <span class="n">first_async_recv</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">get_pipeline_recv_data</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents</span><span class="o">=</span><span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">prompt_attention_mask</span><span class="o">=</span><span class="n">prompt_attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">added_cond_kwargs</span><span class="o">=</span><span class="n">added_cond_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">guidance_scale</span><span class="o">=</span><span class="n">guidance_scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>  <span class="c1"># pred noise</span>
</span></span><span class="line"><span class="cl">                <span class="n">last_patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>  <span class="c1"># last timestep noise</span>
</span></span><span class="line"><span class="cl">                <span class="n">t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">extra_step_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_isend</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span> <span class="n">segment_idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_isend</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span> <span class="n">segment_idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">patch_idx</span> <span class="o">==</span> <span class="n">num_pipeline_patch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">pass</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">recv_next</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">next_patch</span><span class="p">()</span>  <span class="c1"># switch to next: (self.pipeline_patch_idx + 1) % self.num_pipeline_patch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">or</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_warmup_steps</span>
</span></span><span class="line"><span class="cl">        <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">callback</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&#34;callback not supported in async &#34;</span> <span class="s2">&#34;pipeline&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">            <span class="ow">and</span> <span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">%</span> <span class="n">callback_steps</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">step_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span><span class="p">)</span> <span class="o">//</span> <span class="nb">getattr</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&#34;order&#34;</span><span class="p">,</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">callback</span><span class="p">(</span><span class="n">step_idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="construct-final-latents">Construct Final Latents</h2>
<p>timestep 遍历完成后，仍然有最后的操作要进行，这些操作的主要目的是将流水线并行中各个 patch 的结果拼接起来，形成完整的输出结果。尤其是对于最后一个设备，还需要处理 序列并行（sequence parallelism） 的合并操作。通过 all_gather 操作将每个设备上处理的 patch 结果收集起来，然后从每个设备的 <code>sp_latents_list</code> 中，提取出对应于 <code>pp_patch_idx</code> 的 patch 数据并将它们拼接起来。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">latents</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">patch_latents</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">get_sequence_parallel_world_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">sp_degree</span> <span class="o">=</span> <span class="n">get_sequence_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">sp_latents_list</span> <span class="o">=</span> <span class="n">get_sp_group</span><span class="p">()</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents</span><span class="p">,</span> <span class="n">separate_tensors</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">pp_patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents_list</span> <span class="o">+=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="n">sp_latents_list</span><span class="p">[</span><span class="n">sp_patch_idx</span><span class="p">][</span>
</span></span><span class="line"><span class="cl">                    <span class="o">...</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_start_idx_local</span><span class="p">[</span><span class="n">pp_patch_idx</span><span class="p">]</span> <span class="p">:</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_start_idx_local</span><span class="p">[</span><span class="n">pp_patch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="p">:,</span>
</span></span><span class="line"><span class="cl">                <span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">sp_patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sp_degree</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">latents_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">latents</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="decode-latents">Decode Latents</h1>
<p>为了避免 VAE 中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/vae.py#L185">Decoder</a> 在对 8192px 分辨率图像进行 conv2D 的过程中出现 OOM 的问题， xDiT 使用了序列并行和 patch 并行的 <a href="https://github.com/xdit-project/DistVAE/blob/a7e7ee7ec222f45af1214984561c8c645be8aece/distvae/models/layers/conv2d.py#L13">PatchConv2d</a> 和 <a href="https://github.com/xdit-project/DistVAE/blob/a7e7ee7ec222f45af1214984561c8c645be8aece/distvae/models/layers/normalization.py#L59">PatchGroupNorm</a> 来替换掉原有 Decoder 中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d_blocks.py#L2682">UpDecoderBlock2D</a> 对应的层。</p>
<h2 id="patchgroupnorm">PatchGroupNorm</h2>
<p>PatchGroupNorm 在 H 维度上划分为多个 patch，每个设备求自己所负责的部分和。
<details class="custom-details">
    <summary class="custom-summary">GroupNorm Principles</summary>
    <div>假设输入张量 x 的形状为 [N, C, H, W]，其中 N 表示批量大小（Batch Size），C 表示通道数（Channels），H 和 W 分别表示高度和宽度。在 GN 中，通道数 C 被划分为 G 组，每个组包含 C/G 个通道。计算每个组内即 [C/G, H, W] 维度上的均值和方差。特别的 G=1 时，GN 退化为 BN。G=C 时，GN 退化为 LN。</div>
</details><br></p>
<ol>
<li>获取高度信息</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchGroupNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; def __init__(self, ...)&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">height</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">height</span><span class="p">)</span>  <span class="c1"># 收集所有进程的高度并汇总。最终每个进程的 height 都将表示全局的高度和。</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>计算每个组的通道数量以及每个进程内的元素数量</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">channels_per_group</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span>  <span class="c1"># 每个组的通道数量</span>
</span></span><span class="line"><span class="cl"><span class="n">nelements_rank</span> <span class="o">=</span> <span class="n">channels_per_group</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 当前进程负责的每个组中的元素总</span>
</span></span><span class="line"><span class="cl"><span class="n">nelements</span> <span class="o">=</span> <span class="n">channels_per_group</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 所有进程的每个组中的元素总数</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>计算每个组的均值</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1">#  [batch_size, num_groups, channels_per_group, height, width]</span>
</span></span><span class="line"><span class="cl"><span class="n">group_sum</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 对每个组的所有元素 (channels_per_group, height, width) 求平均</span>
</span></span><span class="line"><span class="cl"><span class="n">group_sum</span> <span class="o">=</span> <span class="n">group_sum</span> <span class="o">*</span> <span class="n">nelements_rank</span>  <span class="c1"># 加权后的局部和 = 局部均值 * 当前进程的元素数量</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">group_sum</span><span class="p">)</span>  <span class="c1"># 收集并汇总所有进程的局部和，得到全局和</span>
</span></span><span class="line"><span class="cl"><span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_sum</span> <span class="o">/</span> <span class="n">nelements</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># 计算全局的均值 E</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>计算每个组的方差</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 和计算均值同样的操作</span>
</span></span><span class="line"><span class="cl"><span class="n">group_var_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">out</span><span class="o">=</span><span class="n">group_var_sum</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="n">group_var_sum</span> <span class="o">=</span> <span class="n">group_var_sum</span> <span class="o">*</span> <span class="n">nelements_rank</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">group_var_sum</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">var</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_var_sum</span> <span class="o">/</span> <span class="n">nelements</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>归一化并缩放 $y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">E</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchconv2d">PatchConv2d</h2>
<p><code>PatchConv2d</code> 将潜在空间中的特征映射分割成多个 patch，跨不同设备进行序列并行 VAE 解码。这种技术将中间激活所需的峰值内存减少到 1/N，其中 N 是所使用的设备数量。对于 VAE 中的卷积算子，需要对如下图所示的 halo 区域数据进行通信。</p>
<p>
<figure class="post-figure">
    <a href="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/patchvaeconv.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/patchvaeconv.png" alt="Patch VAE Conv">
    </a><figcaption>Patch VAE Conv</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>  <span class="c1"># TODO: refine this type</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">assert</span> <span class="n">dilation</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;dilation is not supported in PatchConv2d&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">assert</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;dilation is not supported in PatchConv2d&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>_conv_forward</code> 函数是 <code>PatchConv2d</code> 类的核心，它负责在输入张量上执行卷积操作，特别是在分布式计算场景下处理跨进程的输入切分、halo 区域的传递和计算。以下是使用的辅助函数的简要功能说明</p>
<ul>
<li><code>_get_world_size_and_rank </code>：获取当前分布式环境中的进程总数 <code>world_size</code> 和当前进程的编号 <code>rank</code></li>
<li><code>_calc_patch_height_index</code>：根据每个进程的输入高度，计算所有进程的起始和结束高度索引。</li>
<li><code>_calc_halo_width_in_h_dim</code>：计算当前进程在 h 维度上所需的上方和下方的 halo 区域宽度。</li>
<li><code>_calc_bottom_halo_width</code>：计算当前进程从下方相邻进程需要接收的 halo 区域的宽度。</li>
<li><code>_calc_top_halo_width</code>：计算当前进程从上方相邻进程需要接收的 halo 区域的宽度。</li>
<li><code>_adjust_padding_for_patch</code>：根据当前进程的 <code>rank</code> 和总进程数调整输入数据的填充方式，防止边界重复计算。</li>
</ul>
<ol>
<li>获取输入信息以及通信组信息</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_world_size_and_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">world_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># 处理非分布式情况</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                                <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>获取输入的元数据</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">patch_height_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">patch_height_list</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">h</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">))</span>  <span class="c1"># 收集所有进程的输入高度</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_height_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_patch_height_index</span><span class="p">(</span><span class="n">patch_height_list</span><span class="p">)</span>  <span class="c1"># 计算每个进程块的起始高度和结束高度的索引</span>
</span></span><span class="line"><span class="cl"><span class="n">halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_halo_width_in_h_dim</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># 计算当前进程块的上下 halo 区域的宽度</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>计算相邻进程的 halo 区域 (也就是自己需要接发送的部分)</li>
</ol>
<p>通过计算前一个进程的 bottom_halo_width 和后一个进程的 top_halo_width 得出自己需要发送的部分</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">prev_bottom_halo_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="n">next_top_halo_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">prev_bottom_halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_bottom_halo_width</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="n">world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_top_halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_top_halo_width</span><span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_top_halo_width</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">next_top_halo_width</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>进行 halo 区域的发送与接收</li>
</ol>
<p>异步发送，同步接收</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">to_next</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">to_prev</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">top_halo_recv</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">bottom_halo_recv</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">next_top_halo_width</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bottom_halo_send</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">next_top_halo_width</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_next</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">bottom_halo_send</span><span class="p">,</span> <span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not rank 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_halo_recv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">top_halo_recv</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">prev_bottom_halo_width</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not rank N-1</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_halo_send</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">prev_bottom_halo_width</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_prev</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">top_halo_send</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bottom_halo_recv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">bottom_halo_recv</span><span class="p">,</span> <span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>拼接 halo 区域</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Remove redundancy at the top of the input</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]:,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">top_halo_recv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># concat the halo region to the input tensor </span>
</span></span><span class="line"><span class="cl">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">top_halo_recv</span><span class="p">,</span> <span class="nb">input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">bottom_halo_recv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="nb">input</span><span class="p">,</span> <span class="n">bottom_halo_recv</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="6">
<li>等待发送完成再开始计算</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">to_next</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_next</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">to_prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_prev</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="7">
<li>进行卷积和后处理</li>
</ol>
<p>为了减少 memory spike 一次计算 block_size*block_size 的区域，并将结果拼接起来</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjust_padding_for_patch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">h</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">and</span> <span class="n">w</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                            <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">conv_res</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s2">&#34;zeros&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks_in_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>  <span class="c1"># h 维度的 block 数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks_in_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>  <span class="c1"># w ...</span>
</span></span><span class="line"><span class="cl">    <span class="n">unit_chunk_size_h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">//</span> <span class="n">num_chunks_in_h</span>
</span></span><span class="line"><span class="cl">    <span class="n">unit_chunk_size_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">//</span> <span class="n">num_chunks_in_w</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">idx_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks_in_h</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">inner_output</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx_w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks_in_w</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_w</span> <span class="o">=</span> <span class="n">idx_w</span> <span class="o">*</span> <span class="n">unit_chunk_size_w</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_h</span> <span class="o">=</span> <span class="n">idx_h</span> <span class="o">*</span> <span class="n">unit_chunk_size_h</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">unit_chunk_size_w</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">unit_chunk_size_h</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算每个块的开始和结束索引，调整块的边界</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 对当前块执行卷积操作</span>
</span></span><span class="line"><span class="cl">        <span class="n">inner_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_h</span><span class="p">:</span><span class="n">end_h</span><span class="p">,</span> <span class="n">start_w</span><span class="p">:</span><span class="n">end_w</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">inner_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>VLLM Sourse Code Reading</title>
      <link>http://localhost:1313/blogs/vllm/</link>
      <pubDate>Sat, 07 Jun 2025 18:15:55 +0800</pubDate>
      <guid>http://localhost:1313/blogs/vllm/</guid>
      <description>vllm structure</description>
      <content:encoded><![CDATA[<h1 id="basic">Basic</h1>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample prompts.</span>
</span></span><span class="line"><span class="cl"><span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Hello, my name is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The president of the United States is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The capital of France is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The future of AI is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a sampling params object.</span>
</span></span><span class="line"><span class="cl"><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create an LLM.</span>
</span></span><span class="line"><span class="cl"><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&#34;facebook/opt-125m&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generate texts from the prompts. The output is a list of RequestOutput objects</span>
</span></span><span class="line"><span class="cl"><span class="c1"># that contain the prompt, generated text, and other information.</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Print the outputs.</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
</span></span><span class="line"><span class="cl">    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="architecture">Architecture</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB713ed0d773cac101706cdaa862d71dda?method=download&amp;shareKey=09c7c358d0427427384e027f0ced662a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB713ed0d773cac101706cdaa862d71dda?method=download&amp;shareKey=09c7c358d0427427384e027f0ced662a" alt="VLLM Architecture Overview">
    </a><figcaption>VLLM Architecture Overview</figcaption></figure></p>
<ul>
<li>LLM: 最上层的类，构造函数中会根据传入的参数构建 EngineArgs 然后创建 LLMEngine 对象。</li>
<li>LLMEngine: 包含一些组件 InputPreprocessor, ExecutorBase 负责模型推理的最上层的类</li>
<li>ExecutorBase 会初始化 N 个 WorkerWrapperBase (包装实际的 worker，类比成 GPU)
<ul>
<li>Worker: 在 GPU 上执行 (一部分) 模型推理。每个 worker 与一个 GPU 相关联，负责维护 KV Cache 并在 GPU 上执行模型推理。在分布式推理的情况下，每个 worker 被分配模型的一部分。
<ul>
<li>ModelRunner:  执行模型推理并负责采样新 token.</li>
<li>CacheEngine: 负责初始化和管理 GPU 和 CPU KV Cache. 还提供了对 KV Cache 进行操作的方法。通过 <code>initialize_cache()</code> 初始化。</li>
</ul>
</li>
</ul>
</li>
<li>Scheduler: 负责推理时候对请求的调度。组件包括一个 BlockSpaceManager (KV Cache blocks 管理的核心类) 以及三个队列 waiting, running &amp; swapped.</li>
</ul>
<h1 id="llmengine--initialization">LLMEngine  Initialization</h1>
<ul>
<li>InputPreprocessor: 主要是在 <code>add_request()</code> 方法中将输入的 prompt 放入 tokenizer 进行处理。</li>
<li>InputRegistry: 根据目标模型对 InputPreprocessor 之后的数据进行处理。</li>
</ul>
<h2 id="init-executor">Init Executor</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistributedExecutorBase</span><span class="p">(</span><span class="n">ExecutorBase</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Abstract superclass of distributed executor implementations.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This is non-None when the execute model loop is running</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># in the parallel workers. It&#39;s a coroutine in the AsyncLLMEngine case.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_worker_tasks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Awaitable</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ExecutorBase 的构造函数中会调用 <code>self._init_executor()</code> 对应到具体子类的函数。如果采用 TP 或 PP 的话 对应到的是 RayDistributedExecutor，否则对应到的是 UniProcExecutor. 下面以后者为例。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">UniProcExecutor</span><span class="p">(</span><span class="n">ExecutorBase</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">uses_ray</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_init_executor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Initialize the worker and load the model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">driver_worker</span> <span class="o">=</span> <span class="n">WorkerWrapperBase</span><span class="p">(</span><span class="n">vllm_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                               <span class="n">rpc_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">distributed_init_method</span> <span class="o">=</span> <span class="n">get_distributed_init_method</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_ip</span><span class="p">(),</span> <span class="n">get_open_port</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">local_rank</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># set local rank as the device index if specified</span>
</span></span><span class="line"><span class="cl">        <span class="n">device_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">device_config</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_info</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">device_info</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">vllm_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_rank</span><span class="o">=</span><span class="n">local_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">distributed_init_method</span><span class="o">=</span><span class="n">distributed_init_method</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">is_driver_worker</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="ow">or</span> <span class="p">(</span><span class="n">rank</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;init_worker&#34;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">([</span><span class="n">kwargs</span><span class="p">],</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;init_device&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;load_model&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">collective_rpc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                       <span class="n">method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                       <span class="n">timeout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                       <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(),</span>
</span></span><span class="line"><span class="cl">                       <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="n">answer</span> <span class="o">=</span> <span class="n">run_method</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">driver_worker</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># 初始化 Worker</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="n">answer</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>Executor: 初始化具体的继承自 ExecutorBase 的对象，该对象的初始化过程中会调用 <code>init_worker()</code> 初始化 Worker (被 WorkerWrapperBase 包装)，调用 <code>init_device()</code> 初始化设备，和调用具体 Worker 对象的 model_runner 的 <code>load_model()</code> 将模型加载到设备上。
<ul>
<li>Worker: 构造函数中会初始化 <code>GPUModelRunnerBase</code> 对象，确定计算 attention 使用的 backend 还有 CUDAGraphRunner 用于将模型的计算过程记录为一个静态图，在后续的推理中，通过直接 replay 这个静态图来避免动态调度和重复的内核启动开销。</li>
</ul>
</li>
</ul>
<h2 id="initialize_kv_caches">initialize_kv_caches</h2>
<p>LLMEngine 构造函数在初始化 ExecutorBase 后会调用 <code>initialize_kv_caches()</code> 来初始化 Worker 中的 KV Cache，流程如下:</p>
<ol>
<li>该函数会首先通过 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/neuron_worker.py#L69">Worker.determine_num_available_blocks()</a> 确定 GPU 和 CPU 可用的 block 数量。后者在 <code>memory_profiling</code> 上下文中进行 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/model_runner.py#L1239">profile_run()</a> 模拟模型在最大负载 (max_num_batched_tokens 和 max_num_seqs) 下执行一次推理。测量内存使用并分解为权重、激活张量和非 PyTorch 部分。留给 KV Cache 的内存大小为 <code>total_mem * max_utilization - weight_mem - act_mem - nontorch_mem</code>.  再除以每一个 block 能存储的的 KV Cache 大小 <code>cache_size = Cache_config.block_size * num_attention_layers * 2*num_heads*head_size</code> 即可得到最多能分配多少个 GPU block. 而 CPU block 数量由预设的 <code>swap_size // cache_size</code> 所确定。</li>
<li>确定了 GPU 和 CPU 的 block 数量后会调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/worker.py#L285">Worker.initialize_cache()</a> 方法，里面首先会调用 <code>Worker._init_cache_engine()</code> 根据传入的 GPU block 个数初始化 CacheEngine (初始化 attn_backend，调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/cache_engine.py#L68">CacheEngine._allocate_kv_cache()</a> 为模型的每一层 transformer 开辟 CPU 和 GPU 的 KV Cache 内存)，然后会调用 <a href="https://github.com/vllm-project/vllm/blob/main/vllm/utils.py#L2163">bind_kv_cache()</a> 将 GPU KV Cache Tensor 绑定到对应的模型的注意力层，它筛选需要 KV Cache 的注意力层，按层索引排序并去重后为每个设备绑定对应的 Tensor.</li>
<li>预热之后进行 capture_model 记录计算图。</li>
</ol>
<h2 id="init-scheduler">Init Scheduler</h2>
<p>构造函数中会初始化 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block_manager.py#L61">BlockSpaceManager</a>. 首先会创建一个 <code>CpuGpuBlockAllocator</code>，为 CPU 和 GPU 块维护单独的内存池，并允许在这些内存池中分配、释放、分叉和交换块。它会为 CPU 和 GPU 中的 blocks 分别创建一个 <code>BlockAlloctor</code>. 还会初始化一个空的 <code>Dict[SeqId, BlockTable]</code>， 表示对应 seq 的 KV Cache 所使用的物理内存块。还会初始化一些调度时所需要的数据，后文再谈。</p>
<p>还会初始化 waiting(包含新的或 preempted prefill 请求), running &amp; swapped(被换出的 decoding 请求), 它们是 <code>Deque[SequenceGroup]</code>，其中 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/sequence.py#L633">SequenceGroup(SG)</a> 是一组由同一个 prompt 生成的 Sequences 和对应的采样参数。</p>
<ul>
<li>SequenceGroupOutputProcessor: 抽象基类借接口，会分为 SingleStepOutputProcessor (支持 beam seaching) 和 MultiStepOutputProcessor (支持 speculatice decoding)</li>
</ul>
<h1 id="llm-generate">LLM Generate</h1>
<h2 id="_validate_and_add_requests">_validate_and_add_requests</h2>
<p>里面会调用 <code>_add_request()</code> 给 prompt 分配 reqest_id 后会调用 <code>LLMEngine.add_request()</code> 将其添加到请求池中，并将在调用 <code>LLMEngine.step()</code> 时由调度器处理。确切的调度策略由调度程序确定。主要就是进行 tokenize，然后打包成 SG 后加入 waiting.</p>
<h2 id="__run_engine">__run_engine</h2>
<p>调用 generate 时首先会将 prompt 包装成 SG，它是包含某个 prompt 生成的所有 Sequence，以及一些其他在调度时需要的信息的结构。Scheduler 里面包含三个 <code>Deque[SequenceGroup]</code>: waiting, running &amp; swapped.
generate() &ndash;&gt; _run_engine() &ndash;&gt; step() &ndash;&gt; Scheduler.schedule() &ndash;&gt; Scheduler._schedule()
Scheduler 的一些操作与 BlockManager 息息相关，我们在下面先简要说明逻辑，有关其具体结构和操作流程在后文中解释。</p>
<h2 id="step">step</h2>
<p>执行一次 decoding 迭代并返回新生成的结果。

<figure class="post-figure">
    <a href="https://i.imgur.com/sv2HssD.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://i.imgur.com/sv2HssD.png" alt="Overview of the step function">
    </a><figcaption>Overview of the step function</figcaption></figure>
主要流程如下</p>
<ol>
<li>调度要在下一次迭代中执行的 seq 和要交换入/出/复制的令牌块。根据调度策略，Sequences 可能被抢占/重新排序。</li>
<li>调用分布式执行器来执行模型。</li>
<li>处理模型输出。主要包括： decoding 相关输出，使用 _beam_search 与否的模型输出更新调度 seq 组和释放已完成的 seq 组。</li>
<li>读取上一次调度的元数据和输出</li>
<li>如果没有剩余步骤且，调用 <code>Scheduler.schedule()</code> 执行新调度，生成 seq 组元数据、调度输出和异步标志。</li>
<li>获取并重置已完成请求 ID，清理内存</li>
<li>如果不允许异步且有输出队列，处理模型输出。</li>
<li>从 Cache 获取上一次迭代的 sampled_token_ids，构造 ExecuteModelRequest 后调用 <code>Executor.execute_model()</code> (最后是由 ModelRunner) 执行模型推理，获取输出。</li>
</ol>
<h2 id="_schedule_prefill">_schedule_prefill()</h2>
<ol>
<li>检查 budget 是否耗尽</li>
<li>取出队列head 部的 SequenceGroup (prefill 阶段 SequenceGroup 只有一个初始 prompt Sequence)</li>
<li>计算 uncached 和 cached 的新 token 数</li>
<li>调用 <code>BlockSpaceManager.can_allocate()</code> 检查是否能分配足够内存。</li>
<li>若能满足 budget，从 waiting 中移除 SequenceGroup. 调用 <code>_allocate_and_set_running()</code> 分配内存并设置为 RUNNING 状态。</li>
</ol>
<h2 id="_schedule_running">_schedule_running()</h2>
<ol>
<li>取出队列head 部 SequenceGroup 并计算其包含 seq 的 #uncached_token. 这里不需要 #cached_token 因为若使用 chunked prefill，该信息已经在第一次 prefill 时使用，如果不使用那么他就是进行 decoding 的 seq ，不需要用到这个信息。</li>
<li>从 running 移除该 SequenceGroup. 循环调用 <code>Scheduler._can_append_slots()</code> 检查是否有足够的空间存储该 SequenceGroup 的 KV Cache，若不能，进入抢占逻辑</li>
<li>从 budget 中减去当前 SequenceGroup 的 token 和 seq 数</li>
<li>若 running 有其他 SequenceGroup，抢占最低优先级（队列尾部）的，若该 SequenceGroup 只有一个正在运行的 Sequence 则抢占模式为 RECOMPUTE 加入到 <code>preempted</code>，否则为 SWAP 加入到 <code>swapped_out</code>.</li>
<li>分配 slot 并更新 blocks_to_copy，根据该 Sequence 处于 decoding(生成 1 个 token 的 KV Cache ) 或者 prefill(生成 #uncached_token 的 KV Cache) 加入到 <code>prefill_seq_group</code> 或者 <code>decode_seq_groups</code>，并更新 budget.</li>
<li>返回 decode_seq_groups：存储 decoding  SequenceGroup. prefill_seq_groups：存储分块 prefill  SequenceGroup. preempted：被抢占需重新计算的 SequenceGroup. swapped_out：被交换到 CPU 的 SequenceGroup. keys_to_swap_out 和 keys_to_copy：内存块交换和复制的映射</li>
</ol>
<h2 id="_schedule_swapepd">_schedule_swapepd()</h2>
<ol>
<li>循环遍历 swapped 队列，取出队列head 部的 SequenceGroup，调用 <code>BlockManager.can_swap_in()</code> (实际上是 SWAPPED 状态的 <code>can_swap</code>)</li>
<li>获取 SequenceGroup 中处于 SWAPPED 的 Sequence 个数和 token 个数，是否满足预算。</li>
<li>调用 <code>_swap_in</code>(实际上是 <code>BlockManager.swap_in()</code>) 执行交换，更新 blocks_to_swap_in，将 Sequence 状态由 SWAPPED 变为 RUNNING.</li>
<li>调用 <code>_append_slots</code> 给被换入的 Sequence 分配 block.</li>
<li>根据 SequenceGroup 的状态添加到不同队列。</li>
<li>返回blocks_to_swap_in：记录需要从 CPU 交换到 GPU 的块映射。blocks_to_copy：记录需要复制的块映射（例如写时复制）。decode_seq_groups 和 prefill_seq_groups：分别存储 decoding 和 prefill  SequenceGroup. infeasible_seq_groups：存储无法调度的 SequenceGroup. swapped_queue：引用交换队列。leftover_swapped：暂存无法立即调度的 SequenceGroup.</li>
</ol>
<h2 id="_schedule_chunked_prefill">_schedule_chunked_prefill()</h2>
<p>主要思想是: 1.安排尽可能多的 decoding 请求。2.调度未完成的 prefill 请求。3.调度交换请求。4.安排新的 prefill 请求。</p>
<ol>
<li>初始化 budget，限制最大批处理 token 数和 seq 数。</li>
<li>从 running 和 waiting 生成 <code>PartialPrefillMetadata</code></li>
</ol>
<ul>
<li>prefills: running 和 waiting 中未完成 prefill 的 #SequenceGroup.</li>
<li>long_prefills: running 中需要进行 prefill 的 token 数很多的 #SequenceGroup.</li>
<li>waiting_long_prefills: waiting 中需要进行且能进行的 (未超过 ScheduleConfig 限制) prefill 的 token 数很多的 #SequenceGroup.</li>
</ul>
<ol start="3">
<li>调用 <code>_schedule_running</code>.</li>
<li>在 running 调度返回中无无抢占或交换时(说明有足够空间) 执行 <code>_schedule_swapped</code></li>
<li>调用 <code>_schedule_prefills</code>.</li>
<li>更新 waiting，添加 running 调度中返回的被抢占的 seq  <code>running_scheduled.preempted</code>.</li>
<li>按优先级更新 running.</li>
<li>swapped_in.decode_seq_groups：交换回来的 decoding 请求。</li>
<li>swapped_in.prefill_seq_groups：交换回来的 prefill 请求。</li>
<li>running_scheduled.decode_seq_groups：运行中的 decoding 请求。</li>
<li>running_scheduled.prefill_seq_groups（按完成顺序）：未完成的分块 prefill 。使用 _order_finishing_prefills_first 确保即将完成的 prefill 优先，便于下一轮转为 decoding.</li>
<li>prefills.seq_groups：新 prefill 请求。</li>
<li>将运行队列中交换出去的 <code>running_scheduled.swapped_out</code> 添加到 swapped.</li>
<li>按顺序组合所有调度的 SequenceGroup: prefill 优先（满足注意力机制假设），decoding 次之。</li>
<li>调整 lookahead_slots 数量。若所有被调度的均为 prefill 且未启用多步调度，设置 num_lookahead_slots = 0(避免推测 decoding 路径). 否则，使用 running 计算的 lookaheadh slots 数量。</li>
</ol>
<h2 id="_schedule_default">_schedule_default</h2>
<p>尽可能多地批处理 prefill 请求，然后调度 decoding 请求. 在 GPU 内存压力下，需要 preempt 或 swap out 运行中的 decoding 请求。</p>
<ol>
<li>swapped 为空则进行 <code>_schedule_prefills</code>.</li>
<li>如果没有调度任何 prefill 请求，调用 <code>_schedule_running</code>.</li>
<li>如果 running 调度结果中没有发生抢占或换出时 (否则说明资源不够)，执行 <code>_schedule_swapped</code>.</li>
<li>更新 waiting, running &amp; swapped 三个队列。</li>
</ol>
<h2 id="after-schedule">After schedule</h2>
<p>调度结果返回后，</p>
<ol>
<li>遍历调度结果中的 SequenceGroup</li>
<li>遍历该 SequenceGroup 中状态为 RUNNING 的 Sequence. 获取其数据，对应的 BlockID 列表，并更新其访问时间。若使用 prefix_caching, 则调用 <code>BlockManager.get_common_computed_block_ids()</code> 获取共享的已计算的部分的 BlockID 列表。</li>
<li>如果该 SequenceGroup 处于 prefill 阶段，则判断这次调度后是否能完成 prefill.</li>
<li>构造返回结果，标记所有调度 SequenceGroup 的 blocks 为已计算。</li>
</ol>
<h1 id="blockspacemanager">BlockSpaceManager</h1>
<p>用于将 SequenceGroup 操作映射到其包含的对应组件的操作。</p>
<ul>
<li>CpuGpuBlockAlloctor: 根据是否采用 prefix caching 分别为 CPU 和 GPU 初始化一个 Alloctor
<ul>
<li><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/prefix_caching_block.py#L53">PrefixCachingBlockAlloctor</a>: 基于哈希值维护 block 的Cache)重用具有相同哈希值的 block，以避免冗余的内存分配。
<ul>
<li><code>Dict[PrefixHash, BlockId]</code> 将用于 prefix caching blocks 的哈希值与其 BlockID 对应。</li>
<li><code>Dict[BlockId, BlockTracker]</code> 为每个物理 block 初始化一个 BlockTracker.</li>
<li><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/naive_block.py#L13">NaiveBlockAllocator</a> 用于分配不作为 prefix caching 的 blocks. 有一个 <code>RefCounter</code> 表示某个物理 block 被多少逻辑 block 指向。</li>
<li><code>Evictor</code> 采用 LRU 策略驱逐已经Cache) blocks.</li>
<li><code>CopyOnWriterTracker</code> 用于将原先的 block ID 映射到目的 block ID.</li>
</ul>
</li>
</ul>
</li>
<li>Dict[SeqId, BlockTable]: <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/block_table.py#L11">BlockTable</a> 用于将单个 seq 的 KV Cache 映射到物理内存分配。会在调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block_manager.py#L148">_allocate_sequence()</a> 时被初始化。包含一个 <a href="https://github.com/vllm-project/vllm/blob/main/vllm/core/block/common.py#L231">BlockList</a> (block 列表和一个表示对应 ID 的 int 列表) 和 BlockpaceManager 的 BlockAllocator.</li>
<li>ComputedBlocksTracker: 维护一个 <code>Dict[SeqId, List[int]]</code> ( seq id到 seq 块哈希列表的映射)。Cache)个 seq 的完整块 (块全部被占满) 的哈希值。当一个 seq 进行 decoding 时，也相应更新 seq 的哈希值。还有一个 <code>Dict[int, int]</code> ( seq id到已计算 token 数的映射)</li>
</ul>
<h2 id="can_allocate">can_allocate</h2>
<p>在 <code>_schedule_prefills</code> 中被调用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">can_allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol>
<li>取出该 SequenceGroup 中处于 WAITING 状态的第一个 Sequence (i.e. prompt).</li>
<li>调用 <code>BlockTable.get_num_required_blocks()</code> 计算存储 token 和 lookahead slots 所需的最小 block 数 (假设无 prefix caching), i.e. <code>cdiv(len(token_ids) + num_lookahead_slots, block_size)</code>.</li>
<li>调用 <code>BlockAlloctor.get_num_free_blocks()</code> 获取 GPU 上空闲的 block 数 (非 prefix_caching 中的空闲个数 + 可以被驱逐的个数).</li>
<li>返回分配状态</li>
</ol>
<ul>
<li>NEVER: <code>#total - #required &lt; #watermark</code></li>
<li>OK: <code>#free  - #required &gt;= #watermark</code></li>
<li>LATER: <code>#free  - #required &lt; #watermark</code></li>
</ul>
<h2 id="allocate">allocate</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 <code>_schedule_prefills</code> 中步骤 4 中调用的 <code>_allocate_and_set_running</code> 内部被调用。</p>
<ol>
<li>取出该 SequenceGroup 中处于 WAITING 状态的第一个 Sequence (i.e. prompt).</li>
<li>调用 <code>BlockManager._allocate_sequence()</code> 创建一个 BlockTable，在获取 token_ids 列表后调用 <code>BlockTable.allocate()</code> 为该 Sequence 分配 blocks.</li>
<li>将 token_ids 按 _block_size 大小进行分块。最后一块可能不能占满一个 block.</li>
<li>对于能够占满一个 block 的 token_ids 分块，调用 <code>BlockAlloctor.allocate_immutable_block()</code>. 该函数优先从Cache)查找是否已有相同内容的块，若有则直接复用该块并增加其引用计数；否则调用 <code>BlockAlloctor.allocate_mutable_blocks()</code> 分配一个新的 block，并将 token_ids 添加到该 block 中. 该函数会尝试从非 prefix caching blocks 中分配一个 block_id，若没找到则会驱逐一个。</li>
<li>对于最后一个可能被没占满的 block 调用 <code>BlockAlloctor.allocate_mutable_blocks()</code>.</li>
</ol>
<h2 id="can_append_slots">can_append_slots</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">can_append_slots</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>确定 GPU KV Cache 中是否有足够的空间来继续生成指定的 SequenceGroup. 上层接口为 <code>Scheduler._can_append_slots()</code>，在 <code>_schedule_running</code> 中步骤 2 中确定是否需要进行抢占时被调用。</p>
<ol>
<li>遍历该 Sequence Group 中处于 RUNNING 状态的 Sequence 对应的 BlockTable</li>
<li>调用 <code>BlockTable.get_unseen_token_ids()</code> 获取该 Sequence 还未被Cache) token 部分。</li>
<li>调用 <code>BlockTable.get_num_blocks_touched_by_append_slots()</code> 获取Cache)余部分和 lookahead 部分需要几个 block.</li>
<li>调用 <code>BlockAlloctor.get_num_free_blocks()</code> 获取 GPU 上空闲的 block 数.</li>
<li>需要个数小于空闲个数返回 True.</li>
</ol>
<h2 id="append_slots">append_slots</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">append_slots</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上层接口为 <code>Scheduler._append_slots()</code>. 在 <code>_schedule_running</code> 中检查到有空间添加，<code>_schedule_swapped</code> 中有 budget 进行换入，<code>_schedule_prefills</code> 中允许进行 chunked prefill 时被调用。</p>
<ol>
<li>调用 <code>BlockTable.append_token_ids()</code>. 该方法将 tokens 添加到 BlockTable 中的现有 block 中。会调用 <code>BlockTable.ensure_num_empty_slots()</code>， 它查看当前能够容纳多少个 token. 如果没有足够的空间，则使用 <code>BlockAlloctor.allocate_mutable_block()</code> 方法分配新 block.</li>
<li>调用 <code>BlockAllocator.clear_copy_on_writes()</code> 返回一个映射源 block ID 到当前 COW 的目标 block ID 的元组的列表.</li>
</ol>
<h2 id="_can_swap">_can_swap</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_can_swap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">status</span><span class="p">:</span> <span class="n">SequenceStatus</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>根据 status 区分上层接口: RUNNING/SWAPPED 表示需要把该 SequenceGroup 处于 RUNNING/SWAPPED 状态的 Sequence 对应的 blocks 从 GPU/CPU 换到 CPU/GPU.</p>
<ol>
<li>获取 SequenceGroup 中符合指定状态的 seq  Sequence，然后根据 SeqID 获取对应的 BlockTable.</li>
<li>调用 <code>BlockTable.get_num_blocks_touched_by_append_slots()</code> 计算添加未存储 token 加上 lookahead_slots 所需的 block 数量。</li>
<li>调用 <code>BlockAlloctor.get_num_full_blocks_touched()</code> 获取当前有被使用的 block 数量。</li>
<li>如果总块数小于被使用的加上需要的 block 数量 返回 Never. 如果空闲块减去 被使用的加上需要的 block 数量后仍大于等于 watermark_blocks，返回 OK. 否则为 LATER.</li>
</ol>
<h2 id="swap_in">swap_in</h2>
<p>调用的是  <code>self.block_allocator.swap(blocks=blocks, src_device=Device.CPU, dst_device=Device.GPU)</code>，即 blocks 从原设备的换出，换入到目的设备。
进一步则是 <code>BlockAlloctor.swap_in()</code>，该函数遍历传入的 blocks，若已经被占满调用 <code>BlockAlloctor.allocate_immutable_block()</code>. 否则调用 <code>BlockAlloctor.allocate_mutable_blocks()</code> 分配一个新的 block 后将原 block的 token 数据追加到新 block.</p>
<h2 id="swap_out">swap_out</h2>
<p>同上，最终调用的是 <code>BlockAlloctor.swap_out()</code>. 该函数对传入的每个 block 调用 <code>_free_block_id</code>，逐个处理释放逻辑。若 block 有哈希值，refcount -1，若减去后为 0 则将 block 信息添加到 evictor 中，从跟踪系统中移除，然后设置 BlockId 为 None. 否则就直接设置为 None. 若无哈希值则释放 BlockId，减去对应的 refcount，但保留 block 对象本身.</p>
<h1 id="attention">Attention</h1>
<p><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/attention/backends/xformers.py#L354">XFormersImpl</a> 中使用了 vllm 自己写的 PagedAttention kernel.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">XFormersImpl</span><span class="p">(</span><span class="n">AttentionImpl</span><span class="p">[</span><span class="n">XFormersMetadata</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">head_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">num_kv_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">alibi_slopes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">      <span class="n">sliding_window</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">      <span class="n">kv_cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">blocksparse_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">logits_soft_cap</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">attn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其中 <code>attn_type</code> 分为四种，下面我们主要分析 DECODER 的情况。</p>
<ul>
<li>DECODER: 使用 decoding 器的 self-attention block table 来Cache)KV(GPT).</li>
<li>ENCODER: 不进行 KV Cache)用于 Encoder-Decoder 模编码器分支。编码器通常一次性处理整个输入 seq 。</li>
<li>ENCODER-ONLY: 不进行 KV Cache)BERT).</li>
<li>ENCODER_DECODER: 用于编码器- decoding 器模型中的交叉注意力部分，其中 KV  seq 长度与编码器 seq 长度一致(T5).</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">layer</span><span class="p">:</span> <span class="n">AttentionLayer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [2, num_blocks, block_size * num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/attention/backends/.py#L104">AttentionMetadata</a> 类定义如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@dataclass</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AttentionMetadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Attention metadata for prefill and decode batched together.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_prefills</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># prefill 请求的总数</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_prefill_tokens</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># 所有 prefill 请求中的 token 总数。</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_decode_tokens</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># decodeing token 的数量，等同于 decoding 请求的数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">slot_mapping</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>  <span class="c1"># (num_tokens,)，指定每个输入 token 存储到 KV cache 中的 slot 索引</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># block_idx = x // block_size, block_offset = x % block_size</span>
</span></span><span class="line"><span class="cl">    <span class="n">multi_modal_placeholder_index_maps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="nb">str</span><span class="p">,</span> <span class="n">MultiModalPlaceholderMap</span><span class="o">.</span><span class="n">IndexMap</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">    <span class="n">enable_kv_scales_calculation</span><span class="p">:</span> <span class="nb">bool</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>forward 方法如下，简化了成了 DECODER 情况的逻辑。
主要流程为</p>
<ol>
<li>调用 <code>PagedAttention.split_kv_cache</code> 分离并 reshape KV Cache 张量后 调用 PagedAttention.write_to_paged_cache`
写入当前 key 和 value 到Cache)。</li>
<li>分离 prefill 和 decoding 的 token，初始化输出。对于 prefill 部分根据是否采用了 prefix_caching 调用 <code>self._run_memory_efficient_xformers_forward</code> 或 <code>PagedAttention.forward_prefix</code> 计算注意力。</li>
<li>调用 <code>get_seq_len_block_table_args</code> 获取 decoding Sequence 对应的 BlockTable后调用 <code>PagedAttention.forward_decode</code> 计算注意力。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">layer</span><span class="p">:</span> <span class="n">AttentionLayer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>    <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [2, num_blocks, block_size * num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">    <span class="c1"># 将 query 重塑为 [num_tokens, num_heads, head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># key 和 value 必须非空（自注意力要求），重塑为 [num_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果 KV Cache)空，处理Cache)辑</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kv_cache</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 从 kv_cache 分离出 key_cache 和 value_cache</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># key_cache: [num_blocks, num_kv_heads, head_size/x, block_size, x]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># value_cache: [num_blocks, num_kv_heads, head_size, block_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">split_kv_cache</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv_cache</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新自注意力的 KV Cache)        # 使用 attn_metadata.slot_mapping 指定 token 存储位置</span>
</span></span><span class="line"><span class="cl">        <span class="n">PagedAttention</span><span class="o">.</span><span class="n">write_to_paged_cache</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">slot_mapping</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取 prefill 和 decoding 阶段的 token 数量</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">num_prefill_query_tokens</span><span class="p">,</span> <span class="n">num_prefill_kv_tokens</span><span class="p">,</span> <span class="n">num_decode_query_tokens</span><span class="p">)</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">        <span class="n">get_num_prefill_decode_query_kv_tokens</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 创建输出张量与 query 相同</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 分离 prefill 和 decoding 的 QKV</span>
</span></span><span class="line"><span class="cl">    <span class="n">decode_query</span> <span class="o">=</span> <span class="n">query</span><span class="p">[</span><span class="n">num_prefill_query_tokens</span><span class="p">:]</span>  <span class="c1">#</span>
</span></span><span class="line"><span class="cl">    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span>     
</span></span><span class="line"><span class="cl">    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[:</span><span class="n">num_prefill_kv_tokens</span><span class="p">]</span>             
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[:</span><span class="n">num_prefill_kv_tokens</span><span class="p">]</span>         
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 prefill 阶段（如果存在）</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">prefill_meta</span> <span class="o">:=</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">prefill_metadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">kv_cache</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 普通注意力（无Cache)缀）</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_memory_efficient_xformers_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="p">,</span> <span class="n">attn_type</span><span class="o">=</span><span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 前缀Cache)意力</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">forward_prefix</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">prefill_meta</span><span class="o">.</span><span class="n">block_tables</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">query_start_loc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">prefill_meta</span><span class="o">.</span><span class="n">seq_lens_tensor</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">max_query_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 decoding 阶段（如果存在）</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">decode_meta</span> <span class="o">:=</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">decode_metadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 获取 decoding 所需的 seq 长度和 BlockTable 参数</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq_lens_arg</span><span class="p">,</span> <span class="n">max_seq_len_arg</span><span class="p">,</span> <span class="n">block_tables_arg</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">get_seq_len_block_table_args</span><span class="p">(</span><span class="n">decode_meta</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 运行 decoding 注意力</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">[</span><span class="n">num_prefill_query_tokens</span><span class="p">:]</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">forward_decode</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">decode_query</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">block_tables_arg</span><span class="p">,</span> <span class="n">seq_lens_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_seq_len_arg</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 将输出 reshape 为 [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="write_to_paged_cache">write_to_paged_cache</h2>
<p>调用的是已经注册到 torch.ops 中的 CUDA 函数。其对应的 host 函数为每个 token 分配一个 CUDA block，每个 CUDA block 的线程数被限制在最多 512 个。主要的 kernel 函数如下。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// scalar_t: 输入 key 和 value 的数据类型（如 float、half）
</span></span></span><span class="line"><span class="cl"><span class="c1">// cache_t: Cache)key_cache 和 value_cache 的数据类型（如 half、uint8_t）
</span></span></span><span class="line"><span class="cl"><span class="c1">// kv_dt: KV Cache) FP8 数据类型（如 kAuto 或具体 FP8 格式）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">template</span> <span class="o">&lt;</span><span class="kr">typename</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="kr">typename</span> <span class="kt">cache_t</span><span class="p">,</span> <span class="n">Fp8KVCacheDataType</span> <span class="n">kv_dt</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">reshape_and_cache_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">key</span><span class="p">,</span>    <span class="c1">// [num_tokens, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">value</span><span class="p">,</span>  <span class="c1">// [num_tokens, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">key_cache</span><span class="p">,</span>     <span class="c1">// [num_blocks, num_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">value_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">slot_mapping</span><span class="p">,</span>  <span class="c1">// [num_tokens]，指定每个 token 的Cache)置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">key_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">value_stride</span><span class="p">,</span>  <span class="c1">// key 和 value 在 token 维的步幅
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_heads</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">head_size</span><span class="p">,</span>      <span class="c1">// 注意力head 数和每个head 的维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">block_size</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">x</span><span class="p">,</span>             <span class="c1">// Cache)大小和 key_cache 中 head_size 的拆分因子
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">k_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v_scale</span><span class="p">)</span>    <span class="c1">// key 和 value 的缩放因子，用于数据类型转换
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  <span class="c1">// host 函数定义 block 个数与 token 个数相同
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">slot_idx</span> <span class="o">=</span> <span class="n">slot_mapping</span><span class="p">[</span><span class="n">token_idx</span><span class="p">];</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// Cache Block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">slot_idx</span> <span class="o">/</span> <span class="n">block_size</span><span class="p">;</span>  <span class="c1">// 块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">block_offset</span> <span class="o">=</span> <span class="n">slot_idx</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">;</span>  <span class="c1">// 块内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">;</span>  <span class="c1">// 每个 token 的维度数目
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// CUDA Block 级别并行，每个线程处理token 的一个维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算输入 key 和 value 的源索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">src_key_idx</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">*</span> <span class="n">key_stride</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">src_value_idx</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">*</span> <span class="n">value_stride</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算当前处理的head 索引和head 内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">head_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="n">head_size</span><span class="p">;</span>      <span class="c1">// 第几个head 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">head_offset</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">head_size</span><span class="p">;</span>   <span class="c1">// head 内的第几个元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 将 head_offset 拆分为 x_idx 和 x_offset（仅用于 key_cache）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">x_idx</span> <span class="o">=</span> <span class="n">head_offset</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>       <span class="c1">// head_size/x 维的索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">x_offset</span> <span class="o">=</span> <span class="n">head_offset</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>    <span class="c1">// x 维的偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算 key_cache 的目标索引，按维度逐步偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">tgt_key_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">head_size</span> <span class="o">/</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>  <span class="c1">// 块偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_idx</span> <span class="o">*</span> <span class="p">(</span><span class="n">head_size</span> <span class="o">/</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>               <span class="c1">// head 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">x_idx</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>                                    <span class="c1">// head_size/x 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">block_offset</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">;</span>                                <span class="c1">// 块内和 x 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算 value_cache 的目标索引，按维度逐步偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">tgt_value_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>            <span class="c1">// 块偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_idx</span> <span class="o">*</span> <span class="n">head_size</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>                         <span class="c1">// head 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_offset</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>                                  <span class="c1">// head_size 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">block_offset</span><span class="p">;</span>                                               <span class="c1">// 块内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 从输入张量读取当前元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">scalar_t</span> <span class="n">tgt_key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="n">src_key_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">scalar_t</span> <span class="n">tgt_value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="n">src_value_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 根据 kv_dt 类型决定存储方式
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="nf">constexpr</span> <span class="p">(</span><span class="n">kv_dt</span> <span class="o">==</span> <span class="n">Fp8KVCacheDataType</span><span class="o">::</span><span class="n">kAuto</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 如果是 kAuto，直接存储，不进行类型转换
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">key_cache</span><span class="p">[</span><span class="n">tgt_key_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_key</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">value_cache</span><span class="p">[</span><span class="n">tgt_value_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_value</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 否则，使用 scaled_convert 进行类型转换（如 FP8 量化）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">key_cache</span><span class="p">[</span><span class="n">tgt_key_idx</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">          <span class="n">fp8</span><span class="o">::</span><span class="n">scaled_convert</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="n">kv_dt</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tgt_key</span><span class="p">,</span> <span class="o">*</span><span class="n">k_scale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="n">value_cache</span><span class="p">[</span><span class="n">tgt_value_idx</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">          <span class="n">fp8</span><span class="o">::</span><span class="n">scaled_convert</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="n">kv_dt</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tgt_value</span><span class="p">,</span> <span class="o">*</span><span class="n">v_scale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="_run_memory_efficient_xformers_forward">_run_memory_efficient_xformers_forward</h2>
<p>也同样简化成 DECODER 的逻辑的情况</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_memory_efficient_xformers_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_prefill_tokens, num_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>    <span class="c1"># [num_prefill_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_prefill_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">original_query</span> <span class="o">=</span> <span class="n">query</span>  <span class="c1"># 保存原始 query，用于最后 reshape 输出</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 GQA/MQA</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape Q to [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># expand K to [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># expand V to  [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                            <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取或设置 attention bias</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">_get_attn_bias</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attn_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># 确保 seq 长度信息存在</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 创建 causal mask</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">BlockDiagonalCausalMask</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># 如果有滑动窗口，应用局部注意力</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">make_local_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="p">[</span><span class="n">attn_bias</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 使用 ALiBi 偏置（线性偏置注意力）</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">_make_alibi_bias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_set_attn_bias</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 执行 xFormers 高效注意力计算</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 为 QKV 添加 batch</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ALiBi 模式直接使用 attn_bias</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">original_query</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># xformers 不支持在自定义 bias 的情况下每个 seq 的长度不同</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">):</span> 
</span></span><span class="line"><span class="cl">            <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">key</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">value</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">original_query</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">            <span class="n">start</span> <span class="o">+=</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 将输出 reshape 为原始 query </span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">original_query</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="forward_prefix">forward_prefix</h2>
<p>不考虑 ALiBi 的情况调用的是 triton 编写的 <a href="https://github.com/vllm-project/vllm/blob/d1695758b2f65fd314d1aee71ba2469ceba67a5b/vllm/attention/ops/prefix_prefill.py#L22">_fwd_kernel()</a> 每个线程块独立处理一个 Q 的一部分，对 KV Cache 和 当前 KV 分别采取 flash-attention 的计算策略。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">triton</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">triton.language</span> <span class="k">as</span> <span class="nn">tl</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@triton.jit</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_fwd_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 输入张量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">Q</span><span class="p">,</span>  <span class="c1">#  Query 张量: [total_seq_len, num_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># total_seq_len 是所有 batch  seq 长度的总和，当前块为 [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="n">K</span><span class="p">,</span>  <span class="c1"># 键张量（当前输入）: [total_seq_len, num_kv_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">    <span class="n">V</span><span class="p">,</span>  <span class="c1"># 值张量（当前输入）: [total_seq_len, num_kv_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">    <span class="n">K_cache</span><span class="p">,</span>  <span class="c1"># 键Cache) [num_blocks, num_kv_heads, head_dim, block_size, x]</span>
</span></span><span class="line"><span class="cl">              <span class="c1"># 用于存储上下文部分的 K</span>
</span></span><span class="line"><span class="cl">    <span class="n">V_cache</span><span class="p">,</span>  <span class="c1"># 值Cache) [num_blocks, num_kv_heads, head_dim, block_size]</span>
</span></span><span class="line"><span class="cl">              <span class="c1"># 用于存储上下文部分的 V</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Loc</span><span class="p">,</span>  <span class="c1"># 块索引表: [batch_size, max_seq_len // block_size]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 记录每个 batch 中每个块的块编号</span>
</span></span><span class="line"><span class="cl">    <span class="n">sm_scale</span><span class="p">,</span>  <span class="c1"># softmax 缩放因子，通常为 1/sqrt(head_dim)</span>
</span></span><span class="line"><span class="cl">    <span class="n">k_scale</span><span class="p">,</span>  <span class="c1"># 用于 FP8 精度转换的缩放因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">v_scale</span><span class="p">,</span>  <span class="c1"># 用于 FP8 精度转换的缩放因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Start_Loc</span><span class="p">,</span>  <span class="c1">#  batch 起始位置: [batch_size + 1]</span>
</span></span><span class="line"><span class="cl">                  <span class="c1"># 每个 batch 的全局 seq 起始索引，最后一个元素是总长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Seqlen</span><span class="p">,</span>  <span class="c1">#  batch  seq 长度: [batch_size]</span>
</span></span><span class="line"><span class="cl">               <span class="c1"># 每个 batch 的总 seq 长度（上下文 +  Query ）</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_size</span><span class="p">,</span>  <span class="c1"># 每个Cache)的大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span>  <span class="c1"># K_cache 的额外维度分片因子（通常为 1 或小整数）</span>
</span></span><span class="line"><span class="cl">    <span class="n">Out</span><span class="p">,</span>  <span class="c1"># 输出张量: [total_seq_len, num_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># 存储注意力计算结果</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 步幅参数 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_b_loc_b</span><span class="p">,</span>  <span class="c1"># B_Loc 的 batch 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_b_loc_s</span><span class="p">,</span>  <span class="c1"># B_Loc 的 seq 块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qbs</span><span class="p">,</span>  <span class="c1"># Q 的 batch / seq 步幅，通常为 num_heads * head_dim</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qh</span><span class="p">,</span>   <span class="c1"># Q 的head 步幅，通常为 head_dim</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qd</span><span class="p">,</span>   <span class="c1"># Q 的head_size步幅，通常为 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kbs</span><span class="p">,</span>  <span class="c1"># K 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kh</span><span class="p">,</span>   <span class="c1"># K 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kd</span><span class="p">,</span>   <span class="c1"># K 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vbs</span><span class="p">,</span>  <span class="c1"># V 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vh</span><span class="p">,</span>   <span class="c1"># V 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vd</span><span class="p">,</span>   <span class="c1"># V 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_obs</span><span class="p">,</span>  <span class="c1"># Out 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_oh</span><span class="p">,</span>   <span class="c1"># Out 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_od</span><span class="p">,</span>   <span class="c1"># Out 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_bs</span><span class="p">,</span>  <span class="c1"># K_cache 的块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_h</span><span class="p">,</span>   <span class="c1"># K_cache 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_d</span><span class="p">,</span>   <span class="c1"># K_cache 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_bl</span><span class="p">,</span>  <span class="c1"># K_cache 的块内偏移步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_x</span><span class="p">,</span>   <span class="c1"># K_cache 的额外维度步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_bs</span><span class="p">,</span>  <span class="c1"># V_cache 的块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_h</span><span class="p">,</span>   <span class="c1"># V_cache 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_d</span><span class="p">,</span>   <span class="c1"># V_cache 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_bl</span><span class="p">,</span>  <span class="c1"># V_cache 的块内偏移步幅</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 超参数 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_queries_per_kv</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>  <span class="c1"># 每个 KV head 对应的 Query head 数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">IN_PRECISION</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 输入精度（例如 tl.float32）</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#  Query 块大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_DMODEL</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># head 维度大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># head 维度填充到 2 的幂次</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># KV 块大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">SLIDING_WINDOW</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 滑动窗口大小，0 表示无窗口</span>
</span></span><span class="line"><span class="cl">    <span class="n">SKIP_DECODE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 是否跳过解码（仅处理上下文）</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 网格定义 ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># grid = (batch_size, num_heads, max_seq_len // BLOCK_M)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 当前 batch 索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_head</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># 当前head 索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">start_m</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>    <span class="c1"># 当前 Query 块索引</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 KV head 索引 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_kv_head</span> <span class="o">=</span> <span class="n">cur_head</span> <span class="o">//</span> <span class="n">num_queries_per_kv</span>  <span class="c1"># 当前 KV head 索引</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 加载 batch 信息 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_seq_len</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Seqlen</span> <span class="o">+</span> <span class="n">cur_batch</span><span class="p">)</span>  <span class="c1"># 当前 batch 总 seq 长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_in_all_start_index</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Start_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span><span class="p">)</span>  <span class="c1"># 当前 batch 全局起始索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_in_all_stop_index</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Start_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 下一 batch 起始索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_query_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">cur_batch_in_all_stop_index</span> <span class="o">-</span> 
</span></span><span class="line"><span class="cl">                          <span class="n">cur_batch_in_all_start_index</span><span class="p">)</span>  <span class="c1"># 当前 batch  Query 长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_ctx_len</span> <span class="o">=</span> <span class="n">cur_batch_seq_len</span> <span class="o">-</span> <span class="n">cur_batch_query_len</span>  <span class="c1"># 上下文长度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 Query 块起始位置 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_start_loc</span> <span class="o">=</span> <span class="n">BLOCK_M</span> <span class="o">*</span> <span class="n">start_m</span>  <span class="c1"># 当前 Query 块的起始位置</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 初始化索引范围 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>  <span class="c1"># KV 块内偏移: [0, BLOCK_N)</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_d</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">)</span>  <span class="c1"># head_size 偏移: [0, BLOCK_DMODEL_PADDED)</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">)</span>  <span class="c1">#  Query 块内偏移: [start_m * BLOCK_M, (start_m + 1) * BLOCK_M)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 Q 的偏移量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q: [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 定位当前 Query 块在 Q 张量中的内存地址</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_q</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">stride_qbs</span> <span class="o">+</span>  <span class="c1">#  batch 和 seq 偏移</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_head</span> <span class="o">*</span> <span class="n">stride_qh</span> <span class="o">+</span>  <span class="c1"># head 偏移</span>
</span></span><span class="line"><span class="cl">        <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_qd</span>  <span class="c1"># head_size偏移</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 示例: 假设 Q [100, 4, 64], stride_qbs=256, stride_qh=64, stride_qd=1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># cur_batch_in_all_start_index=20, cur_head=1, start_m=1, BLOCK_M=16</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># offs_m=[16, 17, ..., 31], offs_d=[0, 1, ..., 63]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q[0, 0] = (20 + 16) * 256 + 1 * 64 + 0 * 1 = 9216 + 64 = 9280</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q[0, 1] = (20 + 16) * 256 + 1 * 64 + 1 * 1 = 9281</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 创建head_size维度掩码 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">dim_mask</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BLOCK_DMODEL</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">int1</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 屏蔽填充部分，例如 BLOCK_DMODEL=64, BLOCK_DMODEL_PADDED=128，则后 64 个值为 0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 加载 Q 数据 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">q</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">off_q</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加载当前 Query 块，掩码确保不加载超出 Query 长度和填充维度的数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 初始化online softmax 变量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">m_i</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;inf&#34;</span><span class="p">)</span>  <span class="c1"># 最大值</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_i</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 归一化因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 注意力累加</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算上下文注意力（Q 对 KV Cache) ---</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">start_n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">multiple_of</span><span class="p">(</span><span class="n">start_n</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>  <span class="c1"># 确保 start_n 是 BLOCK_N 的倍数</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 Cache 索引 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">bn</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span> <span class="o">*</span> <span class="n">stride_b_loc_b</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                     <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">)</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_b_loc_s</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">mask</span><span class="o">=</span><span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">other</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># bn 是当前 KV Cache的块编号</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: B_Loc=[0, 1, 2, ...], cur_batch=0, start_n=16, block_size=16, offs_n=[0, 1, 2, 3]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># bn = B_Loc[0, 1]（若 stride_b_loc_b=8, stride_b_loc_s=1，则地址为 0*8 + 1*1 = 1）</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 K_cache 偏移量 ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># off_k: [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">off_k</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">bn</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_k_cache_bs</span> <span class="o">+</span>  <span class="c1"># 块偏移</span>
</span></span><span class="line"><span class="cl">            <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_k_cache_h</span> <span class="o">+</span>   <span class="c1"># head 偏移</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">//</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_d</span> <span class="o">+</span>  <span class="c1"># head_size偏移（分片）</span>
</span></span><span class="line"><span class="cl">            <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_bl</span> <span class="o">+</span>  <span class="c1"># 块内偏移</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">%</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_x</span>  <span class="c1"># 额外维度偏移</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: bn=[1], cur_kv_head=1, stride_k_cache_bs=4096, stride_k_cache_h=1024, stride_k_cache_d=16</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># offs_d=[0, 1, ..., 63], start_n=16, offs_n=[0, 1, 2, 3], block_size=16, x=1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># off_k[0, 0] = 1*4096 + 1*1024 + (0//1)*16 + (16+0)%16*256 + (0%1)*1 = 4096 + 1024 = 5120</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 K_cache 数据 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_load</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">K_cache</span> <span class="o">+</span> <span class="n">off_k</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 处理 FP8 精度</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">k_load</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_fp8</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="n">k_load</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_scale</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">k_load</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 QK 注意力分数 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">qk</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>  <span class="c1"># [BLOCK_M, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">*=</span> <span class="n">sm_scale</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">SLIDING_WINDOW</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">cur_batch_ctx_len</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-</span> 
</span></span><span class="line"><span class="cl">                          <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">SLIDING_WINDOW</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="o">-</span><span class="mi">10000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- online softmax 更新 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [BLOCK_M]</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">qk</span> <span class="o">-</span> <span class="n">m_ij</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>  <span class="c1"># [BLOCK_M, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [BLOCK_M]</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i_new</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">m_i</span><span class="p">,</span> <span class="n">m_ij</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_i</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">beta</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_ij</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l_i</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">l_ij</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 更新累加器 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_scale</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">p_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc_scale</span> <span class="o">=</span> <span class="n">l_i</span> <span class="o">/</span> <span class="n">l_i_new</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">acc</span> <span class="o">*</span> <span class="n">acc_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 加载 V_cache</span>
</span></span><span class="line"><span class="cl">        <span class="n">off_v</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">bn</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_v_cache_bs</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_v_cache_h</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_v_cache_d</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">%</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">stride_v_cache_bl</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_load</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">V_cache</span> <span class="o">+</span> <span class="n">off_v</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">v_load</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_fp8</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">v_load</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_scale</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="n">v_load</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">acc</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新 m_i 和 l_i</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i</span> <span class="o">=</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i</span> <span class="o">=</span> <span class="n">m_i_new</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算自注意力（Q 对当前 K 和 V） ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算 K 和 V 的初始偏移</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_kbs</span> <span class="o">+</span> <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_kh</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">             <span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_kd</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_v</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_vbs</span> <span class="o">+</span> <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_vh</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">             <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_vd</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="n">k_ptrs</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">off_k</span>  <span class="c1"># 初始指针</span>
</span></span><span class="line"><span class="cl">    <span class="n">v_ptrs</span> <span class="o">=</span> <span class="n">V</span> <span class="o">+</span> <span class="n">off_v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 检查当前 Query 块是否有效</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_mask</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">block_start_loc</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 遍历当前输入的 K 和 V</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">start_n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">block_mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">start_m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">multiple_of</span><span class="p">(</span><span class="n">start_n</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 K 数据 ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 全局偏移: (cur_batch_in_all_start_index + start_n) * stride_kbs 定位 batch 和 seq 块</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: K [100, 4, 64], stride_kbs=256, cur_batch_in_all_start_index=20, start_n=8</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 基地址偏移 = (20 + 8) * 256 = 7168</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># k_ptrs[0, 0] = K + 0 + 1*64 + 0*1 + 7168 = K + 7232</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_ptrs</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">start_n</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_kbs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 QK 注意力分数 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">qk</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">*=</span> <span class="n">sm_scale</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 应用因果掩码</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]),</span> <span class="n">qk</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">SLIDING_WINDOW</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">SLIDING_WINDOW</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="o">-</span><span class="mi">10000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- online softmax 更新 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">qk</span> <span class="o">-</span> <span class="n">m_ij</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i_new</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">m_i</span><span class="p">,</span> <span class="n">m_ij</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_i</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">beta</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_ij</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l_i</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">l_ij</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 更新累加器 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_scale</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">p_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc_scale</span> <span class="o">=</span> <span class="n">l_i</span> <span class="o">/</span> <span class="n">l_i_new</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">acc</span> <span class="o">*</span> <span class="n">acc_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_ptrs</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">start_n</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_vbs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">acc</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新 m_i 和 l_i</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i</span> <span class="o">=</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i</span> <span class="o">=</span> <span class="n">m_i_new</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 存储输出 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_o</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">stride_obs</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_head</span> <span class="o">*</span> <span class="n">stride_oh</span> <span class="o">+</span> <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_od</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_ptrs</span> <span class="o">=</span> <span class="n">Out</span> <span class="o">+</span> <span class="n">off_o</span>
</span></span><span class="line"><span class="cl">    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">out_ptrs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="forward_decode">forward_decode</h2>
<p>调用的是 <a href="https://github.com/vllm-project/vllm/blob/400d483e87b71315bbb73edb0da9fd629212ca82/csrc/attention/attention_kernels.cuh#L90">paged_atention_kernel</a>
gridDim = (num_heads, num_seqs, 1). decode 的时候每个 seq 的 Query 的 toekn 数目都是 1，</p>
<ul>
<li>gridDim = (num_heads, num_seqs, 1): 每个线程块负责一个 seq 的 一个 head，函数定义如下</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="n">template</span> <span class="o">&lt;</span><span class="kr">typename</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="kr">typename</span> <span class="kt">cache_t</span><span class="p">,</span> <span class="kt">int</span> <span class="n">HEAD_SIZE</span><span class="p">,</span> <span class="kt">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>  <span class="c1">// default 16
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="kt">int</span> <span class="n">NUM_THREADS</span> <span class="cm">/*=128*/</span><span class="p">,</span> <span class="n">vllm</span><span class="o">::</span><span class="n">Fp8KVCacheDataType</span> <span class="n">KV_DTYPE</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">          <span class="kt">bool</span> <span class="n">IS_BLOCK_SPARSE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="kt">int</span> <span class="n">PARTITION_SIZE</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>  <span class="c1">// Zero means no partitioning.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__device__</span> <span class="kt">void</span> <span class="nf">paged_attention_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">exp_sums</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads, max_num_partitions]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">max_logits</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                     <span class="c1">// max_num_partitions]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">out</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">q</span><span class="p">,</span>       <span class="c1">// [num_seqs, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">k_cache</span><span class="p">,</span>  <span class="c1">// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">v_cache</span><span class="p">,</span>  <span class="c1">// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_kv_heads</span><span class="p">,</span>               <span class="c1">// [num_heads]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">float</span> <span class="n">scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">block_tables</span><span class="p">,</span>  <span class="c1">// [num_seqs, max_num_blocks_per_seq]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">seq_lens</span><span class="p">,</span>      <span class="c1">// [num_seqs]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">max_num_blocks_per_seq</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">alibi_slopes</span><span class="p">,</span>  <span class="c1">// [num_heads]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 矩阵每一维度的 stride，便于移动指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">q_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_block_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_head_stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">k_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">tp_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_local_blocks</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_vert_stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_block_size</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_head_sliding_step</span><span class="p">)</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>首先先计算一下当前线程对应的各种参数，这里根据模板函数定义不使用 PARTITIONING.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// grid = (num_heads, num_seqs, 1) 一个 thread block 处理一个 seq 的 一个 head
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">partition_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">max_num_partitions</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>  <span class="c1">// 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_lens</span><span class="p">[</span><span class="n">seq_idx</span><span class="p">];</span>  <span class="c1">// 该 seq token 数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 计算块范围和 token 范围
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_seq_blocks</span> <span class="o">=</span> <span class="nf">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>  <span class="c1">// seq 要分几块读取
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks_per_partition</span> <span class="o">=</span>  <span class="n">num_seq_blocks</span><span class="p">;</span>  <span class="c1">// 分了几块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">start_block_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>  <span class="c1">// 起始块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">end_block_idx</span> <span class="o">=</span> <span class="n">num_seq_blocks</span><span class="p">;</span>  <span class="c1">// 结束块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">end_block_idx</span> <span class="o">-</span> <span class="n">start_block_idx</span><span class="p">;</span>  <span class="c1">// 当前分区块数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">start_token_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>  <span class="c1">// 起始 token 索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">end_token_idx</span> <span class="o">=</span> <span class="nf">MIN</span><span class="p">(</span><span class="n">start_token_idx</span> <span class="o">+</span> <span class="n">num_blocks</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">);</span>  <span class="c1">// 结束 token 索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_tokens</span> <span class="o">=</span> <span class="n">end_token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">;</span>  <span class="c1">// 当前分区 token 数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 线程组织参数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">THREAD_GROUP_SIZE</span> <span class="o">=</span> <span class="nf">MAX</span><span class="p">(</span><span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// 几个 thread 处理一个 token 32/16=2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_THREAD_GROUPS</span> <span class="o">=</span> <span class="n">NUM_THREADS</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 一个 thread block 被分成几组 128/2=64
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span> <span class="o">=</span> <span class="nf">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>  <span class="c1">// 每线程处理的 token 数 16/32=1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_WARPS</span> <span class="o">=</span> <span class="n">NUM_THREADS</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// warp 个数 128/32=4
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">thread_idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  <span class="c1">// 线程索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_idx</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程位于第几个 warp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">lane</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">%</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程是该 warp 中的第几个
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">head_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 考虑 GQA MQA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_queries_per_kv</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">/</span> <span class="n">num_kv_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_head_idx</span> <span class="o">=</span> <span class="n">head_idx</span> <span class="o">/</span> <span class="n">num_queries_per_kv</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">float</span> <span class="n">alibi_slope</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">alibi_slopes</span> <span class="o">==</span> <span class="n">nullptr</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">alibi_slopes</span><span class="p">[</span><span class="n">head_idx</span><span class="p">];</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>定义 thread group ，保证其一次访问的数据为 16 Bytes，需要计算其中每个 thread 处理几个元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// VEC_SIZE 即为一个 thread group 中每个线程需要处理元素个数，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">VEC_SIZE</span> <span class="o">=</span> <span class="nf">MAX</span><span class="p">(</span><span class="mi">16</span> <span class="o">/</span> <span class="p">(</span><span class="n">THREAD_GROUP_SIZE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">scalar_t</span><span class="p">)),</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// 16/2/2=4 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">using</span> <span class="n">K_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">using</span> <span class="n">Q_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">using</span> <span class="n">Quant_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">=</span> <span class="n">HEAD_SIZE</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 每个 thread 处理几个元素 64/2=32
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_VECS_PER_THREAD</span> <span class="o">=</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">/</span> <span class="n">VEC_SIZE</span><span class="p">;</span>  <span class="c1">// 这几个元素相当于几个向量  32/4=8
</span></span></span><span class="line"><span class="cl"><span class="c1">// thread_idx = thread_group_idx * THREAD_GROUP_SIZE + thread_group_offset
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">thread_group_idx</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程位于第几个 thread group
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">thread_group_offset</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">%</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程是该 thread group 中第几个线程
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>下面将 Q 加载进共享内存。

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7a7b85b64fbddcf13d703135a4bf6d32?method=download&amp;shareKey=6ca032c977b9f14a0864999633e8e08f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7a7b85b64fbddcf13d703135a4bf6d32?method=download&amp;shareKey=6ca032c977b9f14a0864999633e8e08f" alt="loadQ">
    </a><figcaption>loadQ</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">q_ptr</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">q_stride</span> <span class="o">+</span> <span class="n">head_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">__shared__</span> <span class="n">Q_vec</span> <span class="n">q_vecs</span><span class="p">[</span><span class="n">THREAD_GROUP_SIZE</span><span class="p">][</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>  <span class="c1">// HEAD_SIZE * VEC_SIZE * sizeof(scalar_t) 大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_group_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREAD_GROUPS</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// NUM_ELEMS_PER_THREAD / VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 使得每个 thread group 的线程访问相邻的 vec
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Q_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">q_ptr</span> <span class="o">+</span> <span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>假设块不稀疏并且把不采用量化，加载 K 并计算 <a href="mailto:Q@K.T">Q@K.T</a>. 核心思想是一个 thread group 访问 16 Bytes. 一个 thread 访问一个 vec，一个向量包含的元素个数 <code>VEC_SIZE = 16 / sizeof (scalar_t) / THREAD_GROUP_SIZE</code></p>
<ol>
<li>1st for 循环确定的是每次迭代中每个 warp 处理的是哪一个 block，一共要循环 num_seq_blocks / NUM_WARPS 次</li>
<li>2nd for 循环确定的是该 warp 中的每个 thread group 访问的是该 block 的第几个 token. 即每个线程组处理一个 token.</li>
<li>3rd for 循环确定的是该 thread group 中的每个 thread 访问的是第几个 vec. 该循环使得该 thread group 里面的线程读取一个完整的 headsize. 一次迭代读取的大小为 16 Bytes.</li>
</ol>
<p>首先将 block_table 指针移动到存储该 kv cache 的首个 blockID 处，取出实际的物理块 ID，用在第三个 for 循环中将指针移动到该 K cache block 起始处. 由于
k_cache 的 shape 是 <code>[num_blocks, num_kv_heads, head_size/x, block_size, x]</code>，在第三个 for 循环中 k_ptr 被移动到了该 thread_group 要读取的 block 的 token 的 head 处。<code>vec_idx * VEC_SIZE</code> 即为 thread 要读取的元素开始位置，/x 表示对应的是第几个 16Bytes 划分, offset1 移动的是 dim3，offset2 移动的 则是 dim4.</p>
<p>3rd loop 结束后已经读取了一个 K cache 的完整 head_size 到寄存器中，因此 qk 为一个 token 的一个 head 的 Score Matrix. 根据 token_idx 由每个 thread group 里的 第一个线程负责将累加和到 logits 中并更新 qk_max。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="c1">// Memory planning.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">char</span> <span class="n">shared_mem</span><span class="p">[];</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span><span class="o">*</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">shared_mem</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Workspace for reduction.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">red_smem</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">NUM_WARPS</span><span class="p">];</span>  <span class="c1">// 前一半用于存储 qk_max 后一半用于存储 exp_sum
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 每次 thread group 一次取的元素数量 保证为 16 bytes
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">cache_t</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="kt">float</span> <span class="n">qk_max</span> <span class="o">=</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 指针移动到当前 seq 对应的首个 blockID
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">block_table</span> <span class="o">=</span> <span class="n">block_tables</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">max_num_blocks_per_seq</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">    <span class="c1">// 每个 warp 处理一个 block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">physical_block_number</span> <span class="o">=</span> <span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">block_table</span><span class="p">[</span><span class="n">block_idx</span><span class="p">]);</span>  <span class="c1">// 该 warp 当前处理的 block 对应的 id
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Load a key to registers.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// BLOCK_SIZE(16) / WARP_SIZE(32) = 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">thread_group_idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>  <span class="c1">// thread group 处理的是该 block 的第几个 token
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">const</span> <span class="kt">int</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>  <span class="c1">// 该 token 是该 seq 的第几个
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// NUM_ELEMS_PER_THREAD(32) / VEC_SIZE(4) = 8
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">k_ptr</span> <span class="o">=</span> <span class="n">k_cache</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">physical_block_number</span> <span class="o">*</span> <span class="n">kv_block_stride</span> <span class="o">+</span>  <span class="c1">// 移动到该 block 起始处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">kv_head_idx</span> <span class="o">*</span> <span class="n">kv_head_stride</span> <span class="o">+</span>  <span class="c1">// 移动到对应的 head 处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">physical_block_offset</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 移动到对应的 token 处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 该 thread 要读取 head_size 划分成的第几个 vec
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">offset1</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 第几个 16Bytes 划分
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">offset2</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 划分的第几个元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nf">constexpr</span> <span class="p">(</span><span class="n">KV_DTYPE</span> <span class="o">==</span> <span class="n">Fp8KVCacheDataType</span><span class="o">::</span><span class="n">kAuto</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">k_vecs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">*</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">K_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">k_ptr</span> <span class="o">+</span> <span class="n">offset1</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">offset2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// Compute dot product.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// This includes a reduction across the threads in the same thread group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="nf">dot</span><span class="p">(</span><span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// Add the ALiBi bias if slopes are given.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">qk</span> <span class="o">+=</span> <span class="p">(</span><span class="n">alibi_slope</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">alibi_slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// 每个线程组的第一个线程进行更新 max
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">bool</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">&gt;=</span> <span class="n">seq_len</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span><span class="p">[</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">qk</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">qk_max</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="nl">qk_max</span> <span class="p">:</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">qk</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB36d66a13612972c7f567ed8f20600664?method=download&amp;shareKey=9a305814befc64b17e64feb1c8d76b17" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB36d66a13612972c7f567ed8f20600664?method=download&amp;shareKey=9a305814befc64b17e64feb1c8d76b17" alt="load k &amp; QK Mul">
    </a><figcaption>load k &amp; QK Mul</figcaption></figure></p>
<p>上面这一段结束后下面每个 warp 内 thread group 中的第一个线程已经记录了该 group 的 qk_max. 下一步则是在 warp 内进行 qk_max 归约，存储在共享内存 red_smem 中。 由于一个 warp 处理的是一个 block，相当于现在 red_smem 每个元素存储了对应 block 内的 qk_max.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="nf">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">red_smem</span><span class="p">[</span><span class="n">warp_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">qk_max</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下一步则是在 thread block 内对所有 warp 进行规约，得到该 seq 最后的 qk_max. 然后广播到所有线程中。之后每个线程计算 exp 存入 logits，每个 warp 内的 exp 求和结果存储在 red_smem 的后一半中。最后则是计算 softmax 存到 logits.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="n">qk_max</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">&lt;</span> <span class="n">NUM_WARPS</span> <span class="o">?</span> <span class="n">red_smem</span><span class="p">[</span><span class="n">lane</span><span class="p">]</span> <span class="o">:</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_WARPS</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="nf">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Broadcast the max qk value to all threads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">VLLM_SHFL_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Get the sum of the exp values.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="nf">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">qk_max</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">exp_sum</span> <span class="o">=</span> <span class="n">block_sum</span><span class="o">&lt;</span><span class="n">NUM_WARPS</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">red_smem</span><span class="p">[</span><span class="n">NUM_WARPS</span><span class="p">],</span> <span class="n">exp_sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Compute softmax.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">float</span> <span class="n">inv_sum</span> <span class="o">=</span> <span class="nf">__fdividef</span><span class="p">(</span><span class="mf">1.f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">inv_sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>加载 v 的逻辑与 k 相同，但没有使用 thread group 概念，而是让一个 thread 一次加载 16 Bytes.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Functional Test of Hugo</title>
      <link>http://localhost:1313/blogs/functiontest/</link>
      <pubDate>Sat, 07 Jun 2025 16:05:12 +0800</pubDate>
      <guid>http://localhost:1313/blogs/functiontest/</guid>
      <description>function test</description>
      <content:encoded><![CDATA[<h1 id="github-card">Github Card</h1>
<div class="github">
    <div class="github_bar">
        <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" viewBox="0 0 50 50"><path d="M17.791,46.836C18.502,46.53,19,45.823,19,45v-5.4c0-0.197,0.016-0.402,0.041-0.61C19.027,38.994,19.014,38.997,19,39 c0,0-3,0-3.6,0c-1.5,0-2.8-0.6-3.4-1.8c-0.7-1.3-1-3.5-2.8-4.7C8.9,32.3,9.1,32,9.7,32c0.6,0.1,1.9,0.9,2.7,2c0.9,1.1,1.8,2,3.4,2 c2.487,0,3.82-0.125,4.622-0.555C21.356,34.056,22.649,33,24,33v-0.025c-5.668-0.182-9.289-2.066-10.975-4.975 c-3.665,0.042-6.856,0.405-8.677,0.707c-0.058-0.327-0.108-0.656-0.151-0.987c1.797-0.296,4.843-0.647,8.345-0.714 c-0.112-0.276-0.209-0.559-0.291-0.849c-3.511-0.178-6.541-0.039-8.187,0.097c-0.02-0.332-0.047-0.663-0.051-0.999 c1.649-0.135,4.597-0.27,8.018-0.111c-0.079-0.5-0.13-1.011-0.13-1.543c0-1.7,0.6-3.5,1.7-5c-0.5-1.7-1.2-5.3,0.2-6.6 c2.7,0,4.6,1.3,5.5,2.1C21,13.4,22.9,13,25,13s4,0.4,5.6,1.1c0.9-0.8,2.8-2.1,5.5-2.1c1.5,1.4,0.7,5,0.2,6.6c1.1,1.5,1.7,3.2,1.6,5 c0,0.484-0.045,0.951-0.11,1.409c3.499-0.172,6.527-0.034,8.204,0.102c-0.002,0.337-0.033,0.666-0.051,0.999 c-1.671-0.138-4.775-0.28-8.359-0.089c-0.089,0.336-0.197,0.663-0.325,0.98c3.546,0.046,6.665,0.389,8.548,0.689 c-0.043,0.332-0.093,0.661-0.151,0.987c-1.912-0.306-5.171-0.664-8.879-0.682C35.112,30.873,31.557,32.75,26,32.969V33 c2.6,0,5,3.9,5,6.6V45c0,0.823,0.498,1.53,1.209,1.836C41.37,43.804,48,35.164,48,25C48,12.318,37.683,2,25,2S2,12.318,2,25 C2,35.164,8.63,43.804,17.791,46.836z"></path></svg>
        <a class="github_name" href="" target="_blank"></a>
    </div>
    <div class="github_description">this is a github card</div>
    <div class="github_language">
        
    </div>
</div>

<h1 id="big-quote">Big Quote</h1>
<blockquote class="quote"><p>Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell</p></blockquote>
<h1 id="margin-note">Margin Note</h1>
<p>这是一段正常的文本，我们正在讨论一个非常重要的概念。<span class="sidenote-number"><small class="sidenote">这就是<a href="https://www.bilibili.com/">bilibili</a>对那个重要概念的解释和补充说明。你甚至可以在这里使用 <strong>Markdown</strong> 语法！</small></span>
 这个概念源于古希腊，对后世影响深远。</p>
<p>继续你的文章&hellip; 另一处需要注解的地方。<span class="sidenote-number"><small class="sidenote">这是第二个旁注，它会自动对齐，不会和第一个重叠。</small></span>
</p>
<h1 id="various-notice">Various Notice</h1>
<p>关于以下notice 请参考 <span class="sidenote-number"><small class="sidenote"><a href="https://github.com/martignoni/hugo-notice?tab=readme-ov-file">hugo_notice</a></small></span>

<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>This is a warning notice. Be warned!</p></div>
</p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>This is a very good tip.</p></div>

<div class="notice info" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="92 59.5 300 300">
  <path d="M292 303.25V272c0-3.516-2.734-6.25-6.25-6.25H267v-100c0-3.516-2.734-6.25-6.25-6.25h-62.5c-3.516 0-6.25 2.734-6.25 6.25V197c0 3.516 2.734 6.25 6.25 6.25H217v62.5h-18.75c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h87.5c3.516 0 6.25-2.734 6.25-6.25Zm-25-175V97c0-3.516-2.734-6.25-6.25-6.25h-37.5c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h37.5c3.516 0 6.25-2.734 6.25-6.25Zm125 81.25c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Info</p><p>This is a use info.</p></div>

<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>This is a note.</p></div>

]]></content:encoded>
    </item>
    <item>
      <title>All2All Communication Cost</title>
      <link>http://localhost:1313/blogs/all2allcommcost/</link>
      <pubDate>Sun, 12 Jan 2025 16:05:23 +0800</pubDate>
      <guid>http://localhost:1313/blogs/all2allcommcost/</guid>
      <description>Introduction of Transformer Family</description>
      <content:encoded><![CDATA[<p>在 All2All 通信中，每个设备给其他设备发送大小为 m 的不同的消息。此操作相当于使用一维数组分区对分布在 p 个进程中的二维数据数组进行转置，因此也被称作全交换 (<strong>total exchange</strong>)</p>
<h2 id="ring--bidirectional-linear-array">Ring / Bidirectional Linear Array</h2>
<p>线性数组拓扑结构的 All2All 通信中，每个设备需要发送 p-1 份大小为 m 的数据。用 {i,j} 表示消息需要从设备 i 发送到设备 j. 首先，每个节点将所有要发送的数据作为一个大小为 m(p-1) 的合并消息发送给它邻居 (假设所有设备通信方向相同)。当邻居收到这个消息后提取他所需要的那一部分，发送剩下的大小为 m(p-2). 每个设备一共发送 p-1 次，每次要发送的消息大小减少 m.</p>
<p>由此可以得出在 p 个设备组成的线性数组拓扑上进行 All2All 每个设备需要向相邻设备通信 p-1 次，第 i 次通信的消息大小为 m(p-i). 如果向两个方向都进行发送，那么每个方向都只用发送原先一半的数据。</p>
$$
\begin{aligned}T_{ring}&=\quad\sum_{i=1}^{p-1}(t_{s}+t_{w}m(p-i))\\&=\quad t_{s}(p-1)+\sum_{i=1}^{p-1}it_{w}m\\&=\quad(t_{s}+t_{w}mp/2)(p-1).\end{aligned}
$$<p>环状网络中每份消息的平均传输跳数是 $\frac{\sum_{d=1}^{p-1}i}{p-1} = p/2$，因此 p 个节点总共的通信量之和为  $p\times m(p-1)\times\frac p2$  环状网络中总的链路数目为 p. 因此负载平均的情况下，最少需要的时间为 $\frac{m(p-1)\times\frac p2\times p}p = m(p-1)\frac p2$ ，因此算法时间为最优的。</p>
<p>跳数为 d 的消息数量对应于相距 d 的节点对 (i, j)，其中 |i-j|=d</p>
<ul>
<li>(0, d),(1, d+1), \ldots,(p-1-d, p-1)，即 i 从 0 到 p-1-d, j=i+d ，共有 p-d 对。</li>
<li>(d, 0),(d+1,1), \ldots,(p-1, p-1-d)，即  i  从  d  到  p-1, ~ j=i-d  ，也有 p-d 对。
总共有 2(p-d) 条消息的跳数为 d</li>
</ul>
<p>总跳数</p>
$$
\begin{aligned}
\text { 总跳数 } & =\sum_{d=1}^{p-1} d \times 2(p-d) \\
& =2 \sum_{d=1}^{p-1} d(p-d)=2\left(p \sum_{d=1}^{p-1} d-\sum_{d=1}^{p-1} d^{2}\right) \\
& = p \cdot \frac{(p-1) p}{2}-\frac{(p-1) p(2 p-1)}{6} \\
& = =\frac{(p-1) p(p+1)}{6}
\end{aligned}
$$<p>因此平均跳数 =$\frac{\text { 总跳数 }}{\text { 总消息数 }}=\frac{\frac{(p-1) p(p+1)}{3}}{p(p-1)}=\frac{p+1}{3}$</p>
<h2 id="mesh">Mesh</h2>
<p>若 p 个设备组成大小为 $\sqrt{p} \times \sqrt{p}$ 的 mesh 进行 All2All 通信，每个设备首先将其 p 个数据按照目的设备的列进行分组，即分成 $\sqrt{p}$ 组，每组包含大小为 $m\sqrt{p}$ 的消息。假设 3x3 的 mesh，则第一组消息的目的节点为 {0,3,6}，第二组消息的目的节点为 {1,4,7}，第三组消息的目的节点为 {2,5,8}</p>
<p>首先同时分别在每一行中进行 All2All 通信，每一份数据大小为 $m\sqrt{p}$. 通信结束后每个设备拥有的是该行目的设备为所在列的所有数据。然后将数据按照目的设备所在的行进行分组。即设备 {0,3,6} 第一组消息的目的节点为 0，第二组消息的目的节点为 3，第三组消息的目的节点为 6. 然后同时分别在每一列中进行 All2All 通信。</p>
<p>我们只需要将 Linear Array 拓扑结构中的公式的 p 换成 $\sqrt{p}$ ，m 换成 $m\sqrt{p}$，再乘以 2 就得到在 mesh 上进行 All2All 的时间</p>
$$
T_{mesh}=(2t_{s}+t_{w}mp)(\sqrt{p}-1).
$$<h2 id="hypercube">Hypercube</h2>
<p>超立方体拓扑在每个维度上都有两个节点，一共有 $\log{p}$ 个维度。在一共有 p 个节点超立方体中，在某个维度 $d$ 上，超立方体可以被划分为两个 (n−1) 维的子立方体，这两个子立方体通过维度 d 上的 p/2 条链路相连。</p>
<p>在 All2All 通信的任何阶段，每个节点都持有 $p$ 个大小为 $m$ 的数据包。当在特定维度上通信时，每个节点发送 $p/2$ 个数据包 (合并为一条消息)。这些数据包的目的地是由当前维度的链路连接的另一个子立方体包含的节点。在上述过程中，节点必须在每个 $\log{p}$ 通信步骤之前在本地重新排列消息。</p>
<p>$\log{p}$ 步中的每一步，每个设备沿当前维度的双向链路交换大小为 mp/2 的数据。因此在 hypercube 上进行 All2All 的时间为</p>
$$
T_{hcube}=(t_{s}+t_{w}mp/2)\log p.
$$<p>值得注意的是与 ring 和 mesh 算法不同，超立方体算法不是最优的。每个设备发送和接收大小为 m(p- 1) 的数据，超立方体上任意两个节点之间的平均距离为 $\log{p}/2$ . 因此，网络上的总数据流量为 $p\times m(p - 1)\times(\log{p})/2$. 每个超立方体一共有 $p\log{p}/2$  条双向链路，如果流量能够被平分，则通信用时下界应该为</p>
$$
\begin{aligned}T_{min}&=\frac{t_{w}pm(p-1)(\log p)/2}{(p\log p)/2}\\&=t_{w}m(p-1).\end{aligned}
$$<h2 id="optimal-algorithm-in-hypercube">Optimal Algorithm in Hypercube</h2>
<p>在超立方体上，执行 All2All 的最佳方法是让每一对节点彼此直接通信。因此，每个节点只需执行 p-1 次通信，每次与不同设备交换大小为 m 的数据。设备必须在每次通信中选择不会出现拥塞的通信对象。在第 j 次通信中，节点 i 与节点 $i \oplus j$ 交换数据。在超立方体上，从节点 i 到节点 j 的消息必须经过至少 l 条链路，其中 l 是 i 和 j 之间的汉明距离 (即 $i \oplus j$ 的二进制表示中的非零比特数). 我们通过 E-cube 路由来选择路径：</p>
<ol>
<li>将当前节点地址 C 与目标节点地址 D 进行 XOR 操作，得到 $R=C\oplus D$.</li>
<li>找到 R 的最低有效非零位，决定下一步跳转的维度。</li>
<li>沿选定维度跳转到下一个节点，更新当前节点地址。</li>
<li>重复上述步骤，直到 R=0， 即到达目标节点。
对于节点i和节点j之间的消息传输，该算法保证每一步的通信时间为 t_s + t_wm，因为在节点 i 和节点 j 之间的链路上沿着同一方向传播的任何其他消息都不存在竞争，切每一步只切换一个维度，通信距离为 1. 整个 All2All 的总通信时间为</li>
</ol>
$$T_{xor}=(t_{s}+t_{w}m)(p-1).$$<h1 id="bruck-algorithm-in-full-connected-network">Bruck Algorithm in Full-connected Network</h1>
<p>Bruck是一种存储-转发 (store-and-forward) 算法，需要 log(P) 次通信步骤。这意味着发送缓冲区 S 和接收缓冲区 R 都用于在中间通信轮次中发送、接收和存储数据。因为某些接收到的数据块必须在后续通信步骤中使用。这种存储-转发的特性对通信轮次的顺序提出了约束。与线性步骤实现不同，Bruck 必须保持明确的通信顺序，其中第 i+1 次迭代必须在第 i 次迭代之后物理时间上发生。

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1b5aaffb71ec91ead2f725d9249728f1?method=download&amp;shareKey=e4eea75f3b72f77982d47b17590c24b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1b5aaffb71ec91ead2f725d9249728f1?method=download&amp;shareKey=e4eea75f3b72f77982d47b17590c24b3" alt="Bruck">
    </a><figcaption>Bruck</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">Algorithm 2 NCCL Bruck algorithm
</span></span><span class="line"><span class="cl">P ← total number of processes.
</span></span><span class="line"><span class="cl">for i ∈ [0, P] do
</span></span><span class="line"><span class="cl">   R[i] = S[(p+i) % P] // S and R are send and receive buffers, and p is rank id of each process;
</span></span><span class="line"><span class="cl">end for
</span></span><span class="line"><span class="cl">allocate temporary buffer T with SC × (P+1) / 2 elements; // SC is number of elements per data-block.
</span></span><span class="line"><span class="cl">for k = 1; k &lt; P; k &lt;&lt;= 1 do
</span></span><span class="line"><span class="cl">   allocate send indexes array SB with (P+1) / 2 integers;
</span></span><span class="line"><span class="cl">   number of send data-blocks NB ← 0;
</span></span><span class="line"><span class="cl">   for i ∈ [k, P] do
</span></span><span class="line"><span class="cl">      if i &amp; k then
</span></span><span class="line"><span class="cl">            SB[NB] ← i;
</span></span><span class="line"><span class="cl">            copy R[i] into T[NB];
</span></span><span class="line"><span class="cl">            NB ← NB + 1;
</span></span><span class="line"><span class="cl">      end if
</span></span><span class="line"><span class="cl">      sendproc ← (p + k) % P;
</span></span><span class="line"><span class="cl">      recvproc ← (p - k + P) % P;
</span></span><span class="line"><span class="cl">      ncclGroupStart()
</span></span><span class="line"><span class="cl">      send data in T to sendproc;
</span></span><span class="line"><span class="cl">      receive data from recvproc into S;
</span></span><span class="line"><span class="cl">      ncclGroupEnd()
</span></span><span class="line"><span class="cl">      for i ∈ [0, SB] do
</span></span><span class="line"><span class="cl">            copy T[i] into R[SB[i]];
</span></span><span class="line"><span class="cl">      end for
</span></span><span class="line"><span class="cl">   end for
</span></span><span class="line"><span class="cl">   for i ∈ [0, P] do
</span></span><span class="line"><span class="cl">      R[i] = R[(p - i + P) % P] // final rotation;
</span></span><span class="line"><span class="cl">   end for
</span></span><span class="line"><span class="cl">end for
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>line(2-4): 将每个设备发送缓冲区 S 中的数据按照 rank 偏移重新排列拷贝到接收缓冲区 R 中。</li>
<li>line(5): 为通信阶段准备一个临时缓冲区 T</li>
<li>line(6): 通信步开始 k 以指数方式增长 (1, 2, 4, &hellip;)，总共执行 logP 次迭代
<ul>
<li>line(7-14): 用索引数组 SB，记录需要发送的数据块位置。遍历 k~P-1 同通过对 i&amp;k 判断哪些数据块需要在此轮发送. (若 P 是 2 的指数幂，因为 k 是 2 的指数幂，因此只有一位为 1，那么就是每轮发送 p/2 个数据块) 将接收缓冲区 R 中满足条件的数据拷贝到临时缓冲区 T，并记录索引。</li>
<li>line(15-16): 确定要接收和发送的目标。</li>
<li>line(17-20): 进行通信操作，将数据发送到目标的发送缓冲区。</li>
<li>line(21-23): 更新接收缓冲区。</li>
<li>line(25-27): 反向调整接收缓冲区数据的位置。</li>
</ul>
</li>
</ul>
<p>总共 log(p) 步骤每步发送 m 消息。</p>
<h1 id="tree-based">Tree-based</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6ed5f5f2681e4f2c3a57bfb7b901515a?method=download&amp;shareKey=7aafd92596dbc981100138525e0f6d09" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6ed5f5f2681e4f2c3a57bfb7b901515a?method=download&amp;shareKey=7aafd92596dbc981100138525e0f6d09" alt="Tree">
    </a><figcaption>Tree</figcaption></figure></p>
<p>采用先在行上进行 All-gather, 再在列上进行 Scatter. 也需要 log(p) 步，其中 gather 阶段第一步通信量为 m(p-1)，一共进行 0.5log(p) 步每一步通信量翻倍，跳数也翻倍；scatter阶段则是相反，因此两步的通信时间相同总共 t_s*log(p) + m(p-1)^2/3</p>
]]></content:encoded>
    </item>
    <item>
      <title>DistriFusion</title>
      <link>http://localhost:1313/blogs/distrifusion/</link>
      <pubDate>Wed, 23 Oct 2024 14:28:37 +0800</pubDate>
      <guid>http://localhost:1313/blogs/distrifusion/</guid>
      <description>Paper reading about DistriFusion.</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>DistriFusion 将模型输入分割成多个 patch 后分配给 GPU。但是直接实现这样的算法会破坏 patch 之间的交互并失去保真度，而同步 GPU 之间的激活将产生巨大的通信开销。为了克服这一困境，根据观察到的相邻扩散步输入之间的高度相似性提出了 <strong>displaced patch parallelism</strong>，该方法通过重用前一个时间步骤中预先计算的 feature map 来利用扩散过程的顺序性，为当前步提供 context. 该方法支持异步通信，可以通过计算实现流水线化。</p>
<h1 id="introduction">Introduction</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" alt="Original, Navie Patch &amp; DistriFusion">
    </a><figcaption>Original, Navie Patch &amp; DistriFusion</figcaption></figure></p>
<p>加速扩散模型推理主要集中在两种方法上：减少采样步骤和优化网络推理。随着计算资源的快速增长，利用多个 GPU 来加速推理是很有吸引力的。例如在 NLP 中， LLM 已经成功地利用了 GPU 之间的张量并行性，从而显著降低了延迟。然而，对于扩散模型，由于激活尺寸大，张量并行这样的技术不太适合扩散模型。多个 GPU 通常只用于 batch 推理，当生成单个图像时，通常只涉及一个GPU.</p>
<blockquote>
<p>Techniques like tensor parallelism are less suitable for diffusion models due to the large activation size, as communication costs outweigh savings from distributed computation.</p></blockquote>
<p>自然而然的一种方法是将图像分成几个 patch 后分配给不同的设备进行生成。由于各个 patch 之间缺乏相互作用，它在每个 patch 的边界处都有一个清晰可见的分界线。</p>
<p>DistriFusion 也是基于 patch parallelism. 关键在于扩散模型中相邻去噪步骤的输入是相似的，因此，只在第一步采用同步通信。后续步骤重用前一步中预先计算的激活，为当前步骤提供全局上下文和 patch 交互。通过异步通信有效地隐藏了计算中的通信开销。并且还稀疏地在指定的区域上进行卷积和注意力计算，从而按比例减少每个设备的计算量。</p>
<h1 id="method">Method</h1>
<h2 id="displaced-patch-parallelism">Displaced Patch Parallelism.</h2>
<p>在预测 $\epsilon_{\theta}(\mathbf{x}_{t})$ 时 (忽略条件 c 和时间步 t 的输入) ，首先将 $\mathbf{x}_{t}$ 分割成多个 patch $\mathbf{x}_t^{(1)},\mathbf{x}_t^{(2)},\ldots,\mathbf{x}_t^{(N)}$ ，对于每一层 l 和设备 i，在获得输入激活 patch $\mathbf{A}_{t}^{l,(i)}$ 后异步处理两个操作：首先，对于设备i， 激活 $\mathbf{A}_{t}^{l,(i)}$ 首先 scatter 到上一步旧的激活 $\mathbf{A}_{t+1}^{l}$ 中。然后将此分散操作的输出送入稀疏算子 Fl (线性、卷积或注意层)，该算子专门对新区域执行计算并产生相应的输出。同时，对 $\mathbf{A}_{t}^{l,(i)}$ 执行 AllGather 操作，为下一步的全尺寸激活 $\mathbf{A}_{t}^{l}$ 做准备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" alt="Overview of DistriFusion">
    </a><figcaption>Overview of DistriFusion</figcaption></figure></p>
<p>我们对除第一层 (采用同步通信获得其他设备上的输入) 外的每一层重复这个过程。然后将最终输出 Gather 在一起以近似 $\epsilon_{\theta}(\mathbf{x}_{t})$，用于计算 $\mathbf{x}_{t-1}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" alt="Timeline Visualization on Each Device">
    </a><figcaption>Timeline Visualization on Each Device</figcaption></figure></p>
<h2 id="sparse-operations">Sparse Operations</h2>
<p>对于每一层 l，如果原始算子 Fl 是一个卷积层、线性层或交叉注意层，调整使其专门作用于新激活的区域。这可以通过从 scatter 输出中提取最新部分并将其输入到 Fl 中来实现。对于 self-attention，将其转换为 cross-attention，仅在设备上保留来自新激活的 Q，而 KV 仍然包含整个特征图。</p>
<h2 id="corrected-asynchronous-groupnorm">Corrected Asynchronous GroupNorm</h2>
<p>仅对新 patch 进行归一化或重用旧特征都会降低图像质量。同步 AllGather 所有均值和方差将产生相当大的开销。为了解决这一困境，DistriFusion 在陈旧的统计数据中引入了一个校正项。计算公式如下</p>
$$
\mathbb{E}[\mathbf{A}_t]\approx\underbrace{\mathbb{E}[\mathbf{A}_{t+1}]}_{\text{stale global mean}}+\underbrace{\mathbb{E}[\mathbf{A}_t^{(i)}]-\mathbb{E}[\mathbf{A}_{t+1}^{(i)}]}_{\text{correction}}
$$<p>同样对二阶矩 $\mathbb{E}[\mathbf{A}^2_t]$ 也采用这种计算方式，然后通过 $\mathbb{E}[\mathbf{A}^2_t] - \mathbb{E}[\mathbf{A}_t]^2$ 来计算方差。对于方差结果为负的部分，将使用新鲜 patch 的局部方差代替。</p>
<h1 id="code-implementation">Code Implementation</h1>
<p>Distrifusion 中主要就是将 <a href="https://github.com/huggingface/diffusers/blob/9366c8f84bfe47099ff047272661786ebb54721d/src/diffusers/models/unets/unet_2d_condition.py#L71">UNet2DConditionModel</a> 中的 Conv2d, Attention 和 GroupNorm 替换成对应的 patch 实现的网络结构 <a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L15">DistriUNetPP</a>. 这里继承的 BaseModel 类为集成了 PatchParallelismCommManager 类 (介绍见后文) 的网络。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" alt="UNet2DConditionModel">
    </a><figcaption>UNet2DConditionModel</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriUNetPP</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>  <span class="c1"># for Patch Parallelism</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">UNet2DConditionModel</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">UNet2DConditionModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39; 
</span></span></span><span class="line"><span class="cl"><span class="s1">                Substitute Conv2d, Attention, GroupNorm with DistriConv2dPP, DistriSelfAttentionPP, DistriCrossAttentionPP, DistriGroupNorm 
</span></span></span><span class="line"><span class="cl"><span class="s1">                &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">subname</span><span class="p">,</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>  
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">kernel_size</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriConv2dPP</span><span class="p">(</span>  
</span></span><span class="line"><span class="cl">                            <span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="o">=</span><span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;conv_in&#34;</span>
</span></span><span class="line"><span class="cl">                        <span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">Attention</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn1&#34;</span><span class="p">:</span>  <span class="c1"># self attention</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="k">else</span><span class="p">:</span>  <span class="c1"># cross attention</span>
</span></span><span class="line"><span class="cl">                            <span class="k">assert</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn2&#34;</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriCrossAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriGroupNorm</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriUNetPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchparallelismcommmanager">PatchParallelismCommManager</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/utils.py#L112">PatchParallelismCommManager</a> 类主要处理异步通信的部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchParallelismCommManager</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span> <span class="o">=</span> <span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 已经注册的张量的累计总元素数量</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel_dict</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># 记录每个 layer_type 所注册的张量的累计元素数量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 在每个设备上存储所有注册张量的数据，通信所用的 buffer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">starts</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的起始位置 (在 buffer_list 中的起始索引)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ends</span> <span class="o">=</span> <span class="p">[]</span>    <span class="c1">#                 结束                       结束</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shapes</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的 shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">idx_queue</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 需要进行通信的张量索引的队列</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 存储每个设备通信操作的句柄的 list, 用于检查通信是否完成</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>成员函数功能介绍如下</p>
<ol>
<li>
<p><code>register_tensor(self, shape: tuple[int, ...] or list[int], torch_dtype: torch.dtype, layer_type: str = None) -&gt; int</code>: 用于注册张量的形状和数据类型，同时计算并记录张量在缓冲区中的起始位置和结束位置。</p>
<ul>
<li>如果尚未指定 <code>torch_dtype</code>，则将传入的 <code>torch_dtype</code> 设为类成员的默认数据类型。</li>
<li>计算传入张量形状的总元素数 <code>numel</code>，并更新 <code>starts</code>、<code>ends</code> 和 <code>shapes</code> 列表。</li>
<li>如果指定了 <code>layer_type</code>，更新 <code>numel_dict</code> 中该层类型对应的元素数目。</li>
</ul>
</li>
<li>
<p><code>create_buffer(self)</code> : 每个设备上为所有注册的张量创建一个统一的缓冲区。</p>
<ul>
<li>为每个设备创建一个形状为 <code>(numel,)</code> 的张量，并将其放入 <code>buffer_list</code> 中。</li>
<li>输出在各设备上创建的缓冲区总参数量。</li>
</ul>
</li>
<li>
<p><code>get_buffer_list(self, idx: int) -&gt; list[torch.Tensor]</code>: 返回每个设备上对应于指定索引 <code>idx</code> 的缓冲区张量。</p>
<ul>
<li>根据 <code>starts</code> 和 <code>ends</code> 信息，从 <code>buffer_list</code> 中提取指定索引 <code>idx</code> 的张量片段并调整其形状。</li>
</ul>
</li>
<li>
<p><code>communicate(self)</code>: 调用 <code>dist.all_gather</code> 将缓冲区中的张量在不同设备间进行广播。</p>
<ul>
<li>确定当前需要通信的张量范围 (根据 <code>idx_queue</code> 中的索引).</li>
<li>调用 <code>dist.all_gather</code> 在设备组内进行异步广播通信，并将句柄存储在 <code>handles</code> 中。</li>
</ul>
</li>
<li>
<p><code>enqueue(self, idx: int, tensor: torch.Tensor)</code>: 将指定索引 <code>idx</code> 处的张量数据复制到 <code>buffer_list</code> 中，并将索引添加到通信队列 <code>idx_queue</code>。</p>
<ul>
<li>如果通信队列不为空且索引为 0，则先执行一次通信操作。</li>
<li>将张量数据复制到 <code>buffer_list</code> 中的对应位置。</li>
<li>当通信队列长度达到 <code>distri_config</code> 中设定的通信检查点值时，进行通信。</li>
</ul>
</li>
<li>
<p><code>clear(self)</code>: 执行一次所有待通信张量的通信，并等待所有异步操作完成。</p>
<ul>
<li>如果通信队列不为空，则进行通信操作。</li>
<li>遍历所有句柄，等待所有异步操作完成后，将句柄设为 <code>None</code>.</li>
</ul>
</li>
</ol>
<h2 id="districonv2dpp">DistriConv2dPP</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L10">DistriConv2dPP</a> 计算自己负责 patch 部分的卷积，需要通信其他设备需要自己负责 patch 的上下 padding 部分。</p>
<ul>
<li><code>__init__</code>：构造函数，初始化成员变量，设置是否为第一层卷积。</li>
<li><code>naive_forward</code>：执行标准的前向传播，不进行任何切片操作。这是单个设备处理时的普通卷积操作。</li>
<li><code>sliced_forward</code>：处理输入张量的切片操作。根据当前设备索引 (<code>split_idx</code>) 计算输入张量在高度方向的起始和结束位置，并在必要时为切片后的张量添加 padding 后进行卷积操作。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriConv2dPP</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriConv2dPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_layer</span> <span class="o">=</span> <span class="n">is_first_layer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">naive_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#  x: [B, C, H, W]</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sliced_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;...&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 等待上一步通信完成</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">boundary_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># buffer_list 存储的是每个 devive 进行卷积所需要的其他 devive 的数据</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;conv2d&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">            <span class="k">def</span> <span class="nf">create_padded_x</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39;拼接接收到的数据&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># rank 0</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">elif</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># rank n-1</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>  <span class="c1"># other ranks</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="p">[</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                            <span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">padded_x</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 提取当前输入张量需要发送给其他设备的部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">boundary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">boundary_size</span><span class="p">,</span> <span class="p">:],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">boundary_size</span><span class="p">:,</span> <span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 直接用上一步的 buffer 拼接</span>
</span></span><span class="line"><span class="cl">            <span class="n">padded_x</span> <span class="o">=</span> <span class="n">create_padded_x</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">padded_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">boundary</span><span class="p">)</span>  <span class="c1"># 插入自己要发送的数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distriselfattentionpp">DistriSelfAttentionPP</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/attn.py#L107">DistriSelfAttentionPP</a> 只负责计算自己 patch 的输出，需要完整的 KV，将 self attention 运算变成 cross-attention 计算。需要通信自己的 KV.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">DistriAttentionPP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriSelfAttentionPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>  <span class="c1"># 获取 Attention 模块</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>  <span class="c1"># 残差连接</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">args</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">USE_PEFT_BACKEND</span> <span class="k">else</span> <span class="p">(</span><span class="n">scale</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Q Projection</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_kv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>  <span class="c1"># KV Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># 如果缓冲区未创建</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">buffer</span> <span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span> <span class="o">=</span> <span class="n">kv</span>
</span></span><span class="line"><span class="cl">        <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_buffer_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将 full_kv 分割为 key 和 value</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">full_kv</span><span class="p">,</span> <span class="n">full_kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># multi-head attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># O Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># Dropout</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">hidden_states</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distrigroupnorm">DistriGroupNorm</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/groupnorm.py#L9">DistriGroupNorm</a> 根据上一步全特征图的以及当前步 patch 的均值和二阶矩近似当前步的全特征图均值和方差。需要通信 patch 均值和二阶矩。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriGroupNorm</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriGroupNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl">        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_groups</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="n">group_size</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>  <span class="c1"># register for E[x], E[x^2]</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;gn&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算 patch 均值和二阶矩</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x2_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x2_mean</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="n">slice_mean</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Equation 2 in the paper E[A_t] = E[A_(t+1)] + (E[A^i_t] - E[A^i_(t+1)]), same for E[A^2_t]</span>
</span></span><span class="line"><span class="cl">            <span class="n">correction</span> <span class="o">=</span> <span class="n">slice_mean</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">)</span> <span class="o">/</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">+</span> <span class="n">correction</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">slice_mean</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">full_x_mean</span><span class="p">,</span> <span class="n">full_x2_mean</span> <span class="o">=</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">full_x2_mean</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算方差</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_x_mean</span><span class="p">,</span> <span class="n">slice_x2_mean</span> <span class="o">=</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_var</span> <span class="o">=</span> <span class="n">slice_x2_mean</span> <span class="o">-</span> <span class="n">slice_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">var</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">slice_var</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>  <span class="c1"># Correct negative variance</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">num_elements</span> <span class="o">=</span> <span class="n">group_size</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scale and shift</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>DeepSpeedUlysses</title>
      <link>http://localhost:1313/blogs/deepspeedulysses/</link>
      <pubDate>Mon, 21 Oct 2024 11:09:12 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepspeedulysses/</guid>
      <description>Paper reading of Deepseed Ulysses.</description>
      <content:encoded><![CDATA[<h1 id="deepspeed-ulysses-core-design">DeepSpeed-Ulysses Core Design</h1>
<h2 id="system-design">System Design</h2>
<p>原理如下图所示，假设设备数 P 等于多头注意力的头数 hc. 输入 <code>x[N,d]</code> 被切分到每个设备上 <code>[N/p, d]</code>，之后进行 QKV Projection，随后将 K 进行转置后进行一次 all-to-all 通信，这样每个设备上就有 <code>Q[N, d/P], K[d/P, N], V[N, d/P]</code>, 再执行标准的 attention 计算 $Outputcontext=Softmax((QK^T)/\sqrt{d})V$. 再进行一次 all-to-all 通信使得每个设备上有 <code>[N, d/P]</code> 结果再进行后续操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB06300727bd2f239239db47091e81223c?method=download&amp;shareKey=6abbc645b3fa3a039b464dd405f96d4a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB06300727bd2f239239db47091e81223c?method=download&amp;shareKey=6abbc645b3fa3a039b464dd405f96d4a" alt="DeepSpeed Sequence Parallelism (DeepSpeed-Ulysses) Design">
    </a><figcaption>DeepSpeed Sequence Parallelism (DeepSpeed-Ulysses) Design</figcaption></figure></p>
<h2 id="communication-analysis">Communication Analysis</h2>
<p>在采用节点内 NVSwitch 互连和节点间 fat tree IB 拓扑的集群中，对于总消息大小为 M 的 all-to-all 通信，每条链路通过 P 个 gpu 传输的通信量为 M/P。对于隐藏层大小为 h、序列长度为 N、并行度为 P 的 transform 模型，DS-Sequence 对注意力计算前总消息大小为 3Nh 的 QKV Projection 执行 all-to-all 通信，对每个 transformer block 的输出执行 all-to-all 通信，大小为 Nh. 因此，DeepSpeed 序列下每条链路的总通信量为 4Nh/P (或复杂度为 O(N/P)). 也就是说当 N 和 P 按比例增加时，该通信量是恒定的。</p>
<h2 id="comparison-of-other-works">Comparison of Other Works</h2>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBff8d584feabe45900c3a57eea94a78a0?method=download&amp;shareKey=7bae2e87b18707dabcd5e5ae7976e644" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBff8d584feabe45900c3a57eea94a78a0?method=download&amp;shareKey=7bae2e87b18707dabcd5e5ae7976e644" alt="Comparison of DS-Ulysses to Other Sequence Parallelism Methods">
    </a><figcaption>Comparison of DS-Ulysses to Other Sequence Parallelism Methods</figcaption></figure></p>
<ul>
<li>ColAI-SP 发明了 Ring-Attention，Q 存储在本地 而 KV 以环形方式传输以计算全局注意力，导致通信复杂度与消息大小 M 呈线性关系。</li>
<li>Megatron-LM 序列并行方法与 Megatron 张量并行紧密集成。Megatron-LM 沿着序列维度划分序列，并应用 all gather 和 reduce scatter 来聚合 QKV 注意力计算的投影。并行通信量随消息大小 M 线性增加。</li>
<li>DeepSpeed-Ulysses 通过增加与序列长度成比例的设备数来保持通信量恒定。同时将 Zero3 扩展到数据并行和序列并行的组合。ZeRO 跨序列和数据并行组划分模型状态，并在需要时使用 allgather 收集每个 rank 的部分。</li>
</ul>
<h2 id="general-and-attention-agnostic-solution">General and Attention Agnostic Solution</h2>
<p>DeepSpeed-Ulysses 的优势在于一种以注意力为中心的序列并行设计。在注意力计算是 N/P 划分的序列并行之前，注意力计算是头并行，每个头的注意力都是完整的，但只有较少的头，因此注意力计算可以被任何类型的注意机制所取代，例如 dense attention 和各种形式的 sparse attention.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Efficient Large-Scale Language Model Training on GPU</title>
      <link>http://localhost:1313/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/</link>
      <pubDate>Sat, 05 Oct 2024 10:09:35 +0800</pubDate>
      <guid>http://localhost:1313/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/</guid>
      <description>Paper reading about Efficient Large-Scale Language Model Training on GPU Clusters.</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>本文展示了如何将张量、流水线和数据并行性组合起来以扩展到数千个gpu。我们提出了一种新的交错流水线调度，可以在内存占用与现有方法相当的同时将吞吐量提高 10%.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb0e290b7248b1bb11233e55661bbb736?method=download&amp;shareKey=c1cd9d017d6f99b06b1fc2635f48fc19" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb0e290b7248b1bb11233e55661bbb736?method=download&amp;shareKey=c1cd9d017d6f99b06b1fc2635f48fc19" alt="Trend of Sizes of SOTA NLP Models">
    </a><figcaption>Trend of Sizes of SOTA NLP Models</figcaption></figure></p>
<h1 id="introduction">Introduction</h1>
<p>张量（层内）模型并行对于较大的模型会崩溃。较大的模型在多个多 GPU 服务器上进行切分会导致两个问题：</p>
<ol>
<li>张量并行所需的 all-reduce 通信需要通过服务器间链路进行，这比多 GPU 服务器内可用的高带宽 NVLink 要慢</li>
<li>高度模型并行会产生小规模的矩阵乘法（GEMM），从而可能降低 GPU 利用率。</li>
</ol>
<p>流水线模型并行化是指模型的各层在多个 GPU 上进行条带化处理。batch 被拆分成更小的 microbatch ，并在这些 microbatch 之间流水线执行。无论进度如何，为了保持严格的优化器语义，优化器步骤需要跨设备同步，从而在每个 batch 结束时进行流水线刷新 (<em>pipeline flush</em>)，允许 microbatch 完成执行 (不再注入新的 microbatch). microbatch 数量与流水线级数的比例越大，流水线刷新所花费的时间就越少。</p>
<p>我们展示了如何结合流水线、张量和数据并行性，我们称之为PTD-P. 配置分布式训练的指导原则如下:</p>
<ul>
<li>不同形式的并行性以不同的方式相互作用: 并行策略对通信量、执行内核的计算效率以及由于流水线冲洗 (流水线气泡) 而花费的等待计算的空闲时间有影响。</li>
<li>用于流水线并行性的调度对通信量、流水线气泡大小和用于存储激活的内存有影响。</li>
<li>超参数 (如 microbatch 大小) 的值会影响内存占用、在工作线程上执行的内核的算术效率和流水线气泡大小。</li>
<li>随着规模扩展分布式训练是通信密集型的。使用较慢的节点间互连或更密集的通信分区会影响扩展性能。</li>
</ul>
<h1 id="model-parallelism">Model Parallelism</h1>
<p>本节中将讨论有助于不适合单个 GPU 内存的大模型的并行训练方法。我们将流水线模型并行和张量模型并行 (如图 2 所示的组合) 与数据并行结合起来，简称为PTD-P.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB09f440d2c0b5bd3b923d2d09dbf47eb5?method=download&amp;shareKey=59701400f477d8dd03c4ea43138c933f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB09f440d2c0b5bd3b923d2d09dbf47eb5?method=download&amp;shareKey=59701400f477d8dd03c4ea43138c933f" alt="Combination of Tensor and Pipeline Model Parallelism (MP)">
    </a><figcaption>Combination of Tensor and Pipeline Model Parallelism (MP)</figcaption></figure></p>
<h2 id="data-parallelism">Data Parallelism</h2>
<p>使用数据并行时，每个 worker 都有一个完整模型的副本，输入数据集被分片， worker 定期汇总他们的梯度，以确保所有 worker 看到一个一致的权重版本。</p>
<h2 id="pipeline-parallelism">Pipeline Parallelism</h2>
<p>通过流水线并行，模型的层被分散到多个设备上。一个 batch 被分成更小的 microbatch. 在 microbatch 之间进行流水线执行。为了准确地保持优化器语义，我们引入了定期的流水线刷新，以便在设备之间同步优化器步骤。在每个 batch 处理的开始和结束时，设备都是空闲的。我们称这段空闲时间为流水线气泡 (<em>pipeline bubble</em>).</p>
<h3 id="default-schedule">Default Schedule</h3>
<p>GPipe 提出了一个调度方案，如图 3 所示 (假设<strong>反向传播的时间是前向传播的两倍</strong>，管道调度的效率并不取决于这个因素)，首先执行一个 batch 中所有 microbatch 的前向传播，然后执行所有 microbatch 的反向传播。设 GPipe 流水线气泡的大小为 t_pb，microbatch 的数量为 m，流水线阶段数量 (用于流水线并行的设备数量) 表示为 p，每次迭代的理想时间表示为 t_id (假设理想缩放)，执行单个 microbatch 的向前和反向传播的时间表示为 t_f 和 t_b. 在该调度中，流水线气泡由批处理开始时的 p−1 个前向传播和 p−1 个反向传播组成。则流水线气泡总时间为 t_pb=(p−1)·(t_f+t_b). batch 的理想执行时间为 t_id=m·(t_f+t_b)。因此，在流水线气泡中花费与理想计算时间的比例为:</p>
<p>流水线气泡占比 = t_pb / t_id = (p−1) / m.</p>
<p>为了使流水线气泡占比小，我们需要 m 远大于 p. 然而 m 非常大时这种方法的内存占用很高，因为它需要在训练一次迭代时间内为所有 m 个 microbatch 保存中间激活.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB04e4031b886573061f614e73854d1f43?method=download&amp;shareKey=97391411ed3d06fc2c6de5de5f20d1d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB04e4031b886573061f614e73854d1f43?method=download&amp;shareKey=97391411ed3d06fc2c6de5de5f20d1d0" alt="GPipe Pipeline Schedule">
    </a><figcaption>GPipe Pipeline Schedule</figcaption></figure></p>
<h3 id="schedule-with-interleaved-stages">Schedule with Interleaved Stages</h3>
<p>为了缩小流水线气泡的大小，每个设备都可以对多个层的子集（称为模型块）进行计算，流水线中的每个设备都被分配了多个流水线阶段（与之前相比，每个流水线阶段的计算量更少），而不是单个连续的层。</p>
<details class="custom-details">
    <summary class="custom-summary">An Example</summary>
    <div>例如，如果每个设备之前被分配 4 层 (即设备 1 有 1 - 4 层，设备 2 有 5 - 8层&hellip;)，我们可以让每个设备为两个模型块执行计算 (每个模型块被分配 2 层)，即设备 1 有 1、2、9、10 层; 设备 2 具有第3、4、11、12层&hellip;</div>
</details><br>
<p>和上一小节一样，我们可以执行完所有 microbatch 的前向传播然后执行所有反向传播 (all-forward, all-backward)，但这将占用大量内存 (与 m 成正比). 因此如图 4 所示，我们设计了一个适配于之前的内存高效 1F1B 的交错调度。它要求 <strong>microbatch 数量是流水线并行度 (流水线中的设备数量) 的整数倍</strong>。</p>
<p>如果每个设备都有 v 个阶段 (模型块)，那么每个阶段 microbatch 的前向和反向传播的时间分别为 t_f/v 和 t_b/v. 流水线气泡时间因此减少到 𝑡^int_pb=(p−1)·(tf+tb)/v，</p>
<p>流水线气泡占比为 𝑡^int_pb / t_id = (p−1) / (m·v).</p>
<p>这意味着该调度减少气泡时间到原先的 1/v，但该计划需要额外的通信，因此通信量也为原来的 v 倍。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb74cfaaf752e14cf2e44f4abd7e3e7bf?method=download&amp;shareKey=4558481494c1d4b22574e739743b123d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb74cfaaf752e14cf2e44f4abd7e3e7bf?method=download&amp;shareKey=4558481494c1d4b22574e739743b123d" alt="Default and Interleaved 1F1B Pipeline Schedules">
    </a><figcaption>Default and Interleaved 1F1B Pipeline Schedules</figcaption></figure></p>
<h2 id="tensor-model-parallelism">Tensor Model Parallelism</h2>
<p>详情见 Megatron-LM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB02b1b15f4d736bfee41738f3c3ee72b3?method=download&amp;shareKey=b1a7e4a6171585f4f0a8a39fb4b2d8b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB02b1b15f4d736bfee41738f3c3ee72b3?method=download&amp;shareKey=b1a7e4a6171585f4f0a8a39fb4b2d8b3" alt="Blocks of Transformer Model Partitioned with Tensor Model Parallelsim">
    </a><figcaption>Blocks of Transformer Model Partitioned with Tensor Model Parallelsim</figcaption></figure></p>
<h1 id="performance-analysis-of-parallelization-configurations">Performance Analysis of Parallelization Configurations</h1>
<p>首先定义下符号含义</p>
<ul>
<li>(p,t,d): 并行化维度。p 表示流水线模型并行大小，t 表示张量模型并行大小，d 表示数据并行大小。</li>
<li>n: GPU 数量，要求 ptd=n.</li>
<li>B: 全局批大小 (作为输入提供)</li>
<li>b: microbatch 大小。</li>
<li>m = B/(db): 一个 batch 中每个流水线中的 microbatch 的数量。</li>
</ul>
<h2 id="tensor-and-pipeline-model-parallelism">Tensor and Pipeline Model Parallelism</h2>
<p>如前所述，使用带有周期性冲洗的流水线并行会产生大小为 (p−1)/m 的流水线气泡. 固定 d=1，则 tp=n，气泡大小可以用 t 表示为</p>
<p>(p−1)/m=(n/t-1)/m.</p>
<p>GPU 之间的通信量也受 p 和 t 大小的影响。流水线模型并行的特点是更便宜的点对点通信，每个 microbatch 的每对连续设备之间 (前向或后向传递) 需要执行的通信总量为 bsh. 张量模型并行则使用 all-reduce 通信，总大小为 bsh 的张量需要在每层的前向和后向传递中，在 t 个模型副本之间进行两次 all-reduce，因此每个 microbatch 每层每个设备的总通信量为 4bsh(t-1)/t. 每个设备通常有多个层，则每个设备上每个 microbatch 的张量并行通信总量为 l^stage4bsh(t-1)/t, 其中 l^stage 为流水线阶段的层数。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 1: 当 t 大于单个节点中的 GPU 数量时，在较慢的节点间链路上执行张量模型并行的开销非常大。在考虑不同形式的模型并行时，使用 g-GPU 服务器时张量模型并行度一般为 g (all-reduce 通信量大，NVLink 带宽高)，然后可以使用流水线模型并行来扩展到跨服务器的更大模型 (P2P 通信量小，PCIe 带宽低).</p></div>

<h2 id="data-and-model-parallelism">Data and Model Parallelism</h2>
<p>管道模型并行性。设 t=1，每个管道的 microbatches 数量 m=𝐵/(db)=b&rsquo;/d, b&rsquo;:=B/b. 设 GPU 总数为 n ，流水线阶段数为 p=n/d，气泡大小为</p>
<p>(p−1)/m=(n/d-1)/(b&rsquo;/d)=(n-d)/b'</p>
<p>管道气泡随着 d 变大而变小。如果数据并行所需的 all-reduce 通信不会随着 d 的变大而急剧增加，那么总体吞吐量将会增加，因为基于环的实现的通信时间随着 d 的变化为 (d−1)/d=1−1/d.同样对于给定的并行配置，随着批量大小的增加，b&rsquo; = B/b 增加，因此吞吐量上升。同时数据并行所需的 all-reduce 通信频率也下降，进一步提高了吞吐量。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB27b82e198a75ad564ac917d6d560dec1?method=download&amp;shareKey=cfef60f8f216007803bdafe8b5a5e64c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB27b82e198a75ad564ac917d6d560dec1?method=download&amp;shareKey=cfef60f8f216007803bdafe8b5a5e64c" alt="Fraction of Time Spent Idling due to Pipeline Flush">
    </a><figcaption>Fraction of Time Spent Idling due to Pipeline Flush</figcaption></figure></p>
<p>在张量模型并行下，每个 microbatch 都需要进行 all-reduce 通信，这在多 GPU 服务器上开销很大；而数据并行只需要在每个 batch 中执行一次的 all-reduce通信。此外，使用张量模型并行，每个设备计算每层的一部分，因此对于不够大的层， GPU 可能无法以峰值效率执行这些子矩阵计算。</p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 2：在使用数据和模型并行时，应使用 M=tp 的总模型并行大小，以便模型参数和中间数据满足 GPU 内存限制；数据并行可用于将训练扩展到更多 GPU.</p></div>

<h2 id="microbatch-size">Microbatch Size</h2>
<p>给定函数 t_f(b) 和 t_b(b)，将 microbatch 大小映射为单个 microbatch 的前向和反向计算时间，计算一个 batch 所花费的总时间 (忽略通信成本) 为</p>
<p>(b&rsquo;/b+p-1)·(t_f(b)+t_b(b)).</p>
<p>microbatch 大小因此既影响运算的算术强度，也影响管道气泡大小。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe3b16b74c3ca1aedb0f81939501da9de?method=download&amp;shareKey=cd1c360e2597b37f705bd5f4906d64b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe3b16b74c3ca1aedb0f81939501da9de?method=download&amp;shareKey=cd1c360e2597b37f705bd5f4906d64b3" alt="Per-GPU Throughput versus Microbatch Size for a GPT Model">
    </a><figcaption>Per-GPU Throughput versus Microbatch Size for a GPT Model</figcaption></figure></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3de398053a7040a405722f3b8c929bf1?method=download&amp;shareKey=7014b2696bffc97d5646b4b2614bd3fb" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3de398053a7040a405722f3b8c929bf1?method=download&amp;shareKey=7014b2696bffc97d5646b4b2614bd3fb" alt="">
    </a><figcaption>Behavior of Throughput for the same GPT Model</figcaption></figure></p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 3：最佳 microbatch 大小 b 取决于模型的吞吐量和内存占用特征，以及流水线深度 p、数据并行大小 d 和批量大小 B.</p></div>

<h2 id="activation-recomputation">Activation Recomputation</h2>
<p>激活重计算通过在向后传递之前运行第二次正向传播 (并且仅存储给定流水线阶段的输入激活)，来权衡所执行的计算操作数量的增加对额外内存占用的影响。设 A^input 为一层的输入激活的大小，A^intermediate 为每层的中间激活的大小，一个模型阶段有 l 层， 激活保存点的数量为 c，那么总内存占用为 c·A^input + l/c·A^intermediate. 因此取 c = \sqrt(l·A^input·A^intermediate) 时内存占用最小。</p>
<h1 id="implementation">Implementation</h1>
<h2 id="communication-optimizations">Communication Optimizations</h2>
<p>使用流水线并行时，我们希望在正向和反向并行发送和接收张量。每台 DGX A100 都配备了 8 个 InfiniBand（IB）网卡。然而发送和接收都是点对点的，只发生在两台服务器上的一对 GPU 之间，因此很难充分利用所有网卡。对于流水线内的单次通信，每个 transformer 层的输出都会在张量并行的设备中复制。为了减少这种冗余，我们可以在发送端将张量分割成大小相等的块，然后使用每个 rank 自己的 InfiniBand 发送. 在接收端通过比 InfiniBand 互连快得多的 NVLink 执行 all-gather，重新组装整个张量。通过 scatter-gather 通信优化，将每对连续流水线阶段之间需要执行的通信总量减少为 bsh/t.</p>
<h2 id="computation-optimizations">Computation Optimizations</h2>
<p>将数据布局从 (b,s,a,h) 更改为 (s,b,a,h). 其次，使用 PyTorch JIT 为一系列元素操作 (bias+GeLU 和 bias+dropout+add) 生成融合算子。</p>
<h1 id="evaluation">Evaluation</h1>
<p>在 Selene 超级计算机上以混合精度运行。每个集群节点有</p>
<ul>
<li>8 个 NVIDIA 80GB A100 GPU，通过 NVLink 和 NVSwitch 互连。</li>
<li>8 个 NVIDIA Mellanox 200Gbps HDR Infiniband HCA 用于应用程序通信</li>
<li>额外有 2 个 HCA 用于专用存储。
节点以三级 (leaf, spine, core) 胖树拓扑结构连接，一共有 850个交换机。集群使用 all-NVME 共享并行文件系统进行高性能数据访问和存储。16 位精度的 A100 GPU 的峰值设备吞吐量为 312 teraFLOP/s.</li>
</ul>
<p>QKV 变换的线性层权重参数量均为 h^2, attention 后的线性层权重参数量为 h^2, 两层前馈网络每个线性层的权重参数量为 4h^2，因此每一个 transformer block 的所有线性层的参数量为 12h^2. 词嵌入的参数量为 Vh，位置编码的参数量为 sh.</p>
<p>一个 $A_{m\times k}\times X_{k\times n}$ 矩阵乘法需要 2mkn FLOPs( 2 是因为乘法和加法). transformer block 包含一个注意力块和一个两层前馈网络组成。对于注意力块，主要的 FLOP 来源于 QKV 转换 (6Bsh^2 次操作)、注意力矩阵计算 (2Bhs^2 次操作)、注意力乘 Value (2Bhs^2 次操作) 和 attention 后的线性层 (2Bsh^2 次操作). 前馈网络将隐藏维度放大到 4h，然后再减小到 1h，需要 16Bsh^2 次操作。将这些加在一起，每个 transformer block 一共有 24Bsh^2+4Bhs^2 FLOPs. 反向传播需要两倍的计算量，因为需要计算关于输入张量和权重张量的梯度。此外，使用激活重计算需要在反向传播之前进行额外的正向传播。因此，每一层的总计算量为 FLOPs 为 4*(24Bsh^2+4Bhs^2).</p>
<p>计算量另一方面来源于 head 的 logit 层，它将维度的特征 h 转换为词汇表维度的特征 V. 该操作所需的计算量为正向传播的 2BshV 和反向传播的 4BshV，总共 6BshV FLOPs.</p>
<h2 id="result">Result</h2>
<p>Pipeline-parallel 并行度增加降低 GPU 的计算效率，因为 bubble 变多了。
Batchsize 的增大可以减少 pipeline-parallel 并行度大小带来的影响。</p>
<p>Batch size增加有助于提高GPU的计算效率。
Interleaved schedules 能显著提高GPU的计算效率。</p>
<p>不使用激活重计算的话单位时间内的训练的吞吐是要高于使用重计算的，因为重计算在反向传播中引入额外的计算量。
由于重计算可以节省显存，batchsize 可以相应提高不少。由于 batchsize 的提高，训练吞吐量也得到了提高，从而达到了优化的效果。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Megatron-LM</title>
      <link>http://localhost:1313/blogs/megatronlm/</link>
      <pubDate>Wed, 02 Oct 2024 15:51:50 +0800</pubDate>
      <guid>http://localhost:1313/blogs/megatronlm/</guid>
      <description>Paper reading about Megatron-LM</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>我们的方法不需要新的编译器或更改库，与流水线模型并行 (<em>pipeline model parallelism</em>) 正交互补，并且可以通过在原生 PyTorch 中插入一些通信操作来实现。为了阐述我们的方法，使用 512 个 GPU 将基于 transformer 的模型扩展到 83 亿个参数。与可保持 39 TeraFLOPs (峰值 FLOPs 的 30%) 的强大单 GPU 基准相比，我们在整个应用中保持了 15.1 PetaFLOPs，扩展效率高达 76%.</p>
<h1 id="introduction">Introduction</h1>
<p>随着 LLM 变得越来越大，它们会超出现代处理器的内存限制，并需要如激活检查点 (activation checkpoint) 等额外的内存管理技术。广泛使用的优化算法 (如ADAM) 需要每个参数额外的内存来存储动量和其他优化器状态。这减少了可以有效训练的模型的大小。模型并行性的几种方法克服了这一限制，它们对模型进行分区，使权重及其相关的优化器状态不需要并发地驻留在处理器上。</p>
<details class="custom-details">
    <summary class="custom-summary">Activation Checkpoint</summary>
    <div>在深度学习模型的训练过程中，前向传播会计算并存储每一层的激活值，这些激活值在后向传播时被用来计算梯度。然而，对于深度很大的模型因为需要存储大量的激活值，可能会导致内存溢出。激活检查点技术通过在前向传播过程中只存储一部分的激活值来解决内存占用问题，如果在后向传播过程中需要没有存储的激活值就进行重新计算。</div>
</details><br>
<p>为了证明方法的可扩展性，通过在单个英伟达 V100 32GB GPU 上训练一个包含 12 亿个参数的模型来建立基准。训练该模型可维持 39 TeraFLOPs 的算力，是在 DGX-2H 服务器中配置的单个 GPU 理论峰值 FLOPS 的 30%. 在 512 个 GPU 上将模型扩展到 83 亿个参数，并采用 8 路模型并行，在整个应用中实现了高达 15.1 PetaFLOPs 的持续运行速度。与单 GPU 情况相比，扩展效率提高了 76%. 下图展示了更详细的扩展结果。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" alt="Model (blue) and model&#43;data (green) parallel FLOPS">
    </a><figcaption>Model (blue) and model&#43;data (green) parallel FLOPS</figcaption></figure></p>
<h1 id="background--chanllenges">Background &amp; Chanllenges</h1>
<h2 id="neural-language-model-pretraining">Neural Language Model Pretraining</h2>
<p>早期的预训练和传递语言神经表示的例子表明，与从头开始学习的词嵌入表相比，预训练的词嵌入表改善了下游任务的结果。目前的技术水平已经从传输单词嵌入表发展到传输整个数十亿参数的语言模型。这种方法的进步要求硬件、系统技术和框架能够高效地大规模运行。</p>
<h2 id="transformer-language-models-and-multi-head-attention">Transformer Language Models and Multi-Head Attention</h2>
<p>下图展示了使用的 transformer 模型的示意图。最近利用 transformer 进行语言建模的工作，如 BERT 和 GPT-2 根据需要分别只使用编码器和解码器。</p>
<blockquote>
<p>GPT-2 和 BERT 都对多头注意和 FFN 的输入使用 GeLU 非线性和层归一化，而原始 transformer 使用 ReLU 非线性并对输出进行层归一化。</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" alt="Transformer Architecture">
    </a><figcaption>Transformer Architecture</figcaption></figure></p>
<h2 id="data-and-model-parallelism-in-deep-learning">Data and Model Parallelism in Deep Learning</h2>
<p>将深度神经网络训练扩展到多硬件加速器有两种范式:</p>
<ul>
<li>Data Parallelism (DP): 将 batch 拆分到多个 worker</li>
<li>Model Parallelism (MP): 将模型的内存使用和计算分布在多个 worker 中。
<ul>
<li>Pipeline Parallelism (PP): 一组操作在一个设备上执行，然后将输出传递到流水线中的下一个设备执行另一组操作。</li>
<li>Distributed Tensor Computation: 将张量运算分割到多个设备上，以加速计算或增加模型大小。</li>
</ul>
</li>
</ul>
<p>然而，这些技术有一个基本的限制: 模型权重必须能加载进 worker. 我们的方法是利用模型并行性在多个加速器之间分割模型。</p>
<h1 id="model-parallel-transformers">Model Parallel Transformers</h1>
<p>我们利用 transformer 网络的结构 (self-attention 和 FFN (2*MLP) 组成)，通过添加一些同步原语，创建了一个简单的并行计算模型。下面分别阐述对 FFN 和 self-attention 的并行化。</p>
<p>FFN 第一个 MLP 由一个 GEMM，后跟一个 GeLU 非线性组成:</p>
$$
Y=\text{GeLU}(XA)
$$<p>并行化 GEMM 的一种选择是将权重矩阵 A 沿着行切分，并将 X 沿着其列切分:</p>
$$
X=[X_1,X_2], A=\begin{bmatrix}A_1\\A_2\end{bmatrix}
$$<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" alt="Row Split of Weight">
    </a><figcaption>Row Split of Weight</figcaption></figure></p>
<p>可以得出 $Y = X_1A_1+X_2A_2$. 由于 GeLU 是非线性函数，因此这种方法需要在 GeLU 函数之前进行同步。</p>
<p>另一个选择是沿着列切分 $A=\begin{bmatrix}A_1,A_2\end{bmatrix}$. 这样可以让 GeLU 独立地应用于每个 GEMM 的输出</p>
<p>$[Y_1, Y_2]=\begin{bmatrix}\text{GeLU}(XA_1),\text{GeLU}(XA_2)\end{bmatrix}$.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" alt="Column Split of Weight">
    </a><figcaption>Column Split of Weight</figcaption></figure></p>
<p>这种切分方式的优点是不需要进行同步操作。</p>
<p>如下图所示，以列并行方式切分第一个 GEMM，并沿着行切分第二个GEMM。然后，在将输出传递给 dropout 层之前，第二个GEMM 的输出在 GPU 之间进行 all-reduce 操作。这种方法将 FFN 中的两个 GEMM 拆分到多个 GPU 上执行，并且只需要在正向传播 (g 操作符) 和反向传播 (f 操作符) 中分别执行一次 all-reduce 操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" alt="Parallelism of MLP">
    </a><figcaption>Parallelism of MLP</figcaption></figure></p>
<p>如下图所示，利用多头注意力操作中本身存在的并行性，以列并行的方式划分与 QKV 相关的 GEMM，以便每个注意力头对应的矩阵乘法在一个 GPU 上独立完成。输出线性层的 GEMM 沿着其行并行化，并直接获取并行 attention 的输出。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" alt="Parallelism of Self-Attention">
    </a><figcaption>Parallelism of Self-Attention</figcaption></figure></p>
<p>如下图所示，这使能够仅在正向传播和反向传播中分别中使用两个 all-reduce 操作执行 transformer 中所有的 GEMM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" alt="Parallelism of Transformer Layer">
    </a><figcaption>Parallelism of Transformer Layer</figcaption></figure></p>
<p>基于 transformer 的语言模型的输出嵌入维度为隐藏层大小 (H) 乘以词汇表大小 (v). 我们沿着词汇表维度 $E = \begin{bmatrix}E_1,E_2\end{bmatrix}$ 并行化权重矩阵。每一块现在只包含嵌入表的一部分，输入嵌入后需要一个 all-reduce (g 算子).</p>
<p>对于输出嵌入，一种方法是通过并行 $\mathrm{GEMM} [Y_{1},Y_{2}]=[XE_{1},XE_{2}]$ 来获得 logits，并对结果 all-gather 后送入交叉熵损失函数。在这种情况下，all-gather 通信量为 bsv 个元素 (b 是批处理大小，s 是序列长度). 为了减小通信规模，我们将输出与交叉熵损失融合，这样通信量降为 bs.</p>
<p>我们在每个 GPU 上维护层归一化参数的副本，并在将这些张量作为输入送到下一个模型并行区域之前，在本地输出上进行 dropout 和残差连接。为了优化模型，我们允许每个模型并行 worker 优化自己的一组参数。因为所有的值要么是本地的，要么是在 GPU上 重复的，所以在这个公式中不需要通信更新的参数值。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Ring Attention Principle</title>
      <link>http://localhost:1313/blogs/ringattention/</link>
      <pubDate>Thu, 26 Sep 2024 22:59:35 +0800</pubDate>
      <guid>http://localhost:1313/blogs/ringattention/</guid>
      <description>This is a brief introduction to the Ring Attention Principle.</description>
      <content:encoded><![CDATA[<h1 id="background">Background</h1>
<p>如今 LLM 的 token 长度显著增加，从 GPT-3.5 的 16k 到 Claude 2 的 200k，现在 Gemini 1.5 Pro 甚至有 1M 的 token 长度。如此长的 token 在计算 attention 时对显存的需求非常大。<a href="https://arxiv.org/abs/2310.01889">Ring Attention</a> 便是为了并行计算 attention 而提出的一种方法<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<blockquote>
<p>Ring Attention 和 Flash Attention 可以同时使用。</p></blockquote>
<h1 id="attention-and-memory">Attention and Memory</h1>
<p>要计算 attention， 我们需要三个大小为 (s, d) 的矩阵：Q (query)、K (key)、V (value)，其中 s 为序列长度，d 为模型维度。attention 的计算公式为</p>
$$
Attention(Q, K, V) = softmax(QK^T / \sqrt{d})V
$$<p>忽略 sqrt(d) 项，我们记 Score Matrix 为 S = QK^T / \sqrt{d}，然后对 S 进行 softmax 归一化，得到 Attention Matrix. 可以发现它们占用显存大小是 O(s*s) 数量级。即使使用 <a href="https://arxiv.org/abs/2205.14135">Flash Attention</a>，显存占用量也是 O(s) 数量级。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe66e94e161b89a4ba25d05b67a47e393?method=download&amp;shareKey=742185dd412edbdb3266fa16ab91d787" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe66e94e161b89a4ba25d05b67a47e393?method=download&amp;shareKey=742185dd412edbdb3266fa16ab91d787" alt="Attention Compute Process">
    </a><figcaption>Attention Compute Process</figcaption></figure></p>
<p>我们希望如果在 N 个设备上并行计算 attention，每个设备的显存占用量为整个的 1/N, 因此就需要对 Q、K、V 的 sequence 长度进行切分。但是如果得到的最终 attention 矩阵需要在设备间进行集合通信组装每个的计算结果，通信量也和 sequence 长度成正比。Ring Attention 提出了一个巧妙的解决方案：在设备之间进行轮转，并行化所有计算而且完全隐藏通信的开销。</p>
<blockquote>
<p>We will rotate between devices to parallelize all computation and hide the communication overhead completely.</p></blockquote>
<h1 id="splitting-the-query">Splitting the Query</h1>
<p>假设我们有 N 个设备，我们将 Q 沿着 sequence 维度切分为 N 份，每份大小为 (s/N, d). 由于计算 Score 和 Attention 需要完整的 K 和 V，这样它们也被切分成 N 份，每份大小为 (s/N, d). 计算示意图如下。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB170087e68345309f813b8edee9487b92?method=download&amp;shareKey=f848ff8adb5676443347921c65a3b104" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB170087e68345309f813b8edee9487b92?method=download&amp;shareKey=f848ff8adb5676443347921c65a3b104" alt="Split Q">
    </a><figcaption>Split Q</figcaption></figure></p>
<h1 id="splitting-the-key-and-value">Splitting the Key and Value</h1>
<p>对 K 和 V 的切分并不能像 Q 那样直接。因为 softmax 的计算公式如下，要得到分母的值意味着我们需要对每一行进行计算。</p>
$$
softmax(s_i) = \frac{\exp(s_i)}{\sum_{j=i}^d{\exp(s_j)}}
$$<p>如果我们能对 K 和 V 进行切分并正确计算 softmax，那么计算过程可以由下图所示的那样完成 (忽略 softmax). 外循环遍历 Q 的所有分块，内循环遍历 K 和 V 的所有分块，一次计算一部分的 attention. Ring Attention 示意图如下所示，顾名思义所有设备组成一个环状，每个设备存储 Q 的一部分，每次迭代过程会传递 K 和 V 到下一个设备，最终每个设备将得到计算自己 Q 部分的 attention 矩阵所需要的 K 和 V. 每个设备被分配 Q 的一部分 (即一个外层循环索引)，并迭代计算每个 K 和 V 的分块 (内循环)。每个设备只需要跟踪形状为 (s/N, s/N) 的累积和 A_j。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbc9ef7d01431fe639ecf44842bce0e1a?method=download&amp;shareKey=03d587a38ca574ed1547f2594a45ab4c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbc9ef7d01431fe639ecf44842bce0e1a?method=download&amp;shareKey=03d587a38ca574ed1547f2594a45ab4c" alt="Attention Parallel Computation">
    </a><figcaption>Attention Parallel Computation</figcaption></figure></p>
<h1 id="online-softmax">Online Softmax</h1>
<p>在内循环的每次迭代中我们可以更新部分和为 $l^j = l^{j-1} + \sum_{k_t\in K_j}{\exp(Q_ik_t^T)}$. 在内循环结束后我们就可以获得每一行的指数和。归一化和与 V 的相乘顺序不会影响结果，我们可以先累加总和，并在所有其他计算完成后再执行实际的归一化操作。</p>
<p>因此，设备 i 除了计算当前的累计和 $A^j = A^{j-1} + \exp(Q_i K_j^T) V_j$ 外，还需要在内循环每次迭代中更新部分和 $l^j \in \mathbb{R}^{B_q}$ ，其中 $B_q$ 为 Q 的分块大小。</p>
<h1 id="safe-softmax">Safe softmax</h1>
<p>由于指数运算经常容易出现溢出，我们通常减去 max(s_i) 后进行指数运算，公式如下，这样并不会影响结果。</p>
$$
\mathrm{softmax}(s_{1:N})=\frac{\exp(s_{1:N})}{\sum_i\exp(s_i)}\cdot\frac{\exp(-s_{max})}{\exp(-s_{max})}=\frac{\exp(s_{1:N}-s_{max})}{\sum_i\exp(s_i-s_{max})}
$$<p>所以我们在内循环每次迭代中需要先更新当前的最大值 $m^{j+1}=\max(m^j,\max(Q_iK_{j+1}^T))$，然后更新之前迭代的计算结果 A_j 和 部分和 l_j. 最后再计算本次迭代的结果。</p>
$$
A^{j+1}=A^j\cdot\exp(m^j-m^{j+1})+\exp(Q_iK_{j+1}^T-m^{j+1})\cdot V_j
$$<p>更新部分和</p>
$$
l^{j+1}=l^j\cdot\exp(m^j-m^{j+1})+\exp(Q_iK_{j+1}^T-m^{j+1})
$$<h1 id="putting-it-together">Putting it Together</h1>
<p>Ring Attention 计算步骤如下：</p>
<ol>
<li>沿着 Q 的 sequence 长度拆分为一个独立的外循环。</li>
<li>应用 Online Safe Softmax，以便沿着 K 和 V 的sequence 长度拆分，从而在内层循环中累积计算注意力。</li>
</ol>
<p>这种并行化的方式是通过将每个设备分配一个 Q_i 块来实现的。因此，我们需要将 Q 拆分为 N 个相等的部分 (B_Q=N). 每个设备将分别计算它的输出块 $\text{Output}(Qi,K,V)= \text{softmax}(Q_i K^T)V ，通过在 K 和 V 块上执行内循环来迭代计算。难点挑战在于设备无法一次存储完整的 K 和 V 矩阵。</p>
<p>如果我们有 4 个 GPU，那么我们将把每个设备的 Q 按序列维度分成 4 个块，K 和 V 被分割成 B_K=B_Q=N 个块，并对设备进行初始化，使每个设备都持有一个 Qi 块、 一个 Kj 块和 一个 Vj 块。为简单起见，我们可以假设设备 i 在开始时持有 Qi, Ki 和 Vj 块。在设备计算完与其当前 vj kj 相对应的一个内循环步骤后，每个设备都需要接收下一个 Key 和 Value 块，以继续内循环。 我们将 N 个设备围成一个环，其中设备 i 可以向设备 i+1 以此类推，如图所示：</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5d0930a41cedf1d4e46af9baa5071f78?method=download&amp;shareKey=0e898be310f92f54f0b065a38771eb5f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5d0930a41cedf1d4e46af9baa5071f78?method=download&amp;shareKey=0e898be310f92f54f0b065a38771eb5f" alt="KV-overlap">
    </a><figcaption>KV-overlap</figcaption></figure></p>
<p>如果在设备 i 上计算内循环的一个步骤 Qi,Vj,Kj 的这段时间内，设备 i 还能向设备 i+1 发送其当前 Kj Vj，并同时从设备 i-1 接收 V_j-1,K_j-1，那么只要发送和接收密钥和值块的时间低于计算时间，那么发送和接收 Key 和 Value 块的延迟就会隐藏在执行实际计算时间之内。一个例子如下图所示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB935e1d2c0eb43c35c5c828abe8a44612?method=download&amp;shareKey=9ec41a1c178534620d9f7274ff2ce9d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB935e1d2c0eb43c35c5c828abe8a44612?method=download&amp;shareKey=9ec41a1c178534620d9f7274ff2ce9d0" alt="KV-rotate">
    </a><figcaption>KV-rotate</figcaption></figure></p>
<h1 id="memory-and-arithmetic-complexity">Memory and Arithmetic Complexity</h1>
<p>以深度学习中常用的 bfloat16 数据类型为例。GPU 或 TPU 等并行处理加速器通常以 FLOP:=F 来衡量，即设备理论上每秒可执行的浮点运算次数。我们假设硬件被完全利用。此外，我们设不同设备之间的连接带宽为:=B (Bytes/sec).</p>
<p>内存复杂度: 为了同时进行接收发送和计算，我们需要有用于接收新 KV 块的寄存器器。存储当前 KV  值块需要 2dc 浮点数或 4dc 字节。用于接收新的 KV 块的内存大小也是 2dc 浮点数或 4dc 字节。假设计算本身不需要更多内存 (利用 Flash Attention 或 Blockwise Attention)，计算当前步骤的输出需要 dc 个浮点数或 2dc 字节。此外，每个设备还需要存储其 Qi 块，这也需要 dc 个浮点数或 2dc 字节。总共需要 6dc 个浮点或 12dc 字节。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>Ring Attention 与 Flash Attention 是正交的，可以一起使用 (Flash Attention 实际上用于 Ring Attention 的内循环). Flash Attention 目标是不将整个 Score Matrix 加载到全局内存中，从而在序列长度上获得线性内存复杂度。Ring Attention 将 原始注意力方法和 Flash Attention 的内存复杂度至少降低了 N 倍，使用 N 个设备的内存复杂度至少降低 N 倍，因为它将所有矩阵都拆分为至少 N 个或更多部分 (将 QKV 分别分成 N 份，并将 Score Matrix 分成 N^2 分). 无论内存复杂度是由 QKV，还是由 Score Matrix 主导，Ring Attention 都能将内存成本降低至少 N 倍。</p></div>

<p>通信开销: 在内循环每一步中，每个设备需要通过带宽为 B 的信道向下一个设备发送 2⋅c_Q⋅d 浮点数。每个 bf16 大小为 2字节，因此，所需的时间约为 4⋅c⋅d/B.</p>
<p>运算强度： 一个内循环步骤，计算局部注意力需要 2⋅d⋅c^2 次浮点计算，计算 softmax，归一化向量和最大值向量需要 2⋅c⋅d 次浮点计算，计算局部注意力与 Vj 块的乘积需 2⋅d⋅c^2 次浮点计算。因此，总计算所需时间≈4⋅d⋅c^2/F.</p>
<p>为了重叠通信和计算 (隐藏通信开销)，我们需要 KV 块的传输时间小于等于计算本地 QKV 所需的时间：</p>
$$
4\cdot c\cdot d/B\leq4\cdot d\cdot c^2/F\iff B\geq F/c\iff s/N\geq F/B 
$$<h1 id="futher-optimization">Futher Optimization</h1>
<p>Ring Attention 的一个应用是用于因果 Transformal 模型时，加上三角形掩码用于注意力计算。这意味着有些 GPU 不需要对整个序列进行计算，导致它们大部分时间处于闲置状态。作为 Ring Attention 的扩展，<a href="https://arxiv.org/pdf/2311.09431.pdf">Stripe Attention</a> 解决了这一问题，并提供了一种分配计算更均匀的方案，从而使 Ring Attention 的计算速度更快。</p>
<p>除了 Ring Attention 和 Flash Attention 等使标准 Transformer 架构能有更长的上下文长度的技术外，人们还尝试使用 <a href="https://arxiv.org/abs/2312.00752">Mamba</a> 等具有线性注意力的状态空间模型（SSM）等模型架构。</p>
<h1 id="references">References</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://coconut-mode.com/posts/ring-attention/">https://coconut-mode.com/posts/ring-attention/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Comparsion of Parallelsim Metods in ViT</title>
      <link>http://localhost:1313/blogs/comparsion-of-parallelsim-metods-in-vit/</link>
      <pubDate>Mon, 13 Nov 2023 16:05:23 +0800</pubDate>
      <guid>http://localhost:1313/blogs/comparsion-of-parallelsim-metods-in-vit/</guid>
      <description>Paper reading of .</description>
      <content:encoded><![CDATA[<h1 id="basic-transformer-block">Basic Transformer Block</h1>
<p>符号含义表示如下</p>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Description</th>
          <th>Symbol</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>a</td>
          <td>注意力头数</td>
          <td>n</td>
          <td>并行度大小</td>
      </tr>
      <tr>
          <td>b</td>
          <td>batchsize</td>
          <td>s</td>
          <td>序列长度</td>
      </tr>
      <tr>
          <td>h</td>
          <td>隐藏层维度</td>
          <td>v</td>
          <td>词汇表大小</td>
      </tr>
      <tr>
          <td>L</td>
          <td>tranformer layer 层数数</td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>基本 transformer block 结构如下，输入是形状为 (b, s, h) 的三维张量，其中 b 为 batchsize. 每个变压器层由一个具有注意头的自注意块组成，随后是一个具有两层的 MLP，第一层将隐藏维度增加到 4h，然第二层将其减少到 h. 每个变压器层的输入和输出具有相同的形状.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6446c9e0a905932db1f9e39fa91c01ba?method=download&amp;shareKey=f26e075bcfc51b8c093388f69d39b40d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6446c9e0a905932db1f9e39fa91c01ba?method=download&amp;shareKey=f26e075bcfc51b8c093388f69d39b40d" alt="Basic Transformer Architecture">
    </a><figcaption>Basic Transformer Architecture</figcaption></figure>

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfbdc229aca70349939d6e3306e78c434?method=download&amp;shareKey=cff3f2903a8e16c5c46d607749a4b3c1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfbdc229aca70349939d6e3306e78c434?method=download&amp;shareKey=cff3f2903a8e16c5c46d607749a4b3c1" alt="Self-attention Block">
    </a><figcaption>Self-attention Block</figcaption></figure></p>
<h2 id="model-parameters">Model Parameters</h2>
<p>QKVO Linear 的权重形状均为 <code>h*h</code>, 偏置形状均为 <code>h*1</code>；MLP 两个 Linear 的权重形分别为 <code>h*4h</code> 和 <code>4h*h</code>，偏置形状分别为 <code>4h*1</code> 和 <code>h*1</code>. 因此每个模型的参数量为 <code>(12hh+13h)L</code>，占用大小还要 <code>x2</code>.</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在传统的 LLM 中最后还需要经过 logits layer，将隐藏层维度 <code>h</code> 转换成词汇表大小 <code>v</code>，参数量还要加上 <code>hv</code>.</p></div>

<h2 id="flops-calculation">FLOPs Calculation</h2>
<p>对于浮点数计算量 (FLOPs)，只考虑占主要部分的通用矩阵乘法 (GEMMs). 对于 Attention 部分，QKV Linear 的计算量为 <code>6bshh</code>，attention matrix (<a href="mailto:Q@K.T">Q@K.T</a>) 的计算量为 <code>2bssh</code>, attention@V 的计算量为 <code>2bssh</code>, O Linear 的计算量为 <code>2bshh</code>. MLP 的两个线性层的每一个计算量都为 <code>8shh</code>. 相加后得到正向传播中总计算量为 <code>(24bshh + 4bssh)L</code> bytes.</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在传统的 LLM 中最后还需要经过 logits layer，将隐藏层维度 <code>h</code> 转换成词汇表大小 <code>v</code>，其计算量为 <code>2bshv</code>.</p></div>

<p>反向传播因为要计算输入和权重的梯度，其计算量为正向传播的两倍，因此整个模型的计算量为 <code>72BLshh(1+s/(6h))</code>.</p>
<h1 id="activation-memory">Activation Memory</h1>
<p>激活的定义为在前向传播中产生并且需要在反向传播中进行梯度计算的张量，即不包括模型参数和优化器状态。并且不考虑相对非常小的激活。例如 LayerNorm 层的输入还需要张量每个通道的均值和方差 (大小均为 bs)，由于 h 大小通常超过 1k，因此只考虑输入张量所占激活的大小 bsh，忽略掉 2bs. 假设数据格式为 fp16/bf16，即每个数据占用 2 bytes 的存储空间，需要特殊处理的是 dropout 层的 mak，每个元素均为 unsigned int，只占用 1 byte.</p>
<p>Attention 部分激活占用如下 (共计 11bsh + 5bssa)</p>
<ul>
<li>QKV Linear: 三个线性层需要的输入相同，占用 2bsh bytes.</li>
<li><a href="mailto:Q@K.T">Q@K.T</a>: 需要存储 Q 和 K，占用 4bsh bytes.</li>
<li>Softmax: 需要存储大小为 2bssa bytes 的输入</li>
<li>Softmax droppot: 需要存储一个大小为 bssa bytes 的 mask.</li>
<li>attention@V: 需要存储 dropout 的输出和 V，分别占用 2bssa 和 2bsh bytes.</li>
<li>O Linear: 需要存储注意力的输出，占用 2bsh bytes.</li>
<li>O dropout 需要存储一个大小为 bsh bytes 的 mask;</li>
</ul>
<p>MLP (共计 18bsh): 第一层和第二层的输入分别占用 2bsh 和 8bsh bytes. GeLU 层需要第二层的输入用于反向传播，占用大小为 8bsh bytes. dropout 需要一个大小为 bsh bytes 的 mask.</p>
<p>LayerNorm (共计 4bsh): 需要存储该层的输入，占用 2bsh bytes. 一共有两个 LayerNorm.</p>
<p>加起来就可以得到每个 transformer block 需要激活大小为 bsh(34+5sa/h) bytes.</p>
<h1 id="tensor-parallelsim">Tensor Parallelsim</h1>
<p><a href="https://darkenstar.github.io/2024/10/02/MegatronLM/#Model-Parallel-Transformers">Megatron 张量并行</a> 的思想是将输入进行连续的两个矩阵乘法的第一个按列切分成 t 份，第二个按行切分成 t 份. 在 Transformer block 中体现为利用多头注意力本身的并行性将 Attention 计算中的 QKV 按列进行切分，O Linear 的权重按行进行切分；MLP 中第一个线性层的权重按列进行切分，第二个权重按行进行切分。</p>
<p>在这种并行方式下，前向传播和反向传播均需要进行 2 次 All-Reduce 通信，由于每次 All-Reduce 通信可以看作 Reduce-Scatter + All-Gather, 因此每次每个设备的通信量为 8αbsh bytes，其中 α=(n-1)/n.</p>
<p>对于激活，2*LayerNorm, QKV Linear 的输入, O dropout mask，MLP 第一层的输入和 MLP dropout 不会被切分，因此每个设备每个 block 要占用的激活为 bsh(10+24/n+5as/(hn))</p>
<p>2D Tensor Parallelsim</p>
<p>2D张量并行将激活第一个矩阵的列切分成 m*n 份，第二个权重 (权重形状为 he) 的行被切分成 m 份，列被切分成 n 份。以下图为例，Rank0-Rank2为通信组 x，Rank0-Rank1为 通信组 y. 第一个矩阵经过一次通信组 y 的 AllGather 后与本设备第二个矩阵进行矩阵乘积，得到的部分和经过一次通信组 x 间的ReduceScatter，计算出正确结果。第一次 AllGather 通信每个设备通信的大小为 bsh(n-1)/(mn). 第二次 ReduceScatter 通信每个设备通信的大小为 bse(m-1)/n.</p>
<h1 id="megatron-sequence-parallelsim">Megatron Sequence Parallelsim</h1>
<p>Megatron 张量并行中 LayerNorm 以及 O Linear 和 MLP 之后的 dropouts 在每个设备中都有一个副本。这些模块不需要大量的计算，但需要占用 10bsh bytes 大小的激活内存。<a href="">Megatron-SP</a> 沿着序列维度划分这些模块来减少激活内存，但需要配合 TP 一起使用，本质上是将 TP 中的 All-Reduce 拆成了在 TP 前进行 All-Gather 和在 TP 后进行 Reduce-Scatter. 但除去第一个 LayerNorm 外的每一个模块的激活都得到了切分。Megatron-SP 这里选择每个设备存储自己的部分并在反向传播中插入一次额外的 All-Gather 通信。因此通信量为 10bsh, 每个设备每个 block 需要占用的激活为 bsh/n*(34+5as/h)</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6800d68e35ee4215289de6aa75f01884?method=download&amp;shareKey=e67ffd54e4d1fe7cf3a10e81108af366" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6800d68e35ee4215289de6aa75f01884?method=download&amp;shareKey=e67ffd54e4d1fe7cf3a10e81108af366" alt="Transformer layer with Megatron-SP">
    </a><figcaption>Transformer layer with Megatron-SP</figcaption></figure></p>
<h1 id="pipeline-parallelsim">Pipeline Parallelsim</h1>
<p>流水线张量并行仅仅将 L 个 Transformer block 平均分到 p 个设备上，并没有划分激活所要占用的内存。在考虑 1F1B 策略下 batchsize 进一步被划分成 p 个 micro batch. 第一个 stage 必须存储 p 个 micro batch 的激活。每个 stage 包含 L/p 层，所以无论流水线并行大小 p 如何，第一个 stage 必须存储 p × L/p = L 层的激活值。在 Megatron-LM 中的 interleaving schedule 需要存储 L(1 + (p−1)/(pm)) 层的激活，其中 m 是 interleaving 的数量。</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在使用 output-tensor-deallocation 优化 (输出传到下一个 stage 后就释放) 的情况下，可以为为每个设备节省 bshr 内存，其中 r 是每个设备正在运行的 micro batch 的数量，在第一个 stage r=p 时达到峰值。</p></div>

<h1 id="deepseed-ulysses-sequence-parallel">Deepseed-Ulysses Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/10/21/Deepseed%20Ulysses/">DS-SP</a> 也是利用多头注意力的并行性，首先将输入按序列维度切分到每个设备上，每个设备占有的输入形状为 b*(s/n)*h. 在计算 Attention 之前对 QKV 进行 All-to-All 通信变成按隐藏层维度切分 ((a 要能整除 n))，通信量为 6αbsh/n bytes. 计算完 score@v 之后再进行一次 All-to-All 通信，通信量为 2αbsh/n bytes，总计通信量为 8αbsh/n bytes. 激活占用上 Attention 中 Softmax 及其 dropout mask 和 attention 没有被切分，激活占用量为 bsh(34/n+5sa/h). 因此，它不适合 GQA 和 MQA 情况, GQA 的并行度被限制在了组数，MQA 则完全没法使用。而且由于张量并行也需要在 a 维度上进行划分，SP-Ulysses 和 TP 是冲突的。</p>
<h1 id="ring-attention-sequence-parallel">Ring-Attention Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/09/26/Ring_Attention/#Putting-it-Together">Ring-SP</a> 实际上为环状的 FlashAttention，将输入沿着序列维度切分到每个设备上，在 Attention 计算过程中每个设备向相邻设备通信 KV 并更新自己的 Softmax 矩阵，通信量为 4bsh bytes. 激活占用和 DS-SP 一样为 bsh(34/n+5sa/h).</p>
<h1 id="unified-sequence-parallel">Unified Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/11/14/USP-A%20Unified%20Sequence%20Parallelism%20Approach%20for%20Long%20Context%20Generative%20AI/#Unified-Ulysses-Ring-Sequence-Parallelism">USP</a> 将 SP 进程组分割成两个正交的进程组：SP-Ring 进程组和 SP-Ulysses 进程组。可以将其视为一个 2D mesh ，每一列上运行 SP-Ring，每一行上运行 SP-Ulysses. 具体方法为 QKV 的切分 和 All-to-All 和 DS-Ulysses 相同，然后采用 Ring-Attention 的方式进行计算。如果遇到使用 casual mask 的情况需要加上 balance load 策略，把序列长度分为 2*(ring_degree) 大小，按照 0-&gt;1-&gt;&hellip;-&gt;(ring_degree-1)-&gt;(ring_degree-1)-&gt;&hellip;-&gt;0 的顺序进行分配。USP 消除了 SP-ulysses的头数限制。并且 USP可以通过调整 SP-Ulysses 进程组数目来更好的适应不同带宽的网络结构，可以让 All-to-All 操作在高带宽中运行，而异步 P2P 通信在低带宽部分运行。</p>
<h1 id="comparsion-of-different-parallelsim-in-training">Comparsion of Different Parallelsim in Training</h1>
<table border="1">
  <tr>
    <th rowspan="2"></th>
    <th colspan="4" style="text-align: center;">Communication (FWD+BWD)</th>
    <th rowspan="2">Split Dim</th>
    <th colspan="3" style="text-align: center;">Memory</th>
  </tr>
  <tr>
    <th>Param</th>
    <th>Cost</th>
    <th>Act</th>
    <th>Cost</th>
    <th>P/G</th>
    <th>OS</th>
    <th>Act</th>
  </tr>
  <tr>
    <td>DS-SP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>8*All2All</td>
    <td>(8/N)O(bsh)</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>Ring-SP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>P2P</td>
    <td>4O(bsh)</td>
    <td>L/L</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>DP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>0</td>
    <td>0</td>
    <td>b/b</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>ZeRO1</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>0</td>
    <td>0</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P/N </td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO1</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO2</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>P+(G/N)</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO3</td>
    <td>2*AllGather + ReduceScatter</td>
    <td>18O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>TP</td>
    <td>0</td>
    <td>0</td>
    <td>4*AllReduce</td>
    <td>8O(bsh)</td>
    <td>a/h</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>αA</td>
  </tr>
  <tr>
    <td>Megatron-SP</td>
    <td>0</td>
    <td>0</td>
    <td>6*AllGather + 4*ReduceScatter</td>
    <td>10O(bsh)</td>
    <td>a/h</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
</table>
<h1 id="analysis">Analysis</h1>
<ol>
<li>All2All 通信使得 DS-SP 的通信开销大于 DP. 使用 Ring-SP 时，尽管异步的 P2P 通信是可以重叠的，理想的性能也是只与 DP 相同。因此只有当批 batchsize 不足以进行切分时才考虑使用 SP.</li>
<li>Megatron-SP 通信量高于 DS-SP 和 Ring-SP. SP-Ring 对于 KV 的通信可以与计算重叠。Megatron-SP 的通信量不会随着并行度的增加而减少，而 DS-SP 可以做到。 DS-SP 和 Ring-SP 具有较低的激活通信成本，但需要同步梯度和参数。不过参数通信量相对于激活通信量较小，可以通过计算进行重叠。GQA/MQA 也可以降低它俩的通信成本，而 Megatron-SP 不受影响。</li>
<li>相同配置下使用 USP+Zero3 来代替 Megatron-SP 并不会增加可训练序列的长度。但与 Megatron-SP 相比，USP 能在通过提高并行度来增加可以训练的序列长度。</li>
<li>Megatron-SP 并行维度受限于注意力头数目。USP 可以通过提高 Ring-SP 的并行度来扩展，以在大规模配置下训练更大模型。</li>
</ol>
<h1 id="sora-inference-modeling-analysis-process">Sora Inference Modeling Analysis Process</h1>
<p>我们需要准备模型的输入：</p>
<ol>
<li>隐空间采样的噪声 z，形状与想生成的视频时常和分辨率相关。生成 1s 的视频为 25.5 frames，经过 VAE Encoder 后输出的通道数为 4，帧数会被压缩到 <code>num_frame*5//17</code>，分辨率的长宽分别被压缩到原来的 1/8. 因此 z 的形状应该为 <code>(B, 4, num_frame*5//17, img_size[0]//8, img_size[1]//8)</code>.</li>
<li>输入的 prompt 会经过 DeepFloyd/t5-v1_1-xxl 编码，该编码器最大的 token 数为 300，编码维度为 4096，文本长度不足时会填充到 300. 因此编码后的 prompt 的形状为 <code>(B, 1, 300, 4096)</code>.</li>
<li>当前去噪的时间步 t，形状为 <code>(B, )</code></li>
<li>生成视频的 fps，形状为 <code>(1, )</code></li>
</ol>
<p>还需要准备相关的模型配置，包括 mesh 形状，sub_mesh 的形状，并行策略以及 stage_ids. 如果需要将模型的 transformer block 切分成多段，则需要配置 sub_mesh 和 stage_ids.</p>
<ul>
<li>mesh_shape: (num_x, num_y)</li>
<li>submesh_shape: <code>[(num_x, num_y, loc_x, loc_y), ]</code></li>
<li>stage_ids: <code>[(submesh0_start, submesh0_end), ]</code></li>
<li>strategy: 并行策略</li>
</ul>
<p>然后初始化模型，Sora 的整体结构如下 我们初始化一个 Pipeline(包含整个流程的函数)，它会有一个或多个 Stage 用于保存模型的不同层，与 stage_ids 中对应。我们将模型分解成 Embedding_blocks(PatchEmbed3D, TimestepEmbedder, SizeEmbedder, Captionembedder, t_block), STDiT3_blocks 和 T2IFinalLayer. 将这个分解函数作为 Pipeline 的 sharding_func.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB9688e46ada2523df1ec522a7649be19a?method=download&amp;shareKey=991eba9aad6eca9f41599d2ad4f75c34" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB9688e46ada2523df1ec522a7649be19a?method=download&amp;shareKey=991eba9aad6eca9f41599d2ad4f75c34" alt="Open-Sora">
    </a><figcaption>Open-Sora</figcaption></figure></p>
<h2 id="init-pipeline">Init Pipeline</h2>
<p>我们需要根据配置以及 PipePatch 并行度和 SP 并行度初始化 Pipeline. 这其中会根据 stage_ids 分配每个 Stage 保存模型的哪些层以及对应的 submesh 大小。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">construct_stages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">submeshes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">],</span> <span class="n">stages_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># construct layers for each stage</span>
</span></span><span class="line"><span class="cl">    <span class="n">first_part</span><span class="p">,</span> <span class="n">module_list</span><span class="p">,</span> <span class="n">last_part</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stages_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">submesh</span> <span class="o">=</span> <span class="n">submeshes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage_id</span> <span class="o">=</span> <span class="n">stages_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get stage layers from user config stage ids in module list</span>
</span></span><span class="line"><span class="cl">        <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">module_list</span><span class="p">[</span><span class="n">stage_id</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">stage_id</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">first_part</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># concat module first part(if exists) bef module list to stage_0</span>
</span></span><span class="line"><span class="cl">            <span class="n">layers</span> <span class="o">=</span> <span class="n">first_part</span> <span class="o">+</span> <span class="n">layers</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">num</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">last_part</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># concat module last part(if exists) aft module list to last stage</span>
</span></span><span class="line"><span class="cl">            <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">last_part</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># deepcopy module for xla device tracing use</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage_module</span> <span class="o">=</span> <span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">Stage</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stage_module</span><span class="p">,</span> <span class="n">submesh</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">modules</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="write-sharding-function">Write Sharding Function</h2>
<p>要根据选择的不同的并行策略对每个 Stage 的模型权重，输入，输出进行切分。这里同样我们单独处理 Embedding_blocks, STDiT3_blocks 和 T2IFinalLayer. 让 stage0 包括对 Embedding_blocks 的处理，stage(N-1) 包括对 T2IFinalLayer 的处理。需要注意的是 DS-ulysses 我们需要对 <a href="mailto:Q@K.T">Q@K.T</a> 的结果 和 S@V 的结果也进行切分 SPMD 才会插入正确的 All2All，因此这部分只能放在网络的 forward 里面进行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_one_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># first 5 modules are embedding layers</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_embedding</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">total_len</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">    <span class="n">shard_sora_final</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_first_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_embedding</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_last_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_len</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># skip norm layer mark sharding</span>
</span></span><span class="line"><span class="cl">    <span class="n">shard_sora_final</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">total_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="construct-pipeline">Construct Pipeline</h2>
<p>然后为了处理多 stage 的情况，我们需要保存每个 stage 的输入和输出的形状。这一步相当于放到 cuda 上重走一遍整个模型的 forward，记录下每一层输入和输出的形状，保存为 json 一遍。实际上对于每个固定生成大小的视频进行一次就行，下次直接读取这个文件。因为现在都采用 <a href="https://facebookresearch.github.io/xformers/components/ops.html">xformers.ops.memory_efficient_attention</a>，需要输入张量在 cuda 上，我们需要手动在模型的 forward 函数中写一个 navie 的 attention 计算流程好让 torch_xla 能对张量进行跟踪。</p>
<h2 id="trace-mhlo-graph">Trace mhlo Graph</h2>
<p>根据上一步得到的每个 Stage 的输入形状，创建输入张量，放入 xla_device 上，执行 forward. 最后导出输出的 mhlo 计算图。这里需要注意第一个 stage 包含多个非连续的模块，因此需要单独处理，最后一个 stage 最后一层的输入与其他 block 不同，因此也要单独处理。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">trace_stage_mhlo_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">check_res</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    trace stage nn modules to mhlo graph
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># (NOTE): construct xla mesh before trace tensors generate,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># i.e., before any xla device call to avoid xla computation client construct</span>
</span></span><span class="line"><span class="cl">    <span class="n">xla_mesh</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">xla_mesh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_construct_stage_xla_mesh</span><span class="p">()</span>  <span class="c1"># create mesh from submesh info</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create xla device trace tensors, move module to xla device</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_trace_tensors</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">y_embedded</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">t_mlp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">t_mlp</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t_mlp</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">mod</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>  <span class="c1"># first load to cpu</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># get pipeline exec mode</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">exec_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">exec_mode</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># load lora cofifg</span>
</span></span><span class="line"><span class="cl">    <span class="n">lora_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">lora_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Enter trace mhlo graph for stage: &#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Trigger shard func to mark sharding the model</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_strategy</span><span class="p">,</span> <span class="n">xla_mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">exec_mode</span> <span class="o">==</span> <span class="n">EXEC_MODE</span><span class="o">.</span><span class="n">INFER</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># set stage name &amp; dump file path</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_set_stage_name_dump_file</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">exec_mode</span><span class="p">,</span> <span class="s2">&#34;fw&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_sampling_steps</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl">        <span class="n">timesteps</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">i</span> <span class="o">/</span> <span class="n">num_sampling_steps</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_timesteps</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_sampling_steps</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># FIXME: 原先是为每个stage单独生成trace_tensor, 现在要把上一个的结果传给下一个 stage</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#for i in range(30):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">pipeline_patches_height_list</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">pipeline_patches_height_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,:]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">xla_mesh</span><span class="p">)</span>  <span class="c1"># outputs is a list</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">,</span> <span class="n">xla_mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">check_res</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># check xla results compared to gpu results</span>
</span></span><span class="line"><span class="cl">            <span class="n">check_result_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># use torch xla _get_xla_tensors_hlo interface</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># to eliminate redundant live tensors as ret values</span>
</span></span><span class="line"><span class="cl">            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;XLA_DUMP_POST_OPTIMIZATIONS&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;true&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch_xla</span><span class="o">.</span><span class="n">_XLAC</span><span class="o">.</span><span class="n">_get_xla_tensors_hlo</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="analyze-mhlo-graph">Analyze mhlo Graph</h2>
<p>接下来我们要遍历上一步得出的 mhlo 图。</p>
<h3 id="opview">OpView</h3>
<p>从根节点的 ir 开始遍历上一步导出的整个计算图。根据传入 ir 的类型定义调用对应的 visit 函数读取其属性进行操作。主要通过 rsqrt 的位置来划分一个 Transformer block 中第几个 dot 和 dot_general 对应的是什么操作。对于 Sora 来说划分情况如下。这里需要注意的是 mhlo 图记录的是拓扑排序的顺序，不是程序顺序执行的顺序，因此第一个 block 会掺杂着 Embedding_blocks 的一些 dot 操作。因此我们从第二个 block 的第一个 rsqrt 位置开始统计。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">collect_rms_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">rms_collector</span> <span class="o">=</span> <span class="n">RMSCollector</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">rms_collector</span><span class="o">.</span><span class="n">visit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span> <span class="o">=</span> <span class="n">rms_collector</span><span class="o">.</span><span class="n">rms_locs</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># construct attention block &amp; ffn block ranges</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># exclude the rsqrt in T2IFinalLayer</span>
</span></span><span class="line"><span class="cl">  <span class="n">att_rm_locs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">att_rm_locs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># a block has 4 rsqrt, start from 2nd block to avoid embedding</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cro_attn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">att_rm_locs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># ORG: range(8, len(att_rm_locs), 4): </span>
</span></span><span class="line"><span class="cl">      <span class="n">start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">4</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">          <span class="n">end</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">end</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ffn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><table>
  <thead>
      <tr>
          <th>module</th>
          <th>operator</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td><code>RMSNorm(x)</code></td>
      </tr>
      <tr>
          <td><strong>Self Attention</strong></td>
          <td><code>dot(x, qkvLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(q)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(q, k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(s, v)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(attn, oLinear.weight)</code></td>
      </tr>
      <tr>
          <td><strong>Cross Attention</strong></td>
          <td><code>dot(x, qLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(y, kvLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(q, k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(s, v)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(attn, oLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(x) </code></td>
      </tr>
      <tr>
          <td><strong>Feed Forward Network</strong></td>
          <td><code>dot(x, upLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(x, downLinear.weight)</code></td>
      </tr>
  </tbody>
</table>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">visit_dot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">dot_lineno</span> <span class="o">=</span> <span class="n">_parse_loc_lineno</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">spt_att_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">cro_att_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cro_attn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">spt_qkv_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">ffn_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># lie in RMS ops closed attention block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">spt_att_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">spt_att_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#import pdb;pdb.set_trace()</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">cro_att_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">cro_att_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cro_att_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># lie ffn block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">spt_qkv_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">spt_qkv_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># pixart pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>                 
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Traversal of one block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_att_dot_cnt</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> \
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">attention_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># reset each block level counters</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_att_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">generic_visit</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>保存好一个 Transformer block 中每个 dot 或 dotgeneral 对应的是什么操作后，我们便可以访问这个 ir. 这里需要注意只要两个相乘的矩阵有一个是二维张量 (比如线性层的权重)，mhlo 都会将另一个 reshape 成二维张量。dot 算子 (<code>jaxlib.mlir.dialects._mhlo_ops_gen.DotOp</code>) 两个操作数都是二维的张量，qkvLinear 对应的是第一个 dot 操作。左操作数的 shape 为 <code>(BST,3C)</code>. 当两个相乘的矩阵都是 3 维及以上张量的时候就会生成 dot_general 该算子的两个相乘的矩阵都会被 reshape 成三维张量。Self-Attention 的第一个 dot_general 左操作数的 shape 为 <code>(BTN_A,S,C)</code>. 这样我们就可以得到 <code>BT=(BST)/S, N_A=(BTN_A)/(BT)</code>. 同样我们可以得到 OLinear, FFN 中 upLinear 和 downLinear 权重的形状. 以及 Cross-Attention 模块的对应信息。由于之前遍历是从第二个 block 开始的，因此总层数要 ＋1. 最后将得到的参数打包成一个字典返回。</p>
<h3 id="communication-view">Communication View</h3>
<p>我们以同样的方式定义各种集合通信算子的 visit 函数用于评估该算子的通信量，遍历到对应的 ir 后调用它。</p>
<p>AllReduce 将所有的数据通过规约操作集成到各个设备中。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6e4d9c026bc0632af5040321998fb3ab?method=download&amp;shareKey=f901430ac6bfa781d0b462f0170981d3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6e4d9c026bc0632af5040321998fb3ab?method=download&amp;shareKey=f901430ac6bfa781d0b462f0170981d3" alt="AllReduce">
    </a><figcaption>AllReduce</figcaption></figure></p>
<p>在 Ring-AllReduce 的 ReduceScatter 步骤中，每个进程发送 M 个元素 N-1 次，总共为 M(N-1). 在 AllGather 步骤中，每个进程发送它计算的块的结果。这是额外的 M 个元素发送了 N-1 次。总的通信量加起来是 2M(N-1).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB69d2b3957cd1863481bff0e785dc9a82?method=download&amp;shareKey=32e60903bafe5dbf240af91c67486e1b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB69d2b3957cd1863481bff0e785dc9a82?method=download&amp;shareKey=32e60903bafe5dbf240af91c67486e1b" alt="Ring-AllReduce">
    </a><figcaption>Ring-AllReduce</figcaption></figure></p>
<p>All-Gather 示意图如下，每个设备开始拥有初始的一部分数据，通信后每个设备都有一份完整的数据。总的通信量为 M(N-1).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe7e6e7a1230ed9ba7ba037556e489d51?method=download&amp;shareKey=5afdf2b669a500a6844aa9e281fe1ac3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe7e6e7a1230ed9ba7ba037556e489d51?method=download&amp;shareKey=5afdf2b669a500a6844aa9e281fe1ac3" alt="AllGather">
    </a><figcaption>AllGather</figcaption></figure></p>
<p>All2All 示意图如下，每个设备把自己的第 i 块数据发送给第 i 个设备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBddc785dcc80dd741fc1f469a85823cd4?method=download&amp;shareKey=085c0a5681116b4e1683d4d6ae5d080f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBddc785dcc80dd741fc1f469a85823cd4?method=download&amp;shareKey=085c0a5681116b4e1683d4d6ae5d080f" alt="All2All">
    </a><figcaption>All2All</figcaption></figure></p>
<p>基于 Bruck 算法的 All2All 流程如下</p>
<ol>
<li>局部循环移位 (Local Shift of Data-Blocks)
每个进程将其本地的数据块重新排列，进行初始的循环移位。对于进程 p 和数据块索引 i: R[i]=S[(p+i)%P]. 其中 S[i] 是进程本地初始的数据，R[i] 是移位后的数据。</li>
<li>全局通信 (Global Communication)
一共进行 log(P) 次通信。
每一步中每个进程将一部分数据发送给相邻的进程，并接收相邻进程发送的数据。若数据块索引 i 用 radix-2 表示的第 k 位为 1，则数据块会被发送到目标进程。
对于进程 p: 发送数据到进程 ((p + 2^k) % P)，接收来自进程 ((p - 2^k) % P) 的数据。
每次发送后，进程将接收到的数据更新到其本地数据中。</li>
<li>局部逆向移位 (Local Inverse Shift of Data-Blocks)
在完成所有全局通信之后，每个进程执行逆向移位，以恢复数据块的正确顺序。对于每个数据块索引 i: R[i]=R[(p−i+P)%P]</li>
</ol>
<p>在进程是 2 次幂的情况下每个设备每次要通信 M*P/2大小的数据，总共为 MPlog(P)/2.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa083a4c6002019e62b23c0b24b59a812?method=download&amp;shareKey=58f1b8055307d53e43ce86b9e1762989" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa083a4c6002019e62b23c0b24b59a812?method=download&amp;shareKey=58f1b8055307d53e43ce86b9e1762989" alt="Example of the Bruck Algorithm with 4 Processes">
    </a><figcaption>Example of the Bruck Algorithm with 4 Processes</figcaption></figure></p>
<h3 id="tflops-view">TFLOPS View</h3>
<p>计算量主要分成两种，element-wise 的操作计算量为元素个数。两个形状分别为 mxk 和 kxn 的矩阵相乘计算量为 2mkn. 被计入 element-wise 操作的算子有 add, subtract, multiply, divide, rsqrt, negate, exponential. 被计入矩阵乘法的算子有 dot, dot_general.</p>
<h2 id="performance-analysis">Performance Analysis</h2>
<p>我们根据提取出的 Transformer block 的信息送入性能分析器进行分析. tx8 的配置如下</p>
<table>
  <thead>
      <tr>
          <th>Parameter</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>TILE_NUM</td>
          <td>16</td>
      </tr>
      <tr>
          <td>SRAM (MB)</td>
          <td>3</td>
      </tr>
      <tr>
          <td>NOC BW (GB/s)</td>
          <td>128</td>
      </tr>
      <tr>
          <td>DRAM BW (GB/s)</td>
          <td>100</td>
      </tr>
      <tr>
          <td>DRAM LATENCY (us)</td>
          <td>0.1</td>
      </tr>
      <tr>
          <td>GEMM (TFLOPS)</td>
          <td>8</td>
      </tr>
      <tr>
          <td>VECTOR (TOPS)</td>
          <td>0.0625</td>
      </tr>
      <tr>
          <td>HOP LATENCY (us)</td>
          <td>0.01</td>
      </tr>
  </tbody>
</table>
<p>根据提取出的信息构建的 STDiT 的 spt_blk, tmp_blk, cross_blk 的参数字典如下.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                  <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_spt&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                  <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_tmp&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span><span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_cro&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;D_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;D_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FD&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FD&#34;</span><span class="p">]}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>根据这些参数再构建每个层的输入输出形状，计算类型和计算量，以 <code>Gate_ResAdd</code> 为例:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">GB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Gate_ResAdd</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">  Construct each op after MHSA on the config file
</span></span></span><span class="line"><span class="cl"><span class="s1">  &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># {name:{type:&#34;&#34;, size:&#34;&#34;, ishape:[], wshape:[]/None, oshape:[]}}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">construct_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">construct_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">GB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;D_O&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_weight_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;D_O&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_output_shape</span> <span class="o">=</span> <span class="n">ResAdd_input_shape</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_compute</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="n">GB</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="o">+</span><span class="s2">&#34;_&#34;</span><span class="o">+</span><span class="s2">&#34;ResAdd&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;ResAdd&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;Vector&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;ishape&#34;</span><span class="p">:</span> <span class="n">ResAdd_input_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;wshape&#34;</span><span class="p">:</span> <span class="n">ResAdd_weight_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;oshape&#34;</span><span class="p">:</span> <span class="n">ResAdd_output_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;compute&#34;</span><span class="p">:</span> <span class="n">ResAdd_compute</span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>就像这样构建整个 Transformer block 的所有操作</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">STDIT2_block</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># {name:{type:&#34;&#34;, size:&#34;&#34;, ishape:[], wshape:[]/None, oshape:[]}}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">construct_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">construct_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_spt&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_tmp&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span><span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_cro&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;D_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;D_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FD&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FD&#34;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cross_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cross_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">mlp_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;mlp&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block</span> <span class="o">=</span> <span class="n">FFN_block</span><span class="p">(</span><span class="n">cross_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">mlp_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;mlp&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">op_list</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">spatial_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="bp">self</span><span class="o">.</span><span class="n">temporal_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="bp">self</span><span class="o">.</span><span class="n">cross_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">op_dict</span> <span class="ow">in</span> <span class="n">op_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">op_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后就可以将构建好的 ops 放入 mapper 进行分析。刚才那些操作会被分成 3 类 <code>vector_mapper</code>, <code>gemm_auto_opt_mapper</code> 和 <code>flashatten_mapper</code>. 我们根据操作的类型送入对应的 mapper 进行分析，具体如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">STDIT2_mapper</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">QKV_fusion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">preset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</span></span><span class="line"><span class="cl">  <span class="n">Layers</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;L&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">ops</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">ops</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;=========================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Spatial Branch Mapping ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  =========================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">TmTn</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_RMSNorm&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_RMSNorm&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_ResAdd&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;==========================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Temporal Branch Mapping ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ==========================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>  <span class="c1"># 切分 30 份也无法满足SRAM要求</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_RMSNorm&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_RMSNorm&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_ResAdd&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;====================================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Cross Branch Mapping 2x per block ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ====================================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#mapping_result[&#39;spatial_RMSNorm&#39;]= vector_mapper(ops[&#39;spatial_RMSNorm&#39;],arch,splits=None,details=details)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_ResAdd&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">  <span class="c1"># HACK: Gate_ResAdd *2 了, cross 无gate 这里无 _2</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;====================================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Feed Forward Network 2x per block ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ====================================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;FFNup&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span><span class="n">fusion_op2</span><span class="o">=</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;SiLU&#39;</span><span class="p">],</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># mapping_result[&#39;FFNgate&#39;] = gemm_auto_opt_mapper(ops[&#39;FFNgate&#39;], arch, TmTn=TmTn, details=details)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># mapping_result[&#39;Hadamard&#39;] = vector_mapper(ops[&#39;Hadamard&#39;], arch, splits=None)</span>
</span></span><span class="line"><span class="cl">  <span class="n">TmTn</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>mapper 会遍历所有可能的切分策略放入 tx8 执行并选择最好的那一个。对于 vector 类型的算子只会沿着 sequence 维度切分；对于 GEMM 算子则会沿着 m, k, n 维度都进行切分；对于 flash-attention 的切分则与原算法相同，外循环遍历 K, V 的每一块，内循环遍历 Q 的每一块。这样就可以得到每个 tx8 上最优的切分方式对应的通信用时，计算用时和利用率。再用之前统计出的每个 die 上通信量除以 die2die 带宽得到通信用时，由此得到总的推理用时。</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
