<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>WITHER</title>
    <link>http://localhost:57770/</link>
    <description>Recent content on WITHER</description>
    <generator>Hugo -- 0.147.7</generator>
    <language>en</language>
    <copyright>2024-2025 WITHER</copyright>
    <atom:link href="http://localhost:57770/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PipeFusion</title>
      <link>http://localhost:57770/blogs/pipefusion/</link>
      <pubDate>Fri, 13 Jun 2025 11:39:32 +0800</pubDate>
      <guid>http://localhost:57770/blogs/pipefusion/</guid>
      <description>Paper Reading of PipeFusion</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>PipeFusion 是一种利用多 GPU 并行来进行 DiT 模型推理的方法。</p>
<ul>
<li>将图像分割成 patch，并将 Transformer Blocks 分布在多个设备上。</li>
<li>通过重用上一步 (<em>one-step stale</em>) 的特征图作为当前步的上下文，消除了流水线中的等待时间。</li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>由于 Attention 的计算特性，计算时间与序列长度的平方成正比，使得 DiT 模型生成高分辨率图形 (长视觉序列) 的推理延迟非常高。
<a href="https://arxiv.org/abs/2402.19481">DistriFusion</a> 观察到在相邻的扩散时间步中输入和激活存在高度相似性，我们将这种现象称为输入时间冗余 (<em>input temporal redundancy</em>). 它保留所有层 KV 的完整形状。内存开销不会随着计算设备数量的增加而减少，在可扩展性方面表现不佳。</p>
<p>如下图所示，DistriFusion 在 2 个设备上都保存一份 DiT 参数。它将图像分成 2 个小块，并对每一层的激活使用异步 allgather. PipeFusion 将 DiT 参数切分到 2 个设备上，将图像分成 4 个 patch ，两个设备之间采用异步 P2P 通信来传输激活。它只在每个设备上传输初始层的输入激活和最终层的输出激活</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB0b7d56775c290d039aba1c7aab220319?method=download&amp;shareKey=6330f70503af01609e9fe13a35826aab" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB0b7d56775c290d039aba1c7aab220319?method=download&amp;shareKey=6330f70503af01609e9fe13a35826aab" alt="Comparsion Between DistriFusion &amp; PipeFusion">
    </a><figcaption>Comparsion Between DistriFusion &amp; PipeFusion</figcaption></figure></p>
<h1 id="background--related-works">Background &amp; Related Works</h1>
<p>扩散模型通常使用 DNN 预测噪声。给定有噪声的图像 xt，模型 ϵθ 将 xt、去噪时间步 t 和附加条件 c (例如文本、图像) 作为输入，以预测 xt 中相应的噪声ϵt.</p>
<p>扩散模型具有较长的序列长度和较小的模型大小，但在推理过程中通信开销仍然很大。DistriFusion 为 U-Net 为主干的扩散模型引入了位移 patch 并行(displacement patch parallelism)，将模型的输入划分为多个 patch，便于激活的异步通信并且使得通信与计算重叠。然而，当将该方法应用于 DiT 时，内存缓冲区的开销将导致巨大的内存开销。</p>
<h1 id="methods">Methods</h1>
<p>不同并行策略下 DiT 单步扩散过程的比较如下表所示。</p>
<ul>
<li>p: 生成的序列长度 (即隐空间下的像素数量).</li>
<li>hs: 模型的隐藏通道大小。</li>
<li>N: 设备数量。</li>
<li>M: 图像切分的 patch 数量。</li>
<li>L: Transformer Blocks 层数。</li>
<li>P: 模型总参数量。</li>
<li>A: Attention 的过程中的激活大小 (Q, K, V, O 大小一样)</li>
</ul>
<p>名称后面的 * 表示采用异步通信，通信开销可以通过计算隐藏。</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>attn-KV</th>
          <th>communication cost</th>
          <th>param</th>
          <th>QO Activations</th>
          <th>KV Activations</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Tensor Parallel</td>
          <td>fresh</td>
          <td>4O(p × hs)L</td>
          <td>P/N</td>
          <td>(2/N) A = (1/N) QO</td>
          <td>(2/N) A = (1/N) KV</td>
      </tr>
      <tr>
          <td>DistriFusion*</td>
          <td>stale</td>
          <td>2O(p × hs)L</td>
          <td>P</td>
          <td>A</td>
          <td>2AL = (KV)L</td>
      </tr>
      <tr>
          <td>Ring Seq Parallel*</td>
          <td>fresh</td>
          <td>NA</td>
          <td>P</td>
          <td>A</td>
          <td>A</td>
      </tr>
      <tr>
          <td>Ulysses Seq Parallel</td>
          <td>fresh</td>
          <td>4O(p × hs)L</td>
          <td>P</td>
          <td>(2/N) A = (1/N) QO</td>
          <td>(2/N) A = (1/N) KV</td>
      </tr>
      <tr>
          <td>PipeFusion*</td>
          <td>stale-</td>
          <td>2O(p × hs)</td>
          <td>P/N</td>
          <td>(2/M) A = (1/M) QO</td>
          <td>(2L/N) A = (1/N) (KV)L</td>
      </tr>
  </tbody>
</table>
<h2 id="sequence-parallelism--tensor-parallelism">Sequence Parallelism &amp; Tensor Parallelism</h2>
<p>针对 LLM 提出的张量并行 (tensor parallelism, TP) 和序列并行 (sequence parallelism, SP) 可以应用于 DiT 推理。因为他们的主干都是 Transformer.
在 TP<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 中，权重矩阵按列被切分为 <em>N</em> 份，这样矩阵乘法后激活值也被切分成 <em>N</em> 份，使得每个设备的参数量和激活量均为原来的 1/N. 在 attention 计算和 FFN 层之后都需要进行两次同步 all-reduce 操作，因此每一层通信量为 4O(p × hs).</p>
<p>在 SP 中，可以将输入图像分割成 patch，DiT 中的多头注意模块可以采用 Ring-Attention<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，DeepSpeed-Ulysses<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，或者两者的组合。Ulysses SP 并行需要 4 次 all-to-all 操作，因此每一层通信量为 4O(p × hs), 和 TP 相同。</p>
<blockquote>
<p>TP 和 SP 可以在 DiT 推理中一起使用。</p></blockquote>
<h2 id="displaced-patch-parallelism">Displaced Patch Parallelism</h2>
<p>输入时间冗余意味着给定层中激活 patch 的计算并不完全取决于其他 patch 的最新激活。在前一个扩散步骤中加入稍微过时的激活是可行的。该方法将输入图像划分为 N 个patch，每个设备计算其各自 patch 的输出结果。 如下图所示 attention 模块需要具有完整形状的 KV 激活。它采用异步 all-gather 收集上一步扩散步骤的 KV 激活，并用其进行当前步的 attention 计算。</p>
<p>DistriFusion<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> 可以看作是异步 SQ 的一种形式。它通过正向计算扩散步骤来隐藏 KV 通信，但代价是消耗更多内存。DistriFusion 利用 N-1/N 的 T+1 步的 KV 激活和 T 步的 1/N 的局部 KV 激活相结合。与 Ring-Attention 相比，DistriFusion 可以更有效地隐藏通信开销，因为它允许 KV 通信与扩散步骤的整个前向计算重叠，而 Ring-Attention 只允许通信在注意模块内部重叠。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB9a64239185e8b3604db9a46098203d05?method=download&amp;shareKey=eab8f5ec3cff754ab7711e87333e8797" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB9a64239185e8b3604db9a46098203d05?method=download&amp;shareKey=eab8f5ec3cff754ab7711e87333e8797" alt="DistriFusion vs. Ring-Attention SQ for an Attention Module">
    </a><figcaption>DistriFusion vs. Ring-Attention SQ for an Attention Module</figcaption></figure></p>
<p>在 Ring-Attention中，其通信缓冲区 c × hs 可由图中块大小 c 控制，其值小于 p/N. DistriFusion要求每个计算设备始终保持 KV 的完整形状的通信缓冲区，因此通信开销总共是 AL.</p>
<h2 id="displaced-patch-pipeline-parallelism">Displaced Patch Pipeline Parallelism</h2>
<p>PipeFusion 相比于 DistriFusion 有着更高的内存效率和更低的通信成本。它将输入图像划分为 M 个不重叠的 patch，DiT Blocks 被划分为 N 个阶段，每个阶段按顺序分配给 N 个计算设备。每个设备在其被分配的阶段以流水线方式处理一个 patch 的计算任务。</p>
<blockquote>
<p>DiT 模型中因有许多相同的 transformer block，很容易将去噪网络的工作负载均匀地划分为 N 个部分。然而，U-Net 扩散模型没有这种重复结构。</p></blockquote>
<p>一个 M=4, N=4 的 PipeFusion 例子如下图所示，利用输入时间冗余，设备不需要等待接收到当前步骤的完整形状激活，利用上一步的激活就可以开始自己所处阶段的计算。考虑流水线气泡，流水线的有效计算比为 MS/MS+N−1，其中 S 为扩散步长数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB157240ab4733f1ca4cca87a2389a7b08?method=download&amp;shareKey=b0593903312b6da2796d081015a30baa" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB157240ab4733f1ca4cca87a2389a7b08?method=download&amp;shareKey=b0593903312b6da2796d081015a30baa" alt="The Workflow of Displaced Patch Pipeline Parallelism">
    </a><figcaption>The Workflow of Displaced Patch Pipeline Parallelism</figcaption></figure></p>
<p>PipeFusion 在计算设备之间仅传输属于一个阶段的 (连续 transformerl blocks) 的输入和输出的激活，因此通信开销为 2O(p × hs). PipeFusion 通过异步 P2P 传输前一步 Patch 数据和接收后一步骤 Patch 数据来与当前 Patch 计算重叠，从而将通信隐藏在计算中。PipeFusion 中的每个设备仅存储与其特定阶段相关的 1/N 份参数。由于使用陈旧 KV 进行注意力计算，要求每个设备保持其阶段对应的 L/N 层的完整 KV.</p>
<p>PipeDiffusion 理论上优于 DistriFusion，因为它利用了更多的新激活。如图所示，在单个扩散步骤内，PipeDiffusion 中最新激活的占比随着流水线执行而增加。而 DistriFusion 中最新激活的占比一直都是 1/N.</p>
<blockquote>
<p>尽管 DiT 没有采用 GroupNorm 层，但在 PipeFusion 中，U-Net 中 DistriFusion 对 GroupNorm 层的精度保持设计，特别是校正异步群归一化 (Corrected Asynchronous GroupNorm)，可以无缝地应用于 PipeFusion.</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf19f26f13cfdf79d2e13e8b012b2954b?method=download&amp;shareKey=9983249909c804bca818835ce2f953ce" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf19f26f13cfdf79d2e13e8b012b2954b?method=download&amp;shareKey=9983249909c804bca818835ce2f953ce" alt="The Fresh Part of Activations">
    </a><figcaption>The Fresh Part of Activations</figcaption></figure></p>
<p>由于使用输入时间冗余需要一个预热期，DistriFusion 使用了几次同步 path 并行的预热步骤作为预备阶段。为了优化预热开销，可以将预热步骤与其余步骤分开，并将其分配给不同的计算资源。</p>
<h1 id="experiments">Experiments</h1>
<p>我们在 Pixart-α 上进行实验 (0.6B)，它支持分辨率 1024px 的高分辨率图像合成，采用标准的 DiT，并结合交叉注意模块注入文本条件。使用了三个 GPU 集群，包括 4xGPU A100 80GB (PCIe) 集群，8xGPU A100 80GB (NVLink) 集群和 8xGPU L20 40GB (PCIe) 集群。测试的 GPU P2P 带宽分别为23 GB/s、268 GB/s 和 26 GB/s. 切分的 patch 数目 M 从 2,4,8,16,32 中搜索来确定最佳延迟性能。</p>
<ul>
<li>TP: 参考 Megatron-LM实 现了一个 TP DiT.</li>
<li>SP: 采用了两种不同的序列并行，DeepSpeed-Ulysses 和 Ring-Attention.</li>
<li>DistriFusion: 将 U-Net 扩散模型中的官方 DistriFusion 应用于DiT.</li>
<li>Original:在单个 GPU 上的串行实现。</li>
</ul>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在 VAE 中由于卷积算子的临时内存使用会产生内存峰值，因此 VAE 比 DiT 层需要更多的内存。为了缓解这个问题，我们将卷积层的输入图像分成几个块，将单个卷积操作转换为按顺序执行的多个操作的序列。</p></div>

<h2 id="quality-results">Quality Results</h2>
<p>使用 20 步 DPM-Solver，预热步骤为 4 步。当 patch 数为 1 时，PipeFusion 的精度与 DistriFusion 相当。当 patch 数超过 1 时，其精度在理论上比 PipeFusion 更接近原始版本。PipeFusion 在 FID 方面略优于 DistriFusion.</p>
<h2 id="latency-and-memory">Latency and Memory</h2>
<p>20 步 DPM-Solver，预热步骤为 1 步。</p>
<ul>
<li>4xA100 (PCIe)集群上: 对于 8192px 的情况，在，DistriFusion 和 SQ 都会遇到内存不足 (OOM) 问题。</li>
<li>8xL20 (PCIe)集群上: 生成 4096px 分辨率的图像时，DistriFusion 和 SQ 都会遇到 OOM 问题。</li>
<li>8xA100 (NVLink) 集群上: 使用异步通信的 SQ (Ulysses) 的延迟与异步 DistriFusion 非常相似，并且优于 Ring 版本。此外，PixArt-α 在跨 8 个设备部署时面临限制，因为28个DiT层不能在均分，从而导致额外的开销。</li>
</ul>
<h3 id="4x-a100-pcie-集群">4x A100 (PCIe) 集群</h3>
<table>
  <thead>
      <tr>
          <th><strong>Latency</strong></th>
          <th><strong>PipeFusion</strong></th>
          <th><strong>Tensor Parallel</strong></th>
          <th><strong>DistriFusion</strong></th>
          <th><strong>Seq Parallel (Ulysses)</strong></th>
          <th><strong>Seq Parallel (Ring)</strong></th>
          <th>Single-GPU</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1024px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>2.41x</td>
          <td>2.69x</td>
          <td>2.01x</td>
          <td>3.04x</td>
          <td>2.4x</td>
      </tr>
      <tr>
          <td><strong>2048px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>3.02x</td>
          <td>1.79x</td>
          <td>1.48x</td>
          <td>2.06x</td>
          <td>3.02x</td>
      </tr>
      <tr>
          <td><strong>4096px</strong></td>
          <td>1.02x</td>
          <td>1.77x</td>
          <td>1.16x</td>
          <td><strong>1.00x</strong></td>
          <td>1.12x</td>
          <td>3.05x</td>
      </tr>
      <tr>
          <td><strong>8192px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>1.10x</td>
          <td>OOM</td>
          <td>OOM</td>
          <td>OOM</td>
          <td>3.1x</td>
      </tr>
  </tbody>
</table>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7384a580336eb8b92972343922c549b6?method=download&amp;shareKey=a203914b94020ad72b87708703fd829f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7384a580336eb8b92972343922c549b6?method=download&amp;shareKey=a203914b94020ad72b87708703fd829f" alt="Overall Latency on a 4×A100-80GB (PCIe)">
    </a><figcaption>Overall Latency on a 4×A100-80GB (PCIe)</figcaption></figure></p>
<p>内存效率方面，PipeFusion优于除了张量并行的其他方法。虽然张量并行的内存占用最低，但与其他并行化策略相比，由于通信量大会导致更高的延迟。</p>
<table>
  <thead>
      <tr>
          <th><strong>Max Memory</strong></th>
          <th><strong>PipeFusion (Baseline)</strong></th>
          <th><strong>Original</strong></th>
          <th><strong>Tensor Parallel</strong></th>
          <th><strong>DistriFusion</strong></th>
          <th><strong>Seq Parallel (Ulysses)</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1024px</strong></td>
          <td>1.00x</td>
          <td>1.04x</td>
          <td>0.98x</td>
          <td>1.21x</td>
          <td>1.21x</td>
      </tr>
      <tr>
          <td><strong>2048px</strong></td>
          <td>1.00x</td>
          <td>0.98x</td>
          <td>0.90x</td>
          <td>1.54x</td>
          <td>1.33x</td>
      </tr>
      <tr>
          <td><strong>4096px</strong></td>
          <td>1.00x</td>
          <td>1.18x</td>
          <td>0.69x</td>
          <td>2.35x</td>
          <td>1.63x</td>
      </tr>
      <tr>
          <td><strong>8192px</strong></td>
          <td>1.00x</td>
          <td>1.41x</td>
          <td>0.71x</td>
          <td>2.34x</td>
          <td>OOM</td>
      </tr>
  </tbody>
</table>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB27159a6d3fb6cafa4dfc7fbc5883a211?method=download&amp;shareKey=90098eb080c078296e3b0fa0fd260ee6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB27159a6d3fb6cafa4dfc7fbc5883a211?method=download&amp;shareKey=90098eb080c078296e3b0fa0fd260ee6" alt="Overall GPU Memory on a 4×A100-80GB (PCIe)">
    </a><figcaption>Overall GPU Memory on a 4×A100-80GB (PCIe)</figcaption></figure></p>
<h3 id="8x-l20-pcie-集群">8x L20 (PCIe) 集群</h3>
<table>
  <thead>
      <tr>
          <th><strong>Latency</strong></th>
          <th><strong>PipeFusion</strong></th>
          <th><strong>Tensor Parallel</strong></th>
          <th><strong>DistriFusion</strong></th>
          <th><strong>Seq Parallel (Ulysses)</strong></th>
          <th><strong>Seq Parallel (Ring)</strong></th>
          <th>Single-GPU</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1024px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>2.46x</td>
          <td>3.26x</td>
          <td>1.48x</td>
          <td>4.42x</td>
          <td>2.46x</td>
      </tr>
      <tr>
          <td><strong>2048px</strong></td>
          <td>0.99x</td>
          <td>2.26x</td>
          <td><strong>1.00x</strong></td>
          <td>1.58x</td>
          <td>1.09x</td>
          <td>4.16x</td>
      </tr>
      <tr>
          <td><strong>4096px</strong></td>
          <td><strong>1.00x</strong></td>
          <td>1.16x</td>
          <td>OOM</td>
          <td>1.31x</td>
          <td>4.40x</td>
          <td>4.30x</td>
      </tr>
  </tbody>
</table>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB883bda408bc38824e7bda7425ae4fb51?method=download&amp;shareKey=403004516bb9ff8d9271e0f8ef88a693" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB883bda408bc38824e7bda7425ae4fb51?method=download&amp;shareKey=403004516bb9ff8d9271e0f8ef88a693" alt="Overall latency on a 8×L20 (PCIe)">
    </a><figcaption>Overall latency on a 8×L20 (PCIe)</figcaption></figure></p>
<h3 id="8x-a100-nvlink-集群">8x A100 (NVLink) 集群</h3>
<table>
  <thead>
      <tr>
          <th><strong>Latency</strong></th>
          <th><strong>PipeFusion</strong></th>
          <th><strong>Tensor Parallel</strong></th>
          <th><strong>DistriFusion</strong></th>
          <th><strong>Seq Parallel (Ulysses)</strong></th>
          <th><strong>Seq Parallel (Ring)</strong></th>
          <th>Single-GPU</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>1024px</strong></td>
          <td>1.26x</td>
          <td>1.59x</td>
          <td><strong>1.00x</strong></td>
          <td>1.79x</td>
          <td>3.38x</td>
          <td>2.15x</td>
      </tr>
      <tr>
          <td><strong>2048px</strong></td>
          <td>1.64x</td>
          <td>2.85x</td>
          <td><strong>1.00x</strong></td>
          <td><strong>1.00x</strong></td>
          <td>1.43x</td>
          <td>3.99x</td>
      </tr>
      <tr>
          <td><strong>4096px</strong></td>
          <td>1.08x</td>
          <td>1.56x</td>
          <td><strong>1.00x</strong></td>
          <td>1.18x</td>
          <td>1.93x</td>
          <td>7.28x</td>
      </tr>
      <tr>
          <td><strong>8192px</strong></td>
          <td>1.35x</td>
          <td><strong>1.00x</strong></td>
          <td>OOM</td>
          <td>OOM</td>
          <td>OOM</td>
          <td>5.98x</td>
      </tr>
  </tbody>
</table>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf294c0c56206b35fb2bd495b65caa1f8?method=download&amp;shareKey=a6780d54a1429bba9fa05174f1ab44e8" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf294c0c56206b35fb2bd495b65caa1f8?method=download&amp;shareKey=a6780d54a1429bba9fa05174f1ab44e8" alt="Overall latency on a 8×A100 (NVLink)">
    </a><figcaption>Overall latency on a 8×A100 (NVLink)</figcaption></figure></p>
<h3 id="scalability">Scalability</h3>
<p>PipeFusion 在 NVLink 和 PCIe 上的时延相似，PCIe 甚至在表现出了轻微的优势。在 PCIe 集群上，对于相同的任务，PipeFusion 总是比 DistriFusion 快。说明 PipeFusion 的通信带宽要求非常低，因此不需要使用 NVLink 等高带宽网络。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbd3f234475f8a4dd0831cb0de02c3023?method=download&amp;shareKey=186d86f87e9eaadc6df309ff516b2b1c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbd3f234475f8a4dd0831cb0de02c3023?method=download&amp;shareKey=186d86f87e9eaadc6df309ff516b2b1c" alt="Scalability of PipeFusion and DistriFusion on A100 PCIe vs. NVLink cluster">
    </a><figcaption>Scalability of PipeFusion and DistriFusion on A100 PCIe vs. NVLink cluster</figcaption></figure></p>
<h2 id="ablation-studies">Ablation Studies</h2>
<p>随着 patch 数目 M 的增加，内存消耗减少，并且对通信没有影响。但在实践中，M 不应该设置得太高。生成 1024px 和 2048px 图像时，当 M 超过一定阈值时，整体延迟增加。然而，这种现象很少出现在高分辨率图像 4K×4K 的情况下。这是因为过于细粒度的计算分区会导致 GPU 的理论吞吐量下降。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBeff3ad6b39db3ce27ce5019245deecbc?method=download&amp;shareKey=65604fe4530f3c7236a57f0497d163c4" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBeff3ad6b39db3ce27ce5019245deecbc?method=download&amp;shareKey=65604fe4530f3c7236a57f0497d163c4" alt="Latency of PipeFusion with various patch numbers M">
    </a><figcaption>Latency of PipeFusion with various patch numbers M</figcaption></figure></p>
<p>绝大多数差异可以忽略不计或接近零，即扩散过程中连续步骤输入之间的高度相似性。</p>
<p>有一些方法可以减轻由预热步骤引起的性能损失: 增加采样步骤，在单独的设备上执行，利用序列或张量并行。</p>
<h1 id="summary">Summary</h1>
<p>我们的方法是先用 Pipeline Parallel 将模型的 transformer block 切分成多个 stage, 再用 Tensor Parallel (Megatron: 切分前一个权重的列，后一个权重的行, Two-dimenson: 切分输入的列，切分权重的行和列)，每一层的 KV 结果需要进行 all-reduce 或者 all-gather + reduce-scatter. 不同 stage 之间是 P2P 通信.</p>
<p>PipeFusion 行为更像单纯的 Pipeline Parallel，利用上一步的 KV 完成当前步的计算，P2P 通信的是自己所处 stage 的激活 (与切分的 patch 数成反比)，与 transformer block 的层数无关。</p>
<p><a href="https://darkenstar.github.io/2024/09/27/xDiT/#Construct-Parallel-Groups">xDiT的分析中</a>提到过将并行维度从小到大可以分为 TP-SP-PP-CFG-DP，其中 CFG 和 DP 实际上是对 数据的 batchsize 维度进行切分，PP 的大小取决于划分的 patch 数，每个 stage 的 transformer block 计算的时候可以进一步再进行 SP 和 TP.</p>
<h1 id="references">References</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://darkenstar.github.io/blogs/MegatronLM/">https://darkenstar.github.io/blogs/MegatronLM/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://darkenstar.github.io/blogs/ringattention/">https://darkenstar.github.io/blogs/ringattention/</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://darkenstar.github.io/blogs/deepspeedulysses/">https://darkenstar.github.io/blogs/deepspeedulysses/</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://darkenstar.github.io/blogs/distrifusion/">https://darkenstar.github.io/blogs/distrifusion/</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Fast-dLLM</title>
      <link>http://localhost:57770/blogs/fast-dllm/</link>
      <pubDate>Thu, 12 Jun 2025 23:01:49 +0800</pubDate>
      <guid>http://localhost:57770/blogs/fast-dllm/</guid>
      <description>Paper Reading of Fast-dLLM</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Diffusion LLMs 被视为下一代文本生成技术的有力竞争者，其核心优势在于理论上可以并行生成多个 token，从而有望实现比自回归模型快几个数量级的推理速度。谷歌的 Gemini Diffusion 和 Inception Labs 的Mercury等模型已经展示了其惊人的潜力，宣称能达到每秒上千 token 的生成速度。</p>
<p>当前开源的扩散LLM (LLaDA、Dream) 在实际应用中的速度远远达不到预期，甚至比优化良好的自回归模型还要慢。这篇论文的工作，就是要拆掉阻碍扩散 LLM 起飞的两座大山。</p>
<ol>
<li>无法使用 KV Cache</li>
</ol>
<p>扩散LLM的注意力机制是双向的，即一个 token 的生成不仅依赖于它前面的内容，也依赖于它后面的内容（尽管后面可能是未知的 MASK token ）。这种特性使得过去的信息和未来的信息相互纠缠，无法像自回归模型那样简单地缓存和复用过去的信息。导致扩散LLM在每一步推理中都需要进行大量的重复计算，严重拖慢了速度。</p>
<p>Fast-dLLM 的第一个核心贡献，就是提出了一种分块近似 (block-wise approximate) KV Cache 机制。</p>
<blockquote class="quote"><p>While the bidirectional nature of attention in Diffusion LLMs precludes a fully equivalent KV Cache, our approximation closely resembles an ideal cache in practice.</p></blockquote>
<p>它将待生成的文本序列分成若干个块. 在生成某一个块 (比如Block 1) 时，它会提前计算并缓存其他所有块 (比如 Prompt 和 Block 0) 的 KV. 在这个块的内部生成过程中，这些缓存被反复利用。当这个块生成完毕后，再整体更新一次所有块的KV缓存 。</p>
<p>这个方法的近似在于，在一个块的生成过程中，缓存是固定的，而实际上随着块内 token 的不断去噪和清晰化，这些缓存理论上也应该随之微调。但论文通过可视化实验（图3）有力地证明，在相邻的推理步骤中，KV 激活值的 余弦相似度非常高，几乎接近于1. 这说明使用固定的近似缓存带来的误差微乎其微，完全可以用极小的精度损失换取巨大的速度提升。</p>
<p>论文还进一步提出了双缓存 (DualCache) 版本，不仅缓存了前面的“前缀”（prefix），还缓存了后面的“后缀”（suffix，通常是 MASK  token ） ，从而进一步压榨了计算优化的空间，实现了更快的速度。</p>
<ol start="2">
<li>并行解码带来的质量下降</li>
</ol>
<p>扩散LLM的另一大理论优势是 并行解码 (Parallel Decoding)，即一次性预测和生成多个 token  。然而，实践再次证明，当并行解码的 token 数量增多时，生成文本的质量会急剧下降 。</p>
<p>论文深刻地剖析了其根源：条件独立性假设 (conditional independence assumption) 的破坏 。在并行解码时，模型是独立地为每个待生成的 MASK 位置预测一个概率分布，然后从中采样。但实际上，一句话中的 token 之间存在着强烈的依赖关系。论文举了一个例子:</p>
<blockquote class="quote"><p>Consider an example from [30]: The list of poker hands that consist of two English words are: The subsequent two words could be, for instance, &ldquo;high card,&rdquo; &ldquo;two pair,&rdquo; &ldquo;full house,&rdquo; or &ldquo;straight flush.&rdquo; [&hellip;] However, the multi-token prediction procedure in MDMs first generates a probability distribution for each token and then samples from these distributions independently. This independent sampling can lead to undesirable combinations, such as &ldquo;high house.&rdquo;</p></blockquote>
<p>模型可能会独立地预测出 &ldquo;high&rdquo; 和 &ldquo;house&quot;这两个词，但把它们组合在一起就成了毫无意义的 high house. 这是因为模型在并行预测时忽略了 token 间的联合概率，而错误地直接使用了边缘概率的乘积。</p>
<p>为了解决这个问题，Fast-dLLM提出了第二个核心贡献：置信度感知并行解码 (Confidence-Aware Parallel Decoding) 策略 。这个想法非常直观且有效：我们只对那些模型非常有把握的 token 进行并行解码。</p>
<p>具体来说，在每一步解码时，模型会为每个待生成的 MASK 位置计算一个 置信度分数 (比如softmax概率的最大值). 然后，设定一个全局的置信度阈值 τ，只有那些置信度超过这个阈值的 token 才会被揭开，而置信度不足的 token 则继续保持 MASK 状态，留到下一步再做决策。为了避免无限循环，如果没有任何 token 的置信度达标，模型会强制解码置信度最高的那一个。</p>
<p>这个策略的精妙之处在于，它在理论上是站得住脚的。论文通过定理一从数学上证明了：当模型对一组 token 的预测置信度足够高时 (即 p&gt;1−ϵ，且 ϵ 足够小)，基于独立边缘概率的“贪心并行解码”与基于真实联合概率的“贪心串行解码”会得到完全相同的结果。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&amp;shareKey=c9e48ddb1e1f0600394ce8baa1d84426" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&amp;shareKey=c9e48ddb1e1f0600394ce8baa1d84426" alt="Effectiveness of Components of Fast-dLLM across Different Approaches">
    </a><figcaption>Effectiveness of Components of Fast-dLLM across Different Approaches</figcaption></figure></p>
<p>Fast-dLLM 的创新性体现在它是一种 training-free 的加速框架。它没有修改模型结构，也不需要重新训练，而是通过两项即插即用的推理策略——“分块近似KV缓存”和“置信度感知并行解码”，分别从减少重复计算和提升并行效率两个维度，精准地解决了当前开源扩散 LLM 面临的核心瓶颈。 实验结果在 LLaDA 和 Dream 等模型上，结合两种策略，实现了高达 27.6 倍的端到端吞吐量提升，同时在多个基准测试上几乎没有精度损失。</p>
<h1 id="2-preliminary">2. Preliminary</h1>
<h3 id="21-masked-diffusion-model">2.1. Masked Diffusion Model</h3>
<p>针对离散数据的扩散模型最早在 Argmax Flows and Multinomial Diffusion 和 Deep Unsupervised Learning using
Nonequilibrium Thermodynamics 中被探提出。随后 D3PM 提出了一个更通用的框架，通过特定的转移矩阵 $Q_{t}$ 定义了前向加噪过程的离散状态马尔可夫链，并通过最大化 ELBO 来学习反向过程的参数化模型 $p_{\theta}(x_{0}|x_{t})$. CTMC 进一步将 D3PM 扩展到连续时间，将其形式化为一个连续时间马尔可夫链 (CTMC) 框架。在另一种不同的方法中，SEDD 通过参数化似然比 $\frac{p_{t}(y)}{p_{t}(x)}$ 来学习反向过程，并采用去噪分数熵来训练该比率。</p>
<p>在各种离散扩散的噪声处理方式中，<strong>Masked Diffusion Models, MDMs</strong>，也被称为吸收状态离散扩散模型，获得了相当大的关注。MDMs 采用一种前向加噪过程，其中 token 被逐步替换为一个特殊的 MASK  token  。这个过程由以下转移概率定义：</p>
$$
q_{t|0}(x_{t}|x_{0})=\prod_{i=1}^{n}q_{t|0}(x_{t}^{i}|x_{0}^{i})=\prod_{i=1}^{n}Cat(x_{t}^{i};(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}) \quad(1)
$$<ul>
<li>$q_{t|0}(x_t|x_0)$: 表示给定原始序列 $x_0$，得到噪声序列 $x_t$ 的概率 。</li>
<li>$\prod_{i=1}^{n}$: 连乘符号，表示整个序列的噪声过程是序列中每个 token （token）独立进行噪声过程的概率乘积 。</li>
<li>$Cat(\cdot)$: 代表<strong>类别分布 (Categorical Distribution)</strong> 。</li>
<li>$t \in [0,1]$: 表示<strong>扩散时间</strong>或<strong>掩码级别</strong>。当 $t=0$ 时，序列完全是原始的；当 $t=1$ 时，序列被完全替换为 <code>[MASK]</code>  token 。</li>
<li>$(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}$: 在时间 <code>t</code>，第 <code>i</code> 个 token 有 $1-t$ 的概率保持其原始身份 $x_0^i$，有 $t$ 的概率变成 <code>[MASK]</code>  token 。<code>$\delta$</code> 是克罗内克函数，用于指定概率。</li>
</ul>
<p>最近，MDLM 和 RADD 的工作表明，对于 MDMs 不同的参数化是等价的。此外，他们证明了 MDMs 的训练目标可以被简化或直接从数据似然中推导出来 。这导出了以下目标函数，即 $log~p_{\theta}(x)$ 的一个 ELBO:</p>
$$
-log~p_{\theta}(x)\le\int_{0}^{1}\frac{1}{t}\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})]dt:=\mathcal{L}_{MDM}. \quad(2)
$$<ul>
<li>$-log~p_{\theta}(x)$: 模型的目标是最大化生成真实数据 $x$ 的对数似然，这等价于最小化它的负对数似然。这个公式给出了负对数似然的一个* ELBO.</li>
<li>$\int_{0}^{1}...dt$: 对所有可能的噪声级别 <code>t</code> (从0到1) 进行积分，意味着模型需要学会在任何噪声水平下都能很好地复原数据 。</li>
<li>$\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[...]$: 表示对所有可能的噪声样本求期望。在训练时，我们根据公式(1)随机生成一个带 <code>[MASK]</code> 的噪声序列 $x_t$.</li>
<li>$\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})$:
<ul>
<li>$\sum_{i:x_{t}^{i}=[MASK]}$: 对所有被 <code>[MASK]</code> 的位置 <code>i</code> 进行求和 。</li>
<li>$-log~p_{\theta}(x_{0}^{i}|x_{t})$: 这是交叉熵损失。它的意思是，给定带有 <code>[MASK]</code> 的序列 $x_t$，模型 $p_{\theta}$ 需要预测在位置 i 上的原始 token  $x_0^i$ 应该是什么。模型预测得越准，这个损失值就越小。</li>
</ul>
</li>
</ul>
<h3 id="22-mdms-的生成过程">2.2. MDMs 的生成过程</h3>
<p>对于公式1中定义的前向过程，其解析上的逆过程在生成时计算效率低下，因为它通常每步只修改一个 token 。一个常见的加速策略是采用 $\tau$-leaping 近似法来处理反向过程。在 MDMs 的背景下，这允许一个迭代式的生成过程，其中多个被掩码的 token 可以从一个噪声水平 t 近似地单步恢复到一个更早的水平 s &lt; t.</p>
$$
q_{s|t}(x_s|x_t)=\prod_{i=0}^{n-1}q_{s|t}(x_{s}^{i}|x_{t})
$$<p>其中</p>
$$
q_{s|t}(x_{s}^{i}|x_{t})=\begin{cases}1, & \text{if } x_{t}^{i}\ne[MASK], x_{s}^{i}=x_{t}^{i} \\ \frac{s}{t}, & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}=[MASK] \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}\ne[MASK]\end{cases} \quad(3)
$$<ul>
<li>$q_{s|t}(x_{s}^{i}|x_{t})$: 表示从 <code>t</code> 时刻的 token  $x_t^i$ 变为 <code>s</code> 时刻的 token  $x_s^i$ 的概率 。</li>
<li><strong>Case 1</strong>: 如果一个 token 在 <code>t</code> 时刻就不是 <code>[MASK]</code>，那么它在更早的 <code>s</code> 时刻也保持不变 。</li>
<li><strong>Case 2</strong>: 一个在 t 时刻是 <code>[MASK]</code> 的 token ，在更早的 s 时刻仍然是 <code>[MASK]</code>.</li>
<li><strong>Case 3</strong>: 这是关键的去噪步骤。如果一个 token 在 <code>t</code> 时刻是 <code>[MASK]</code>，模型会尝试在 s 时刻预测出一个具体的 token.
<ul>
<li>$\frac{t-s}{t}$: 代表一个在 <code>t</code> 时刻被掩码的 token，在 <code>s</code> 时刻被“揭示”出来的概率 。</li>
<li>$q_{0|t}(x_{s}^{i}|x_{t})$: 这是由神经网络模型给出的预测分布。模型会观察整个带有 <code>[MASK]</code> 的上下文 $x_t$，然后为当前位置预测一个最有可能的原始 token ，并给出一个在整个词汇表上的概率分布 。</li>
</ul>
</li>
</ul>
<p>在涉及条件数据的场景中，例如根据一个 propmt p 生成一个回应 $x_{0}$，MDM 的反向过程 (公式3所定义) 需要进行调整。具体来说，模型用于揭示一个 token  $x_{s}^{i}$ 的预测分布 $q_{0|t}(x_{s}^{i}|x_{t})$ 现在也需要以 prompt p 为条件，即 $q_{0|t}(x_{s}^{i}|x_{t},p)$ 。</p>
<p><strong>并行解码的诅咒</strong>
直接逆转公式1的前向过程来进行生成是缓慢的，通常每步只改变一个 token. 一个常见的加速策略是采用 $\tau$-leaping 近似法来处理反向过程。对于 MDMs，这意味着多个被掩码的 token 将在一个步骤中并行生成。然而，由于条件独立性假设，多 token 预测中出现了一个重大挑战。考虑一个例子：由两个英文单词组成的扑克手牌列表是：随后的两个词可能是，例如，high card，two pair，full house，或 straight flush. 值得注意的是，这两个词之间存在着关联。然而，MDMs 中的多 token 预测过程首先为每个 token 生成一个概率分布，然后独立地从这些分布中进行采样。这种独立采样可能导致不希望的组合，例如 high house.</p>
<p>为了将其形式化，考虑揭示两个 token 位置 i 和 j. 由于条件独立性假设，MDMs 从 $p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t})$ 中采样这些 token. 然而，真实的联合概率需要考虑它们之间的依赖关系：</p>
$$
p(x_{s}^{i},x_{s}^{j}|x_{t})=p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t},x_{s}^{i})
$$<p>或者对称地，通过将 i 依赖于条件 j. 这种假设的独立生成与真实的依赖性数据分布之间的差异，会降低生成序列的质量和连贯性。当在单一步骤中同时揭示大量 token 时，这个问题会变得更加严重。</p>
<h1 id="3-methodology">3. Methodology</h1>
<h2 id="31-pipeline-overview">3.1. Pipeline Overview</h2>
<p><strong>Fast-dLLM</strong>，建立在 MDM 架构之上，以实现高效和高质量的序列生成。为了加速推理，整体流水线融合了两大关键策略：通过 KV Cache 实现的高效注意力计算，以及一个由预测置信度引导的 并行解码方案。具体来说，我们采用了分块解码设计的 KV Cache，它允许在不同步骤间复用注意力激活值，并显著减少了冗余计算。在每个块内部，进一步提出了置信度感知的并行解码，它能根据置信度分数选择性地更新 token ，从而在保持输出质量的同时提高效率。通过结合这些策略，Fast-dLLM 在对生成性能影响最小的情况下，显著加快了 MDM 的推理速度。整体流程在算法 1 中进行了总结。</p>
<h2 id="32-key-value-cache-for-block-wise-decoding">3.2. Key-Value Cache for Block-Wise Decoding</h2>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&amp;shareKey=8952caa17d664bd8bcc33b9ebcec321e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&amp;shareKey=8952caa17d664bd8bcc33b9ebcec321e" alt="Illustration of our Key-Value Cache for Block-Wise Decoding">
    </a><figcaption>Illustration of our Key-Value Cache for Block-Wise Decoding</figcaption></figure></p>
<p>如上图所示，我们采用了一种分块解码的策略来支持 KV Cache 的使用。一开始计算并存储 prompt 的 KV 缓存，这个缓存将在整个块 0的解码过程中被复用。在每个块的内部，相同的缓存会被多个解码步骤复用。在完成一个块的解码之后，更新<strong>所有</strong> token  (不仅仅是新生成的 token ) 的缓存。这个缓存更新可以与解码步骤联合执行，因此与不使用缓存相比，没有额外的计算开销。由于掩码扩散模型中使用的是完全注意力机制，这种方法导致了一个近似的解码过程。</p>
<p>我们的近似 KV 缓存方法的有效性，源于我们观察到 KV 激活值在相邻的推理步骤中表现出高度的相似性，如下图所示。图 a 中红色方框区域突显了块内的相似性分数，这些分数始终接近于1。这表明在分块解码期间，前缀 (prefix) 的键和值的差异可以忽略不计，使我们能够安全地复用缓存而不会有显著的准确率损失。 此外，我们实现了一个我们 KV 缓存机制的双向版本，名为 <strong>DualCache</strong>，它不仅缓存前缀 token ，还缓存后缀 (suffix)  token ，在我们的分块解码方案中，后缀完全由掩码 token 组成。如表3所示，DualCache 带来了进一步的加速。图 b 中的红色方框区域进一步证明，在分块解码期间，后缀的键和值的差异也可以忽略不计。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&amp;shareKey=6a5005c556aaa11edb4006a48b755b4a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&amp;shareKey=6a5005c556aaa11edb4006a48b755b4a" alt="Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA">
    </a><figcaption>Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA</figcaption></figure></p>
<h2 id="33-confidence-aware-parallel-decoding">3.3. Confidence-Aware Parallel Decoding</h2>
<p>尽管存在一些方法，例如使用辅助模型来显式地捕捉不同位置 token 之间的依赖关系，但它们通常会增加整个流水线的复杂性。与这些方法相反，我们提出了一个简单而有效的<strong>置信度感知解码算法</strong>，旨在缓解这种条件独立性问题。</p>
<p>在每次迭代中，我们不是冒然地使用它们独立的边缘概率来揭示所有被掩码的 token ，而是为每个 token 计算一个置信度分数 (例如最大的 softmax 概率). 只有那些置信度超过一个阈值的 token 才会在当前步骤被揭示；其余的则保持掩码状态，并在未来的步骤中重新考虑。如果没有 token 的置信度超过阈值，就揭示置信度最高的那一个，以确保过程能够进行并防止无限循环。这个策略在加速生成的同时，减少了由不确定或模糊预测引起的错误。</p>
<p>一个关键问题是</p>
<blockquote class="quote"><p><em>When is it theoretically justifiable to decode tokens in parallel using independent marginals, despite the true joint distribution potentially containing dependencies?</em></p></blockquote>
<p>以下结果来回答了在高置信度情况下，greedy parallel 解码等同于 greedy sequential 解码的条件，并量化了两种分布之间的差异。在给出定理之前，我们将定义其表述中使用的数学符号。</p>
<p>设 $p_{\theta}(\cdot|E)$ 表示一个 MDM 在给定 E (包括 prompt $p_{0}$ 和先前生成的 token) 的条件下给出的 PMF. 假设模型要为不在 E 中的位置 $i_{1},...,i_{n}$ 预测 n 个 token.</p>
<p>令 $X=(X_{i_{1}},...,X_{i_{n}})$ 是 n 个 token 的向量，其中每个 $X_{i_{j}}$ 在词汇表 V 中取值。设 $p(X|E)\equiv p_{\theta}(X_{i_{1}},...,X_{i_{n}}|E)$ 是模型给出的联合条件 PMF。设 $p_{j}(X_{i_{j}}|E)\equiv p_{\theta}(X_{i_{j}}|E)$ 是位置 $i_{j}$ 的边缘条件 PMF。并行解码使用边缘概率的乘积来生成 token ：$q(X|E)=\tilde{\prod}_{j=1}^{n}p_{j}(X_{i_{j}}|E)$。定理1的证明及相关讨论见附录A。</p>
<p><strong>定理 1 (高置信度下的并行解码).</strong> 假设存在一个特定的 token 序列 $x^{*}=(x_{i_{1}},...,x_{i_{n}})$，使得对于每个 $j\in\{1,...,n\}$，模型对 $x_{i_{j}}$ 都有很高的置信度：$p_{j}(X_{i_{j}}=x_{i_{j}}|E)>1-\epsilon$，对于某个很小的 $\epsilon>0$. 那么，以下结论成立：</p>
<ol>
<li><strong>贪婪解码的等价性</strong>：如果 $(n+1)\epsilon\le1$（即 $\epsilon\le\frac{1}{n+1}$），那么
$$
\text{argmax}_{z} p(z|E) = \text{argmax}_{z} q(z|E) = x^{*}. \quad (4)
$$</li>
</ol>
<p>这意味着 greedy parallel 解码 (选择 argmax q) 与贪婪序贯解码 (选择 argmax p) 产生相同的结果。  这个界是紧的：如果 $\epsilon > \frac{1}{n+1}$，则存在满足高置信度边缘假设的分布 $p(X|E)$，使得 argmax $p(z|E)$ ≠ argmax $q(z|E)$。</p>
<ol start="2">
<li><em>Distance and Divergence Bounds</em>：为简洁起见，将 $p(\cdot|E)$ 和 $q(\cdot|E)$ 表示为 p 和 q。</li>
</ol>
<p><strong>$L_p$ Distance ($p \ge 1$)</strong>: 对于 $n>1$，$D_{p}(p,q)<((n-1)^{p}+2n)^{1/p}\epsilon$。特别地，对于总变差距离 ($D_{TV}(p,q)=\frac{1}{2}D_{1}(p,q)$)，$D_{TV}(p,q)<\frac{3n-1}{2}\epsilon$。</p>
<p><strong>Forward KL Divergence</strong>: 对于 $n > 1$，$D_{KL}(p||q)<(n-1)(H_{b}(\epsilon)+\epsilon~ln(|\mathcal{V}|-1))$，其中 $H_{b}(\epsilon)=-\epsilon~ln~\epsilon-(1-\epsilon)ln(1-\epsilon)$ 是二元熵函数，而 $|\mathcal{V}|$ 是词汇表的大小。</p>
<h1 id="4-experiments">4. Experiments</h1>
<hr>
<h2 id="41-experimental-setup">4.1 Experimental Setup</h2>
<ul>
<li><strong>硬件与环境</strong> 🖥️: 所有实验均在单张 <strong>NVIDIA A100 80GB GPU</strong> 上进行，batch size=1.</li>
<li><strong>评测模型</strong> 🧠: <strong>LLaDA</strong>  和 <strong>Dream</strong>.</li>
<li><strong>评测基准</strong> 📊: 采用了四个广泛使用的基准数据集：<strong>GSM8K</strong>、<strong>MATH</strong>、<strong>HumanEval</strong> 和 <strong>MBPP</strong>.</li>
<li><strong>核心指标</strong> ⏱️:
<ul>
<li><strong>准确率 (Accuracy)</strong>: 衡量模型在具体任务上的表现。</li>
<li><strong>吞吐量 (Throughput)</strong>: 以 tokens/sec 为单位，反映端到端的真实解码速度。</li>
</ul>
</li>
<li><strong>超参数</strong> ⚙️:
<ul>
<li><strong>缓存块大小</strong>: 在 4 到 32 之间进行探索。</li>
<li><strong>置信度阈值</strong>: 在 0.5 到 1.0 之间进行探索。</li>
<li>实验默认使用 <strong>PrefixCache</strong>，块大小为 <strong>32</strong>，置信度阈值为 <strong>0.9</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="42-main-results-performance-and-speed">4.2 Main Results: Performance and Speed</h2>
<p>实验结果表明，Fast-dLLM 在各种任务和设置上都取得了显著的速度提升，同时对模型准确率的影响微乎其微 。</p>
<ul>
<li>加速效果:
<ul>
<li>单独引入 KV Cache 机制，通常能带来 <strong>2x-3.6x</strong> 的速度提升。</li>
<li>当 KV Cache 和并行解码两种策略结合使用时，性能提升更为显著。在 LLaDA 模型上，最 高可达 <strong>11.0x</strong> 的吞吐量提升；在 Dream 模型上，最高可达 <strong>7.8x</strong> 的提升 。</li>
</ul>
</li>
<li>极小的精度损失: 在所有基准测试中，加速后模型的准确率与原始基线模型的差距基本保持在 <strong>1-2个百分点</strong> 以内，有时甚至略有提高。</li>
<li>对长序列更友好: 实验还发现，在处理更长的文本序列时 (例如 few-shot 场景或长代码生成)，Fast-dLLM 的加速效果更为明显。</li>
</ul>
<p>下表以 GSM8K (5-shot) 任务为例，直观展示了 Fast-dLLM (即 +Cache+Parallel) 相较于基线模型的性能提升。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">模型</th>
          <th style="text-align: left">生成长度</th>
          <th style="text-align: left">配置</th>
          <th style="text-align: left">准确率 (%)</th>
          <th style="text-align: left">吞吐量 (tok/s)</th>
          <th style="text-align: left">相对加速</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>LLaDA</strong></td>
          <td style="text-align: left">256</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">79.3</td>
          <td style="text-align: left">6.7</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>78.5</strong></td>
          <td style="text-align: left"><strong>54.4</strong></td>
          <td style="text-align: left"><strong>8.1x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">512</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">77.5</td>
          <td style="text-align: left">3.2</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>77.2</strong></td>
          <td style="text-align: left"><strong>35.3</strong></td>
          <td style="text-align: left"><strong>11.0x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Dream</strong></td>
          <td style="text-align: left">256</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">75.0</td>
          <td style="text-align: left">9.1</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>74.8</strong></td>
          <td style="text-align: left"><strong>48.2</strong></td>
          <td style="text-align: left"><strong>5.3x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">512</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">76.0</td>
          <td style="text-align: left">7.7</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>74.0</strong></td>
          <td style="text-align: left"><strong>42.9</strong></td>
          <td style="text-align: left"><strong>5.6x</strong></td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="43-ablations-and-analysis">4.3 Ablations and Analysis</h2>
<p>为了深入理解各个组件的贡献，论文进行了一系列详细的消融实验。</p>
<ul>
<li>
<p><strong>输入与生成长度的影响</strong>:</p>
<ul>
<li>实验证明，更长的上下文 (prefill，如从 5-shot 增加到 8-shot) 和更长的生成长度，都能显著放大加速效果。</li>
<li>在 8-shot 和 1024 生成长度的设置下，<strong>DualCache</strong> 实现了 <strong>27.6x</strong> 端到端加速。</li>
</ul>
</li>
<li>
<p><strong>PrefixCache vs. DualCache</strong>:</p>
<ul>
<li><strong>DualCache</strong> 通常比只缓存前缀的 <strong>PrefixCache</strong> 实现更高的加速比，尤其是在长序列生成任务中 。</li>
</ul>
</li>
<li>
<p><strong>缓存块大小的影响</strong>:</p>
<ul>
<li><strong>small block size</strong>：准确率最高，但因频繁更新缓存导致开销较大，速度提升有限 。</li>
<li><strong>small block size</strong>：速度快，但可能因上下文不匹配导致准确率下降 。</li>
<li>实验发现，块大小为 <strong>32</strong> 时在速度和精度之间取得了最佳平衡。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&amp;shareKey=1e3a007e630de1a9cbf8b3d9f318f307" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&amp;shareKey=1e3a007e630de1a9cbf8b3d9f318f307" alt="Impact of Cache Block Size on Accuracy and Throughput">
    </a><figcaption>Impact of Cache Block Size on Accuracy and Throughput</figcaption></figure></p>
<ul>
<li><strong>动态阈值 vs. 固定步数策略</strong>:
<ul>
<li>论文提出的 <strong>置信度感知并行解码</strong> 策略，在性能上持续优于每步固定解码 K 个 token 的 baseline 方法。</li>
<li>在达到相似甚至更高准确率的同时，该动态策略能实现更高的平均每步解码 token 数，从而获得更高的吞吐量。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&amp;shareKey=88d29eb3e40615a74c4846d278413e5b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&amp;shareKey=88d29eb3e40615a74c4846d278413e5b" alt="Threshold VS Fxied Step">
    </a><figcaption>Threshold VS Fxied Step</figcaption></figure></p>
<h1 id="5-related-work">5. Related Work</h1>
<p>本章节回顾了与 Fast-dLLM 相关的两个核心领域：扩散语言模型的发展，以及大语言模型的通用加速技术。</p>
<hr>
<h2 id="51-diffusion-llm">5.1. Diffusion LLM</h2>
<p>扩散模型作为一种强大的生成范式，最初在图像和音频等连续数据领域取得了巨大成功，随后其影响力扩展到了 NLP. 特别是离散扩散模型的最新进展为大语言模型提供了一种替代自回归 (AR) 范式的可行方案 。</p>
<ul>
<li>
<p><strong>理论基础的发展</strong>:</p>
<ul>
<li>离散数据的扩散模型最早由 [29, 11] 探索 。</li>
<li><strong>D3PM</strong> 提出了一个更通用的框架，将前向加噪过程建模为离散状态马尔可夫链，并通过最大 ELBO 来学习反向过程。</li>
<li><strong>CTMC</strong> 将 D3PM 扩展到连续时间设定 。</li>
<li><strong>SEDD</strong> 采用了不同的方法，通过参数化边际似然比来学习反向过程 。</li>
<li><strong>MDMs</strong> 近期受到了广泛关注，其中 <strong>MDLM</strong> 和 <strong>RADD</strong> 的研究表明，MDMs 的不同参数化方法是等价的，并且其训练目标可以被简化 。</li>
</ul>
</li>
<li>
<p><strong>与预训练语言模型的结合</strong>: 一个关键的突破是将离散扩散与现有的大语言模型架构相结合 。</p>
<ul>
<li><strong>Diffusion-NAT</strong> [40] 将离散扩散的去噪过程与 BART 的非自回归解码相结合，通过迭代式地优化被掩码的 token ，实现了比同类自回归 Transformer 快20倍的生成速度 。</li>
<li><strong>LLaDA</strong> [21]、<strong>DiffuLLaMA</strong> [7] 和 <strong>Dream</strong> [36] 等框架将扩散模型扩展到了 7B 参数的规模，通过在扩散时间步上进行递归式的 token 预测，展现了与 LLaMA3 等主流自回归模型相匹敌的性能 。</li>
</ul>
</li>
</ul>
<h2 id="52-llm-acceleration">5.2. LLM Acceleration</h2>
<ul>
<li>KV Cache</li>
</ul>
<p>由于 LLaDA 等扩散语言模型采用的是 <strong>full attention</strong>，将 KV 缓存直接应用于这类模型并非易事。 一篇相关的研究 <strong>Block diffusion</strong>  通过<strong>分块生成 (block-by-block)</strong> 的方式，克服了先前扩散语言模型的局限，使得缓存和复用先前已解码块的键和值成为可能 。</p>
<ul>
<li>Non-Autoregressive Generation</li>
</ul>
<p>非自回归 (NAR) 生成标志着一种根本性的转变，它通过同时生成多个 token 来显著加速推理过程。NAR 方法最初被用于神经机器翻译，现已扩展到语法纠错、文本摘要和对话系统等多种任务
。
尽管 NAR 在速度上优势巨大，但它通常以牺牲一定的生成质量为代价。扩散语言模型是 NAR 领域一个新兴的范式；然而，先前的工作（如 LLaDA）在实践中难以实现预期的加速，因为并行生成会导致输出质量显著下降。</p>
<h1 id="weakness">Weakness</h1>
<p>近似缓存的误差累积效应：论文证明了在相邻步骤中，KV激活值的差异很小 。但随着生成块的增多，这种“近似”带来的微小误差是否会累积，并在生成非常长的文本（如数万 token 的小说）时导致语义漂移或一致性下降？论文的最长测试序列为1024 ，对于更长的序列，其鲁棒性有待进一步验证。</p>
<p>对模型能力的依赖：“置信度感知解码”策略的有效性，隐式地依赖于模型本身具有良好的“校准度”（calibration），即模型的置信度能够真实反映其预测的正确性。如果模型本身“过于自信”或“不够自信”，可能会导致该策略效果不佳。论文没有对所用模型的校准度进行分析。
定理一的理论与实践差距：论文坦诚地指出了定理一的局限性</p>
<blockquote>
<p>In practice, while MDM may not strictly satisfy this property, its behavior typically offers a close approximation.</p></blockquote>
<p>理论证明假设了一个“理想的”联合概率分布，而真实模型是否以及在多大程度上符合这个理想假设，是一个需要进一步探究的问题。理论和实践之间的差距可能在某些刁钻的（adversarial）或分布外（Out-of-Distribution）的场景下被放大。
超参数的敏感性与调优成本：尽管论文分析了块大小和阈值的影响，但并未提供一套系统性的方法来为新模型或新任务选择最佳超参数。在实际应用中，这可能意味着需要为每个特定用例进行成本不菲的网格搜索（grid search），增加了方法的应用门槛。
评估维度的局限性：论文主要使用了基于准确率的基准测试。但在开放式生成、对话等任务中，评估指标（如流畅度、一致性、多样性）更为复杂。Fast-dLLM是否会在这些“软”指标上引入不易察觉的负面影响，需要更全面的评估。</p>
]]></content:encoded>
    </item>
    <item>
      <title>LLaDA</title>
      <link>http://localhost:57770/blogs/llada/</link>
      <pubDate>Thu, 12 Jun 2025 13:43:16 +0800</pubDate>
      <guid>http://localhost:57770/blogs/llada/</guid>
      <description>Paper Reading of LLaDA</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>LLM 主要的思想是 <em>generative modeling</em> 的思想是通过最大似然估计来优化模型的分布 $\log p_\theta(\cdot)$ 来逼近数据的分布 $\log p_{\text{data}}(\cdot)$
</p>
$$
\underbrace{\max_\theta\mathbb{E}_{p_{\text{data}}(x)}\log p_\theta(x)\Leftrightarrow\min_\theta\operatorname{KL}(p_{\text{data}}(x)||p_\theta(x)).}_{\text{Generative modeling principles}} \quad(1)
$$<p>当前，几乎所有我们熟知的大语言模型，从GPT系列到LLaMA系列，都基于<em>autoregressice modeling</em> 来实现。这种范式的核心是 <strong>next-token prediction</strong> ，即根据已经生成的文本序列，逐 toekn 地预测下一个最有可能出现的 token.</p>
$$
\underbrace{p_\theta(x)=p_\theta(x^1)\prod_{i=2}^Lp_\theta(x^i\mid x^1,\ldots,x^{i-1})}_{\text{Autoregressive formulation}} \quad(2)
$$<p>这种单向、顺序的生成方式在处理需要双向推理的任务时表现不佳，一个典型的例子就是 <strong>Reversal Curse</strong> ——模型知道 A is B，却往往无法推断出 B is A.</p>
<p>LLM 能力的核心基石是生成式建模原理本身，即通过最大似然估计让模型学习真实世界的数据分布 ，而非自回归这一具体的实现形式。</p>
<blockquote class="quote"><p><strong>It is the generative modeling principles (i.e., Eq. (1)), rather than the autoregressive formulation (i.e., Eq. (2)) itself, that fundamentally underpin the essential properties of LLMs.</strong></p></blockquote>
<ol>
<li>
<p>大语言模型的可扩展性 (scalability) ——即模型越大、数据越多、效果越好的特性——并非自回归模型所独有 。相反，这种可扩展性来源于更底层的生成式建模原理，而这些原理恰好保证了<em>fisher consistency</em><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</li>
<li>
<p><em>instruction-following</em> 和 <em>in-context learning</em><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> 并非自回归模型所独有，而是所有设计得当的条件生成模型 (conditional generative models) 在处理结构化语言任务时都应具备的内在属性 。</p>
</li>
</ol>
<p>因此作者提出了<strong>LLaDA</strong> (<strong>L</strong>arge <strong>L</strong>anguage <strong>D</strong>iffusion with m<strong>A</strong>sking)，一个从零开始训练的、参数量达到 8B 的扩散语言模型。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&amp;shareKey=94170299ede39d5102cf1cf6e397c5c7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&amp;shareKey=94170299ede39d5102cf1cf6e397c5c7" alt="Zero&amp;Few-Shot Benchmarks">
    </a><figcaption>Zero&amp;Few-Shot Benchmarks</figcaption></figure></p>
<p>LLaDA 使用了 Masked Diffusion Model (MDM)，该方法结合了离散随机掩蔽过程，并训练了一个掩码预测器来近似其反向过程。</p>
<h1 id="2-approach">2 Approach</h1>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&amp;shareKey=0293b80db53bfd7b8a9ba03f15a6f802" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&amp;shareKey=0293b80db53bfd7b8a9ba03f15a6f802" alt="A Conceptual Overview of LLaDA">
    </a><figcaption>A Conceptual Overview of LLaDA</figcaption></figure></p>
<h2 id="21-probabilistic-formulation">2.1 Probabilistic Formulation</h2>
<p>与公式(2)中的自回归模型不同，LLaDA通过<strong>前向过程 (forward process)</strong> 和 <strong>反向过程 (reverse process)</strong> 来定义模型分布 $p_{\theta}(x_{0})$。</p>
<h3 id="forward-process">Forward Process</h3>
<p>逐步地、独立地 mask $x_{0}$ 中的 token，直到在 $t=1$ 时序列被完全 mask.</p>
<p>给定 $x_{0}$ 时 $x_{t}$ 的条件分布可以被分解为：</p>
$$
q_{t|0}(x_{t}|x_{0}) = \prod_{i=1}^{L} q_{t|0}(x_{t}^{i}|x_{0}^{i})
$$<p>对于 $t \in (0,1)$，序列 $x_{t}$ 是部分被掩码的，其中每个 token 有 $t$ 的概率被mask，或有 $1-t$ 的概率保持不变。</p>
$$
q_{t|0}(x_{t}^{i}|x_{0}^{i}) = \begin{cases} 1-t, & x_{t}^{i} = x_{0}^{i} \\ t, & x_{t}^{i} = M \end{cases}
$$<p>其中 M 表示掩码 token. 直观上，每个 token 要么保持不变，要么被掩码，而被掩码的概率随着 t 从 0 到 1 线性增加。在 $t=1$ 时，所有 token 都被 mask.</p>
<h2 id="reverse-process">Reverse Process</h2>
<p>反向过程则通过在 $t=1\rightarrow 0$ 从完全被掩码的序列中生成新数据。</p>
<p>对于 $0 \le s < t \le 1$，反向过程的条件分布分解为：</p>
$$
q_{s|t}(x_{s}|x_{t}) = \prod_{i=1}^{L} q_{s|t}(x_{s}^{i}|x_{t})
$$<p>其中每个 token 的条件分布为：</p>
$$
q_{s|t}(x_{s}^{i}|x_{t}^{i}) = \begin{cases} 1, & x_{t}^{i} \ne M, x_{s}^{i} = x_{t}^{i} \\ \frac{s}{t}, & x_{t}^{i} = M, x_{s}^{i} = M \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & x_{t}^{i} = M, x_{s}^{i} \ne M \\ 0, & \text{otherwise} \end{cases}
$$<p>需要估计的关键函数是条件分布 $q_{0|t}(x_{s}^{i}|x_{t})$，它在输入 $x_{t}$ 中对应位置被掩码的情况下，预测出原始的 token. 类似于连续扩散模型中的数据预测形式。如 (Ou et al., 2024) 所证明，可以推导出一个等价但无时间依赖的参数化形式</p>
$$
q_{0|t}(x_s^i|x_t)=p_{\text{data}}(x_0^i|x_t^\text{UM}),\quad\forall i\text{ such that }x_t^i=\mathbf{M}
$$<p>其中 $x_{t}^{\text{UM}}$ 表示 $x_{t}$ 中未被掩码 token 的集合，它与原始数据 $x_{0}$ 中对应的 token 相同，因为未掩码的 token 仅由 $x_{0}$ 决定且与时间 t 无关 。直观上，这意味着估计数据预测函数等同于估计在干净数据上的条件分布，而后者是时不变的。因此，时间 t 不需要作为输入提供给参数化模型 。</p>
<p>尽管 MDM 的推导过程不简单，但其实现是直接的。我们首先引入<strong>掩码预测器</strong>，一个参数化模型 $p_{\theta}(\cdot|x_{t})$ (例如一个没有因果掩码的 Transformer)，它将任意 t 时刻的 $x_{t}$ 作为输入，并同时预测所有被 mask 的 token. 然后，我们如下定义模型分布 $p_{\theta}(x_{0})$：从一个被完全 mask 序列的 $x_{1}$ 开始，从 $t=1$ 到 0 模拟一个由 $p_{\theta}(\cdot|x_{t})$ 参数化的近似反向过程。在 $t=0$ 时刻推导出的边缘分布即代表了模型分布 $p_{\theta}(x_{0})$ 。</p>
<p>掩码预测器将 $x_{t}$ 作为输入并同时预测所有被掩码的 token (表示为 M). 它通过一个仅在被掩码 token 上计算的交叉熵损失进行训练：</p>
$$
\mathcal{L}(\theta)\triangleq-\mathbb{E}_{t,x_{0},x_{t}}[\frac{1}{t}\sum_{i=1}^{L}I[x_{t}^{i}=M]log~p_{\theta}(x_{0}^{i}|x_{t})], \quad(3)
$$<p>其中，$x_{0}$ 从训练数据中采样，$t$ 从<code>[0, 1]</code>中均匀采样<span class="sidenote-number"><small class="sidenote">Notably, LLaDA employs a masking ratio that <em>varies randomly</em> between 0 and 1 while masked language models (Devlin, 2018) use a fixed ratio.</small></span>
，$x_{t}$ 从前向过程中采样。指示函数 $I[\cdot]$ 确保损失仅针对被掩码的 token 计算。一旦训练完成，便可以模拟一个由该掩码预测器参数化的反向过程（详见2.4节），并将模型分布 $p_{\theta}(x_{0})$ 定义为该过程的边缘分布。</p>
<p>总的来说该方法通过在正向过程中逐步屏蔽令牌并在反向过程中学习恢复数据分布来训练生成模型，所有这些都在（近似）最大似然估计框架下。</p>
<h2 id="pretraining">Pretraining</h2>
<ul>
<li>
<p>LLaDA 8B 模型在一个包含 2.3T tokens 的高质量、多源数据集上从零开始进行预训练。该数据集覆盖了通用文本、代码、数学和多语言内容 。</p>
</li>
<li>
<p>训练总共消耗了 0.13M H800 GPU小 hours. 训练序列长度固定为4096. 其核心训练步骤是：对每个序列随机采样一个掩码率 t，并独立地以该概率掩码每个 token，然后让模型去预测被掩码的部分 。</p>
</li>
<li>
<p><strong>架构调整</strong> 相较于LLaMA3 8B，LLaDA 8B在架构上做了一些必要调整，如使用标准的 MHA 而非 GQA，并相应地调整了 FFN 的维度以保持模型总参数量相当 。</p>
</li>
<li>
<p><strong>优化器与学习率</strong> 训练使用了 AdamW 优化器和一个特殊的 Warmup-Stable-Decay 学习率调度策略。整个8B模型的训练实验只执行了一次，没有进行任何超参数调优。</p>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center">Our ARM Baseline 1B</th>
          <th style="text-align: center">LLaDA IB</th>
          <th style="text-align: center">Our ARM Baseline 7B</th>
          <th style="text-align: center">LLaDA 8B</th>
          <th style="text-align: center">LLaMA3 8B</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>Layers</strong></td>
          <td style="text-align: center">22</td>
          <td style="text-align: center">22</td>
          <td style="text-align: center">28</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Model dimension</strong></td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">4096</td>
          <td style="text-align: center">4096</td>
          <td style="text-align: center">4096</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Attention heads</strong></td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Vocabulary size</strong></td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126.464</td>
          <td style="text-align: center">128,000</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>FFN dimension</strong></td>
          <td style="text-align: center">5634</td>
          <td style="text-align: center">5634</td>
          <td style="text-align: center">13.440</td>
          <td style="text-align: center">12,288</td>
          <td style="text-align: center">14,336</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Key/Value heads</strong></td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">8</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">8</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Total parameters</strong></td>
          <td style="text-align: center">1.49 B</td>
          <td style="text-align: center">1.49 B</td>
          <td style="text-align: center">6.83 B</td>
          <td style="text-align: center">8.02 B</td>
          <td style="text-align: center">8.03 B</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Non-embedding parameters</strong></td>
          <td style="text-align: center">0.97 B</td>
          <td style="text-align: center">0.97 B</td>
          <td style="text-align: center">5.80 B</td>
          <td style="text-align: center">6.98 B</td>
          <td style="text-align: center">6.98 B</td>
      </tr>
  </tbody>
</table>
<h2 id="supervised-fine-tuning">Supervised Fine-Tuning</h2>
<p>我们通过使用配对数据 $(p_{0}, r_{0})$ 进行监督微调 (SFT)来增强LLaDA遵循指令的能力，其中 $p_{0}$ 是 prompt，$r_{0}$ 表示响应(response). 这是针对LLM最简单、最基础的 post-training 方法。从技术上讲，这要求模型对条件分布 $p_{\theta}(r_{0}|p_{0})$ 进行建模，而非预训练中的 $p_{\theta}(x_{0})$。</p>
<p>其实现方式与预训练类似。如图2(b)所示，保持 prompt 部分不变，并像处理 $x_{0}$ 一样，独立地 mask response 中的 token. 然后，将提示和被掩码的响应 $r_{t}$ 一同送入预训练好的掩码预测器，以计算用于 SFT 的损失</p>
$$
-\mathbb{E}_{t,p_{0},r_{0},r_{t}}[\frac{1}{t}\sum_{i=1}^{L^{\prime}}I[r_{t}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{t})] \quad (5)
$$<p>其中，$L^{\prime}$ 表示稍后指定的动态长度。这种方法与预训练是完全兼容的。本质上，将 $p_{0}$ 和 $r_{0}$ 拼接起来可以被视为干净的预训练数据 $x_{0} $，而将 $p_{0}$ 和 $r_{t}$ 拼接起来则可作为其被掩码后的版本 $x_{t}$. 这个过程与预训练完全相同，唯一的区别在于所有被掩码的 token 恰好都出现在 $r_{0}$ 部分。</p>
<p>LLaDA 8B 模型在一个包含 4.5M 对样本的数据集上进行了 SFT. 与预训练过程一致，数据准备和训练都遵循了现有LLM (Chu et al., 2024; Yang et al., 2024) 中使用的 SFT 协议，没有引入任何额外的技术来优化 LLaDA 的性能。该数据集涵盖了多个领域，包括代码、数学、指令遵循和结构化数据理解。我们在每个 mini-batch 中的短样本对末尾附加 EOS token，以确保所有数据长度相等。在训练期间将 EOS视为一个普通 token ，并在采样时将其移除，使得LLaDA能够自动控制响应的长度。</p>
<p>我们在SFT数据上训练了 3 个 epoch，其调度策略与预训练阶段相似。学习率在最初 50 次迭代中从 0 线性增加到 $2.5 \times 10^{-5}$，然后保持不变。在最后 10% 的迭代中，学习率性降低到 $2.5 \times 10^{-6}$. 此外，我们将权重衰减设置为 0.1，全局 batch size 设置为 256，每个 GPU 的本地 batch size 设置为 2. SFT实验只执行了一次，没有进行任何超参数调优。</p>
<h2 id="inference">Inference</h2>
<p>作为一个生成式模型，LLaDA既能 <strong>采样 (sampling)</strong> 新文本，也能 <strong>评估 (evaluating)</strong> 候选文本的似然。</p>
<p>我们先从采样说起。如图 2(c) 所示，给定一个 prompt $p_{0}$，我们通过离散化反向过程来从模型分布 $p_{\theta}(r_{0}|p_{0})$ 中进行采样，这个过程从一个被完全掩码的 response 开始。总的采样步数是一个超参数，为 LLaDA 提供了一个在效率和样本质量之间的权衡（详见3.3节分析）。我们默认使用均匀分布的时间步。此外，生成长度也被视为一个超参数，它指定了采样过程开始时完全被掩码句子的长度。如附录B.4所述，由于预训练和SFT都是在可变长度的数据集上进行的，最终结果对这个长度超参数不敏感。</p>
<p>在一个从时间 $t \in (0, 1]$ 到 $s \in [0, t)$的中间步骤中，我们将 $p_{0}$ 和 $r_{t}$ 同时送入掩码预测器，并一次性预测所有被掩码的 token. 随后 <em>remask</em> $\frac{s}{t}$ 比例的已预测 token. 得到$r_{s}$，从而确保反向过程的转换与前向过程保持一致，以实现准确采样。</p>
<p>受 LLM 采样中退火技巧的启发，我们探索了两种确定性但有效的重掩码策略。</p>
<ul>
<li><strong>low-confidence remasking</strong>: remask 那些基于预测置信度最低的 $\frac{s}{t}$ 比例的 token.</li>
<li><strong>semi-autoregressive remasking</strong>: 对于经过 SFT 的 LLaDA 模型，将序列分成几个块，并从左到右地生成. 在每个块内部，采用反向过程进行采样。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&amp;shareKey=838350c5b31c7e78112324263cdf5621" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&amp;shareKey=838350c5b31c7e78112324263cdf5621" alt="A Conceptual Overview of the Semi-autoregressive Sampling">
    </a><figcaption>A Conceptual Overview of the Semi-autoregressive Sampling</figcaption></figure></p>
<p>对于条件似然评估，我们自然可以利用公式(5)中的上界。然而，我们发现下面这个等价形式（公式6）表现出更低的方差，在评估时更为稳定：</p>
$$
-\mathbb{E}_{l,r_{0},r_{l}}[\frac{L}{l}\sum_{i=1}^{L}I[r_{l}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{l})] \quad(6)
$$<p>其中，$l$ 从 ${1, 2, ..., L}$ 中均匀采样，$r_{l}$ 是通过从 $r_{0}$ 中不放回地均匀采样 $l$ 个没被 mask 的 token 得到的。此外，我们还采用了 unsupervised classifier-free guidance.</p>
<p>虽然这两个形式的期望值相同，但它们的方差不同。直观上，在公式 (5) 中，我们期望 $x_{t}=[p_0,r_t]$ 有 $t$ 比例的 token 被掩码。然而，前向过程的随机性常常会导致偏差，尤其当 $x_{t}$ 包含的 token 很少时。相比之下，在公式 (6) 中，$r_{l}$ 中被掩码 token 的比例 $\frac{l}{L}$ 是确定的。</p>
<p>虽然理论分析取决于数据分布，但经验结果表明，公式 (5) 需要超过 1000 次蒙特卡洛估计才能得到稳定结果，而公式 (6) 仅需 128 次估计即可达到稳定。</p>
<p>Any-order autoregressive models (AO-ARM)  通过对 L 个变量所有可能的排列顺序进行自回归来描述联合分布。为了学习这样的分布，AO-ARM 利用一个权重共享的神经网络来为所有单变量条件概率建模，并使用掩码 token 来表示缺失的变量。在训练期间，模型会最小化在所有顺序的均匀分布 $U_{\pi}$ 上的期望负对数似然：</p>
$$
-\mathbb{E}_{x_{0},\pi \sim U_{\pi}}[\sum_{i=1}^{L}log~p_{\theta}(x_{0}^{\pi(i)}|x_{0}^{\pi(<i)}; \pi)]
$$<p> (15)</p>
<p>直观上，$x_{0}^{\pi(<i)}$ 可以被理解为一个被掩码的 token 序列 $x_{t}$，其中索引在 $\pi(\ge i)$ 的 token 被掩码 。可以进一步证明，公式 (15) 等价于公式 (12) 。这种联系解释了 LLaDA 的双向推理能力，即使它在推理过程中从未被显式使用 。</p>
<p>Nie et al. (2024) 引入了无监督的无分类器指导，这是一种即插即用的技术，可以平衡与提示的对齐度和文本多样性 。具体来说，无监督的无分类器指导在推理时采用以下修改过的掩码预测器 ：</p>
$$
\tilde{p}_{\theta}(r_{0}|p_{0},r_{t}) \propto \frac{p_{\theta}(r_{0}|p_{0},r_{t})^{1+w}}{p_{\theta}(r_{0}|m,r_{t})^{w}}
$$<p> (16)</p>
<p>其中，$m$ 是一个与 $p_{0}$ 长度相同的掩码序列，$w$ 是一个控制 $p_{0}$ 强度的超参数 。我们在下游任务中采用了无监督的无分类器指导，详见附录 B.5 。</p>
<h1 id="3-experiment">3 Experiment</h1>
<p>实验主要围绕以下三个核心方面展开：</p>
<ol>
<li>
<p>可扩展性 (Scalability)：研究 LLaDA 的性能是否随着计算资源和模型规模的增加而稳定提升。通过与自建的自回归模型 (ARM) 基线在相同数据上进行对比，结果显示 LLaDA 表现出强大的可扩展性，其性能增长趋势与 ARM 相当，甚至在 MMLU 和 GSM8K 等任务上更具优势。</p>
</li>
<li>
<p>基准测试结果 (Benchmark Results)：将 8B 规模的 LLaDA 与 LLaMA3 8B、LLaMA2 7B 等主流模型在涵盖通用，数学，代码和中文四大类的 15 个标准基准上进行对比。</p>
<ul>
<li>
<p>预训练模型：LLaDA 8B Base 模型的性能全面超越 LLaMA2 7B，并与 LLaMA3 8B 整体上具有竞争力，尤其在数学和中文任务上表现突出。</p>
</li>
<li>
<p>微调模型：仅经过 SFT 的 LLaDA 8B Instruct 模型，在未进行强化学习对齐的情况下，其性能在多数任务上得到提升 ，并展现出令人印象深刻的 Instruction Follow 能力。</p>
</li>
</ul>
</li>
<li>
<p>反向推理 (Reversal Reasoning)：为了量化模型克服“反转诅咒”的能力，实验在一个中文古诗补全任务上进行了测试。结果表明，LLaDA 在正向和反向任务上表现均衡，一致性强，而 GPT-4o 等模型则在反向任务上表现出显著的性能下降。</p>
</li>
</ol>
<h1 id="reference">Reference</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>简单来说就是拥有无限数据、一个足够大的网络和最优训练的理想条件下，模型有能力恢复出真实的数据分布。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>在不更新其自身参数的情况下，仅通过在 Prompt 中提供少量示例 (few-shot) 或任务描述 (zero-shot)，就能当场学会并执行一个新任务的能力。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Tx8read</title>
      <link>http://localhost:57770/blogs/tx8read/</link>
      <pubDate>Wed, 11 Jun 2025 10:21:42 +0800</pubDate>
      <guid>http://localhost:57770/blogs/tx8read/</guid>
      <description>tx8 regression</description>
      <content:encoded><![CDATA[<h1 id="testgraphcompute">TestGraphCompute</h1>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">**</span><span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 1. 初始化与命令行参数处理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">Timer</span> <span class="n">timer</span><span class="p">(</span><span class="s">&#34;TestGraphCompute&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">registerAsmPrinterCLOptions</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">registerMLIRContextCLOptions</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">registerPassManagerCLOptions</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 解析命令行参数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cl</span><span class="o">::</span><span class="n">ParseCommandLineOptions</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">,</span> <span class="s">&#34;tx8be compiler</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 2. 初始化 MLIR 模块和上下文
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mlir</span><span class="o">::</span><span class="n">OwningOpRef</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ModuleOp</span><span class="o">&gt;</span> <span class="n">module</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="n">context</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 定义一个正则表达式，用于从命令行选项中提取 codegen_path 参数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">regex</span> <span class="n">pattern</span><span class="p">(</span><span class="s">&#34;codegen_path=([a-zA-Z0-9_]+)&#34;</span><span class="p">);</span>  
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">smatch</span> <span class="n">matches</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">cachePath</span> <span class="o">=</span> <span class="s">&#34;codegen&#34;</span><span class="p">;</span>  <span class="c1">// 默认文件夹名字
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">regex_search</span><span class="p">(</span><span class="n">optionstr</span><span class="p">,</span> <span class="n">matches</span><span class="p">,</span> <span class="n">pattern</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// 寻找命令行选项中是否指定 codegen_path
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">cachePath</span> <span class="o">=</span> <span class="n">matches</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 3. 加载 MLIR 模块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 如果 cache 为 2 或 4，则从缓存路径加载模块；否则，使用默认的 gModelFil
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">gModelFile</span> <span class="o">=</span> <span class="p">(</span><span class="n">cache</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">||</span> <span class="n">cache</span> <span class="o">==</span> <span class="mi">4</span><span class="p">)</span> <span class="o">?</span> <span class="n">cachePath</span> <span class="o">+</span> <span class="s">&#34;/cache.mlir&#34;</span> <span class="o">:</span> <span class="n">gModelFile</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="kt">int</span> <span class="n">error</span> <span class="o">=</span> <span class="n">getMLIRFromFile</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">gModelFile</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">error</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 4. 配置模块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">mconfig</span> <span class="o">=</span> <span class="n">getModuleConfig</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">optionstr</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">mconfig</span><span class="p">.</span><span class="n">option</span> <span class="o">+=</span> <span class="n">optionstr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">mconfig</span><span class="p">.</span><span class="n">constCache</span> <span class="o">=</span> <span class="n">cache</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">updateModuleConfig</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">mconfig</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">mconfig</span> <span class="o">=</span> <span class="n">getModuleConfig</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">showModuleConfig</span><span class="p">(</span><span class="n">mconfig</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 5. 处理多卡信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">json_info_multi_card_t</span> <span class="o">*</span><span class="n">multi_card_jinfo</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">multi_card_jinfo</span> <span class="o">=</span> <span class="p">(</span><span class="n">cache</span> <span class="o">==</span> <span class="mi">2</span> <span class="o">||</span> <span class="n">cache</span> <span class="o">==</span> <span class="mi">4</span><span class="p">)</span> <span class="o">?</span> <span class="n">get_multi_card_info_from_file</span><span class="p">(</span><span class="n">cachePath</span> <span class="o">+</span> <span class="s">&#34;/model_info.json&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                                <span class="o">:</span> <span class="n">parseMultiCardModuleInfo</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">dumpIR</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 6. 读取参考路径
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">fast_codegen</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// NOT fast_codegen
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">in_files</span> <span class="o">=</span> <span class="n">parseStringArgs</span><span class="p">(</span><span class="n">gInputBin</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&#34;,&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">out_files</span> <span class="o">=</span> <span class="n">parseStringArgs</span><span class="p">(</span><span class="n">gOutputBin</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&#34;,&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 定义一个 lambda 函数，用于从文件中读取参考文件路径。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">getRefFiles</span> <span class="o">=</span> <span class="p">[]</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">gFile</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">files</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">gFile</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">llvm</span><span class="o">::</span><span class="n">sys</span><span class="o">::</span><span class="n">fs</span><span class="o">::</span><span class="n">exists</span><span class="p">(</span><span class="n">gFile</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">ifstream</span> <span class="n">gf</span><span class="p">(</span><span class="n">gFile</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">text</span><span class="p">((</span><span class="n">std</span><span class="o">::</span><span class="n">istreambuf_iterator</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">(</span><span class="n">gf</span><span class="p">)),</span> <span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">istreambuf_iterator</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">()));</span>
</span></span><span class="line"><span class="cl">            <span class="n">files</span> <span class="o">=</span> <span class="n">parseStringArgs</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&#34;,&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">};</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">((</span><span class="n">gInputBin</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">gInputFile</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">getRefFiles</span><span class="p">(</span><span class="n">gInputFile</span><span class="p">,</span> <span class="n">in_files</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="n">getRefFiles</span><span class="p">(</span><span class="n">gOutputFile</span><span class="p">,</span> <span class="n">out_files</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// 7. computeGolden
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">cache</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">mconfig</span><span class="p">.</span><span class="n">tile</span><span class="p">.</span><span class="n">chip_num</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// 遍历芯片数量，创建对应的目录结构
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 构造并创建创建目 codegen/node_0_0/chip0/agent/data 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">path</span> <span class="o">=</span> <span class="s">&#34;codegen/node_0_0/chip&#34;</span><span class="p">;</span>  
</span></span><span class="line"><span class="cl">        <span class="n">path</span> <span class="o">+=</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;/agent/data&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">createDir</span><span class="p">(</span><span class="n">path</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 调用 computeGolden 函数，计算参考输出保存到 codegenPath
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">computeGolden</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">multi_card_jinfo</span><span class="p">,</span> <span class="n">in_files</span><span class="p">,</span> <span class="n">out_files</span><span class="p">,</span> <span class="n">mconfig</span><span class="p">.</span><span class="n">codegenPath</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 调用 moduleCompileCodegen 函数，对 MLIR 模块进行编译和代码生成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">moduleCompileCodegen</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">ret</span> <span class="o">==</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// 9. 获取内存大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 从模块中获取立即数 (Immediate) 和常量参数的 DDR 大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint64_t</span> <span class="n">imm_size</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ImmDdrSize</span><span class="p">)</span> <span class="o">?</span>
</span></span><span class="line"><span class="cl">    <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ImmDdrSize</span><span class="p">).</span><span class="n">getInt</span><span class="p">()</span> <span class="o">:</span> <span class="mi">2147483648</span><span class="p">;</span>  
</span></span><span class="line"><span class="cl">    <span class="kt">uint64_t</span> <span class="n">params_size</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ConstDdrSize</span><span class="p">)</span> <span class="o">?</span>
</span></span><span class="line"><span class="cl">        <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ConstDdrSize</span><span class="p">).</span><span class="n">getInt</span><span class="p">()</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1">// 10. 更新每个芯片的内存大小信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">mconfig</span><span class="p">.</span><span class="n">tile</span><span class="p">.</span><span class="n">chip_num</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_infos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">imm_size</span> <span class="o">=</span> <span class="n">imm_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_infos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">params_size</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">params_size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 11. 保存多卡模型信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">chipIds</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ChipIds</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// 获取芯片 ID 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">mlir</span><span class="o">::</span><span class="n">ArrayAttr</span> <span class="n">chipIdsAttr</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ArrayAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">ChipIds</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">chipIdsAttr</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">chipIds</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">chipIdsAttr</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getInt</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 多卡模型文件保存到 codegenPath 路径下
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">saveMultiCardModelJson</span><span class="p">(</span><span class="n">multi_card_jinfo</span><span class="p">,</span> <span class="n">mconfig</span><span class="p">.</span><span class="n">codegenPath</span><span class="p">,</span> <span class="n">chipIds</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// uint64_t ddrSize = getModelDDRSize(multi_card_jinfo);
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="computegolden">computeGolden</h2>
<p>输入参数的来源：</p>
<ul>
<li><code>module</code>：在 main 文件中，通过 <code>getMLIRFromFile</code> 函数从文件中加载 MLIR 模块</li>
<li><code>multi_card_jinfo</code>：在 main 文件中，通过 <code>get_multi_card_info_from_file</code> 或 <code>parseMultiCardModuleInfo</code> 从 JSON 文件或 MLIR 模块中提取多卡信息。</li>
<li><code>in_files</code> 和 <code>out_files</code>：在 main 文件中，通过 <code>parseStringArgs</code> 或 <code>getRefFiles</code> 解析输入和输出文件路径。</li>
<li><code>mconfig.codegenPath</code>：在 main 文件中，通过命令行选项或默认值设置代码生成路径，并传递给 computeGolden。</li>
</ul>
<p>computeGolden 函数生成的数据（输入和输出的二进制文件）将保存到指定路径 mconfig.codegenPath.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">computeGolden</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">OwningOpRef</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ModuleOp</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">module</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">json_info_multi_card_t</span> <span class="o">*</span><span class="n">multi_card_jinfo</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">inFiles</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="o">&amp;</span><span class="n">outFiles</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">file_path</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 定义形状类型，用于存储多维张量的形状信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">using</span> <span class="n">ShapeType</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 用于存储多芯片的输入和输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span> <span class="o">*&gt;&gt;</span> <span class="n">multiInputDdata</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span> <span class="o">*&gt;&gt;</span> <span class="n">multiOutputData</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="o">&gt;</span> <span class="n">threads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">chip_num</span> <span class="o">=</span> <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_num</span><span class="p">;</span>  <span class="c1">// 获取芯片数量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">tile_info</span> <span class="o">=</span> <span class="n">get_tileinfo</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>  <span class="c1">// 从 MLIR 模块中提取 tile 信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ShapeType</span><span class="o">&gt;</span> <span class="n">outShapes</span><span class="p">(</span><span class="n">chip_num</span><span class="p">);</span>  <span class="c1">// 存储每个芯片的输出形状信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">chip_num</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">chip_info_t</span> <span class="o">*</span><span class="n">chip_info</span> <span class="o">=</span> <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_infos</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 分配当前芯片的输入和输出数据指针数组
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span> <span class="o">*&gt;</span> <span class="n">input_data</span><span class="p">(</span><span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input_num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int8_t</span> <span class="o">*&gt;</span> <span class="n">output_data</span><span class="p">(</span><span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output_num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 用于 OneDNN 计算的输入和输出缓冲区
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span> <span class="o">*&gt;</span> <span class="n">computeInputs</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span> <span class="o">*&gt;</span> <span class="n">computeOutputs</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 当前芯片的输入和输出文件路径
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">chipInFiles</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">chipOutFiles</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">parseInOutfile</span><span class="p">(</span><span class="n">inFiles</span><span class="p">,</span> <span class="n">chipInFiles</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">chip_num</span><span class="p">,</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input_num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">parseInOutfile</span><span class="p">(</span><span class="n">outFiles</span><span class="p">,</span> <span class="n">chipOutFiles</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">chip_num</span><span class="p">,</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output_num</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 生成当前芯片的输入输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">genInputs4SingleChip</span><span class="p">(</span><span class="n">computeInputs</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">chip_info</span><span class="p">,</span> <span class="n">chipInFiles</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">genOutputs4SingleChip</span><span class="p">(</span><span class="n">computeOutputs</span><span class="p">,</span> <span class="n">output_data</span><span class="p">,</span> <span class="n">chip_info</span><span class="p">,</span> <span class="n">chipOutFiles</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 如果输入文件为空，则生成随机输入数据并校正
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">chipInFiles</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">updateSpecialInputData</span><span class="p">(</span><span class="n">computeModuleRef</span><span class="p">,</span> <span class="n">computeInputs</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">chip_info</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">multiInputDdata</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">input_data</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">multiOutputData</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">output_data</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">outFiles</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">threads</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">moduleComputeInterface</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">ref</span><span class="p">(</span><span class="n">computeModuleRef</span><span class="p">),</span> <span class="n">std</span><span class="o">::</span><span class="n">ref</span><span class="p">(</span><span class="n">outShapes</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">computeInputs</span><span class="p">,</span> <span class="n">computeOutputs</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 等待所有线程完成计算
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="kr">thread</span> <span class="o">:</span> <span class="n">threads</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kr">thread</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 遍历每个芯片，保存输入和输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">chip_num</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">chip_info_t</span> <span class="o">*</span><span class="n">chip_info</span> <span class="o">=</span> <span class="n">multi_card_jinfo</span><span class="o">-&gt;</span><span class="n">chip_infos</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kt">uint32_t</span> <span class="n">node_id</span> <span class="o">=</span> <span class="n">get_node_id</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tile_info</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int32_t</span> <span class="n">relative_chip_id</span> <span class="o">=</span> <span class="n">get_relative_chip_id</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">tile_info</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 构造当前芯片的数据保存路径  file_path/node_x_y/chip_z/agent/data
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">data_path</span> <span class="o">=</span> <span class="n">file_path</span> <span class="o">+</span> <span class="s">&#34;/node_&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">node_id</span> <span class="o">/</span> <span class="n">tile_info</span><span class="p">.</span><span class="n">node_y</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;_&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">node_id</span> <span class="o">%</span> <span class="n">tile_info</span><span class="p">.</span><span class="n">node_y</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;/chip&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">relative_chip_id</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;/agent/data&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">createDir</span><span class="p">(</span><span class="n">data_path</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// bin格式保存当前芯片的输入数据  
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input_num</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">inout_tensor_info_t</span> <span class="o">*</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="n">saveInOutTensor</span><span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dim</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">layout</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">data_path</span> <span class="o">+</span> <span class="s">&#34;/in&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;.bin&#34;</span><span class="p">,</span> <span class="n">multiInputDdata</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// bin格式保存当前芯片的输出数据  out_j_ref.bin
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output_num</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">inout_tensor_info_t</span> <span class="o">*</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int32_t</span> <span class="n">tensorShape</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">// 根据 outShapes 或原始形状计算输出张量的形状
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dim</span><span class="p">;</span> <span class="o">++</span><span class="n">idx</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">outShapes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tensorShape</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">outShapes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tensorShape</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">shape</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">// 保存输出数据
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">saveInOutTensor</span><span class="p">(</span><span class="n">tensorShape</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dim</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">layout</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="n">data_path</span> <span class="o">+</span> <span class="s">&#34;/out&#34;</span> <span class="o">+</span> <span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span> <span class="s">&#34;_ref.bin&#34;</span><span class="p">,</span> <span class="n">multiOutputData</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 释放当前芯片的输入和输出数据内存
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">input_num</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">free</span><span class="p">(</span><span class="n">multiInputDdata</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">uint32_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">chip_info</span><span class="o">-&gt;</span><span class="n">output_num</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">free</span><span class="p">(</span><span class="n">multiOutputData</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="run_code_gen_layer">run_code_gen_layer</h1>
<p>主要用于运行代码生成 (codegen) 相关的任务，以下是函数的详细功能解释：</p>
<ol>
<li>解析参数：</li>
</ol>
<ul>
<li>接受至少两个参数：$1 是可执行文件的名称，$2 是种子文件 (seed file)</li>
<li>如果有更多参数 ($# &gt; 2)，则将额外参数存储为配置参数 (config_params)</li>
<li>从 config_params 中提取 <code>codegen_path</code> (代码生成输出路径) ，如果未指定则使用默认值 &ldquo;codegen&rdquo;</li>
</ul>
<ol start="2">
<li>切换工作目录：</li>
</ol>
<ul>
<li>切换到 <code>${BEMLIR_PROJECT_ROOT}/build/bin</code> 目录。</li>
<li>删除旧的 <code>codegen_path</code> 目录，确保环境干净。</li>
</ul>
<ol start="3">
<li>执行可执行文件：</li>
</ol>
<ul>
<li>使用 <code>${layer_cmd}</code> (即 ./$1) 运行指定的可执行文件，传入种子文件和配置参数。</li>
<li>检查返回值，如果失败 <code>(ret != 0)</code>，则恢复目录并返回错误。</li>
</ul>
<ol start="4">
<li>处理生成的代码：</li>
</ol>
<ul>
<li>根据参数中的 <code>chip_num</code> 或 <code>static_shape</code> 判断 <code>host_type</code>.</li>
<li>调用 <code>get_codegen_file</code> 处理生成的代码文件。</li>
</ul>
<ol start="5">
<li>运行 cmodel 测试:</li>
</ol>
<ul>
<li>根据参数中的 <code>fast_codegen</code> 或 <code>not_run</code> 设置 cmp_flag.</li>
<li>调用 <code>run_on_cmodel</code> 在 cmodel 上运行生成的代码。</li>
<li>检查返回值，失败则返回错误。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 定义 run_codegen_layer 函数，用于运行代码生成层测试流程</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> run_codegen_layer<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1. 打印开始时间，用于调试和性能追踪</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> -n <span class="s2">&#34;time==&gt;&gt;run_codegen_layer-start   &#34;</span><span class="p">;</span> date<span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 2. 函数参数说明</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># $1: 可执行文件名称</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># $2: 种子文件 (seed file）</span>
</span></span><span class="line"><span class="cl">    <span class="nv">layer_cmd</span><span class="o">=</span><span class="s2">&#34;./</span><span class="nv">$1</span><span class="s2">&#34;</span>  <span class="c1"># 在当前目录下执行的可执行文件路径</span>
</span></span><span class="line"><span class="cl">    <span class="nv">seed_file</span><span class="o">=</span><span class="nv">$2</span>      <span class="c1"># 种子文件或配置文件</span>
</span></span><span class="line"><span class="cl">    <span class="nv">config_params</span><span class="o">=</span><span class="s2">&#34;&#34;</span>  <span class="c1"># 配置参数，默认为空</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 默认代码生成输出路径</span>
</span></span><span class="line"><span class="cl">    <span class="nv">codegen_path</span><span class="o">=</span><span class="s2">&#34;codegen&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 3. 检查是否有超过2个参数</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> <span class="nv">$#</span> -gt <span class="m">2</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 提取除前两个参数外的所有参数作为配置参数</span>
</span></span><span class="line"><span class="cl">        <span class="nv">config_params</span><span class="o">=</span><span class="nv">$*</span>
</span></span><span class="line"><span class="cl">        <span class="nv">config_params</span><span class="o">=</span><span class="si">${</span><span class="nv">config_params</span><span class="p">#*</span><span class="si">}</span>  <span class="c1"># 移除第一个参数 (可执行文件）</span>
</span></span><span class="line"><span class="cl">        <span class="nv">config_params</span><span class="o">=</span><span class="si">${</span><span class="nv">config_params</span><span class="p">#*</span><span class="si">}</span>  <span class="c1"># 移除第二个参数 (种子文件）</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 4. 如果配置参数中包含 --codegen_path，提取其值作为代码生成路径</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">config_params</span><span class="si">}</span> <span class="o">==</span> *<span class="s2">&#34;--codegen_path=&#34;</span>* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 提取 --codegen_path= 后面的值</span>
</span></span><span class="line"><span class="cl">        <span class="nv">codegen_path</span><span class="o">=</span><span class="si">${</span><span class="nv">config_params</span><span class="p">#*codegen_path=</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 移除可能存在的引号或其他字符</span>
</span></span><span class="line"><span class="cl">        <span class="nv">codegen_path</span><span class="o">=</span><span class="si">${</span><span class="nv">codegen_path</span><span class="p">%%</span><span class="se">\&#34;</span><span class="p">*</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="nv">codegen_path</span><span class="o">=</span><span class="si">${</span><span class="nv">codegen_path</span><span class="p">-*</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="nv">codegen_path</span><span class="o">=</span><span class="si">${</span><span class="nv">codegen_path</span><span class="p">*</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 5. 切换到 build/bin 目录执行命令</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span>/build/bin
</span></span><span class="line"><span class="cl">        <span class="c1"># 删除旧的 codegen_path 目录，确保环境干净</span>
</span></span><span class="line"><span class="cl">        rm -rf <span class="si">${</span><span class="nv">codegen_path</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 执行层命令，传入种子文件和配置参数</span>
</span></span><span class="line"><span class="cl">        <span class="si">${</span><span class="nv">layer_cmd</span><span class="si">}</span> <span class="si">${</span><span class="nv">seed_file</span><span class="si">}</span> <span class="si">${</span><span class="nv">config_params</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 捕获命令的返回值</span>
</span></span><span class="line"><span class="cl">        <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果命令执行失败 (返回码非0），恢复目录并返回错误</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">            <span class="nb">echo</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">fi</span>
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>  <span class="c1"># 恢复原始目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 6. 打印代码生成完成的时间</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> -n <span class="s2">&#34;time==&gt;&gt;run_codegen_layer-codegen=== &#34;</span><span class="p">;</span> date<span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 7. 根据参数判断主机类型</span>
</span></span><span class="line"><span class="cl">    <span class="nv">host_type</span><span class="o">=</span><span class="m">0</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果参数中包含 chip_num 或 static_shape，则将 host_type 设为 1</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$*</span> <span class="o">==</span> *<span class="s2">&#34;chip_num&#34;</span>* <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="nv">$*</span> <span class="o">==</span> *<span class="s2">&#34;static_shape&#34;</span>* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nv">host_type</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 8. 调用 get_codegen_file 处理生成的代码文件</span>
</span></span><span class="line"><span class="cl">    get_codegen_file <span class="si">${</span><span class="nv">codegen_path</span><span class="si">}</span> <span class="si">${</span><span class="nv">host_type</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 捕获返回值</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果处理失败，恢复目录并返回错误</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 9. 初始化比较标志</span>
</span></span><span class="line"><span class="cl">    <span class="nv">cmp_flag</span><span class="o">=</span><span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果参数中包含 fast_codegen 或 not_run，则设置 cmp_flag 为 &#34;not_run&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="nv">$*</span> <span class="o">==</span> *<span class="s2">&#34;fast_codegen&#34;</span>* <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="nv">$*</span> <span class="o">==</span> *<span class="s2">&#34;not_run&#34;</span>* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nv">cmp_flag</span><span class="o">=</span><span class="s2">&#34;not_run&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 10. 在 cmodel 上运行生成的代码，传入比较标志</span>
</span></span><span class="line"><span class="cl">    run_on_cmodel <span class="si">${</span><span class="nv">codegen_path</span><span class="si">}</span> <span class="si">${</span><span class="nv">cmp_flag</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 捕获返回值</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果运行失败，恢复目录并返回错误</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[[</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="si">${</span><span class="nv">ret</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 11. 打印结束时间</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> -n <span class="s2">&#34;time==&gt;&gt;run codegen layer-end===   &#34;</span><span class="p">;</span> date<span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="get_codegen_file">get_codegen_file</h2>
<p><code>get_codegen_file</code> 用于整理代码生成的结果 (位于 <code>${BEMLIR_PROJECT_ROOT}/build/bin/${codegen_case}</code>)，为每个节点生成版本信息 (version.txt)，并将生成的文件复制到测试目录 (<code>${BEMLIR_PROJECT_ROOT}/external/tx8be-oplib/tests/test_codegen</code>)，最后调用 <code>get_codegen_host</code> 完成主机相关处理。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Function to process and organize generated codegen files</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> get_codegen_file<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Print all input arguments for debugging</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="nv">$*</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign first argument as the codegen case name or path</span>
</span></span><span class="line"><span class="cl">    <span class="nv">codegen_case</span><span class="o">=</span><span class="nv">$1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Second argument: 0 for host thread mode, 1 for host stream mode</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Note: $2 is passed to get_codegen_host</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Change to the codegen case directory under build/bin</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span><span class="s2">/build/bin/</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Find node directories matching node_[0-9]+_[0-9] pattern (e.g., node_123_4)</span>
</span></span><span class="line"><span class="cl">        <span class="nv">node_dirs</span><span class="o">=</span><span class="k">$(</span>find . -maxdepth <span class="m">1</span> -type d -regex <span class="s1">&#39;.*/node_[0-9]+_[0-9]&#39;</span> -exec basename <span class="o">{}</span> <span class="se">\;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Iterate through each node directory</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> dir in <span class="nv">$node_dirs</span><span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Check if libTX8MLIRTransforms.a exists to determine version type</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="o">[</span> ! -e <span class="s2">&#34;</span><span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span><span class="s2">/lib/libTX8MLIRTransforms.a&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Write &#39;tx8be-mlir&#39; to version.txt if library is absent</span>
</span></span><span class="line"><span class="cl">                <span class="nb">echo</span> -e <span class="s2">&#34;tx8be-mlir&#34;</span> &gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Write &#39;tx8be-mlir-sdk&#39; to version.txt if library is present</span>
</span></span><span class="line"><span class="cl">                <span class="nb">echo</span> -e <span class="s2">&#34;tx8be-mlir-sdk&#34;</span> &gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="k">fi</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Append git status to version.txt to record repository state</span>
</span></span><span class="line"><span class="cl">            git status --porcelain &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Append last two git commits to version.txt for version history</span>
</span></span><span class="line"><span class="cl">            git log -2 &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">done</span>
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Change to the test_codegen directory</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span><span class="s2">/external/tx8be-oplib/tests/test_codegen&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Remove existing codegen_case directory to ensure a clean state</span>
</span></span><span class="line"><span class="cl">        rm -rf <span class="s2">&#34;</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Copy the codegen_case directory from build/bin</span>
</span></span><span class="line"><span class="cl">        cp -r <span class="s2">&#34;</span><span class="si">${</span><span class="nv">BEMLIR_PROJECT_ROOT</span><span class="si">}</span><span class="s2">/build/bin/</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span> .
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Call get_codegen_host to process host-related tasks</span>
</span></span><span class="line"><span class="cl">    get_codegen_host <span class="s2">&#34;</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span> <span class="s2">&#34;</span><span class="nv">$2</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="get_codegen_host">get_codegen_host</h2>
<p><code>get_codegen_host </code> 用于为 host 环境准备代码生成用例的测试文件。它在指定的测试用例目录中处理 node &amp; chip 相关的文件，复制必要的配置文件、源代码和构建脚本，并根据 host_type 选择不同的主机实现文件 host_thread.cpp 或 host_stream.cpp.</p>
<ol>
<li>函数输入参数：</li>
</ol>
<ul>
<li><code>$1 (codegen_case)</code>: 代码生成用例的名称或路径，通常是一个目录 (例如 codegen0 或 codegen1) ，表示测试用例的根目录。</li>
<li><code>$2 (host_type)</code>: 主机执行模式，0: host_thread.cpp，1: host_stream.cpp.</li>
</ul>
<ol start="2">
<li>切换到 <code>${{OPLIB_PROJECT_ROOT}}/tests/test_codegen/${codegen_case}</code> 目录:</li>
</ol>
<ul>
<li>使用 find 命令查找符合 node_[0-9]+_[0-9] 模式 (例如 node_123_4) 的子目录，表示代码生成中的节点。</li>
<li>对每个 node_dir 追加版本信息和复制相关文件。</li>
</ul>
<ol start="3">
<li>处理 chip 目录:</li>
</ol>
<ul>
<li>在每个节点目录下，查找符合 <code> chip[0-9]+</code> 模式 (例如 chip0, chip1) 的子目录</li>
<li>为每个 dir 复制 Makefile_tile 到 <code>./${node_dir}/${dir}/Makefile</code>. 在 <code>./${node_dir}/${dir}/</code> 下创建 16 个子目录 (tiles0 - tiles15)，并为每个子目录复制 Makefile_main 到 t <code>iles$i/Makefile</code></li>
</ul>
<ol start="4">
<li>根据 <code>host_type</code> 复制 host_thread.cpp 或 host_stream.cpp 到当前目录的 host.cpp.</li>
<li>复制 CMakeLists.txt 和 Makefile 到当前目录。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Function to prepare host-related files for a codegen test case</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> get_codegen_host<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Print all input arguments for debugging</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="nv">$*</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign first argument as the codegen case name or path</span>
</span></span><span class="line"><span class="cl">    <span class="nv">codegen_case</span><span class="o">=</span><span class="nv">$1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign second argument as host type (0: thread mode, 1: stream mode)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">host_type</span><span class="o">=</span><span class="nv">$2</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Define relative path for test_codegen directory</span>
</span></span><span class="line"><span class="cl">    <span class="nv">oplib_path</span><span class="o">=</span><span class="s2">&#34;tests/test_codegen&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Change to the test_codegen directory for the codegen case</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tests/test_codegen/</span><span class="si">${</span><span class="nv">codegen_case</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Find node directories matching node_[0-9]+_[0-9] pattern (e.g., node_123_4)</span>
</span></span><span class="line"><span class="cl">        <span class="nv">node_dirs</span><span class="o">=</span><span class="k">$(</span>find . -maxdepth <span class="m">1</span> -type d -regex <span class="s1">&#39;.*/node_[0-9]+_[0-9]&#39;</span> -exec basename <span class="o">{}</span> <span class="se">\;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> node_dir in <span class="nv">$node_dirs</span><span class="p">;</span> <span class="k">do</span>  <span class="c1"># # Iterate through each node directory</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Append oplib version info to version.txt</span>
</span></span><span class="line"><span class="cl">            <span class="nb">echo</span> -e <span class="s2">&#34;\n\ntx8be-oplib:&#34;</span> &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Append git status to version.txt to record repository state</span>
</span></span><span class="line"><span class="cl">            git status --porcelain &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Write last two git commits to version.txt for version history</span>
</span></span><span class="line"><span class="cl">            git log -2 &gt;&gt; <span class="s2">&#34;</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/version.txt&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Copy all stream-related files to node directory</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/stream*&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Copy CMakeLists_chip.txt as CMakeLists.txt for node</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/CMakeLists_chip.txt&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/CMakeLists.txt&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Copy main_kcore.c to node directory</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/main_kcore.c&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Copy Makefile_chip as Makefile for node</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/Makefile_chip&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/Makefile&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Find chip directories matching chip[0-9]+ pattern (e.g., chip0, chip1)</span>
</span></span><span class="line"><span class="cl">            <span class="nv">chip_dirs</span><span class="o">=</span><span class="k">$(</span>find <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">&#34;</span> -maxdepth <span class="m">1</span> -type d -regex <span class="s1">&#39;.*/chip[0-9]+&#39;</span> -exec basename <span class="o">{}</span> <span class="se">\;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Iterate through each chip directory</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> dir in <span class="nv">$chip_dirs</span><span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Copy Makefile_tile as Makefile for chip</span>
</span></span><span class="line"><span class="cl">                cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/Makefile_tile&#34;</span> <span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/Makefile&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Create Makefiles for 16 tiles (tiles0 to tiles15)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="o">((</span><span class="nv">i</span><span class="o">=</span>0<span class="p">;</span> i&lt;16<span class="p">;</span> i++<span class="o">))</span><span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">                    <span class="nv">dst_file</span><span class="o">=</span><span class="s2">&#34;./</span><span class="si">${</span><span class="nv">node_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">${</span><span class="nv">dir</span><span class="si">}</span><span class="s2">/tiles</span><span class="si">${</span><span class="nv">i</span><span class="si">}</span><span class="s2">/Makefile&#34;</span>
</span></span><span class="line"><span class="cl">                    <span class="c1"># Copy Makefile_main to each tile&#39;s Makefile</span>
</span></span><span class="line"><span class="cl">                    cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/Makefile_main&#34;</span> <span class="s2">&#34;</span><span class="nv">$dst_file</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="k">done</span>
</span></span><span class="line"><span class="cl">            <span class="k">done</span>
</span></span><span class="line"><span class="cl">        <span class="k">done</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Copy host implementation based on host_type</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="o">[</span> <span class="nv">$host_type</span> -eq <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Use thread-based host implementation</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/host_thread.cpp&#34;</span> <span class="s2">&#34;host.cpp&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Use stream-based host implementation</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Note: Fixed typo &#39;$t{OPLIB_PROJECT_ROOT}&#39; to &#39;${{OPLIB_PROJECT_ROOT}}&#39;</span>
</span></span><span class="line"><span class="cl">            cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/host_stream.cpp&#34;</span> <span class="s2">&#34;host.cpp&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Copy top-level CMakeLists.txt for test case</span>
</span></span><span class="line"><span class="cl">        cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/CMakeLists.txt&#34;</span> .
</span></span><span class="line"><span class="cl">        <span class="c1"># Copy top-level Makefile for test case</span>
</span></span><span class="line"><span class="cl">        cp <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tools/codegen/Makefile&#34;</span> .
</span></span><span class="line"><span class="cl">    <span class="c1"># Restore original directory</span>
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="run_on_cmodel">run_on_cmodel</h2>
<p><code>run_on_cmodel</code> 用于在指定的测试用例目录中运行 cmodel 仿真任务。函数的主要功能包括环境设置、构建、执行仿真脚本或程序，并处理错误。以下是详细的功能说明：</p>
<ol>
<li>函数输入参数：</li>
</ol>
<ul>
<li>$1 (case_name): 来自 <code>run_codegen_layer</code> 的 <code>codegen_path</code>，可能附加 <code>host_type</code>.</li>
<li>$2 (run_flag): 运行标志，来自 <code>run_codegen_layer</code> 的 <code>cmp_flag</code> 用于控制仿真执行的方式 (例如是否运行或运行模式) 。</li>
</ul>
<ol start="2">
<li>切换工作目录并执行:</li>
</ol>
<ul>
<li>切换到测试用例目录 <code>${{OPLIB_PROJECT_ROOT}}/tests/test_codegen/${case_name}</code></li>
<li>运行 <code>cmake .. -DUSING_RISCV=OFF</code>，配置构建系统，禁用 RISCV 支持。</li>
<li>运行 <code>make -j</code> 并动态设置并行任务数 (基于 CPU 核心数，<code>cat /proc/stat | grep cpu[0-9] -c</code>)</li>
</ul>
<ol start="3">
<li>仿真执行: 根据参数运行仿真脚本 (host_sim.sh) 或 host_sim.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Function to run a cmodel simulation for a given test case</span>
</span></span><span class="line"><span class="cl"><span class="k">function</span> run_on_cmodel<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign first argument as the test case name</span>
</span></span><span class="line"><span class="cl">    <span class="nv">case_name</span><span class="o">=</span><span class="nv">$1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Assign second argument as the run flag (controls execution mode)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">run_flag</span><span class="o">=</span><span class="nv">$2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Check if case_name is empty</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$case_name</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;Error: case_name is empty&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Check if the test case directory exists</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> ! -d <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tests/test_codegen/</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;Can not find </span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Change to the test case directory</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="p">{OPLIB_PROJECT_ROOT</span><span class="si">}</span><span class="s2">}/tests/test_codegen/</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">&#34;</span>  <span class="c1"># FIXED DIR</span>
</span></span><span class="line"><span class="cl">        rm -rf build
</span></span><span class="line"><span class="cl">        mkdir build
</span></span><span class="line"><span class="cl">        <span class="nb">cd</span> build
</span></span><span class="line"><span class="cl">        <span class="c1"># Run cmake to configure the build, disabling RISCV support</span>
</span></span><span class="line"><span class="cl">        cmake .. -DUSING_RISCV<span class="o">=</span>OFF
</span></span><span class="line"><span class="cl">        <span class="c1"># Run make with parallel jobs based on CPU core count</span>
</span></span><span class="line"><span class="cl">        make -j<span class="k">$(</span>cat /proc/stat <span class="p">|</span> grep cpu<span class="o">[</span>0-9<span class="o">]</span> -c<span class="k">)</span>
</span></span><span class="line"><span class="cl">        <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  <span class="c1"># Capture the return code</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># If make fails, restore directory, print error, and exit</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="o">[[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">            <span class="nb">echo</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">        <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Check if run_flag is empty</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$run_flag</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="o">[</span> -e ../host_sim.sh <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># Check if host_sim.sh exists in the parent directory</span>
</span></span><span class="line"><span class="cl">                cp ../host_sim.sh .
</span></span><span class="line"><span class="cl">                sh ./host_sim.sh
</span></span><span class="line"><span class="cl">                <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  <span class="c1"># Capture the return code</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># If script fails, restore directory, print error, and exit</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="o">[[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>  
</span></span><span class="line"><span class="cl">                    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">echo</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                    <span class="k">return</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                <span class="k">fi</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span>
</span></span><span class="line"><span class="cl">                ./host_sim ../  <span class="c1"># Run host_sim with parent directory as argument</span>
</span></span><span class="line"><span class="cl">                <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  <span class="c1"># Capture the return code</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># If host_sim fails, restore directory, print error, and exit</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="o">[[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">                    <span class="nb">echo</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                    <span class="k">return</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                <span class="k">fi</span>
</span></span><span class="line"><span class="cl">            <span class="k">fi</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Check if run_flag is &#34;0&#34; or &#34;1&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="o">[[</span> <span class="nv">$run_flag</span> <span class="o">==</span> <span class="s2">&#34;0&#34;</span> <span class="o">]]</span> <span class="o">||</span> <span class="o">[[</span> <span class="nv">$run_flag</span> <span class="o">==</span> <span class="s2">&#34;1&#34;</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Run host_sim with parent directory and run_flag</span>
</span></span><span class="line"><span class="cl">            ./host_sim ../ <span class="s2">&#34;</span><span class="nv">$run_flag</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span>  <span class="c1"># Capture the return code</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># If host_sim fails, restore directory, print error, and exit</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="o">[[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
</span></span><span class="line"><span class="cl">                <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">                <span class="nb">echo</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="nv">$ret</span>
</span></span><span class="line"><span class="cl">            <span class="k">fi</span>
</span></span><span class="line"><span class="cl">        <span class="k">fi</span>
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> </span><span class="nv">$*</span><span class="s2"> passed&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h1 id="run_codegen_case_soc_rtt">run_codegen_case_soc_rtt</h1>
<p>run_codegen_case_soc_rtt 位于 <code>tx8-oplib/scripts/regression.sh</code>，函数用于在 SOC 环境下运行 RTT (Real-Time Transfer) 测试。其主要流程如下：</p>
<ol>
<li>初始化和参数获取：</li>
</ol>
<ul>
<li>函数从命令行参数中获取 <code>case_name</code>, <code>copy_option</code>, 和 <code>multi_graph_enable</code>.</li>
<li>检查 <code>case_name</code> 是否为空，如果为空则输出错误信息并返回 1.</li>
</ul>
<ol start="2">
<li>环境设置和目录导航：</li>
</ol>
<ul>
<li>将工作目录切换到 <code>${OPLIB_PROJECT_ROOT}/tests/test_codegen/${case_name}</code>. 如果目录不存在，则输出错误信息并返回 1。</li>
</ul>
<ol start="3">
<li>构建和配置：</li>
</ol>
<ul>
<li>执行 <code>rm -rf ${case_name}_build</code> 清理之前的构建文件。</li>
<li>根据 <code>multi_graph_enable</code> 设置 <code>CONFIG_ARGS</code>，如果启用多图则设置为 &ldquo;-DMULTI_GRAPH=1&rdquo;，否则为空。</li>
<li>调用 cmake 命令生成构建文件，指定构建目录为 <code>${case_name}_build</code>，并根据 <code>copy_option</code> 设置 <code>COPY_RTT_FLAG</code>.</li>
<li>执行 make 命令进行实际构建，目标包括 all 和 chip_out.</li>
</ul>
<ol start="4">
<li>错误处理和退出：</li>
</ol>
<ul>
<li>每次关键步骤执行后，检查返回状态 <code>$ret</code>，如果非 0，则弹出目录并返回错误码。</li>
<li>构建成功后输出 <code>${FUNCNAME[0]} &quot;passed&quot;</code> 表示通过。</li>
</ul>
<ol start="5">
<li>清理和返回: 函数结束时弹出目录，恢复原始工作目录。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> run_codegen_case_soc_rtt<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> &#39;start&#39;&#34;</span>  <span class="c1"># 输出函数名和&#34;start&#34;表示开始</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">case_name</span><span class="o">=</span><span class="nv">$1</span>                  <span class="c1"># 获取用例名称</span>
</span></span><span class="line"><span class="cl">    <span class="nv">copy_option</span><span class="o">=</span><span class="nv">$2</span>                 <span class="c1"># 获取复制选项</span>
</span></span><span class="line"><span class="cl">    <span class="nv">multi_graph_enable</span><span class="o">=</span><span class="nv">$3</span>          <span class="c1"># 获取多图启用标志</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$case_name</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>   <span class="c1"># 如果用例名称为空</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;case_name(</span><span class="nv">$case_name</span><span class="s2">) not found &#34;</span>  <span class="c1"># 输出错误信息</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="m">1</span>                   <span class="c1"># 返回错误码 1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">case_dir</span><span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/tests/test_codegen/<span class="si">${</span><span class="nv">case_name</span><span class="si">}</span>  <span class="c1"># 设置用例目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">case_dir</span><span class="si">}</span>              <span class="c1"># 切换到用例目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    rm -rf <span class="si">${</span><span class="nv">case_name</span><span class="si">}</span>_build      <span class="c1"># 清理之前的构建文件</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 检查清理是否成功</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$multi_graph_enable</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># 如果多图启用标志为空</span>
</span></span><span class="line"><span class="cl">        <span class="nv">CONFIG_ARGS</span><span class="o">=</span><span class="s2">&#34;&#34;</span>                 <span class="c1"># 配置参数为空</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span>                                 <span class="c1"># 否则</span>
</span></span><span class="line"><span class="cl">        <span class="nv">CONFIG_ARGS</span><span class="o">=</span><span class="s2">&#34;-DMULTI_GRAPH=1&#34;</span>  <span class="c1"># 设置多图配置参数</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    cmake -B <span class="s2">&#34;</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">_build&#34;</span> -DUSING_RISCV<span class="o">=</span>ON -TX8FW_BASE<span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/release/riscv/tx8-yoc-rt-thread-smp <span class="si">${</span><span class="nv">CONFIG_ARGS</span><span class="si">}</span> <span class="p">;</span> <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 生成构建文件</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    make -j -C <span class="s2">&#34;</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">_build&#34;</span> --target all chip_out <span class="p">;</span> <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 执行构建</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>                          <span class="c1"># 恢复到原始目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> &#39;passed&#39;&#34;</span> <span class="c1"># 输出函数名和&#34;passed&#34;表示通过</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="export_tx8fw_to_env">export_tx8fw_to_env</h2>
<p><code>export_tx8fw_to_env</code> 函数的主要目的是设置与 TX8FW 相关的环境变量，以便后续构建或运行时使用。以下是其流程：</p>
<ol>
<li>设置 SDK 路径：</li>
</ol>
<ul>
<li>定义 TX8FW 的 SDK 路径 <code>soc_sdk_path</code> 为 <code>${OPLIB_PROJECT_ROOT}/3rd_party/tx8-yoc-rt-thread-smp</code>.</li>
</ul>
<ol start="2">
<li>检查路径是否存在:</li>
</ol>
<ul>
<li>检查路径 <code>${soc_sdk_path}/tool/tx8fw-xuantie-sdk</code> 是否存在。如果不存在，打印错误信息并退出，状态码为 1.</li>
</ul>
<ol start="3">
<li>导出环境变量: 打印并设置以下环境变量</li>
</ol>
<ul>
<li>TX8FW_SDK_INSTALL_DIR：指向 ${soc_sdk_path}/tool/tx8fw-xuantie-sdk。</li>
<li>TX8FW_TOOLCHAIN_VARIANT：设置为 cross-compile。</li>
</ul>
<ol start="4">
<li>清理目录: 使用 popd 命令恢复到之前的目录.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> export_tx8fw_to_env<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nv">soc_sdk_path</span><span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/3rd_party/tx8-yoc-rt-thread-smp  <span class="c1"># 设置 TX8FW SDK 路径</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">soc_sdk_path</span><span class="si">}</span>/tool/tx8fw-xuantie-sdk  <span class="c1"># 切换到 TX8FW SDK 工具目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> ! -d <span class="s2">&#34;xuantie-900-gcc-elf-newlib-x86_64-V2.8.0&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># 检查指定 SDK 目录是否存在</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">soc_sdk_path</span><span class="si">}</span><span class="s2">/tool/tx8fw-xuantie-sdk didn&#39;t exist&#34;</span>  <span class="c1"># 如果不存在，打印错误信息</span>
</span></span><span class="line"><span class="cl">        <span class="nb">exit</span> <span class="m">1</span>  <span class="c1"># 退出并返回状态码 1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;export TX8FW_SDK_INSTALL_DIR=</span><span class="si">${</span><span class="nv">soc_sdk_path</span><span class="si">}</span><span class="s2">/tool/tx8fw-xuantie-sdk&#34;</span>  <span class="c1"># 打印并设置 TX8FW SDK 安装目录环境变量</span>
</span></span><span class="line"><span class="cl">    <span class="nb">export</span> <span class="nv">TX8FW_SDK_INSTALL_DIR</span><span class="o">=</span><span class="si">${</span><span class="nv">soc_sdk_path</span><span class="si">}</span>/tool/tx8fw-xuantie-sdk  <span class="c1"># 导出 TX8FW SDK 安装目录环境变量</span>
</span></span><span class="line"><span class="cl">    <span class="nb">export</span> <span class="nv">TX8FW_TOOLCHAIN_VARIANT</span><span class="o">=</span>cross-compile  <span class="c1"># 导出工具链变体为 cross-compile</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>  <span class="c1"># 恢复到之前的目录</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="build_oplib_with_soc-函数">build_oplib_with_soc 函数</h2>
<p><code>build_oplib_with_soc</code> 函数用于构建 OPLib 并结合特定 SoC 配置。以下是其流程：</p>
<p>打印项目根目录：
打印 OPLIB_PROJECT_ROOT 环境变量，用于调试或日志记录。</p>
<ol>
<li>切换目录和初始化：</li>
</ol>
<ul>
<li>使用 pushd 切换到 <code>OPLIB_PROJECT_ROOT</code> 目录。</li>
<li>定义变量 <code>rm=rf build</code>, <code>mkdir=build</code> 和 <code>cd=build</code>，这些变量实际上是模拟命令（rm -rf build、mkdir build 和 cd build）。</li>
</ul>
<ol start="2">
<li>设置复制标志：</li>
</ol>
<ul>
<li>检查 <code>$1</code> (即 <code>copy_option</code>) 是否为 &ldquo;NOT_COPY&rdquo;，如果是，则设置 <code>COPY_RTT_FLAG</code> 为 <code>--DRTT_HOST_COPY=OFF</code>，否则为空。</li>
</ul>
<ol start="3">
<li>导出环境变量并构建：</li>
</ol>
<ul>
<li>调用 <code>export_tx8fw_to_env</code> 函数设置 TX8FW 相关环境变量。</li>
<li>运行 cmake 命令，生成构建文件，指定构建选项 <code>-DUSING_RISCV=ON</code> 和 <code>TX8FW_BASE</code>，并根据 <code>COPY_RTT_FLAG</code> 添加额外参数。</li>
<li>使用 make 命令执行构建，目标包括 grep epilog 和 c</li>
</ul>
<ol start="4">
<li>清理目录: 使用 popd 命令恢复到之前的目录.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> build_oplib_with_soc<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>  <span class="c1"># 打印 OPLib 项目根目录路径</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>  <span class="c1"># 切换到 OPLib 项目根目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">rm</span><span class="o">=</span>rf build  <span class="c1"># 定义清理构建目录的命令</span>
</span></span><span class="line"><span class="cl">    <span class="nv">mkdir</span><span class="o">=</span>build  <span class="c1"># 定义创建构建目录的命令</span>
</span></span><span class="line"><span class="cl">    <span class="nv">cd</span><span class="o">=</span>build     <span class="c1"># 定义切换到构建目录的命令</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">COPY_RTT_FLAG</span><span class="o">=</span><span class="s2">&#34;&#34;</span>  <span class="c1"># 初始化 RTT 复制标志</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> <span class="s2">&#34;</span><span class="nv">$1</span><span class="s2">&#34;</span> <span class="o">==</span> <span class="s2">&#34;NOT_COPY&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># 如果传入的复制选项为 NOT_COPY</span>
</span></span><span class="line"><span class="cl">        <span class="nv">COPY_RTT_FLAG</span><span class="o">=</span><span class="s2">&#34;--DRTT_HOST_COPY=OFF&#34;</span>  <span class="c1"># 设置 RTT 复制标志为关闭</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    export_tx8fw_to_env  <span class="c1"># 调用函数导出 TX8FW 相关环境变量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    cmake .. -DUSING_RISCV<span class="o">=</span>ON -TX8FW_BASE<span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/release/riscv/tx8-yoc-rt-thread-smp <span class="si">${</span><span class="nv">COPY_RTT_FLAG</span><span class="si">}</span>  <span class="c1"># 生成构建文件，指定 RISCV 和 TX8FW 路径</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 检查 cmake 是否成功，失败则返回</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    make -j cat /proc/stat <span class="p">|</span> grep epilog -c  <span class="c1"># 执行构建并检查 epilog 相关信息</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 检查 make 是否成功，失败则返回</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>  <span class="c1"># 恢复到之前的目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span> <span class="s2">&#34;passed&#34;</span>  <span class="c1"># 输出函数名和&#34;passed&#34;表示构建成功</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h2 id="run_on_soc_rtt">run_on_soc_rtt</h2>
<p><code>run_on_soc_rtt</code>，用于在特定 SoC 和 RTT 环境下运行测试用例。以下是其主要流程：</p>
<ol>
<li>初始化和参数获取:</li>
</ol>
<ul>
<li>函数从命令行参数中获取 <code>case_name</code>, <code>rtt_option</code> 和 <code>multi_graph_enable</code>.</li>
<li>检查 <code>case_name</code> 是否为空，如果为空则输出错误信息并返回 1。</li>
</ul>
<ol start="2">
<li>目录切换和清理:</li>
</ol>
<ul>
<li>将工作目录切换到 <code>${OPLIB_PROJECT_ROOT}/tests/test_codegen/${case_name}</code>.</li>
<li>执行 <code>rm -rf ${case_name}_build</code> 清理之前的构建文件。</li>
</ul>
<ol start="3">
<li>配置设置: 根据 <code>multi_graph_enable</code> 设置 CONFIG_ARGS，如果启用多图则设置为 &ldquo;-DMULTI_GRAPH=1&rdquo;，否则为空。</li>
<li>构建和运行：</li>
</ol>
<ul>
<li>使用 cmake 生成构建文件，指定构建目录为 <code>${case_name}_build</code>，并设置 <code>-DUSING_RISCV=ON</code> 和 <code>-TX8FW_BASE</code> 路径。</li>
<li>使用 make 命令执行构建，目标包括 all 和 chip_out.</li>
</ul>
<ol start="5">
<li>清理和返回: 使用 popd 恢复到原始目录。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> run_on_soc_rtt<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> &#39;start&#39;&#34;</span>  <span class="c1"># 输出函数名和&#34;start&#34;表示开始</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">case_name</span><span class="o">=</span><span class="nv">$1</span>                  <span class="c1"># 获取用例名称</span>
</span></span><span class="line"><span class="cl">    <span class="nv">rtt_option</span><span class="o">=</span><span class="nv">$2</span>                 <span class="c1"># 获取 RTT 选项</span>
</span></span><span class="line"><span class="cl">    <span class="nv">multi_graph_enable</span><span class="o">=</span><span class="nv">$3</span>         <span class="c1"># 获取多图启用标志</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$case_name</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>   <span class="c1"># 如果用例名称为空</span>
</span></span><span class="line"><span class="cl">        <span class="nb">echo</span> <span class="s2">&#34;case_name(</span><span class="nv">$case_name</span><span class="s2">) not found &#34;</span>  <span class="c1"># 输出错误信息</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="m">1</span>                   <span class="c1"># 返回错误码 1</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">case_dir</span><span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/tests/test_codegen/<span class="si">${</span><span class="nv">case_name</span><span class="si">}</span>  <span class="c1"># 设置用例目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">pushd</span> <span class="si">${</span><span class="nv">case_dir</span><span class="si">}</span>              <span class="c1"># 切换到用例目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    rm -rf <span class="si">${</span><span class="nv">case_name</span><span class="si">}</span>_build      <span class="c1"># 清理之前的构建文件</span>
</span></span><span class="line"><span class="cl">    <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 检查清理是否成功，失败则返回</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="o">[</span> -z <span class="s2">&#34;</span><span class="nv">$multi_graph_enable</span><span class="s2">&#34;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>  <span class="c1"># 如果多图启用标志为空</span>
</span></span><span class="line"><span class="cl">        <span class="nv">CONFIG_ARGS</span><span class="o">=</span><span class="s2">&#34;&#34;</span>                 <span class="c1"># 配置参数为空</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span>                                 <span class="c1"># 否则</span>
</span></span><span class="line"><span class="cl">        <span class="nv">CONFIG_ARGS</span><span class="o">=</span><span class="s2">&#34;-DMULTI_GRAPH=1&#34;</span>  <span class="c1"># 设置多图配置参数</span>
</span></span><span class="line"><span class="cl">    <span class="k">fi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    cmake -B <span class="s2">&#34;</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">_build&#34;</span> -DUSING_RISCV<span class="o">=</span>ON -TX8FW_BASE<span class="o">=</span><span class="si">${</span><span class="nv">OPLIB_PROJECT_ROOT</span><span class="si">}</span>/release/riscv/tx8-yoc-rt-thread-smp <span class="si">${</span><span class="nv">CONFIG_ARGS</span><span class="si">}</span> <span class="p">;</span> <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 生成构建文件，指定 RISCV 和 TX8FW 路径</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    make -C <span class="s2">&#34;</span><span class="si">${</span><span class="nv">case_name</span><span class="si">}</span><span class="s2">_build&#34;</span> --target all chip_out <span class="p">;</span> <span class="nv">ret</span><span class="o">=</span><span class="nv">$?</span><span class="p">;</span> <span class="k">if</span> <span class="o">[</span> <span class="o">[</span> <span class="nv">$ret</span> -ne <span class="m">0</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span> popd<span class="p">;</span> <span class="nb">echo</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">return</span> <span class="nv">$ret</span><span class="p">;</span> <span class="k">fi</span>  <span class="c1"># 执行构建，目标为 all 和 chip_out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">popd</span>                          <span class="c1"># 恢复到原始目录</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">FUNCNAME</span><span class="p">[0]</span><span class="si">}</span><span class="s2"> &#39;passed&#39;&#34;</span> <span class="c1"># 输出函数名和&#34;passed&#34;表示通过</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div>]]></content:encoded>
    </item>
    <item>
      <title>astra-Sim</title>
      <link>http://localhost:57770/blogs/astra-sim/</link>
      <pubDate>Mon, 09 Jun 2025 13:34:39 +0800</pubDate>
      <guid>http://localhost:57770/blogs/astra-sim/</guid>
      <description>source code reading of astra-sim</description>
      <content:encoded><![CDATA[<h1 id="build-analytical-backend">Build Analytical Backend</h1>
<p><code>build.sh</code> 脚本是构建过程的高级控制器。其核心职责是解析用户意图，执行预构建步骤，并以正确的参数调用底层的 CMake 工具链。</p>
<ol>
<li>
<p><strong>选项解析</strong>: 脚本通过 <code>getopts</code> 处理以下命令行标志：</p>
<ul>
<li><code>-t &lt;target&gt;</code>: 指定编译目标。有效值为 <code>all</code>, <code>congestion_unaware</code>, <code>congestion_aware</code>。此值将作为变量传递给 CMake。</li>
<li><code>-l</code>: 触发清理 (<code>cleanup</code>) 流程，删除所有构建产物并终止脚本。</li>
<li><code>-d</code>: 启用调试 (<code>Debug</code>) 模式进行编译。</li>
</ul>
</li>
<li>
<p><strong>环境准备 (<code>setup</code>, <code>compile_chakra_et</code>)</strong>:</p>
<ul>
<li><code>setup</code> 函数负责创建用于存放中间文件和最终产物的 <code>build</code> 目录，确保源码树的清洁。同时，它会根据系统核心数设置一个上限为 16 的并发编译线程数，以优化编译效率。</li>
<li><code>compile_chakra_et</code> 函数负责处理 <code>et_def.proto</code> 这一 Protobuf 依赖。它检查目标文件是否存在，若不存在，则调用 <code>protoc</code> 编译器生成相应的 C++ 和 Python 源码。</li>
</ul>
</li>
<li>
<p><strong>构建执行 (<code>compile_astrasim_analytical</code>, <code>compile_astrasim_analytical_as_debug</code>)</strong>:</p>
<ul>
<li>这两个函数是脚本与 CMake 交互的核心。它们根据用户是否指定 <code>-d</code> 标志，决定是执行标准 <code>Release</code> 构建还是 <code>Debug</code> 构建。关键在于它们会将用户指定的 <code>build_target</code> 作为 <code>-DBUILDTARGET</code> 参数传递给 CMake。</li>
</ul>
</li>
<li>
<p><strong>后处理 (<code>create_symlink_*</code>)</strong>:</p>
<ul>
<li>编译完成后，<code>create_symlink_congestion_unaware</code> 和 <code>create_symlink_congestion_aware</code> 等函数会为生成的二进制文件创建符号链接。此举旨在维持对旧文件路径的向后兼容性。</li>
</ul>
</li>
</ol>
<hr>
<p><code>CMakeLists.txt</code> 文件是项目的构建蓝图，它向 CMake 阐述了项目的结构、依赖关系以及编译规则。</p>
<ol>
<li>
<p><strong>编译环境设定</strong>:</p>
<ul>
<li><code>cmake_minimum_required(VERSION 3.15)</code>: 规定了运行此配置所需的最低 CMake 版本。</li>
<li><code>set(CMAKE_CXX_STANDARD 17)</code> 和 <code>set(CMAKE_CXX_STANDARD_REQUIRED ON)</code>: 强制项目必须在支持 C++17 标准的编译环境中构建。</li>
</ul>
</li>
<li>
<p><strong>编译标志 (Compiler Flags)</strong>:</p>
<ul>
<li>此文件为不同的构建类型（<code>CMAKE_BUILD_TYPE</code>）定义了不同的编译器标志。</li>
<li><strong><code>Release</code></strong> (默认模式): <code>set(CMAKE_CXX_FLAGS_RELEASE &quot;-O3&quot;)</code> 指示编译器进行高等级优化，以追求最大化程序性能。</li>
<li><strong><code>Debug</code></strong>: <code>set(CMAKE_CXX_FLAGS_DEBUG &quot;...&quot;)</code> 包含一系列用于调试的标志：
<ul>
<li><code>-O0</code>: 关闭所有优化，确保编译后的代码与源码行为一致。</li>
<li><code>-g</code>: 在可执行文件中包含调试符号，这是 GDB 等调试器工作的前提。</li>
<li><code>-fsanitize=address,undefined,leak</code>: 启用 AddressSanitizer、UndefinedBehaviorSanitizer 和 LeakSanitizer。这些是强大的运行时诊断工具，用于捕获内存访问错误、未定义行为及内存泄漏。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>项目结构与依赖</strong>:</p>
<ul>
<li><code>project(AstraSim_Analytical)</code>: 声明项目名称。</li>
<li><code>add_subdirectory(...)</code>: 此指令是组织项目的关键。它将 <code>AstraSim</code> 核心库、<code>Analytical</code> 网络后端和 <code>AstraSim_Analytical</code> 前端等多个子模块纳入构建过程。</li>
</ul>
</li>
<li>
<p><strong>用户自定义选项</strong>:</p>
<ul>
<li><code>set(BUILDTARGET &quot;all&quot; CACHE STRING ...)</code>: 此行定义了一个名为 <code>BUILDTARGET</code> 的可缓存变量。这使得用户可以通过 <code>cmake -D</code> 命令从外部注入该变量的值。此变量随后会被子目录中的 <code>CMakeLists.txt</code> 文件用来实现条件编译。</li>
</ul>
</li>
</ol>
<h1 id="build-ns-3-backend">Build ns-3 Backend</h1>
<p>构建命令为 <code>./build/astra_ns3/build.sh -c</code>，他会执行该脚本里的 compile 函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> compile <span class="o">{</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">NS3_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">./ns3 configure --enable-mpi
</span></span><span class="line"><span class="cl">./ns3 build AstraSimNetwork -j <span class="m">12</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="ns3-configure---enable-mpi"><code>./ns3 configure --enable-mpi</code></h2>
<ol>
<li>参数解析 (<code>parse_args</code>): 脚本的 <code>argparse</code> 模块会识别出 <code>configure</code> 子命令和 <code>--enable-mpi</code> 选项。<code>--enable-mpi</code> 是一个预定义的&quot;On-Off&quot;选项，用于控制 MPI (Message Passing Interface) 分布式仿真功能的支持。</li>
<li>进入配置步骤 (<code>configuration_step</code>): 由于检测到 configure 命令，脚本会调用 <code>configuration_step</code> 函数。</li>
<li>调用 CMake (<code>configure_cmake</code>): <code>configuration_step</code> 函数内部会调用 <code>configure_cmake</code>. 这个函数是会动态地构建一个 cmake 命令。
<ul>
<li>它会检测到 <code>--enable-mpi</code> 选项，并通过 <code>on_off_condition</code> 函数将其转换为 CMake 变量 <code>-DNS3_MPI=ON</code>.</li>
<li>最终组装出的命令为为 <code>cmake -S . -B cmake-cache -G &quot;Unix Makefiles&quot; -DCMAKE_BUILD_TYPE=default -DNS3_ASSERT=ON -DNS3_LOG=ON -DNS3_WARNINGS_AS_ERRORS=OFF -DNS3_MPI=ON --warn-uninitialized</code></li>
</ul>
</li>
<li>执行配置: 脚本通过 <code>subprocess.run()</code> 执行这条 cmake 命令</li>
</ol>
<h2 id="ns3-build-astrasimnetwork--j-12"><code>./ns3 build AstraSimNetwork -j 12</code></h2>
<ol>
<li>参数解析 (<code>parse_args</code>): 脚本识别出 <code>build</code> 子命令，目标 <code>AstraSimNetwork</code>，以及并行任务数 <code>-j 12</code>. 前者会被存入 <code>args.build</code> 列表，后者会被存入 <code>args.jobs</code>.</li>
<li>进入构建步骤 (<code>build_step</code>): 脚本检测到 <code>build</code> 命令，并调用 <code>build_step</code> 函数。</li>
<li>调用 CMake 构建 (<code>cmake_build</code>): <code>build_step</code> 函数会遍历 <code>args.build</code> 列表中的所有目标。在这里，它会为 <code>AstraSimNetwork</code> 这个目标调用 <code>cmake_build</code> 函数。
<ul>
<li>cmake_build 函数会组装出一条 <code>cmake --build</code> 命令。</li>
<li>将目标 AstraSimNetwork 转换为 <code>--target AstraSimNetwork</code>.</li>
<li>将并行任务数 12 转换为 <code>-j 12</code>.</li>
<li>最终组装出的命令为 <code>cmake --build cmake-cache --target AstraSimNetwork -j 12</code>.</li>
</ul>
</li>
</ol>
<h1 id="error-when-building-ns-3">Error When Building ns-3</h1>
<h2 id="call-of-overloaded-format-is-ambiguous-">call of overloaded ‘format(&hellip;)’ is ambiguous ❌</h2>
<h3 id="问题诊断-">问题诊断 🩺</h3>
<p>错误信息 <code>call of overloaded ‘format(...)’ is ambiguous</code> 的意思是，编译器在你的代码中遇到了一个名为 <code>format</code> 的函数调用，但它找到了多个同名的、并且参数类型都能匹配的 <code>format</code> 函数定义，导致编译器不知道该选择哪一个，因此产生了“歧义”（ambiguous）。</p>
<p><strong>这个歧义的来源是：</strong></p>
<ol>
<li><strong><code>std::format</code> (来自 C++20 标准库)</strong>: 你的项目很可能正在使用支持 C++20 或更高版本的现代编译器（如 GCC 11+）。C++20 标准库引入了一个新的格式化函数 <code>std::format</code>。</li>
<li><strong><code>fmt::format</code> (来自 {fmt} 库)</strong>: <code>spdlog</code> 这个日志库是基于一个非常流行的第三方格式化库 <code>{fmt}</code> 构建的。这个库也提供了一个功能几乎完全相同的 <code>fmt::format</code> 函数。在 <code>spdlog</code> 的上下文中，它通常可以直接以 <code>format</code> 的形式被调用。</li>
</ol>
<p>当你的代码（这里是 <code>spdlog_setup</code> 的一部分）简单地调用 <code>format(...)</code> 时，如果 C++20 的 <code>&lt;format&gt;</code> 头文件被包含，编译器就会同时看到 <code>std::format</code> 和 <code>spdlog</code> 内部的 <code>fmt::format</code>。由于两者都能处理字符串字面量 (<code>const char[]</code>) 和 <code>std::string</code>，编译器无法决定用哪个，从而报错。</p>
<hr>
<h3 id="关于-using-fmtformat-为何仍然无效的解释">关于 <code>using fmt::format;</code> 为何仍然无效的解释</h3>
<p>原因是，除了常规的命名空间查找规则，C++ 还有一个更强大的规则叫做<strong>参数依赖查找（Argument-Dependent Lookup, ADL）</strong>，有时也被称为 Koenig 查找。</p>
<hr>
<p>我们来梳理一下编译器在看到 <code>format(...)</code> 这行代码时的“思考过程”：</p>
<ol>
<li>
<p><strong>在当前作用域查找</strong></p>
<p>编译器看到了你的 <code>using fmt::format;</code> 声明。很好，它在当前作用域里找到了一个叫做 <code>format</code> 的函数（也就是 <code>fmt::format</code>）。这成为了<strong>候选者 A</strong>。</p>
</li>
<li>
<p><strong>参数依赖查找 (ADL) —— 问题的根源</strong></p>
<p>接下来，编译器会检查 <code>format(...)</code> 函数的所有参数类型。在你的错误日志里，我们看到了 <code>const std::string&amp;</code> 这样的参数。</p>
<ul>
<li>ADL 规则规定：如果一个函数的参数是某个命名空间 <code>N</code> 下的类型（比如 <code>std::string</code> 是 <code>std</code> 命名空间下的），那么编译器<strong>也必须</strong>去那个命名空间 <code>N</code> (这里是 <code>std</code>) 里面去查找同名的函数。</li>
<li>由于 <code>std::string</code> 是 <code>std</code> 命名空间的成员，ADL 规则被触发，编译器自动地去 <code>std</code> 命名空间里寻找名为 <code>format</code> 的函数。</li>
<li>因为你使用了 C++20 编译器，它在 <code>std</code> 命名空间里成功找到了 <code>std::format</code>。这成为了<strong>候选者 B</strong>。</li>
</ul>
</li>
<li>
<p><strong>产生歧义</strong></p>
<p>现在编译器陷入了困境。它手头有两个同样匹配的候选函数：</p>
<ul>
<li><strong>候选者 A</strong>: <code>fmt::format</code> (通过 <code>using</code> 声明找到)</li>
<li><strong>候选者 B</strong>: <code>std::format</code> (通过 ADL 在参数的命名空间里找到)</li>
</ul>
<p><code>using</code> 声明只是将一个名字引入当前作用域，它并**没有足够的“特权”**去压制一个通过 ADL 找到的同样优秀的候选者。因为两个函数都能完美处理你传入的参数，编译器无法做出选择，所以它只能放弃并报告“调用是模糊的 (ambiguous)”。</p>
</li>
</ol>
<h3 id="结论与最终解决方案-">结论与最终解决方案 ✅</h3>
<p>这个 C++ 的特性意味着，只要你的函数参数中包含了 <code>std</code> 命名空间里的类型（如 <code>std::string</code>, <code>std::vector</code> 等），ADL 就有可能被触发，从而把 <code>std</code> 里的函数（如 <code>std::format</code>, <code>std::to_string</code> 等）也拉入候选列表，造成意想不到的冲突。</p>
<p>因此，唯一能 100% 消除歧义、让编译器别无选择的方法，就是使用<strong>显式的命名空间限定</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// 这样做，是在直接告诉编译器：“别去猜了，我就是要调用 fmt 命名空间里的这个 format！”
</span></span></span><span class="line"><span class="cl"><span class="c1">// 这会完全绕过 ADL 和其他查找规则，直达目标。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(...);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="runing-arguments">Runing Arguments</h1>
<p>执行仿真需要传递一些参数，命令模板如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">{</span>ASTRA_SIM_BIN<span class="o">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --workload-configuration<span class="o">=</span><span class="si">${</span><span class="nv">WORKLOAD_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --system-configuration<span class="o">=</span><span class="si">${</span><span class="nv">SYSTEM_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --network-configuration<span class="o">=</span><span class="si">${</span><span class="nv">NETWORK_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --remote-memory-configuration<span class="o">=</span><span class="si">${</span><span class="nv">REMOTE_MEMORY_CONFIG</span><span class="si">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="workload_config">WORKLOAD_CONFIG</h2>
<p>astra-sim 使用的是 Chakra (Execution Trace) 作为 workload 层的输入。将 chakra 作为 python package 安装后有几个命令通过 pyproject.toml 对应到 python函数。</p>
<details class="custom-details">
    <summary class="custom-summary">Explanation of toml file</summary>
    <div><p><code>pyproject.toml</code> 是一个标准化的配置文件，用于定义 Python 项目的元数据、依赖关系以及构建和开发工具的配置。</p>
<hr>
<ol>
<li><code>[build-system]</code> 构建系统配置，这部分定义了如何构建你的 Python 包。</li>
</ol>
<ul>
<li><code>**requires**</code>: 列出了构建项目本身所必需的包。这些是构建环境的依赖，而不是你代码运行时的依赖。
<ul>
<li><code>setuptools</code>, <code>setuptools-grpc</code>: 表明此项目使用 <code>setuptools</code> 作为其构建工具，并需要 <code>setuptools-grpc</code> 插件。</li>
</ul>
</li>
<li><code>**build-backend**</code>: 指定了构建工具中实际执行构建过程的 Python 对象（入口点）。
<ul>
<li><code>setuptools.build_meta</code>: 这是 <code>setuptools</code> 提供的标准构建后端。</li>
</ul>
</li>
</ul>
<hr>
<ol start="2">
<li><code>[project]</code>：这部分包含了项目的基本信息，这些信息会展示在 PyPI (Python Package Index) 上。</li>
</ol>
<ul>
<li><code>**name**</code>: 包的名称，即 <code>pip install chakra</code> 中的 <code>chakra</code>。</li>
<li><code>**requires-python**</code>: 运行此包所需的最低 Python 版本，这里是 <code>3.7</code> 或更高。</li>
<li><code>**version**</code>: 当前包的版本号。</li>
<li><code>**readme**</code>: 指向一个文件，该文件的内容将作为项目在 PyPI 上的详细描述。</li>
<li><code>**license**</code>: 指向包含许可证信息的文件。</li>
<li><code>**authors**</code>：项目的作者信息。</li>
<li><code>**dependencies**</code>: <strong>项目运行时的依赖项</strong>。当用户 <code>pip install chakra</code> 时，这些包也会被一并安装。
<ul>
<li><code>protobuf==5.*</code>: 需要版本为 5.x 的 <code>protobuf</code> 库。</li>
<li><code>graphviz</code>, <code>networkx</code>, <code>pydot</code>: 其他标准的第三方库依赖。</li>
<li><code>HolisticTraceAnalysis @ git+...</code>: 这是一个特殊的依赖。它直接从 GitHub 仓库的一个<strong>特定 commit</strong> (<code>d731cc...</code>) 来安装。这确保了项目依赖于一个稳定且不会意外变动的版本。</li>
</ul>
</li>
</ul>
<hr>
<ol start="3">
<li><code>[project.urls]</code>：项目相关链接，这些链接会显示在 PyPI 页面的侧边栏，为用户提供更多信息的入口。</li>
</ol>
<ul>
<li><code>**Homepage**</code>, <code>**Documentation**</code>, <code>**Repository**</code>: 分别指向项目主页、文档和代码仓库的 URL。</li>
</ul>
<hr>
<ol start="4">
<li><code>[tool.setuptools]</code>：这部分是针对构建工具 <code>setuptools</code> 的详细配置。</li>
</ol>
<ul>
<li><code>**package-dir**</code>: 定义了 Python 包名与实际源代码目录之间的映射关系。
<ul>
<li>例如，<code>&quot;chakra.src.converter&quot; = &quot;src/converter&quot;</code> 表示当用户 <code>import chakra.src.converter</code> 时，Python 会从 <code>src/converter/</code> 目录下寻找代码。这使得项目可以使用 <code>src</code> 布局。</li>
</ul>
</li>
<li><code>**package-data**</code>: 指定需要包含在最终发布包中的非 Python 文件。
<ul>
<li><code>&quot;chakra.schema.protobuf&quot; = [&quot;et_def.proto&quot;]</code>: 表示需要将 <code>et_def.proto</code> 这个文件打包到 <code>chakra.schema.protobuf</code> 这个包里。</li>
</ul>
</li>
</ul>
<hr>
<ol start="5">
<li><code>[project.scripts]</code>：这部分定义了在安装包时应创建的命令行工具。</li>
</ol>
<ul>
<li><code>**chakra_converter = &quot;chakra.src.converter.converter:main&quot;**</code>: 这行配置意味着，当用户安装此包后，他们可以在终端中直接运行 <code>chakra_converter</code> 命令。执行此命令时，系统会调用 <code>chakra.src.converter.converter</code> 模块中的 <code>main</code> 函数。</li>
</ul>
<hr>
<ol start="6">
<li><code>[tool.ruff]</code>：这部分是用于配置 <code>Ruff</code> 高性能代码检查（Linter）和格式化（Formatter）工具。</li>
</ol>
<ul>
<li><code>**target-version**</code>, <code>**line-length**</code>, <code>**exclude**</code>: 基本配置，如目标 Python 版本、每行最大长度和要排除检查的文件。</li>
<li><code>**[tool.ruff.lint]**</code>: Linter 的具体配置。
<ul>
<li><code>**select**</code>: 启用一系列代码规则集（例如 <code>D</code> 代表文档字符串 <code>pydocstyle</code>，<code>I</code> 代表导入排序 <code>isort</code>）。</li>
<li><code>**ignore**</code>: 全局禁用的特定规则。注释中解释了忽略它们的原因（例如，规则冲突或待办事项）。</li>
<li><code>**per-file-ignores**</code>: 针对特定文件或目录禁用规则。例如，<code>&quot;**/tests/*&quot; = [&quot;D&quot;]</code> 表示在所有测试文件中都禁用文档字符串检查。</li>
</ul>
</li>
<li><code>**[tool.ruff.format]**</code>: 格式化器的配置，如使用空格作为缩进风格。</li>
</ul>
<hr>
<ol start="7">
<li><code>[tool.pyright]</code>：这部分配置了 <code>Pyright</code>，一个由微软开发的静态类型检查工具。</li>
</ol>
<ul>
<li><code>**typeCheckingMode**</code>: 类型检查的严格程度，这里是 <code>basic</code>（基础模式）。</li>
<li><code>**exclude**</code>：在进行类型检查时要忽略的文件和目录。</li>
<li><code>**report...**</code>：关闭特定的错误或警告报告。</li>
</ul>
<hr>
<ol start="8">
<li><code>[tool.vulture]</code>：这部分配置了 <code>Vulture</code>，一个用于发现项目中未使用（&ldquo;死&rdquo;）代码的工具。</li>
</ol>
<ul>
<li><code>**ignore_names**</code>: 让 Vulture 忽略某些特定的变量名或函数名，即使它们看起来未使用。</li>
<li><code>**min_confidence**</code>: 设置报告问题的最低置信度阈值。<code>100</code> 表示只有在 Vulture 100% 确定代码是无用的时候才会报告，这可以有效减少误报。</li>
</ul></div>
</details><br>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-toml" data-lang="toml"><span class="line"><span class="cl"><span class="p">[</span><span class="nx">project</span><span class="p">.</span><span class="nx">scripts</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_converter</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.converter.converter:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_generator</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.generator.generator:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_jsonizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.jsonizer.jsonizer:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_timeline_visualizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.timeline_visualizer.timeline_visualizer:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_trace_link</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.trace_link.trace_link:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_visualizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.visualizer.visualizer:main&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="generate-execution-trace">Generate Execution Trace</h3>
<p>ASTRA-sim 的 ET 命名格式为 <code>{path prefix/trace name}.{npu_id}.et</code>. Chakra ET 的获取流程如下图所示<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<ol>
<li>Collect ET from PyTorch
<ul>
<li>PyTorch ET 负责 CPU 算子，并明确表示它们之间的依赖关系。</li>
<li>Kineto Trace 编码 GPU 算子及其开始和结束时间。</li>
</ul>
</li>
<li>Merge Trace by <code>chkra_trace_link</code>：将它们合并为一个 PyTorch ET+. 该格式本质上遵循 PyTorch ET 的模式，但同时也编码了 GPU 操作符及其依赖关系。</li>
<li>Convert to Chakra ET by <code>chakra_converter</code>

<figure class="post-figure">
    <a href="https://private-user-images.githubusercontent.com/7621438/294028976-67228699-cec5-4a4d-b03e-e76647a80ce8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1NDQxNDUsIm5iZiI6MTc0OTU0Mzg0NSwicGF0aCI6Ii83NjIxNDM4LzI5NDAyODk3Ni02NzIyODY5OS1jZWM1LTRhNGQtYjAzZS1lNzY2NDdhODBjZTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MTBUMDgyNDA1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWE4NzAyMGQ0NWQ0MDA2MzIzMmY1MmNhYWU4YWUzNTJiNjI3OTAzZDk2ZDU3NDIwMWJhZTFlMjNjZDhjN2JmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.-DDH2mackHVASqoCbmyvN2xl8vZemaa73OiLmBER1o0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://private-user-images.githubusercontent.com/7621438/294028976-67228699-cec5-4a4d-b03e-e76647a80ce8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1NDQxNDUsIm5iZiI6MTc0OTU0Mzg0NSwicGF0aCI6Ii83NjIxNDM4LzI5NDAyODk3Ni02NzIyODY5OS1jZWM1LTRhNGQtYjAzZS1lNzY2NDdhODBjZTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MTBUMDgyNDA1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWE4NzAyMGQ0NWQ0MDA2MzIzMmY1MmNhYWU4YWUzNTJiNjI3OTAzZDk2ZDU3NDIwMWJhZTFlMjNjZDhjN2JmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.-DDH2mackHVASqoCbmyvN2xl8vZemaa73OiLmBER1o0" alt="Overview of Trace Collection">
    </a><figcaption>Overview of Trace Collection</figcaption></figure></li>
</ol>
<p>具体的教程和例子可以在 <a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#3-from-raw-traces-to-chakra-a-step-by-step-conversion-guide">Conversion Guide</a> 和 <a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#3-from-raw-traces-to-chakra-a-step-by-step-conversion-guide">Practical Example</a> 找到。</p>
<h3 id="using-et-converter">Using ET Converter</h3>
<p>可以将 astra-sim 1.0 的文本输入转换成 Chakra ET.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> ./extern/graph_frontend/chakra/
</span></span><span class="line"><span class="cl">pip3 install .
</span></span><span class="line"><span class="cl">chakra_converter Text <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --input ../../../examples/text_converter/text_workloads/Resnet50_DataParallel.txt <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --output ../../../examples/text_converter/text_workloads/Resnet50_DataParallel <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --num-npus <span class="m">8</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --num-passes <span class="m">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>workload 文本格式要求如下，其中通信大小单位是字节，计算时间以周期数表示。</p>
<ul>
<li>第一行：(DATA/HYBRID_TRANSFORMER/HYBRID_DLRM)
<ul>
<li>该行指定训练循环的并行化类型。DATA 表示纯数据并行方法，HYBRID_TRANSFORMER 表示专为 Transformer DNN 网络设计的混合并行方法，而 HYBRID_DLRM 表示专为 DLRM DNN 网络优化的混合并行方法。</li>
</ul>
</li>
<li>第二行：(int)
<ul>
<li>该行表示 DNN 的层数。</li>
</ul>
</li>
<li>后续行：每行描述一层。层的描述格式如下：
<ul>
<li>{(string: 层名称)</li>
<li>(int: 保留变量)</li>
<li>(int: 前向计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 前向通信类型)</li>
<li>(int: 前向通信大小)</li>
<li>(int: 输入梯度计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 输入梯度通信类型)</li>
<li>(int: 输入梯度通信大小)</li>
<li>(int: 权重梯度计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 权重梯度通信类型)</li>
<li>(int: 权重梯度通信大小)</li>
<li>(集合通信完成后，权重/输入/输出更新的延迟)}`</li>
</ul>
</li>
</ul>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>每一层的参数写要在同一行！！！</p></div>

<h3 id="enable-communicator-groups">Enable Communicator Groups</h3>
<p>astra-sim 2.0 支持<a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html">通信组</a>。可以通过指定 <code>--comm-group-configuration</code> JSON 文件来指定，默认只有一个通信组。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// The first/second communicator group, with ID 0/1, includes GPU IDs from 0-3/4-7. 
</span></span></span><span class="line"><span class="cl"><span class="c1">//   &#34;0&#34;: [0, 1, 2, 3],
</span></span></span><span class="line"><span class="cl"><span class="c1">//   &#34;1&#34;: [4, 5, 6, 7]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="nt">&#34;&lt;communicator_group_id&gt;&#34;</span> <span class="p">:</span> <span class="p">[</span><span class="err">gpu_ids</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="system_config">SYSTEM_CONFIG</h2>
<h1 id="system-layer">System Layer</h1>
<p>Workload 层会遍历 Chakra ET 中的节点，并为每个节点所指代的操作发出相应的命令。System 层接收这些命令，并将其转换为适合网络、计算或内存后端的格式，从而正确模拟操作。根据操作的类型，系统层的行为会有所不同，具体如下：</p>
<ul>
<li>计算操作：向计算后端发出调用，以模拟操作的持续时间。</li>
<li>内存操作：  内存</li>
<li>通信操作：将集合通信分解为点对点的发送和接收消息，并向网络后端发出“发送”或“接收”调用，以模拟消息的传输过程。</li>
</ul>
<h2 id="collective-scheduler">Collective Scheduler</h2>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-sim-docs/_images/system_overview_queue.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-sim-docs/_images/system_overview_queue.svg" alt="Collective Scheduler">
    </a><figcaption>Collective Scheduler</figcaption></figure></p>
<p>每个队列都有许多 <code>StreamBaseline</code> 对象 (图中右上角)，代表了整个集合通信的流程，<code>phase_to_go</code> 是一个用于表示这些阶段的队列，<code>my_current_phase</code> 是指向当前执行阶段的指针。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">StreamBaseline</span> <span class="o">:</span> <span class="k">public</span> <span class="n">BaseStream</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">StreamBaseline</span><span class="p">(</span><span class="n">Sys</span><span class="o">*</span> <span class="n">owner</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">DataSet</span><span class="o">*</span> <span class="n">dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="kt">int</span> <span class="n">stream_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">list</span><span class="o">&lt;</span><span class="n">CollectivePhase</span><span class="o">&gt;</span> <span class="n">phases_to_go</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="kt">int</span> <span class="n">priority</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// my_current_phase[CollectivePhase] is defined in BaseStream
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">void</span> <span class="nf">init</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">call</span><span class="p">(</span><span class="n">EventType</span> <span class="n">event</span><span class="p">,</span> <span class="n">CallData</span><span class="o">*</span> <span class="n">data</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">consume</span><span class="p">(</span><span class="n">RecvPacketEventHandlerData</span><span class="o">*</span> <span class="n">message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于每个 stream <code>proceed_to_next_vnet_baseline</code> (astra-sim/system/Sys.cc) 用于推进通信阶段并且负责在队列之间移动 stream 对象。以下几种情况会调用该函数：</p>
<ol>
<li>stream 第一次被移动出 ready_list 并且将被插入到 <code>active_streams</code>.</li>
<li>stream 完成了一个通信阶段并且等待下一个阶段。</li>
<li>stream 完成了所有的通信阶段。</li>
</ol>
<p>(2-1) 到 (2-5) 描述了该函数的行为</p>
<ol>
<li>
<p>查看当前持有 stream 的队列: 从队列中删除 <code>StreamBaseline</code> 对象 (流的完成顺序可能与它们开始执行的顺序不同)。</p>
</li>
<li>
<p>修改 <code>StreamBaseline</code> 对象: 已完成的集合通信阶段从 <code>phases_to_go</code> 中弹出，<code>my_current_phase</code> 现在指向下一个待执行的阶段。</p>
</li>
<li>
<p>使用 <code>insert_stream</code> 将 <code>StreamBaseline</code> 对象插入到下一个队列中。</p>
</li>
<li>
<p>调用函数 <code>notify_stream_removed</code> 函数查看前一个队列的头部。 <code>stream_pointer</code> 指向队列中第一个未运行的 stream (标记为蓝色)。该函数通过调用 <code>StreamBaseline::init()</code> 来启动 stream 的下一个阶段的执行。</p>
</li>
<li>
<p>使用 <code>notify_stream_added</code> 触发新队列头部 stream 的通信阶段执行。</p>
</li>
</ol>
<p>在其他情况下，<code>proceed_to_next_vnet_baseline</code> 会执行上述步骤的一部分。具体如下：</p>
<ol>
<li>
<p>刚从 <code>ready_list</code> 中移除：<br>
<code>proceed_to_next..</code> 会初始化 stream (1-2)，将其插入到第一个队列中 (1-3)，并触发该队列头部的流执行。</p>
</li>
<li>
<p>stream 完成：<br>
该函数会从之前的队列中删除 stream (3-1)，并触发之前队列头部的 stream 执行。此外，<code>StreamBaseline</code> 对象会被删除，并调用 <code>notify_stream_finished</code>，以通知 <code>Sys</code> 对象 stream 已经结束 (3-6)</p>
</li>
</ol>
<h2 id="collective-implementation">Collective Implementation</h2>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-sim-docs/_images/coll_implementation.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-sim-docs/_images/coll_implementation.svg" alt="Overview of Collective Implementation">
    </a><figcaption>Overview of Collective Implementation</figcaption></figure>
模拟器将集体通信分解为发送和接收消息的方式有两种。目前最常用的方法是模拟器实现一组预定义的常见算法 (例如 Ring、DoubleBinary、HalvingDoubling 等)。这种“原生”实现逻辑位于模拟器的代码库中，允许用户快速探索一组预定义的算法。</p>
<p>自 2024 年 8 月以来，ASTRA-sim 支持了一种新的集合通信算法表示方式。System 层通过暴露一个集体 API，可以接收任意集体算法的定义。</p>
<p>这两种方法都是对 <code>CollectivePhase::Algorithm</code> 对象的实现，该对象是 System 层中的调度单元. <a href="https://github.com/astra-sim/astra-sim/blob/15a4334ade00fe1040fd00495cd13fd1ea5177e4/astra-sim/system/Sys.cc#L1037">generate_collective_phase</a> 会根据不同的算法在创建 <a href="https://github.com/astra-sim/astra-sim/blob/15a4334ade00fe1040fd00495cd13fd1ea5177e4/astra-sim/system/CollectivePhase.hh#L17">CollectivePhase</a> 的时候传入对应的 Algorithm.</p>
<h3 id="astra-sim-native-implementation">ASTRA-Sim Native Implementation</h3>
<p>相关的实现都位于<a href="https://github.com/astra-sim/astra-sim/tree/master/astra-sim/system/collective">该文件夹</a>下, naive 实现的限制是当需要模拟一个新的集合通信算法时算法，必须实现整个集合？随着不规则集合通信 (如 TACOS(Topology Aware CollectiveS), MSCCLang(基于 DSL)) 中工作的增加，快速模拟和迭代各种算法的需求变得越来越多。</p>
<h3 id="chakra-based-arbitrary-definition-through-collective-api">Chakra Based Arbitrary Definition Through Collective API</h3>
<p>因此一个新的 AP来接受任何集合通信算法的定义，而不局限于预定义的规则通信模式。对于通信表示，使用 Chakra ET 模式作为单独的图。将集合通信算法表示为Chakra ET 模式中 COMM_SEND，COMM_RECV 节点的图。也就是说，System 层不是将集合通信分解为发送和接收消息，而是简单地遵循 Chakra 图中已经表示的分解。由于已经使用 Chakra ET 来表示 workload，使用 Chakra ET 来额外定义集合通信算法提供了一种轻松简单的方式来遍历整个图。</p>
<p>如上图所示当 workload 层发出 AllReduce 集体操作时，System 层不会运行模拟器代码库中已有的原生实现逻辑，而是会遍历通过 API 提供的 Chakra ET，该 ET 表示集合通信算法。需要注意 workload Chakra 图和集合通信算法的 Chakra 图是解耦的，并通过不同的输入点提供。最终，asytra-sim 模拟器会将通信节点替换为集体实现。</p>
<h2 id="input-files-for-collective-api">Input Files for Collective API</h2>
<h3 id="astra-sim-native">ASTRA-sim Native</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="s2">&#34;active-chunks-per-dimension&#34;</span><span class="err">:</span> <span class="mi">1</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-reduce-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-gather-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">,</span> <span class="s2">&#34;doubleBinaryTree&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-to-all-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">,</span> <span class="s2">&#34;doubleBinaryTree&#34;</span><span class="p">,</span> <span class="s2">&#34;halvingDoubling&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span></code></pre></div><p><code>all-*-implementation</code> 指定了模拟器将如何将给定的集合通信分解为发送和接收消息。All-Gather 操作列表中的两个条目表示模拟器将按两个维度分解 ——第一个维度使用 Ring 算法，第二个维度使用 doubleBinaryTree 算法。</p>
<blockquote class="quote"><p>Native Implementation Requires That the Dimensions for Collective Algorithms Are Same Across All Collectives.</p></blockquote>
<div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p><strong>Native 实现要求所有集体操作的维度必须相同</strong>。换句话说，如果一个集合通信算法被定义为二维的，那么其他集合通信算法也必须是二维操作。上述只是一个例子。</p></div>

<h3 id="collective-api">Collective API</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="s2">&#34;active-chunks-per-dimension&#34;</span><span class="err">:</span> <span class="mi">1</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;all-reduce-implementation-chakra&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;/app/hoti2024/demo5/inputs/custom_ring&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span></code></pre></div><p>需要注意这里要使用 <code>all-*-implementation-chakra</code>，而不是 <code>all-*-implementation</code>. 另外  Chakra ET 文件与传递给 workload 层的文件是不同的，每一项的值是 Chakra ET 文件的绝对路径，不包括最后的 <code>{rank}.et</code> 字符串 (类似于 Workload 层输入)。此外，即使有许多维度，列表也只接受一个值。这是因为跨维度通信的概念已经包含在 ET 中。</p>
<div class="github">
    <div class="github_bar">
        <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" viewBox="0 0 50 50"><path d="M17.791,46.836C18.502,46.53,19,45.823,19,45v-5.4c0-0.197,0.016-0.402,0.041-0.61C19.027,38.994,19.014,38.997,19,39 c0,0-3,0-3.6,0c-1.5,0-2.8-0.6-3.4-1.8c-0.7-1.3-1-3.5-2.8-4.7C8.9,32.3,9.1,32,9.7,32c0.6,0.1,1.9,0.9,2.7,2c0.9,1.1,1.8,2,3.4,2 c2.487,0,3.82-0.125,4.622-0.555C21.356,34.056,22.649,33,24,33v-0.025c-5.668-0.182-9.289-2.066-10.975-4.975 c-3.665,0.042-6.856,0.405-8.677,0.707c-0.058-0.327-0.108-0.656-0.151-0.987c1.797-0.296,4.843-0.647,8.345-0.714 c-0.112-0.276-0.209-0.559-0.291-0.849c-3.511-0.178-6.541-0.039-8.187,0.097c-0.02-0.332-0.047-0.663-0.051-0.999 c1.649-0.135,4.597-0.27,8.018-0.111c-0.079-0.5-0.13-1.011-0.13-1.543c0-1.7,0.6-3.5,1.7-5c-0.5-1.7-1.2-5.3,0.2-6.6 c2.7,0,4.6,1.3,5.5,2.1C21,13.4,22.9,13,25,13s4,0.4,5.6,1.1c0.9-0.8,2.8-2.1,5.5-2.1c1.5,1.4,0.7,5,0.2,6.6c1.1,1.5,1.7,3.2,1.6,5 c0,0.484-0.045,0.951-0.11,1.409c3.499-0.172,6.527-0.034,8.204,0.102c-0.002,0.337-0.033,0.666-0.051,0.999 c-1.671-0.138-4.775-0.28-8.359-0.089c-0.089,0.336-0.197,0.663-0.325,0.98c3.546,0.046,6.665,0.389,8.548,0.689 c-0.043,0.332-0.093,0.661-0.151,0.987c-1.912-0.306-5.171-0.664-8.879-0.682C35.112,30.873,31.557,32.75,26,32.969V33 c2.6,0,5,3.9,5,6.6V45c0,0.823,0.498,1.53,1.209,1.836C41.37,43.804,48,35.164,48,25C48,12.318,37.683,2,25,2S2,12.318,2,25 C2,35.164,8.63,43.804,17.791,46.836z"></path></svg>
        <a class="github_name" href="https://github.com/astra-sim/collectiveapi" target="_blank">Collective API</a>
    </div>
    <div class="github_description">参考该仓库实现</div>
    <div class="github_language">
        
    </div>
</div>

<h1 id="network-backend">Network Backend</h1>
<h2 id="analytical-network-backend">Analytical Network Backend</h2>
<p>Analytical Network 模拟器通过数学方程模拟所有网络行为。因此，该后端最适合于大规模分布式平台的建模和仿真。目前支持两种分析模式</p>
<ul>
<li>congestion_<strong>unaware</strong> analytical network simulator</li>
<li>congestion_<strong>aware</strong> analytical network simulator</li>
</ul>
<hr>
<ul>
<li>T<strong>Topology</strong></li>
</ul>
<p>Analytical Network 支持三种拓扑结构: Ring, FullConnected, Switch. 并且可以堆叠来表示多维网络。</p>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/network-building-blocks.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/network-building-blocks.svg" alt="Basic Network Building Block">
    </a><figcaption>Basic Network Building Block</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">topology</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="l">Ring, Switch ] </span><span class="w"> </span><span class="c"># 2D topology</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">topology</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="l">Ring, Ring, Ring ] </span><span class="w"> </span><span class="c"># 3D topology</span><span class="w">
</span></span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/multidim-network-example.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/multidim-network-example.svg" alt="Example of 2D &amp; 3D Topologies">
    </a><figcaption>Example of 2D &amp; 3D Topologies</figcaption></figure></p>
<hr>
<ul>
<li><strong>NPUs Count</strong></li>
</ul>
<p>指定了每个维度上的设备数目</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 5 NPUs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 4 × 2 = 8 NPUs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 4 × 2 × 2 = 16 NPUs</span><span class="w">
</span></span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/npus-count-example.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/npus-count-example.svg" alt="NPUs Count Example">
    </a><figcaption>NPUs Count Example</figcaption></figure></p>
<hr>
<ul>
<li><strong>Bandwidth</strong> &amp; <strong>Latency</strong></li>
</ul>
<p><code>latency</code> 定义了每条单向链路的延迟 (ns).
<code>bandwidth</code> 定义了每条单向链路的带宽 (GB/s).</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>$1 GB = 2^{30} B$ and $1 s = 10^9 ns$</p></div>

<h2 id="ns3-backend">ns3 backend</h2>
<p>下面是用 ns3 后端进行方针的一个执行命令。这里使用了 <code>--network-backend</code> 和 <code>--logical-topology</code> 这两个参数。需要说明的是，Analytical Backend 中仅使用了-<code>-network-backend</code> 参数，这是因为分析型后端的逻辑拓扑与物理拓扑是相同的，而 ns3 则允许我们将逻辑拓扑与物理拓扑分离。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">   <span class="c1"># {NS3_DIR} is the directory of the ns-3 backend. That is, &#39;{ASTRA_SIM_ROOT_DIRECTORY}/extern/network_backend/ns-3&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">NS3_DIR</span><span class="si">}</span><span class="s2">/build/scratch&#34;</span>
</span></span><span class="line"><span class="cl">    ./ns3.42-AstraSimNetwork-default <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --workload-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../extern/graph_frontend/chakra/one_comm_coll_node_allgather  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --system-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/system/Switch.json  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --network-configuration<span class="o">=</span><span class="s2">&#34;../../../ns-3/scratch/config/config.txt&#34;</span>   <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --remote-memory-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/remote_memory/analytical/no_memory_expansion.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --logical-topology-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/network/ns3/sample_8nodes_1D.json   <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --comm-group-configuration<span class="o">=</span><span class="se">\&#34;</span>empty<span class="se">\&#34;</span>
</span></span></code></pre></div><hr>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#2-overview-of-trace-collection-and-simulation-methodology">Overview of Trace Collection</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Transformer Family</title>
      <link>http://localhost:57770/blogs/transformerfamily/</link>
      <pubDate>Sat, 07 Jun 2025 21:24:13 +0800</pubDate>
      <guid>http://localhost:57770/blogs/transformerfamily/</guid>
      <description>Introduction of Transformer Family</description>
      <content:encoded><![CDATA[<h1 id="origin-of-transformer">Origin of Transformer</h1>
<p>Transformer 由谷歌研于 2017 年在一篇名为 <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> 的论文中提出。与 RNN 的输入仅为一个 token 不同，Transformer 一次性可以输入一整个完整的序列。总体结构如下图所示，包含一个 Encoder 和一个 Decoder.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBd293bc1a46904e1af31ce993b83c68f1?method=download&amp;shareKey=47cf357e488e7da5483a1b98f3257ab1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBd293bc1a46904e1af31ce993b83c68f1?method=download&amp;shareKey=47cf357e488e7da5483a1b98f3257ab1" alt="Transformers Architecture">
    </a><figcaption>Transformers Architecture</figcaption></figure></p>
<h2 id="embedding">Embedding</h2>
<p>Embedding 是一种将离散的、稀疏的输入 (如词语、字符、类别标签&hellip;) 转换为连续的、密集的向量表示的技术，核心是通过一个映射函数将离散的输入符号 (如单词) 映射到一个低维向量空间中。假设我们有一个包含 V 个单词的 Vocabulary，维度为 d，那么 Embedding Matrix 将是一个大小为 V×d 的矩阵，其中每一行是一个单词的向量表示。通过嵌入层，输入的词索引 (通常是整数) 就会被映射到该矩阵的对应行，从而得到词的向量表示。常见的预训练词嵌入方法包括：</p>
<ul>
<li>Word2Vec：通过上下文预测词语的方式学习词向量。</li>
<li>GloVe：通过统计词共现信息来学习词向量。</li>
<li>FastText：考虑了子词信息的词嵌入方法，能更好地处理词形变化。</li>
</ul>
<p>在 PyTorch 和 TensorFlow 等框架中，通常有专门的 Embedding 层，Hugging Face 也有 tokenizer 将句子划分成单词并转换成对应的索引：</p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Positional Encoding 作用是为输入的序列中的每个元素提供位置信息。由于 Transformer 架构并没有使用递归或卷积结构，本身无法捕捉输入序列中元素的相对位置关系，因此需要通过位置编码来显式地引入这些位置信息。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>Transformer 的主要优势是通过 Self-Attention 并行处理序列中的每个元素，但是这也意味着它没有自带顺序感知能力，它并不会自动知道一个单词是在句子的开头还是结尾，因此需要额外的机制来编码每个元素在序列中的位置。</p>
<p>位置编码 通过将每个单词的位置信息 (即它在序列中的位置) 编码为一个向量，并将该向量添加到单词的嵌入表示中，从而让模型能够感知每个元素的相对或绝对位置。</p></div>

<p>经典的 Transformer 位置编码使用 正弦和余弦函数的组合，为每个位置生成的向量在不同维度上具有不同的周期性，这能够捕捉到不同级别的相对位置关系。假设输入的序列中有 N 个单词，每个单词的嵌入维度为 d，那么 Positional Encodin(PE) 的计算公式如下:</p>
$$
\begin{aligned}
&PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{\frac{2i}d}}\right)\\
&PE_{(pos,2i+1)}=\cos\left(\frac{pos}{10000^{\frac{2i}d}}\right)
\end{aligned}
$$<p>其中：</p>
<ul>
<li>pos 是单词在序列中的位置索引 (位置从 0 开始).</li>
<li>i 是位置编码的维度索引，表示该位置编码向量中的第 i 个元素。</li>
<li>d 是 Embedding 的维度</li>
</ul>
<p>这些位置编码与单词的词嵌入 (Word Embedding) 相加，最终形成输入模型的向量表示。</p>
<h2 id="masked-multi-head-attention">(Masked) Multi-Head Attention</h2>
<p>Multi-Head Attention (MHA) 的目的是通过并行地计算多个注意力头 (Attention Head)，从多个子空间中学习输入序列的不同表示。经过 Word Embedding 后的输入 X 形状为 Nxd. 计算步骤如下</p>
<ol>
<li>
<p>通过学习的变换矩阵将 X 映射到查询 (Q)、键 (K) 和值 (V) 空间。
</p>
$$
\begin{aligned}&Q=XW^{Q}\\&K=XW^{K}\\&V=XW^{V}\end{aligned}
$$<p>
其中 $W^{Q},W^{K}\in\mathbb{R}d_{model}\times d_{k},W^{Q},W^{V}\in\mathbb{R}d_{model}\times d_{v}$</p>
</li>
<li>
<p>根据 QKV 计算 Attention
每个查询向量会与所有键向量进行相似度计算 (一般采用 scaled inner product)，从而获得权重，然后利用这些权重对所有值向量进行加权求和。</p>
</li>
</ol>
$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p><br>在多头注意力中，为了增加模型的表达能力，通常将 Q、K 和 V 通过多个不同的线性变换矩阵进行多次计算，得到多个注意力头 (Attention Heads). 每个头的计算是独立的，但它们的结果会在最后进行拼接并经过线性变换。最终的 Multi-Head Attention 公式为：</p>
$$
\text{MultiHead}(Q,K,V)=\text{Concat}(head_1,head_2,\ldots,head_h)W^O
$$<p><br>每个头 $head_i$ 计算公式为</p>
$$
\text{MultiHead}(Q,K,V)=\text{Concat}(head_1,head_2,\ldots,head_h)W^O
$$<p><br>这里的 $W^{Q}_{i},W^{K}_{i},W^{V}_{i}$ 是为每个头学习到的不同权重矩阵，$W^O$ 是输出的线性变换矩阵。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB85e0bf86b5d9f2c649bbc3f08c03d203?method=download&amp;shareKey=b5e662d324237709f786beb08c27b774" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB85e0bf86b5d9f2c649bbc3f08c03d203?method=download&amp;shareKey=b5e662d324237709f786beb08c27b774" alt="Multi-Head Attention">
    </a><figcaption>Multi-Head Attention</figcaption></figure></p>
<p>Decoder 中的 Masked MHA 确保模型只能在解码序列的当前位置及其之前的位置上操作，而不能 “看到” 将要生成的未来信息。与标准的 MHA 相同，注意力分数 $\mathrm{Attention Scores}=\frac{QK^T}{\sqrt{d_k}}$ 是通过 Q 和 K 的点积计算得到的。计算完成后我们给其加上一个下三角元素 (包含主对角线) 为 0，上三角元素为 —∞ 的 mask，这样未来的信息经过 Softmax 后的权重为 0，被完全屏蔽。</p>
<h2 id="grouped-query-attentiongqa-multi-query-attention-mqa">Grouped Query Attention（GQA）&amp; Multi-query Attention (MQA)</h2>
<p><a href="https://arxiv.org/pdf/2305.13245">GQA</a> 将多个 Q 分成若干组，每一组共享相同的权重矩阵。这使得每组查询可以共同处理同一个 K 和 V，降低了计算量和内存需求。在 MHA 中，所有的头共享相同的输入 X，但使用不同的投影矩阵来生成 K 和 V. GQA 中 K 和 V 通常是对输入 X 进行一次性线性变换，并在所有同一分组中的 Q 共享。MQA 更为极端，所有的 Q 共享一个 K 和 V.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3c7dc003db55abf4b8a1ebeb4aabd667?method=download&amp;shareKey=f1570d975432b38d6f74742e9bb4cf6e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3c7dc003db55abf4b8a1ebeb4aabd667?method=download&amp;shareKey=f1570d975432b38d6f74742e9bb4cf6e" alt="Overview of MHA, GQA &amp; MQA">
    </a><figcaption>Overview of MHA, GQA &amp; MQA</figcaption></figure></p>
<h2 id="multi-head-cross-attention">Multi-Head Cross Attention</h2>
<p>Multi-Head Cross Attention 是 Transformer Decoder 中的一个核心组件。与 Self-Attention 不同，Cross Attention 负责将解码器的隐藏状态与编码器的输出上下文信息进行交互，允许解码器的每一个解码时间步的状态 <strong>查看整个编码器的输出</strong>。每个解码的时间步 t，Decoder 的隐藏状态作为 Q，Encoder 的输出作为 K 和 V，计算过程与 标准的 Self-Attention 相同。</p>
<h1 id="evolution-tree-of-transformer">Evolution Tree of Transformer</h1>
<p>后续的研究逐渐把 Encoder 和 Decoder 分离开来，形成 Encoder-Only 和 Decoder-Only 的模型。如下图所示</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa2db49ee75b563db2d846dab14947060?method=download&amp;shareKey=12514a3314f3bb4c5e30936c2d634650" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa2db49ee75b563db2d846dab14947060?method=download&amp;shareKey=12514a3314f3bb4c5e30936c2d634650" alt="Transformer Evolution Tree">
    </a><figcaption>Transformer Evolution Tree</figcaption></figure></p>
<h2 id="feed-forward-network">Feed Forward Network</h2>
<p>FFN 是一个两层的前馈全连接网络，中间有一个非线性激活函数。第一层全连接将 $d_model$ 映射到 $4d_model$ ，经过非线性激活函数后，第二层全连接再重新映射回 $d_model$.</p>
<h1 id="decoder-only-transformer">Decoder-Only Transformer</h1>
<p>Decoder-Only 删除了原先 Transformer Encoder 的部分以及 Encoder 和 Decoder 进行 Cross Attention 的部分。它具有三个必要的特征:</p>
<ol>
<li>在给定编码器输入作为上下文的情况下基于迄今为止生成的 token 自动回归预测下一个。</li>
<li>在评估对输入序列的 Q 时看不到未来值。这就是为什么仅解码器的模型通常被称为 Casual Language Model (CLM).</li>
<li>训练模型以在给定当前输入序列的情况下预测下一个 token. 这种训练方法与回归相结合，允许模型自回归生成任意长 (最高达输入序列的最大长度) 的序列。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa6c37075488053053efa01808163d0ba?method=download&amp;shareKey=5542015805dbda24ff7ab5dbf44a368b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa6c37075488053053efa01808163d0ba?method=download&amp;shareKey=5542015805dbda24ff7ab5dbf44a368b" alt="Decoder-only (left) and Encoder-only (right) Transformer Architectures">
    </a><figcaption>Decoder-only (left) and Encoder-only (right) Transformer Architectures</figcaption></figure></p>
<h1 id="llama-transformer-architecture">LLaMA Transformer Architecture</h1>
<p>LLaMA Transformer 结构如下，主要有以下变化</p>
<ol>
<li>使用 RoPE (Rotary Position Embedding) 替代传统的位置编码。</li>
<li>RMSNorm 替代 LayerNorm</li>
<li>引入 Gated Linear Unit (GLU)</li>
</ol>
<h2 id="rotary-position-embedding">Rotary Position Embedding</h2>
<p>传统的 Transformer 模型使用可学习的绝对位置编码 (如 sinusoidal position embedding)，但 RoPE 采用了旋转矩阵的思想，将位置编码与输入的 token 表示直接结合，而不依赖于额外的可学习参数。</p>
<p>输入向量的旋转角度为 $\theta(p,i)=p\cdot10000^{-2i/d}$. p 表示位置索引，i 表示维度索引，d 为向量的总维度。对于输入的 token 向量 x 中的每一对偶数和奇数维度 $(x_{2i},x_{2i+1})$，旋转操作可以用 2D 旋转矩阵表示为</p>
$$\begin{bmatrix}x_{2i}^{\prime}\\x_{2i+1}^{\prime}\end{bmatrix}=\begin{bmatrix}\cos(\theta)&-\sin(\theta)\\\sin(\theta)&\cos(\theta)\end{bmatrix}\cdot\begin{bmatrix}x_{2i}\\x_{2i+1}\end{bmatrix}$$<p><br>对于输入的 token 向量 $\mathbf{x}\left[x_{0},x_{1},x_{2},x_{3},\cdots,x_{d-1}\right]$, RoPE 将其两两一组配对，每一组都会与位置相关的旋转角度 θ 对应地应用旋转操作。这个过程的本质是对输入 token 的表示做了旋转变换，使得这些特征不仅依赖于输入的特征，还隐含了该 token 在序列中的位置。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf24aca24d7ff8bc2901ca4983cbf6c47?method=download&amp;shareKey=9ac054d415fe2e172bb8a719935d4793" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf24aca24d7ff8bc2901ca4983cbf6c47?method=download&amp;shareKey=9ac054d415fe2e172bb8a719935d4793" alt="RoPE">
    </a><figcaption>RoPE</figcaption></figure></p>
<h2 id="rmsnorm">RMSNorm</h2>
<p>RMSNorm 相对于 LayerNorm 去掉了均值计算，仅基于输入的均方根进行归一化 $\mathrm{RMSNorm}(\mathbf{x})=\frac{\mathbf{x}}{\mathrm{RMS}(\mathbf{x})+\epsilon}\cdot\gamma$</p>
<p>其中</p>
<ul>
<li>$\mathrm{RMS}(\mathbf{x})=\sqrt{\frac1d\sum_{i=1}^dx_i^2}$ 为输入的均方根。</li>
<li>$\gamma{:}$ 为可学习的缩放参数。</li>
<li>$\epsilon{:}$ 为防止除以 0 的小数。</li>
</ul>
<h2 id="silu">SiLU</h2>
<p>SiLU (Sigmoid Linear Unit) 是一种激活函数，也称为 Swish，其定义为输入 x 和 Sigmoid 函数输出的乘积。其定义为
</p>
$$\mathrm{SiLU}(x)=x\cdot\sigma(x)$$<p>
其中 $\sigma(x)=\frac1{1+e^{-x}}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB552a846c520bf2b5194c621e7b8e224e?method=download&amp;shareKey=519f3a1e4cce59da1895fa7bc2bcc842" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB552a846c520bf2b5194c621e7b8e224e?method=download&amp;shareKey=519f3a1e4cce59da1895fa7bc2bcc842" alt="SiLU">
    </a><figcaption>SiLU</figcaption></figure></p>
]]></content:encoded>
    </item>
    <item>
      <title>ZeRO, ZeRO-Offload, ZeRO-Infinity</title>
      <link>http://localhost:57770/blogs/zero/</link>
      <pubDate>Sat, 07 Jun 2025 21:11:32 +0800</pubDate>
      <guid>http://localhost:57770/blogs/zero/</guid>
      <description>Paper reading of ZeRO.</description>
      <content:encoded><![CDATA[<h1 id="zero">ZeRO</h1>
<p>Zero 用于优化内存，极大地提高了训练速度，同时增加了可以训练的模型大小。ZeRO 消除了数据和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度，能够以持续的高效率按设备数量等比例扩展可训练模型的大小。</p>
<h2 id="introduction">Introduction</h2>
<p>ZeRO 首先总结了下当前并行方法存在的问题</p>
<ul>
<li>Basic DP: 没有减少每个设备的内存，在 32GB 内存的 GPU 上训练超过 1.4B 参数的模型便会 OOM.</li>
<li>Model Parallelsim (MP): 切分了每一层的计算和激活到每个设备上，但引入了大量的通信 (前向和反向都需要 2xAll-Reduce)，因此扩展性差，通常只在一个节点内的高带宽连接的 GPU 中进行。在 DGX-2 节点训练 40B 参数的模型每个 V100 GPU 仅能达到硬件峰值的 5% 算力 (5T flops).</li>
</ul>
<p>模型状态通常占据了训练时的大部分内存，但 DP 在所有数据并行进程中保存一份模型状态，导致冗余内存消耗；而 MP 对这些状态进行切分以获得高内存效率，但通常会导致过于细粒度的计算和昂贵的通信，扩展效率较低。此外，这些方法都静态地维护整个训练过程所需的<strong>整个模型状态</strong>。</p>
<p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO-DP</a> 通过在数据并行过程中划分模型状态 (参数、梯度和优化器状态) 消除了数据并行过程中的内存冗余。</p>
<p>结论：如下图所示 ZeRO-DP 有三个主要的优化阶段，它们对应于优化器状态、梯度和参数的划分。
对于使用 FP16 的模型，内存占用包括参数 (FP16)、梯度 (FP16)、Adam 优化器状态 (动量 (FP32)，方差 (FP32) 以及更新后的参数 (FP32), 因此 K=12).</p>
<ol>
<li>优化器状态划分 (Pos) —— 内存减少 4 倍，需要对梯度进行 reduce-scatter，用各自的优化器状态更新梯度后进行 All-gather 使所有设备都有最新的梯度，通信量与数据并行性相同 (对 Loss 进行一次 All-reduce).</li>
<li>添加梯度划分  (Pos+g) &ndash; 内存减少 8 倍，每个设备需要将自己的梯度 scatter 到负责更新那部分参数的设备上，然后使用 Gather 将其他设备更新后的模型参数同步到自己上面，通信量与数据并行性相同。</li>
<li>添加参数划分 (Pos+g+p) &ndash; 内存减少与数据并行度 Nd 呈线性关系。通信量增加了50%，因为在前向/反向传播中需要每个设备需要额外广播自己存储的模型参数 <code>2*(N-1)/N*P</code>，反向传播时需要对发送梯度到对应的设备上 <code>(N-1)/N*P</code>.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBcfab82173b0f76eb5b3c8396e81e238a?method=download&amp;shareKey=1b8bb86256be5b15bec039beecee062b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBcfab82173b0f76eb5b3c8396e81e238a?method=download&amp;shareKey=1b8bb86256be5b15bec039beecee062b" alt="Memory Savings and Communication Volume for the 3-stage of ZeRO">
    </a><figcaption>Memory Savings and Communication Volume for the 3-stage of ZeRO</figcaption></figure></p>
<p>激活、临时缓冲区和不可用的内存片段会成为次要内存瓶颈。作者开发了 ZeRO-R 优化了这三个因素分别消耗的剩余内存。</p>
<ol>
<li>对于激活 (在前向传播中存储，反向传播中使用)，仅仅使用激活检查点是不够的。ZeRO-R 通过激活划分识别和删除现有 MP 方法中重复存储的激活，并且在适当时候将激活存储在 CPU 中。</li>
<li>ZeRO-R 定义了适当大小的临时缓冲区，以实现内存和计算效率的平衡。</li>
<li>由于不同张量的寿命存在差异，ZeRO-R 根据张量的不同生命周期主动管理内存，防止内存碎片。</li>
</ol>
<p>在某些情况下，MP 仍可以和 ZeRO 一起使用：i）当与 ZeRO-R 一起使用时，MP 可以减少超大模型的激活内存占用。ii）对于较小模型，当单独使用 DP 的 batchsize 太大而无法实现良好的收敛时，MP 也可以带来好处。</p>
<h2 id="where-did-all-the-memory-go">Where Did All the Memory Go?</h2>
<p>在模型训练期间，大部分内存被模型状态消耗 (优化器状态、梯度和参数). 除了这些模型状态之外，剩余的内存被激活、临时缓冲区和碎片内存所消耗，称之为剩余状态。</p>
<h3 id="model-states-optimizer-states-gradients-and-parameters">Model States: Optimizer States, Gradients and Parameters</h3>
<p>Adam 优化器需要存储两个优化器状态：时间平均动量和梯度方差来计算更新后的参数。此外，还需要有足够的内存来存储梯度和权重本身。</p>
<p><strong>混合精度训练 (Mixed-Precision Training)</strong> 中参数和激活以 fp16 格式存储并且在前向和反向传播中也使用 fp16 格式的权重和激活。Adam 优化器存储 fp32 格式的参数副本、动量和方差以保证更新的精度。</p>
<p>假设模型参数量为 ψ，模型参数需要占用 2ψ 字节的内存，反向传播中产生的 fp16 梯度需要占用 2ψ 字节的内存。Adam 优化器存储 fp32 格式的参数副本、动量和方差每个都需要占用 4ψ 字节的内存。因此训练时总共需要 16ψ 字节的内存，为存储模型参数的 8x.</p>
<h3 id="residual-memory-consumption">Residual Memory Consumption</h3>
<p>在训练过程中，<strong>激活</strong>会占用大量的内存。基于 transformer 的模型的激活内存占用与层数×隐藏维度×序列长度×批大小成正比。对于类似 GPT-2的结构，总激活约为 12×隐藏亮度×批大小×序列长度×变层数 (<code>QKV(h*3h) + O(h*h) + MLP(h*4h+4h*h)=12h*h</code>，没有考虑 mask). 激活重计算可以以 33% 的额外计算开销 (之前是一次前向，一次反向，反向因为需要对输入和参数都进行求导所以计算量是前向的两倍，现在多了一次前向) 换取接近原先激活大小平方级别的内存占用。</p>
<p>对于大型模型，用于存储中间结果的<strong>临时缓冲区</strong>会消耗大量内存。对梯度进行 All-Reduce 或梯度归一化计算等操作倾向于在操作之前将所有梯度融合到单个扁平缓冲区中，以提高吞吐量。</p>
<p><strong>碎片化内存</strong>会导致即使有足够的内存但没有足够大的连续块进行分配时的 OOM，作者观察到极端情况下在有 30% 剩余内存时也会产生 OOM.</p>
<h2 id="zero-insight-and-overview">ZeRO: Insight and Overview</h2>
<p>ZeRO有两组优化：ZeRO-DP 旨在减少模型状态的内存占用；ZeRO-R 旨在减少剩余内存消耗。</p>
<p>ZeRO-DP 基于三个关键见解：</p>
<ol>
<li>DP 比 MP 具有更好的扩展效率，因为 MP 减少了计算的粒度，同时也增加了通信开销。</li>
<li>DP 内存效率低下，因为模型状态被在所有数据并行进程中都存有一份。</li>
<li>DP 和 MP 都保留了整个训练过程中所需的所有模型状态，但并非所有状态在整个训练期间都需要。</li>
</ol>
<p>ZeRO-DP 划分模型状态，并使用动态通信调度利用模型状态的内在的暂时性，同时最小化通信量。</p>
<p>ZeRO-R 基于两个关键见解：</p>
<ol>
<li>MP 对模型状态进行切分，但通常需要重复存储激活。</li>
<li>对于GPT-2或更大的模型，算术强度 (每次迭代计算量与激活检查点数量的比值) 非常大 (≥10K)，并且随着隐藏维数的增加而线性增加，即使在带宽较低的情况下，也可以隐藏激活检查点的数据移动成本。</li>
</ol>
<p>ZeRO 通过跨 GPU 划分激活检查点来消除 MP 中的内存冗余，并根据需要使用 All-Gather 来重建；使用恒定大小的缓冲区来避免临时缓冲区随着模型大小的增加而爆炸；通过将激活检查点和梯度移动到预分配的连续内存缓冲区来执行动态内存碎片整理。</p>
<h2 id="deep-dive-into-zero-dp">Deep Dive into ZeRO-DP</h2>
<p>下表显示了逐渐切分 (1) 优化器状态、(2) 梯度和 (3) 参数冗余后的内存占用。称为ZeRO-DP的三个优化阶段：Pos， Pg和Pp，将在下面详细说明。</p>
<table border="1" cellspacing="0" cellpadding="5">
  <thead>
    <tr>
      <th rowspan="2">DP</th>
      <th colspan="3">7.5B Model (GB)</th>
      <th colspan="3">128B Model (GB)</th>
      <th colspan="3">1T Model (GB)</th>
    </tr>
    <tr>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>120</td>
      <td>120</td>
      <td>120</td>
      <td>2048</td>
      <td>2048</td>
      <td>2048</td>
      <td>16000</td>
      <td>16000</td>
      <td>16000</td>
    </tr>
    <tr>
      <td>4</td>
      <td>52.5</td>
      <td>41.3</td>
      <td><b>30</b></td>
      <td>896</td>
      <td>704</td>
      <td>512</td>
      <td>7000</td>
      <td>5500</td>
      <td>4000</td>
    </tr>
    <tr>
      <td>16</td>
      <td>35.6</td>
      <td><b>21.6</b></td>
      <td>7.5</td>
      <td>608</td>
      <td>368</td>
      <td>128</td>
      <td>4750</td>
      <td>2875</td>
      <td>1000</td>
    </tr>
    <tr>
      <td>64</td>
      <td><b>31.4</b></td>
      <td>16.6</td>
      <td>1.88</td>
      <td>536</td>
      <td>284</td>
      <td><b>32</b></td>
      <td>4187</td>
      <td>2218</td>
      <td>250</td>
    </tr>
    <tr>
      <td>256</td>
      <td>30.4</td>
      <td>15.4</td>
      <td>0.47</td>
      <td>518</td>
      <td>263</td>
      <td>8</td>
      <td>4046</td>
      <td>2054</td>
      <td>62.5</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>30.1</td>
      <td>15.1</td>
      <td>0.12</td>
      <td>513</td>
      <td>257</td>
      <td>2</td>
      <td>4011</td>
      <td>2013</td>
      <td><b>15.6</b></td>
    </tr>
  </tbody>
</table>
<h3 id="pos-optimizer-state-partitioning">Pos: Optimizer State Partitioning</h3>
<p>设 DP 并行度为 Nd, 每个数据并行进程只需要存储和更新总优化器状态的 1/Nd，然后只更新参数的 1/Nd. 在每个训练步骤结束时，在数据并行进程中执行一次 All-Gather，以获得所有数据并行过程中完全更新的参数。这使得每个设备上保存模型状态需要的内存从 4ψ+Kψ 变成 4ψ+Kψ/Nd，当使用 Adam 优化器 (K=12) 并且 Nd 很大时，内存需求可以降低接近 4x.</p>
<h3 id="pg-gradient-partitioning">Pg: Gradient Partitioning</h3>
<p>由于每个数据并行进程只用更新自己被分配的参数，因此他也只需要那部分参数 reduce 后的梯度。只在负责更新相应参数的数据并行过程中进行 reduce. 完成后它们的内存可以被释放。这使得了梯度所需的内存占用从 2Ψ 字节减少到 2Ψ/Nd. 更新后的参数再被 scatter 到其他进程。</p>
<p>通常为了效率，将需要 reduce 的梯度按照参数的分区划分成多个 buckets，每个 bucket 对应特定的一组参数，对每个 bucket 进行整体 reduce 操作，而不是对单个梯度进行操作。进一步划分梯度后，每个设备上保存模型状态需要的内存进一步减少到 2ψ+(K+2)ψ/Nd</p>
<p>蓝色箭头串起来的白色长方形代表的是 Transformer Block，蓝色的第一行代表 FP16 参数；橙色的第二行代表 FP16 梯度，反向传播时将用于更新参数；绿色的行代表优化器状态 (FP32 的梯度，动量，方差，以及更新后的参数)，其中在计算完 FP16 梯度以后不再需要保存 FP32 参数。同时也需要 buffer 来保存部分 transformer block 的输出激活。</p>
<h3 id="pp-parameter-partitioning">Pp: Parameter Partitioning</h3>
<p>更进一步我们可以将模型参数也进行划分，当设备所没有的参数需要进行向前和向后传播时，通过广播从其他的的数据并行进程接收。通过前面的分析可知这使得通信量变为原来的 1.5x， 但使得所有的模型参数都被划分到每个设备上，只需要 (4+K)/Nd 字节的内存。</p>
<h3 id="execution-steps-of-zero3">Execution Steps of ZeRO3</h3>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fd41e8b92bcd256a910ce757d4eea21?method=download&amp;shareKey=d82f49f0d59309d987c164a100966895" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fd41e8b92bcd256a910ce757d4eea21?method=download&amp;shareKey=d82f49f0d59309d987c164a100966895" alt="Overview of Memory Consumption">
    </a><figcaption>Overview of Memory Consumption</figcaption></figure></p>
<p>每个 GPU 只需要保存自己部分的 Pos+g+p. 前向传播时保存对应模型参数的 GPU 需要把参数广播到其他 GPU 中，其他 GPU 用自己部分的数据完成前向传播后就可以删除这部分参数 (最后一部分除外). <code>(N-1)/N*P</code></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB82b66a8a7b40a2fdf9512545189cc37a?method=download&amp;shareKey=37a4614d85a77d2ca00e6d5a4769e2f9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB82b66a8a7b40a2fdf9512545189cc37a?method=download&amp;shareKey=37a4614d85a77d2ca00e6d5a4769e2f9" alt="Broadcast of Model Parameters">
    </a><figcaption>Broadcast of Model Parameters</figcaption></figure></p>
<p>前向传播完成后，第一次反向传播可以利用最后一次正向传播已经广播了的模型参数，每个 GPU 计算自己部分的梯度，然后 Reduce 到存储对应模型参数的 GPU 中。之后和前向传播一样，每个 GPU 都需要广播自己的参数，然后其他 GPU 用自己的数据完成梯度计算以后 Reduce 到自己的梯度。<code>(N-1)/N*P + 1/N*G*(N-1)</code></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB575b3869e59814ae1449351cf1b18d01?method=download&amp;shareKey=10f29390f47227ca8eefbcc00f4fca6e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB575b3869e59814ae1449351cf1b18d01?method=download&amp;shareKey=10f29390f47227ca8eefbcc00f4fca6e" alt="Gradient Accumulation">
    </a><figcaption>Gradient Accumulation</figcaption></figure></p>
<p>反向传播结束以后，每个 GPU 使用优化器更新自己的 FP32 模型参数后转换成 FP16 格式。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB19cb21dfa63ab76437a2246ff52b00aa?method=download&amp;shareKey=ddb63d57f3bf976b1dce4596e77a2009" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB19cb21dfa63ab76437a2246ff52b00aa?method=download&amp;shareKey=ddb63d57f3bf976b1dce4596e77a2009" alt="Update Parameters Locally">
    </a><figcaption>Update Parameters Locally</figcaption></figure></p>
<h2 id="deep-dive-into-zero-r">Deep Dive into ZeRO-R</h2>
<h3 id="pa-partitioned-activation-checkpointing">Pa: Partitioned Activation Checkpointing</h3>
<p>一旦计算了模型的一层的前向传播，输入激活将在所有模型并行过程中进行划分，直到在反向传播期间再次需要输入激活。此时，ZeRO 使用一个 All-Gather 操作来重新实现激活的复制副本。称这个优化为 Pa. 将 Pa 与激活检查点结合，只存储分区的激活检查点，这样使得激活占用空间的减少与 MP 并行度成正比。</p>
<h3 id="cb-constant-size-buffers">CB: Constant Size Buffers</h3>
<p>通信的效率不仅仅与数据量相关，还受到固定启动开销和带宽利用率的影响。较大的输入更容易充分利用硬件的带宽和优化机制，因而能显著提高 All-Reduce 操作的效率。因此经常将需要进行通信的数据合并到一个缓冲器。然而，合并缓冲区的内存开销与模型大小成正比，模型过大时容易 OOM. 为了解决这个问题，<strong>当模型很大时，简单地使用一个性能高效的固定大小的合并缓冲区</strong>。</p>
<h3 id="md-memory-defragmentation">MD: Memory Defragmentation</h3>
<p>前向传播中只需要保存检查点的激活而丢弃其他激活会产生碎片化内存。同样的反向传播中只需要保存参数的梯度而丢弃激活的梯度也会产生碎片化内存。内存碎片导致两个问题: (1) 即使有足够的可用内存，由于缺乏连续内存导致 OOM. (2) 由于内存分配器花费大量时间搜索连续内存块以满足内存请求而导致效率低下。ZeRO 通过<strong>为激活检查点和梯度预分配连续内存块，并在它们产生时将它们复制到预分配的内存中</strong>，从而实时地进行内存碎片整理。</p>
<h2 id="communication-analysis-of-zero-dp">Communication Analysis of ZeRO-DP</h2>
<p>使用 Pos 和 Pg 时，ZeRO-DP 不会产生额外的通信，同时可以减少高达 8 倍的内存。使用 Pos+g+p 时，ZeRO-DP 最多会产生 1.5 倍的通信，同时减少内存占用为原来的 1/Nd.</p>
<p>在数据并行训练过程中，在计算下一步的更新之前，在反向传播结束时对所有数据并行进程的梯度使用 All-Reduce 进行平均，因此通信量为 2ψ. 使用 Pos+g 时每个设备需要将自己的梯度 scatter 到负责更新那部分参数的设备上，然后使用 Gather 将其他设备更新后的模型参数同步到自己上面，总通信量仍为 2ψ，与数据并行相同。使用 Pos+g+p 时负责该分区的数据并行进程将权重 brocast 给所有数据并行进程 (前向反向各一次)，最后仍需要 Gather 其他进程上更新好的参数，因此总通信量为 3ψ.</p>
<h2 id="communication-analysis-of-zero-r">Communication Analysis of ZeRO-R</h2>
<p>在使用激活检查点的 Megatron-LM 中，每个 transformer block 在前向传播中执行 2 次大小为 批大小×序列长度×隐藏维度的 All-Reduce 操作，反向传播中执行 2 次同样大小的 All-Reduce 操作，同时激活重计算也需要 2 次同样大小的 All-Reduce 操作。因此每个块的总通信量为 12×序列长度×隐藏维度。</p>
<p>当使用 ZeRO-R 划分激活检查点时，在对每个激活检查点上的反向传播进行前向重新计算之前，需要进行额外的一次 All-Gather 操作。因此，Pa的总通信开销相对于原先 MP 通信量增加了 1/12，但是使得激活内存占用减小到原来的 1/MP_degree.</p>
<p>如果使用了 Pa+cpu，则分区激活检查点将被存储到 CPU，对激活内存需求减少到几乎为零，而代价是与 Pa 相比，需要从 CPU 和内存之间的数据移动增加了 2 倍。</p>
<h1 id="zero-offload">ZeRO-Offload</h1>
<p>ZeRO-Offload 通过将数据和计算下放到 CPU 来实现大型模型训练。为了保持计算效率，它尽可能减少数据在 GPU 和 CPU 之间的移动，同时最大限度地减少 CPU 的计算时间，并最大限度地节省 GPU 上的内存。</p>
<h2 id="introduction-1">Introduction</h2>
<p>PP, MP 和 ZeRO 等并行技术都需要有足够的 GPU 设备，使得它们的内存之和能够容纳训练所需的模型状态的存储。目前基于注意力的大模型训练的主要内存瓶颈是模型状态，而不是激活。现有的异构训练在两个主要方面受到限制：(i) 几乎所有的训练都利用 CPU 内存，而不是 CPU算力。(ii) 它们主要是为单个 GPU 设计和评估的。</p>
<p>ZeRO-Offload 为了提高计算效率采取的设计原则有三条：(i) 它需要的 CPU 计算量与 GPU 相比减少了几个数量级。(ii) 它最小化了 CPU 和 GPU 之间的通信量，防止了通信成为瓶颈。(iii) 可以证明在实现最小通信量的同时最大限度地节省了 GPU 的内存。</p>
<p>ZeRO-Offload 将梯度，优化器状态和优化器计算卸载到 CPU，而将参数和前向和反向计算保留在 GPU上。这样 CPU 上的计算量为 O(M)，而 GPU 上的计算量则为 O(MB)，其中 M 和 B 分别为模型大小和 batchsize. 因为 CPU 只处理模型参数的更新，而不参与与 batch size 相关的梯度求平均的操作。在大多数情况下，batchsize 较大，因此 CPU 计算不是瓶颈。但是对于较小的 batchsize，CPU 计算可能会成为瓶颈。</p>
<h2 id="unique-optimal-offload-strategy">Unique Optimal Offload Strategy</h2>
<p>为了确定最佳的卸载策略，ZeRO-Offload 将 DL 训练建模为如下图所示的数据流，并有效地在 CPU 和 GPU 设备之间进行划分。GPU 和 CPU 之间的卸载策略可以使用该图的二分图来表示，这样一个分区中的计算节点将在拥有该分区的设备上执行，分区中的数据节点也存储在拥有该分区的设备上。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc55ea0bd058b2e603052658a6cb25aa6?method=download&amp;shareKey=e2474b8904eff6869bcdce3b09e545f6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc55ea0bd058b2e603052658a6cb25aa6?method=download&amp;shareKey=e2474b8904eff6869bcdce3b09e545f6" alt="The Dataflow of Fully Connected Neural Networks">
    </a><figcaption>The Dataflow of Fully Connected Neural Networks</figcaption></figure></p>
<p>由于 CPU 的算力远远低于 GPU，所以前向传播和反向传播 (它们的计算复杂度都是 O(MB)) 必须在 GPU上完成，而其余复杂度为 O(M) 的计算 (如归一化计算、权重更新等) 会被卸载到 CPU 上。</p>
<p>CPU 内存带宽 (100xGB) 至少比 CPU 和 GPU 之间的 PCIe 带宽 (10xGB) 快一个数量级，而 GPU 内存带宽比 CPU 内存带宽 (TB) 快另一个数量级。数据流中的每个节点都是环的一部分。因此，对该图进行任何划分都需要切割至少两条边，每条边的权值至少为 2M，从而总通信量至少 4M (通过仅卸载部分模型状态，可以进一步减少通信量). 因此，为了实现最小的通信量，所有卸载策略必须使得关于 fp32 模型状态操作的生产者和消费者相同。fp16 参数节点必须和 FWD-BWD 节点在一个子图中，因为这两个节点之间的边权值是 4M.</p>
<p>下表显示了最小化通信量情况下的所有有效分区策略所节省的内存。通过将 fp16 梯度和 Update Super 节点放到 CPU 可以实现 8x 的最大内存节省。</p>
<table>
  <thead>
      <tr>
          <th>FWD-BWD</th>
          <th>p16</th>
          <th>g16</th>
          <th>Update</th>
          <th>Memory</th>
          <th>Reduction</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>gpu</td>
          <td>gpu</td>
          <td>16M</td>
          <td>1x (baseline)</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>cpu</td>
          <td>gpu</td>
          <td>14M</td>
          <td>1.14x</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>4M</td>
          <td>4x</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>2M</td>
          <td>8x</td>
      </tr>
  </tbody>
</table>
<p>综上所述 ZeRO-Offload 在 CPU 上存储所有 fp32 模型状态以及 fp16 梯度，并且还在 CPU 上计算更新后的参数。fp16 的参数保存在 GPU 上，前向和反向计算也在GPU上完成。</p>
<h2 id="zero-offload-schedule">ZeRO-Offload Schedule</h2>
<p>在训练过程中，首先通过前向传播计算损失。由于 fp16 参数已经存放在GPU上，因此这部分计算不需要与 CPU 通信。在损失的反向传播过程中，不同设备计算不同参数的梯度。ZeRO-Offload 可以在计算完每个参数后，将这些梯度单独或分组传输到 CPU 内存中。由于梯度是逐层传输的，因此 GPU 上只需要很小的缓冲区来存放每一层的梯度。在反向传播之后，ZeRO-Offload 直接在 CPU 上更新 fp32 参数和优化器状态），并将更新后的 fp32 参数从 CPU 内存复制到 GPU 内存上的 fp16 参数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB810cb8d722c2c9e8e140b80084c47cbe?method=download&amp;shareKey=6ab97f8c087ad948e03121afde1c266a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB810cb8d722c2c9e8e140b80084c47cbe?method=download&amp;shareKey=6ab97f8c087ad948e03121afde1c266a" alt="ZeRO-Offload Training Process on a Single GPU">
    </a><figcaption>ZeRO-Offload Training Process on a Single GPU</figcaption></figure></p>
<p>在卸载之前进行如上一节所述的划分的主要好处是，对于具有超过 1 个 GPU 的系统，每个数据并行进程只负责更新参数的一个子集。所有数据并行进程的 GPU 到 CPU 的通信量总和保持不变，CPU 资源可以并行使用，共同计算单个权重更新。ZeRO-Offload 在不同 GPU 之间划分梯度和优化器状态，每个 GPU 将其拥有的部分卸载到 CPU 内存中，并在整个训练过程中将其一直保存在那里。在反向传播过程中，在 GPU上使用 reduce-scatter 计算普遍复核一遍梯度，每个 GPU 只将属于其那一部分的平均梯度卸载到 CPU 内存中。然后优化器状态将由每个数据并行进程直接在 CPU 上并行更新。更新后，参数被移回 GPU，然后在 GPU 上执行类似于 ZeRO-2 的 All-Gather 操作来获取所有更新后的参数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6e4ed2b9f7f37bf8a5f8f9326bb98971?method=download&amp;shareKey=0bf3ca60c8edb9a602a3ae93262c0967" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6e4ed2b9f7f37bf8a5f8f9326bb98971?method=download&amp;shareKey=0bf3ca60c8edb9a602a3ae93262c0967" alt="ZeRO-Offload Data Placement with Multiple GPUs">
    </a><figcaption>ZeRO-Offload Data Placement with Multiple GPUs</figcaption></figure></p>
<h2 id="optimized-cpu-execution">Optimized CPU Execution</h2>
<ol>
<li>作者使用高性能计算技术实现了一个加速版的 CPU Adam 优化器</li>
<li>开发了一个一步延迟参数更新计划，将 CPU 参数更新计算与 GPU 上的前向和反向计算重叠，隐藏了 CPU 执行时间。</li>
</ol>
<h3 id="implementing-the-cpu-optimizer">Implementing the CPU Optimizer</h3>
<p>作者使用三级并行性来提高 CPU 优化器的性能。</p>
<ol>
<li>SIMD 矢量指令，充分利用 CPU 架构的硬件并行性。</li>
<li>循环展开，一种提高指令级并行性的有效技术，能更好地利用内存带宽。</li>
<li>OMP 多线程，可以有效地并行利用 CPU 上的多个内核和线程。</li>
</ol>
<p>算法的输入为 β₁(动量系数), β₂(RMSProp 的平方梯度衰减系数), α(学习率)，以及梯度，动量，方差和 fp32 参数作为输入。我们还使用了一些特定于实现的参数，如 simd_width 和 unroll_width. Adam 优化器分别发送更新的方差、动量和参数的 fp16 和 fp32 格式到 GPU 和 CPU. 首先将数据读入矢量寄存器。然后，主循环中使用 Fused Multiplication Add 矢量操作。其他操作，如乘法、除法和平方根，也在矢量模式下运行。为了获得最佳性能，使用 AVX512 simd 指令集和基于自动调优结果的 unroll_width=8. 除了 CPU-Adam 优化器之外，还以分块的方式实现了 CPU 到 GPU 的 fp16 参数复制。通过并行化 Adam 计算并将参数复制到 GPU 来重叠 CPU 和 GPU 的执行。<strong>当在 CPU 上处理当前数据块的 Adam 计算时，将先前处理过的数据块的参数写回 GPU.</strong></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7e25392bc19670dfd7b80cc9a84a5d73?method=download&amp;shareKey=ac89809b9f51717d9a5429d1cd5d9865" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7e25392bc19670dfd7b80cc9a84a5d73?method=download&amp;shareKey=ac89809b9f51717d9a5429d1cd5d9865" alt="CPU-ADAM Optimizer">
    </a><figcaption>CPU-ADAM Optimizer</figcaption></figure></p>
<h3 id="one-step-delayed-parameter-update">One-Step Delayed Parameter Update</h3>
<p>下图展示了 Delayed Parameter Update(DPU) 的 ZeRO-Offload 训练的工作流程。</p>
<ol>
<li>前 N−1 步不使用 DPU 进行训练，避免在梯度变化迅速的早期阶段破坏训练的稳定性。</li>
<li>在第 N 步中，从 GPU 获取梯度，但跳过 CPU 优化步骤，也不更新 GPU 上的 fp16 参数。</li>
<li>在第 N+1 步中，我们使用第 N 步的梯度计算 CPU 上的参数更新，同时使用第 N-1 步更新的参数并行计算 GPU 上的前向和反向。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc56f9c814071e1766b9f33833024d829?method=download&amp;shareKey=d5be7d41c0247d8243bddb3452eba75b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc56f9c814071e1766b9f33833024d829?method=download&amp;shareKey=d5be7d41c0247d8243bddb3452eba75b" alt="Delayed Parameter Update During the Training Process">
    </a><figcaption>Delayed Parameter Update During the Training Process</figcaption></figure></p>
<h1 id="zero-infinity">ZeRO-Infinity</h1>
<p>ZeRO-Infinity 是一种新的异构系统技术，它利用 GPU, CPU 和 NVMe 内存，在有限的资源上实现前所未有的模型扩展，并且不需要模型代码重构。</p>
<p>目前大型模型训练技术中最先进的是三维并行 (3D parallelism)，它将模型（张量切片）和流水线并行与数据并行相结合。但是 GPU 内存跟不上模型大小的增长。</p>
<p>ZeRO-Infinity 的优势如下</p>
<ol>
<li>通过同时利用 CPU 和 NVMe 内存，在有限的 GPU 资源上支持大模型训练。</li>
<li>引入了一种称为 <em>memory-centric tiling</em> 的 GPU 内存优化技术，以应对 GPU 内存无法一次放下的超大 block 情况。</li>
<li>引入了一种称作 <em>bandwidth-centric partitioning</em> 的数据分区策略，用于利用所有设备上的内存带宽，并将其与重叠通信与计算的技术结合。</li>
</ol>
<h2 id="memory-requirements">MEMORY REQUIREMENTS</h2>
<p><strong>Memory for Model States:</strong> 基于 Transformer 的模型中的参数总数主要取决于隐藏维度 (hd) 和 Transformer 层数 (nl). Transformer block 中的几乎所有参数都来自四个线性层，大小分别为：QKV_Linear(nd,3nd), O_Linear(hd, hd),MLP(hd, 4hd)+(4hd, hd). 因此一个 Transformer block 的参数量约为 <strong>12 x nl x (hd)²</strong>，因此占用的内存大小为 192 x nl x (hd)² 字节。</p>
<p><strong>Memory for Residual States:</strong> 剩余状态主要由激活内存组成，它取决于模型架构、批处理大小 (bsz) 和序列长度 (seq). 存储激活检查点所需的内存估计为 <strong>2×bsz×seq×hd×nl/ci</strong>，其中 ci(checkpoint interval) 是两个激活检查点之间的 Transformer block 的数量。</p>
<p><strong>Model State Working Memory (MSWM)</strong> 是在所有模型状态被卸载到 CPU 或 NVMe 之后，在模型中最大的单个算子上执行前向或反向传播所需的最小 GPU 内存。对于基于 Transformer 的模型，最大的算子是将隐藏维度从 hd 转换为 4hd 的线性层，因此 fp32 格式下需要 <strong>4xhdx4hd</strong> 字节的内存。</p>
<p><strong>Activation Working Memory (AWM):</strong> 是在执行实际的反向传播之前重新计算激活所需的内存，即两个连续激活检查点之间的激活大小 bsz × seq × ci × (16 × hd + 2 × attn_heads × seq) 字节。</p>
<h2 id="bandwidth-requirements">BANDWIDTH REQUIREMENTS</h2>
<p>假设没有任何计算和通信重叠的工作负载执行，我们可以使用峰值计算吞吐量 (peaktp)，数据移动带宽 (bw) 及其算术强度 (ait) 来估计训练效率。需要注意 peaktp 不是理论上的硬件峰值，而是在没有任何通信瓶颈的情况下可以达到的峰值。</p>
<p>算术强度 (AIT) 是总计算量与计算所需数据量之比。它描述了每次数据移动的计算量。</p>
<ol>
<li>compute_time = total_computation / peaktp</li>
<li>ait = total_computation / total_data_movement</li>
<li>communication_time = total_data_movement / bw = total_computation / (ait × bw)</li>
<li>efficiency = compute_time / (compute_time + communication_time) = ait x bw / (ait x bw + peaktp)</li>
</ol>
<h3 id="quantifying-ait-in-dl-training">Quantifying AIT in DL training</h3>
<p>Transformer block 中一次前向传播中的计算量可以近似为输入乘以参数大小 2 × bsz × seq × params. 反向传播则为其 2 倍。如果使用激活检查点则还需要一次额外的前向传播，因此每次迭代的总计算量为 computation_per_iter = 2 × 4 × bsz × seq × parameters = 2 × 4 × 12 × bsz × seq × nl × (hd)²</p>
<p><strong>AIT w.r.t. Parameters and Gradients:</strong> 前向和反向过程中模型参数必须从存储位置位置加载到 GPU 寄存器各次。在使用激活检查点的情况下，还需要加载一次，以便在反向传播期间重新计算。此外，梯度必须从 GPU 寄存器存储到其最终位置至少一次。因此总共要移动模型参数 4 次，总计 2 x 4 x parameters 字节。因此关于参数和梯度的计算强度为 seq x bsz.</p>
<p><strong>AIT w.r.t. Optimizer States:</strong> 优化器状态必须至少读取和写入一次。所以总的数据移动是 2 × optimizer_states，总计 2 × 16 × parameters 字节。因此关于优化器状态的计算强度为 seq x bsz / 4.</p>
<p><strong>AIT w.r.t. Activation Checkpoints:</strong> 前向传播时必须将激活检查点保存到它们的最终位置，然后在反向传播期间加载激活检查点。因此总数据移动量为 4 × nl/ci × hd × seq × bsz 字节。因此关于激活检查点的计算强度为 24 × hd × ci.</p>
<h3 id="bandwidth-requirements-1">Bandwidth Requirements</h3>
<p>通过前面的分析可知模型状态的计算强度仅取决于批大小和序列长度，激活检查点的计算强度仅取决于存储间隔和模型的隐藏维度大小。下图 a 说明当传输参数和梯度的带宽超过 70 GB/s 时，即使是最小的批处理大小，也可以实现超过 50% 的效率。图 b 说明，传输优化器状态需要近 4 倍的带宽才能达到 50% 的效率。并且<strong>优化器状态更新需要等待所有前向和反向传播结束，不能与计算重叠</strong>。图 c 说明，启用激活检查点后，即使隐藏大小为2K，2 GB/s 的带宽也能够保持 50% 以上的效率。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfc095fefa5837b449dcafbd0b9441d63?method=download&amp;shareKey=42ab0064e0986bfde564af6e879f5cd6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfc095fefa5837b449dcafbd0b9441d63?method=download&amp;shareKey=42ab0064e0986bfde564af6e879f5cd6" alt="Impact of Bandwidth on Efficiency with 70 TFlops of single GPU Peak Throughput">
    </a><figcaption>Impact of Bandwidth on Efficiency with 70 TFlops of single GPU Peak Throughput</figcaption></figure></p>
<h2 id="zero-infinity-design-overview">ZERO-INFINITY DESIGN OVERVIEW</h2>
<p>GPU 集群采用异构内存存储，除了 GPU 内存还拥有 CPU 内存以及比 GPU 内存大 50x, 比 CPU 内存大近 20x 的大规模 NVMe 存储。下图为 ZeRO-Infinity 架构，描述了第一层的反向传递的通信。将划分后的参数从慢速内存移动到 GPU，然后 All-Gather 以形成完整的层。在计算梯度之后，参数被聚合和重新划分，然后卸载到慢速内存中。层用下标表示，DP rank 用上标表示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB07d94538fa97fe4b478f44d44eb19e79?method=download&amp;shareKey=82f65d6660819491e1bec3688d2715ae" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB07d94538fa97fe4b478f44d44eb19e79?method=download&amp;shareKey=82f65d6660819491e1bec3688d2715ae" alt="A Snapshot of ZeRO-Infinity Training a Model with 2 Layers on 4 DP Ranks">
    </a><figcaption>A Snapshot of ZeRO-Infinity Training a Model with 2 Layers on 4 DP Ranks</figcaption></figure></p>
<p><strong>Efficiency w.r.t Parameter and Gradients:</strong> 现有的异构解决方案 (例如 ZeRO-Offload) 要求先将参数从 CPU 移动到拥有这些参数的 GPU，然后再进行广播。这种方式需要在每个 GPU 上使用足够大的 batchsize，以确保通信能被计算掩盖。但这带来了两个问题：</p>
<ol>
<li>对于超大规模模型，激活的内存占用会过大，甚至超过 CPU 的内存容量。</li>
<li>当扩展到数百甚至上千个 GPU 时，为了实现有效的收敛，实际的 batchsize 会变得过大。</li>
</ol>
<p><strong>Efficiency w.r.t Optimizer States:</strong> 与在前向和反向传播期间参数和梯度的产生有先后顺序不同，优化器状态可以同时更新。ZeRO-Infinity 建立在 ZeRO-3 之上，因此在将优化器状态卸载到 CPU 内存时，它还可以利用所有的 GPU 和 CPU 内存带宽以及所有 CPU 算力用于优化器状态更新。然而，使用 NVMe 卸载，需要将数据从 NVMe 传入到 CPU 内存中，再从 CPU 内存返回。由于 CPU 内存有限，必须将数据分块从 NVMe 加载到 CPU 内存，进行计算后再写回 NVMe.</p>
<p><strong>Efficiency w.r.t Activations:</strong> 在一台 DGX-2 节点上，每个 GPU 可以通过 PCIe 接口以大约 3 GB/s 的速度并行读写数据到 CPU 内存。这使得在隐藏层大小为 8K 或更大时，可以将激活检查点卸载到 CPU 内存的同时保持超过 80% 的效率。</p>
<h2 id="efficiency-optimizations">EFFICIENCY OPTIMIZATIONS</h2>
<h3 id="bandwidth-centric-partitioning">Bandwidth-Centric Partitioning</h3>
<p>在 ZeRO-3 和 ZeRO-Offload 中每层的参数为单个数据并行进程拥有，在需要时将它们广播给其他进程，ZeRO-Infinity 在所有数据并行进程中划分单个参数，并在需要参数时使用 All-Gather. 相较于广播只用到了单个 PCIe 链路将参数从存储位置加载到 GPU，All-Gather 同时使用所有的 PCIe 链路，每条链路传输 1/dp 的参数。</p>
<h3 id="overlap-centric-design">Overlap Centric Design</h3>
<p>访问 NVMe 内存需要三个步骤：(i) 从 NVMe 读取数据到CPU内存 (nc-transfer). (ii) 将数据从 CPU 内存复制到 GPU 内存 (cg-transfer). (iii) 执行 All-Gather 以在所有 GPU 上获得完整参数 (gg-transfer).</p>
<p>ZeRO-Infinity 的通信重叠有两个组件</p>
<ol>
<li>一个 dynamic prefetcher，在每次迭代期间，跟踪其在算子序列中的位置，并预取未来算子所需的参数。在执行第 i 个操作符之前，prefetcher 可以分别对第 i+3，第 i+2 和第 i+1 个算子所需的参数调用 nc, cg 和 gg-transfer.</li>
<li>一个通信和卸载重叠机制，用于并行执行梯度所需的数据移动和反向计算。将第 i+1 个算子中参数梯度的 Reduce-Scatter 与第 i 个算子的计算重叠，同时将第 i+2 个算子 Reduce-Scatter 划分的梯度传输给 CPU 或 NVMe.</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>xDiT Principle</title>
      <link>http://localhost:57770/blogs/xdit/</link>
      <pubDate>Sat, 07 Jun 2025 20:44:50 +0800</pubDate>
      <guid>http://localhost:57770/blogs/xdit/</guid>
      <description>This is a brief introduction to the xDiT Principle.</description>
      <content:encoded><![CDATA[<h1 id="parse-config-arguments">Parse Config Arguments</h1>
<p>会从命令行参数中获取有关 Model, Runtime, Parallel Processing &amp; Input 有关的信息。前三者被包含在 <code>engine_config</code> 中，而最后者则被包含在 <code>input_config</code> 中。在 <code>create_config()</code> 函数中，会初始化 <code>_WORLD</code> 全局变量，它是一个 <code>GroupCoordinator</code> 实例。很明显它只有一个包含所有的设备进程组。
<details class="custom-details">
    <summary class="custom-summary">GroupCoordinator</summary>
    <div><p><code>GroupCoordinator</code> 类是一个 PyTorch 的进程组封装器，主要用于管理一组进程之间的通信。它可以根据不同的通信后端（如 NCCL、Gloo、MPI 等）来协调进程之间的操作。包含以下信息</p>
<ul>
<li><code>rank</code>: 当前进程的全局索引（全局唯一）。</li>
<li><code>ranks</code>: 组内所有进程的全局索引列表。</li>
<li><code>world_size</code>: 组的大小，即进程的数量 <code>len(ranks)</code></li>
<li><code>local_rank</code>: 当前进程在本地节点中的索引。</li>
<li><code>rank_in_group</code>: 当前进程在组内的索引。</li>
<li><code>cpu_group</code>: 用于 CPU 通信的进程组。</li>
<li><code>device_group</code>: 用于设备（如 GPU）通信的进程组。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">we</span> <span class="n">have</span> <span class="n">a</span> <span class="n">group</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span> <span class="n">across</span> <span class="n">two</span> <span class="n">nodes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">Process</span> <span class="o">|</span> <span class="n">Node</span> <span class="o">|</span> <span class="n">Rank</span> <span class="o">|</span> <span class="n">Local</span> <span class="n">Rank</span> <span class="o">|</span> <span class="n">Rank</span> <span class="ow">in</span> <span class="n">Group</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span>     <span class="o">|</span>   <span class="mi">0</span>  <span class="o">|</span>  <span class="mi">0</span>   <span class="o">|</span>     <span class="mi">0</span>      <span class="o">|</span>       <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span>     <span class="o">|</span>   <span class="mi">0</span>  <span class="o">|</span>  <span class="mi">1</span>   <span class="o">|</span>     <span class="mi">1</span>      <span class="o">|</span>       <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="mi">2</span>     <span class="o">|</span>   <span class="mi">1</span>  <span class="o">|</span>  <span class="mi">2</span>   <span class="o">|</span>     <span class="mi">0</span>      <span class="o">|</span>       <span class="mi">2</span>
</span></span><span class="line"><span class="cl">  <span class="mi">3</span>     <span class="o">|</span>   <span class="mi">1</span>  <span class="o">|</span>  <span class="mi">3</span>   <span class="o">|</span>     <span class="mi">1</span>      <span class="o">|</span>       <span class="mi">3</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>__init__</code> 方法接收以下参数：</p>
<ul>
<li><code>group_ranks</code>: 一个包含多个进程索引列表的列表，每个子列表表示一个进程组。</li>
<li><code>local_rank</code>: 当前进程的本地索引。</li>
<li><code>torch_distributed_backend</code>: 指定用于通信的后端类型 (如 &ldquo;gloo&rdquo; 或 &ldquo;nccl&rdquo;).</li>
</ul>
<p>初始化过程：</p>
<ol>
<li>使用 <code>torch.distributed.get_rank()</code> 获取当前进程的全局索引。</li>
<li>遍历传入的 <code>group_ranks</code> 列表，为每个子列表创建一个新的设备组和 CPU 组。</li>
<li>如果当前进程的索引在当前子列表中，则设置该进程的组内信息 (包括 <code>ranks</code>、<code>world_size</code> 和 <code>rank_in_group</code>).</li>
<li>确保 CPU 组和设备组都已成功创建。</li>
<li>根据是否可用 CUDA 设置当前设备为 GPU 或 CPU.</li>
</ol>
</div>
</details><br></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span> <span class="o">=</span> <span class="n">FlexibleArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&#34;xFuser Arguments&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">xFuserArgs</span><span class="o">.</span><span class="n">add_cli_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>  <span class="c1"># Add Command Line Interface (CLI) arguments</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_args</span> <span class="o">=</span> <span class="n">xFuserArgs</span><span class="o">.</span><span class="n">from_cli_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Extract CLI args and pass them to xFuserArgs Constructor</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_config</span><span class="p">,</span> <span class="n">input_config</span> <span class="o">=</span> <span class="n">engine_args</span><span class="o">.</span><span class="n">create_config</span><span class="p">()</span>  <span class="c1"># Init _WORLD. engine_config: model, run_time &amp; parallel infos, input_config: input shape, prompt &amp; sampler infos</span>
</span></span><span class="line"><span class="cl">    <span class="n">local_rank</span> <span class="o">=</span> <span class="n">get_world_group</span><span class="p">()</span><span class="o">.</span><span class="n">local_rank</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>关于可以支持的并行策略如下，包括 Data Parallel, Sequence Parallel, Pipefusion Parallel &amp; Tensor Parallel.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Parallel Processing Options:
</span></span><span class="line"><span class="cl">  --use_cfg_parallel    Use split batch in classifier_free_guidance. cfg_degree will be <span class="m">2</span> <span class="k">if</span> <span class="nb">set</span>
</span></span><span class="line"><span class="cl">  --data_parallel_degree DATA_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Data parallel degree.
</span></span><span class="line"><span class="cl">  --ulysses_degree ULYSSES_DEGREE
</span></span><span class="line"><span class="cl">                        Ulysses sequence parallel degree. Used in attention layer.
</span></span><span class="line"><span class="cl">  --ring_degree RING_DEGREE
</span></span><span class="line"><span class="cl">                        Ring sequence parallel degree. Used in attention layer.
</span></span><span class="line"><span class="cl">  --pipefusion_parallel_degree PIPEFUSION_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Pipefusion parallel degree. Indicates the number of pipeline stages.
</span></span><span class="line"><span class="cl">  --num_pipeline_patch NUM_PIPELINE_PATCH
</span></span><span class="line"><span class="cl">                        Number of patches the feature map should be segmented in pipefusion parallel.
</span></span><span class="line"><span class="cl">  --attn_layer_num_for_pp <span class="o">[</span>ATTN_LAYER_NUM_FOR_PP ...<span class="o">]</span>
</span></span><span class="line"><span class="cl">                        List representing the number of layers per stage of the pipeline in pipefusion parallel
</span></span><span class="line"><span class="cl">  --tensor_parallel_degree TENSOR_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Tensor parallel degree.
</span></span><span class="line"><span class="cl">  --split_scheme SPLIT_SCHEME
</span></span><span class="line"><span class="cl">                        Split scheme <span class="k">for</span> tensor parallel.
</span></span></code></pre></td></tr></table>
</div>
</div><p>从 CLI 解析的参数后会在 <code>create_config()</code> 中组成如下的 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/config/config.py#L185">ParallelConfig</a>.</p>
<ul>
<li><code>DataParallelConfig</code>: 总的并行度为 <code>dp_degree * cfg_degree</code>.
<ul>
<li><code>dp_degree</code>: 相当于对 batch 维度进行切分，</li>
<li><code>cfg_degree</code>: Class-free Guidance(cfg) 用于控制无条件的图片生成 (若使用相当于 <code>batchsize *= 2</code>).</li>
</ul>
</li>
<li><code>SequenceParallelConfig</code>: 总的并行度为 <code>sp_degree = ulysses_degree * ring_degree</code>
<ul>
<li><code>ulysses_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2309.14509">DeepSeed-Ulesses</a> 的序列并行度。</li>
<li><code>ring_degree</code>: 用于控制计算 Ring Attention 时对 Q K V 沿着 Sequence 维度的切分块数。</li>
</ul>
</li>
<li><code>TensorParallelConfig</code>: 总的并行度为 <code>tp_degree</code>.
<ul>
<li><code>tp_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2104.05343">2D Tensor Parallel</a> 的并行度。</li>
<li><code>split_scheme</code>: 用于控制张量切分方式.</li>
</ul>
</li>
<li><code>PipeFusionParallelConfig</code>: 总的并行度为 <code>pp_degree=num_pipeline_patch</code>.
<ul>
<li><code>pp_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2112.11446">PipeFusion</a> 中模型 Transoformer Blocks 的切分个数。</li>
<li><code>num_pipeline_patch</code>: 用于控制对 latent feature map 的切分块数.</li>
<li><code>attn_layer_num_for_pp</code>: 是一个 list，表示 <code>pp_degree</code> 里每个 stage 的 Transformer 层数。</li>
</ul>
</li>
</ul>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>关于 PipeFusion，原文说切分的 patch 数和 pipeline 大小可以不同，但这里要求 <code>len(attn_layer_num_for_pp)=pp_degree</code></p></div>

<div class="notice info" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="92 59.5 300 300">
  <path d="M292 303.25V272c0-3.516-2.734-6.25-6.25-6.25H267v-100c0-3.516-2.734-6.25-6.25-6.25h-62.5c-3.516 0-6.25 2.734-6.25 6.25V197c0 3.516 2.734 6.25 6.25 6.25H217v62.5h-18.75c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h87.5c3.516 0 6.25-2.734 6.25-6.25Zm-25-175V97c0-3.516-2.734-6.25-6.25-6.25h-37.5c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h37.5c3.516 0 6.25-2.734 6.25-6.25Zm125 81.25c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Info</p><p>设备数必须等于 <code>dp_degree * cfg_degree * sp_degree * tp_degree * num_pipeline_patch</code>，并且 <code>pp_degree</code> 必须小于等于设备数。
<code>ulysses_degree</code> 必须要大于且能被 attention 的头数整除。</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">ParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">dp_config</span><span class="o">=</span><span class="n">DataParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">dp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cfg_parallel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_cfg_parallel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">sp_config</span><span class="o">=</span><span class="n">SequenceParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">ulysses_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ulysses_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">ring_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ring_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">tp_config</span><span class="o">=</span><span class="n">TensorParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">tp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">split_scheme</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">split_scheme</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_config</span><span class="o">=</span><span class="n">PipeFusionParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">pp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pipefusion_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_pipeline_patch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_layer_num_for_pp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layer_num_for_pp</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="construct-pipeline">Construct Pipeline</h1>
<p>解析完配置参数并构建了 <code>engine_config</code> 后，下一步是构建模型的 pipeline.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">pipe</span> <span class="o">=</span> <span class="n">xFuserPixArtAlphaPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>  <span class="c1"># First construct a PixArtAlphaPipeline, then pass it and engine_config to xFuserPipelineBaseWrapper</span>
</span></span><span class="line"><span class="cl">        <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">engine_config</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">engine_config</span><span class="o">=</span><span class="n">engine_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pipe</span><span class="o">.</span><span class="n">prepare_run</span><span class="p">(</span><span class="n">input_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>xFuserPixArtAlphaPipeline 继承自 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/model_executor/pipelines/base_pipeline.py#L61">xFuserPipelineBaseWrapper</a>，_init_runtime_state 函数经过一番调用后会使用 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/core/distributed/parallel_state.py#L265">initialize_model_parallel</a> 初始化 <code>_RUNTIME</code> 有关模型参数的部分和模型并行的全局变量 <code>_DP, _CFG, _PP, _SP, _TP</code>，它是一个 DiTRuntimeState (继承 RuntimeState) 实例，记录了每个 Group 包含的设备索引，除此之外还包括 PipeFusionParallel 中有关 patch 索引的参数 (在稍后 pipeline 执行的时候计算).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xFuserPipelineBaseWrapper</span><span class="p">(</span><span class="n">xFuserBaseWrapper</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pipeline</span><span class="p">:</span> <span class="n">DiffusionPipeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">engine_config</span><span class="p">:</span> <span class="n">EngineConfig</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">:</span> <span class="n">DiffusionPipeline</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_init_runtime_state</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">engine_config</span><span class="o">=</span><span class="n">engine_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># backbone</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;transformer&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;unet&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># vae</span>
</span></span><span class="line"><span class="cl">        <span class="n">vae</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;vae&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scheduler</span>
</span></span><span class="line"><span class="cl">        <span class="n">scheduler</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;scheduler&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">transformer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_transformer_backbone</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">unet</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">unet</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_unet_backbone</span><span class="p">(</span><span class="n">unet</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_scheduler</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">pipeline</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   
</span></span><span class="line"><span class="cl">   
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_convert_transformer_backbone</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">				<span class="c1">#...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&#34;Transformer backbone found, paralleling transformer...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wrapper</span> <span class="o">=</span> <span class="o">**</span><span class="n">xFuserTransformerWrappersRegister</span><span class="o">.</span><span class="n">get_wrapper</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span><span class="o">**</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span> <span class="o">=</span> <span class="n">wrapper</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">transformer</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="initialize_model_parallel">initialize_model_parallel</h2>
<p>该函数中会初始化一个 <code>RankGenerator</code>，它接收每个并行方法的设备组大小和并行度大小顺序。其主要的方法是通过 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/core/distributed/utils.py#L4">generate_masked_orthogonal_rank_groups</a> 函数确定每个并行组由包含哪些设备，先把并行方法按照并行度从小到大排列成 <code>tp-sp-pp-cfg-dp</code>. 再根据要生成的并行组产生对应的 <code>mask</code>. 即如果要生成 <code>pp</code> 组对应的 rank，那么 <code>mask = [0, 0, 1, 0, 0]</code></p>
<p>该函数首先会生成需要生成的并行组的大小组成的 masked_shape 和不需要生成的 unmasked_shape. 首先要用 prefix_product 计算 <code>global_stride</code>，即每个并行度的设备组包含几个设备。再根据 <code>mask</code> 取出对应的 <code>mask_stride</code> 和 <code>unmaskd_stride</code>. <code>group_size = mask_stride[-1]</code> 即为最大并行度的组包含的设备数。<code>num_of_group = num_of_device / mask_stride[-1]</code> 即为要生成几个并行度最大的组。先遍历要生成的每个设备组，并用 decompose 函数确定该设备组在不需要并行维度上的索引；再遍历该组中的每个设备的 lock rank，确定该设备在需要并行维度上的索引，最后用 inner_product 确定该设备的 global rank.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_masked_orthogonal_rank_groups</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">parallel_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">prefix_product</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">init</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>  <span class="c1"># Exclusive</span>
</span></span><span class="line"><span class="cl">        <span class="n">r</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">a</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">init</span> <span class="o">=</span> <span class="n">init</span> <span class="o">*</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">            <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">r</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">inner_product</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">b</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">decompose</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># index: 第几个并行组  # shape: 并行组大小的 list</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function solve the math problem below:
</span></span></span><span class="line"><span class="cl"><span class="s2">            There is an equation: index = sum(idx[i] * stride[i])
</span></span></span><span class="line"><span class="cl"><span class="s2">            And given the value of index, stride.
</span></span></span><span class="line"><span class="cl"><span class="s2">            Return the idx.
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function will used to get the pp/dp/pp_rank from group_index and rank_in_group.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">stride</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">idx</span> <span class="o">=</span> <span class="p">[(</span><span class="n">index</span> <span class="o">//</span> <span class="n">d</span><span class="p">)</span> <span class="o">%</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">)]</span>  <span class="c1">#  计算在每个并行维度上的索引</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># stride is a prefix_product result. And the value of stride[-1]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># is not used.</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stride</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])])</span> <span class="o">==</span> <span class="n">index</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span> <span class="s2">&#34;idx </span><span class="si">{}</span><span class="s2"> with shape </span><span class="si">{}</span><span class="s2"> mismatch the return idx </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">masked_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="p">]</span>  <span class="c1"># 需要采取并行的维度</span>
</span></span><span class="line"><span class="cl">    <span class="n">unmasked_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="p">]</span>  <span class="c1"># 不需要的</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">global_stride</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">)</span>  <span class="c1"># exclusive 前缀积 表示大的并行维度包括几个设备</span>
</span></span><span class="line"><span class="cl">    <span class="n">masked_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_stride</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">unmasked_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_stride</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">group_size</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">masked_shape</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 最大的一个并行维度包括几个设备</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_of_group</span> <span class="o">=</span> <span class="n">world_size</span> <span class="o">//</span> <span class="n">group_size</span>  <span class="c1"># 分成几个大组</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>  
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">group_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_group</span><span class="p">):</span>  <span class="c1"># 遍历每个设备组</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get indices from unmaksed for group_index.</span>
</span></span><span class="line"><span class="cl">        <span class="n">decomposed_group_idx</span> <span class="o">=</span> <span class="n">decompose</span><span class="p">(</span><span class="n">group_index</span><span class="p">,</span> <span class="n">unmasked_shape</span><span class="p">)</span>  <span class="c1"># 得到在不需要采取并行的维度上的索引</span>
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rank_in_group</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">):</span>  <span class="c1"># 遍历该组中的每个设备 local rank</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># get indices from masked for rank_in_group.</span>
</span></span><span class="line"><span class="cl">            <span class="n">decomposed_rank_idx</span> <span class="o">=</span> <span class="n">decompose</span><span class="p">(</span><span class="n">rank_in_group</span><span class="p">,</span> <span class="n">masked_shape</span><span class="p">)</span>  <span class="c1"># 得到最大并行组的每个设备在采取并行的维度上的索引</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>  <span class="o">//</span> <span class="n">相加得到全局rank</span>
</span></span><span class="line"><span class="cl">                <span class="n">inner_product</span><span class="p">(</span><span class="n">decomposed_rank_idx</span><span class="p">,</span> <span class="n">masked_stride</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">                <span class="o">+</span> <span class="n">inner_product</span><span class="p">(</span><span class="n">decomposed_group_idx</span><span class="p">,</span> <span class="n">unmasked_stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">ranks</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="hybrid-parallelsim-design">Hybrid Parallelsim Design</h2>
<p>xDiT支持四种并行方式：PipeFusion、Sequence、Data 和 CFG Parallel。其中，Data 和 CFG Parallel在图像间并行相对简单，而 PipeFusion和 Sequence 在图像内部的不同 Patch 间并行则较为复杂。能</p>
<p>PipeFusion 利用 Input Tempor Redundancy特点，使用过时的 KV（Stale KV）进行 Attention 计算，这使得 PipeFusion 无法像大型语言模型那样轻松地实现并行策略的混合。使用标准的序列并行接口，如RingAttention、Ulysses或 USP，无法满足 SP 与PipeFusion混合并行的需求。</p>
<p>我们对这个问题具体说明，下图展示了pipe_degree=4，sp_degree=2的混合并行方法。设置 <code>num_pipeline_patch</code>=4，图片切分为 M=<code>num_pipeline_patch*sp_degree</code>=8 个 Patch，分别是 P0~P7.</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/hybrid_pp_scheme.png" alt="hybrid process group config"  width="60%">
</div>
<p>Standard SP Attention 的输入Q，K，V 和输出 O 都是沿着序列维度切分，且切分方式一致。如果不同 rank 的输入 patch 没有重叠，每个 micro step 计算出 fresh KV 更新的位置在不同 rank 间也没有重叠。如下图所示，standard SP 的 KV Buffer 中黄色部分是 SP0 rank=0 拥有的 fresh KV，绿色部分是 SP1 rank=1 拥有的fresh KV，二者并不相同。在这个 diffusion step 内，device=0 无法拿到 P1,3,5,7 的 fresh KV 进行计算，但是 PipeFusion 则需要在下一个 diffusion step 中，拥有上一个diffusion step 全部的 KV. standard SP 只拥有 1/sp_degree 的 fresh kv buffer，因此无法获得混合并行推理正确的结果。</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/hybrid_workflow.png" alt="hybrid parallel workflow">
</div>
<p>xDiT专门定制了序列并行的实现方式，以适应这种混合并行的需求。xDiT使用 <code>xFuserLongContextAttention</code> 把SP的中间结果存在 KV Buffer 内。效果如下图，每个 micro-step SP 执行完毕后，SP Group 内不同 rank 设备的 fresh KV是 replicate 的。这样一个 diffusion step 后，SP Group 所有设备的 KV Buffer 都更新成最新，供下一个 Diffusion Step 使用。</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/kvbuffer_hybrid.png" alt="kvbuffer in hybrid parallel">
</div>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>假设一共有 16 个 GPU，索引表示为 g0 &hellip; g15，并行方法和并行度设置如下</p>
<p><code>dp_degree (2) * cfg_degree (2) * pp_degree (2) * sp_degree (2) = 16</code>.</p>
<p>那么一共会创建 2 data parallel-groups, 8 CFG groups, 8 pipeline-parallel groups &amp; 8 sequence-parallel groups:</p>
<ul>
<li>2 data-parallel groups:
[g0, g1, g2, g3, g4, g5, g6, g7],
[g8, g9, g10, g11, g12, g13, g14, g15]</li>
<li>8 CFG-parallel groups:
[g0, g4], [g1, g5], [g2, g6], [g3, g7],
[g8, g12], [g9, g13], [g10, g14], [g11, g15]</li>
<li>8 pipeline-parallel groups:
[g0, g2], [g4, g6], [g8, g10], [g12, g14],
[g1, g3], [g5, g7], [g9, g11], [g13, g15]</li>
<li>8 sequence-parallel groups:
[g0, g1], [g2, g3], [g4, g5], [g6, g7],
[g8, g9], [g10, g11], [g12, g13], [g14, g15]</li>
</ul></div>

<h2 id="convert-model">Convert Model</h2>
<p><a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/model_executor/models/transformers/base_transformer.py#L76">_split_transformer_blocks</a> 会对 transformer block 进行分配，如果 parallel_config 指定了 attn_layer_num_for_pp，即存有每个 pipeFusion 的设备被分配的 transformer block 数量的列表，按其进行分配；否则平均分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_split_transformer_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># omit</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># transformer layer split</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_layer_num_for_pp</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># 获取每个 pipeFusion 的设备被分配的 transformer block 数量</span>
</span></span><span class="line"><span class="cl">        <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pp_config</span><span class="o">.</span><span class="n">attn_layer_num_for_pp</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_rank</span> <span class="o">=</span> <span class="n">get_pipeline_parallel_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_world_size</span> <span class="o">=</span> <span class="n">get_pipeline_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attn_layer_num_for_pp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span> <span class="p">:</span> <span class="n">attn_layer_num_for_pp</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">attn_layer_num_for_pp</span><span class="p">[:</span> <span class="n">pp_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="p">:</span> 
</span></span><span class="line"><span class="cl">                                                                            <span class="nb">sum</span><span class="p">(</span><span class="n">attn_layer_num_for_pp</span><span class="p">[:</span><span class="n">pp_rank</span><span class="p">])]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>  <span class="c1"># 没有指定则平均分</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_blocks_per_stage</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">)</span> <span class="o">+</span> <span class="n">pp_world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">pp_world_size</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">pp_rank</span> <span class="o">*</span> <span class="n">num_blocks_per_stage</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">pp_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_blocks_per_stage</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">),)</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># position embedding</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">norm_out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">transformer</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>同时也会 convert 原先的 transformer backbone 为 <a href="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/models/transformers/pixart_transformer_2d.py#L21">xFuserPixArtTransformer2DWrapper</a>，具体表现为只有 pipeline 的第一阶段进行 position embedding，最后一阶段进行 unpatchify 变为原来的图像形状。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@xFuserTransformerWrappersRegister.register</span><span class="p">(</span><span class="n">PixArtTransformer2DModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xFuserPixArtTransformer2DWrapper</span><span class="p">(</span><span class="n">xFuserTransformerBaseWrapper</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="p">:</span> <span class="n">PixArtTransformer2DModel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">submodule_classes_to_wrap</span><span class="o">=</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">PatchEmbed</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">submodule_name_to_wrap</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;attn1&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@xFuserBaseWrapper.forward_check_condition</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">added_cond_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cross_attention_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>  
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_patch_height_width</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># * only pp rank 0 needs pos_embed (patchify)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">	    <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">output</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">Transformer2DModelOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="pipeline-execution">Pipeline Execution</h1>
<p>在进行 warm up 后便会进行模型推理和采样器的去噪过程。模型推理通过调用 pipeline 的 <code>__call__</code> 方法实现。在原先 diffusers 包中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py">PixaeArtAlphaPipeline</a> 基础上做了一些修改。我们直接看修改的部分。</p>
<p><code>get_runtime_state()</code> 返回 <code>_RUNTIME</code> ，再调用 <code>set_input_parameters</code> 方法，设置输入参数和计算 PipeFusionParallel 中有关 patch 索引的参数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">set_input_parameters</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">num_inference_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该函数会计算</p>
<ul>
<li>pipeline parallel 中每个 patch 的高度，必须是 <code>patch_size * num_sp_patches</code> 的整数倍。</li>
<li>将每个流水线阶段的 patch 高度均匀地分配给 <code>num_sp_patches</code> 个序列并行设备，计算每个设备的 patch 高度和起始索引。</li>
</ul>
<p>然后会对 prompt 嵌入后的正样本和负样本在 cfg parallel 组中的设备进行分割, rank 0 负样本，rank 1 正样本。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">prompt_embeds</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">prompt_attention_mask</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_cfg_split_batch</span><span class="p">(</span><span class="n">negative_prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">negative_prompt_attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">prompt_attention_mask</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_process_cfg_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_0_negative</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_0</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_1_negative</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">get_classifier_free_guidance_world_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">concat_group_0_negative</span><span class="p">,</span> <span class="n">concat_group_0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">concat_group_1_negative</span><span class="p">,</span> <span class="n">concat_group_1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">get_classifier_free_guidance_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">concat_group_0_negative</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">concat_group_1_negative</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">get_classifier_free_guidance_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">concat_group_0</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">concat_group_1</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid classifier free guidance rank&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">concat_group_0</span><span class="p">,</span> <span class="n">concat_group_1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="async-pipeline">Async Pipeline</h1>
<h2 id="initialize-pipeline">Initialize Pipeline</h2>
<p>首先会初始化 pipeline，rank 0 会接收 warmup 阶段的 latents 然后沿着 H 维度进行分块，rank -1 也会沿着 H 维度进行分块。然后为每个 patch 创建接收的任务，注意 rank 0 第一次是从 warmup 阶段接收 latents，所以他的需要接收的 timestep 少一个。
<code>patch_latents</code> 表示当前设备正在处理的 patch 数据，它会在流水线的每一阶段进行处理和传递。<code>last_patch_latents</code> 只在流水线的最后阶段设备中使用，用来存储每个 patch 的最终计算结果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">latents</span>
</span></span><span class="line"><span class="cl"><span class="n">num_pipeline_patch</span> <span class="o">=</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span>
</span></span><span class="line"><span class="cl"><span class="n">num_pipeline_warmup_steps</span> <span class="o">=</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">warmup_steps</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_async_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_timesteps</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span><span class="o">=</span><span class="n">latents</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_pipeline_warmup_steps</span><span class="o">=</span><span class="n">num_pipeline_warmup_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">last_patch_latents</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># 每个 pipeline group 最后的设备接收所有的 patch</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pipeline_patch</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">is_pipeline_last_stage</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_init_async_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_timesteps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_pipeline_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">set_patched_mode</span><span class="p">(</span><span class="n">patch_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get latents computed in warmup stage</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ignore latents after the last timestep</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents</span> <span class="o">=</span> <span class="p">(</span><span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_recv</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                  <span class="k">if</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">                  <span class="k">else</span> <span class="n">latents</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">latents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_height</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">latents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_height</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">recv_timesteps</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_timesteps</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="k">else</span> <span class="n">num_timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># construct receive tasks for each patch</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">recv_timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">add_pipeline_recv_task</span><span class="p">(</span><span class="n">patch_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">patch_latents</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="iterate-over-timesteps">Iterate Over Timesteps</h1>
<p>对于每个 <code>timestep</code>（即每个去噪步骤），会对每个 patch 执行：</p>
<ol>
<li>如果当前设备是流水线的最后一阶段 (<code>is_pipeline_last_stage()</code>)，将当前 patch 的数据保存到 <code>last_patch_latents</code> 中。</li>
<li>如果不是第一阶段的第一个时间步 (<code>i == 0</code>)，调用 <code>recv_next()</code> 来异步接收来自上一设备的 patch 数据（非阻塞操作，通过 <code>irecv</code> 完成）。</li>
<li>对每个 patch 执行模型的前向传播 <code>_backbone_forward</code>，根据当前时间步 <code>t</code> 进行推理和计算。</li>
<li>如果当前设备是最后一阶段，调用 <code>_scheduler_step</code> 来根据噪声进行去噪，并将数据发送给下一个设备 <code>pipeline_isend</code>。</li>
<li>对于非最后阶段的设备，继续将当前 patch 的计算结果发送到下一设备。</li>
</ol>
<p><code>get_pp_group().pipeline_isend</code> 用于将当前 patch 发送到下一个设备，使用的是 torch.distributed.isend，这是非阻塞发送。
<code>get_pp_group().recv_next</code> 会准备好接收来自上一个设备的数据，recv_buffer 用来存放接收到的数据。irecv 实现非阻塞接收，可以在等待数据的同时进行其他操作。</p>
<div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>scheduler_step 只对单独的 patch 进行，原因未知。</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">first_async_recv</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">last_patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">first_async_recv</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">recv_next</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">                <span class="n">first_async_recv</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">get_pipeline_recv_data</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents</span><span class="o">=</span><span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">prompt_attention_mask</span><span class="o">=</span><span class="n">prompt_attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">added_cond_kwargs</span><span class="o">=</span><span class="n">added_cond_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">guidance_scale</span><span class="o">=</span><span class="n">guidance_scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>  <span class="c1"># pred noise</span>
</span></span><span class="line"><span class="cl">                <span class="n">last_patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>  <span class="c1"># last timestep noise</span>
</span></span><span class="line"><span class="cl">                <span class="n">t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">extra_step_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_isend</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span> <span class="n">segment_idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_isend</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span> <span class="n">segment_idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">patch_idx</span> <span class="o">==</span> <span class="n">num_pipeline_patch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">pass</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">recv_next</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">next_patch</span><span class="p">()</span>  <span class="c1"># switch to next: (self.pipeline_patch_idx + 1) % self.num_pipeline_patch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">or</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_warmup_steps</span>
</span></span><span class="line"><span class="cl">        <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">callback</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&#34;callback not supported in async &#34;</span> <span class="s2">&#34;pipeline&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">            <span class="ow">and</span> <span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">%</span> <span class="n">callback_steps</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">step_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span><span class="p">)</span> <span class="o">//</span> <span class="nb">getattr</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&#34;order&#34;</span><span class="p">,</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">callback</span><span class="p">(</span><span class="n">step_idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="construct-final-latents">Construct Final Latents</h2>
<p>timestep 遍历完成后，仍然有最后的操作要进行，这些操作的主要目的是将流水线并行中各个 patch 的结果拼接起来，形成完整的输出结果。尤其是对于最后一个设备，还需要处理 序列并行（sequence parallelism） 的合并操作。通过 all_gather 操作将每个设备上处理的 patch 结果收集起来，然后从每个设备的 <code>sp_latents_list</code> 中，提取出对应于 <code>pp_patch_idx</code> 的 patch 数据并将它们拼接起来。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">latents</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">patch_latents</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">get_sequence_parallel_world_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">sp_degree</span> <span class="o">=</span> <span class="n">get_sequence_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">sp_latents_list</span> <span class="o">=</span> <span class="n">get_sp_group</span><span class="p">()</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents</span><span class="p">,</span> <span class="n">separate_tensors</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">pp_patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents_list</span> <span class="o">+=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="n">sp_latents_list</span><span class="p">[</span><span class="n">sp_patch_idx</span><span class="p">][</span>
</span></span><span class="line"><span class="cl">                    <span class="o">...</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_start_idx_local</span><span class="p">[</span><span class="n">pp_patch_idx</span><span class="p">]</span> <span class="p">:</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_start_idx_local</span><span class="p">[</span><span class="n">pp_patch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="p">:,</span>
</span></span><span class="line"><span class="cl">                <span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">sp_patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sp_degree</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">latents_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">latents</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="decode-latents">Decode Latents</h1>
<p>为了避免 VAE 中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/vae.py#L185">Decoder</a> 在对 8192px 分辨率图像进行 conv2D 的过程中出现 OOM 的问题， xDiT 使用了序列并行和 patch 并行的 <a href="https://github.com/xdit-project/DistVAE/blob/a7e7ee7ec222f45af1214984561c8c645be8aece/distvae/models/layers/conv2d.py#L13">PatchConv2d</a> 和 <a href="https://github.com/xdit-project/DistVAE/blob/a7e7ee7ec222f45af1214984561c8c645be8aece/distvae/models/layers/normalization.py#L59">PatchGroupNorm</a> 来替换掉原有 Decoder 中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d_blocks.py#L2682">UpDecoderBlock2D</a> 对应的层。</p>
<h2 id="patchgroupnorm">PatchGroupNorm</h2>
<p>PatchGroupNorm 在 H 维度上划分为多个 patch，每个设备求自己所负责的部分和。
<details class="custom-details">
    <summary class="custom-summary">GroupNorm Principles</summary>
    <div>假设输入张量 x 的形状为 [N, C, H, W]，其中 N 表示批量大小（Batch Size），C 表示通道数（Channels），H 和 W 分别表示高度和宽度。在 GN 中，通道数 C 被划分为 G 组，每个组包含 C/G 个通道。计算每个组内即 [C/G, H, W] 维度上的均值和方差。特别的 G=1 时，GN 退化为 BN。G=C 时，GN 退化为 LN。</div>
</details><br></p>
<ol>
<li>获取高度信息</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchGroupNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; def __init__(self, ...)&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">height</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">height</span><span class="p">)</span>  <span class="c1"># 收集所有进程的高度并汇总。最终每个进程的 height 都将表示全局的高度和。</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>计算每个组的通道数量以及每个进程内的元素数量</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">channels_per_group</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span>  <span class="c1"># 每个组的通道数量</span>
</span></span><span class="line"><span class="cl"><span class="n">nelements_rank</span> <span class="o">=</span> <span class="n">channels_per_group</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 当前进程负责的每个组中的元素总</span>
</span></span><span class="line"><span class="cl"><span class="n">nelements</span> <span class="o">=</span> <span class="n">channels_per_group</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 所有进程的每个组中的元素总数</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>计算每个组的均值</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1">#  [batch_size, num_groups, channels_per_group, height, width]</span>
</span></span><span class="line"><span class="cl"><span class="n">group_sum</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 对每个组的所有元素 (channels_per_group, height, width) 求平均</span>
</span></span><span class="line"><span class="cl"><span class="n">group_sum</span> <span class="o">=</span> <span class="n">group_sum</span> <span class="o">*</span> <span class="n">nelements_rank</span>  <span class="c1"># 加权后的局部和 = 局部均值 * 当前进程的元素数量</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">group_sum</span><span class="p">)</span>  <span class="c1"># 收集并汇总所有进程的局部和，得到全局和</span>
</span></span><span class="line"><span class="cl"><span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_sum</span> <span class="o">/</span> <span class="n">nelements</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># 计算全局的均值 E</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>计算每个组的方差</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 和计算均值同样的操作</span>
</span></span><span class="line"><span class="cl"><span class="n">group_var_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">out</span><span class="o">=</span><span class="n">group_var_sum</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="n">group_var_sum</span> <span class="o">=</span> <span class="n">group_var_sum</span> <span class="o">*</span> <span class="n">nelements_rank</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">group_var_sum</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">var</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_var_sum</span> <span class="o">/</span> <span class="n">nelements</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>归一化并缩放 $y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">E</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchconv2d">PatchConv2d</h2>
<p><code>PatchConv2d</code> 将潜在空间中的特征映射分割成多个 patch，跨不同设备进行序列并行 VAE 解码。这种技术将中间激活所需的峰值内存减少到 1/N，其中 N 是所使用的设备数量。对于 VAE 中的卷积算子，需要对如下图所示的 halo 区域数据进行通信。</p>
<p>
<figure class="post-figure">
    <a href="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/patchvaeconv.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/patchvaeconv.png" alt="Patch VAE Conv">
    </a><figcaption>Patch VAE Conv</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>  <span class="c1"># TODO: refine this type</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">assert</span> <span class="n">dilation</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;dilation is not supported in PatchConv2d&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">assert</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;dilation is not supported in PatchConv2d&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>_conv_forward</code> 函数是 <code>PatchConv2d</code> 类的核心，它负责在输入张量上执行卷积操作，特别是在分布式计算场景下处理跨进程的输入切分、halo 区域的传递和计算。以下是使用的辅助函数的简要功能说明</p>
<ul>
<li><code>_get_world_size_and_rank </code>：获取当前分布式环境中的进程总数 <code>world_size</code> 和当前进程的编号 <code>rank</code></li>
<li><code>_calc_patch_height_index</code>：根据每个进程的输入高度，计算所有进程的起始和结束高度索引。</li>
<li><code>_calc_halo_width_in_h_dim</code>：计算当前进程在 h 维度上所需的上方和下方的 halo 区域宽度。</li>
<li><code>_calc_bottom_halo_width</code>：计算当前进程从下方相邻进程需要接收的 halo 区域的宽度。</li>
<li><code>_calc_top_halo_width</code>：计算当前进程从上方相邻进程需要接收的 halo 区域的宽度。</li>
<li><code>_adjust_padding_for_patch</code>：根据当前进程的 <code>rank</code> 和总进程数调整输入数据的填充方式，防止边界重复计算。</li>
</ul>
<ol>
<li>获取输入信息以及通信组信息</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_world_size_and_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">world_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># 处理非分布式情况</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                                <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>获取输入的元数据</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">patch_height_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">patch_height_list</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">h</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">))</span>  <span class="c1"># 收集所有进程的输入高度</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_height_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_patch_height_index</span><span class="p">(</span><span class="n">patch_height_list</span><span class="p">)</span>  <span class="c1"># 计算每个进程块的起始高度和结束高度的索引</span>
</span></span><span class="line"><span class="cl"><span class="n">halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_halo_width_in_h_dim</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># 计算当前进程块的上下 halo 区域的宽度</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>计算相邻进程的 halo 区域 (也就是自己需要接发送的部分)</li>
</ol>
<p>通过计算前一个进程的 bottom_halo_width 和后一个进程的 top_halo_width 得出自己需要发送的部分</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">prev_bottom_halo_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="n">next_top_halo_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">prev_bottom_halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_bottom_halo_width</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="n">world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_top_halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_top_halo_width</span><span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_top_halo_width</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">next_top_halo_width</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>进行 halo 区域的发送与接收</li>
</ol>
<p>异步发送，同步接收</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">to_next</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">to_prev</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">top_halo_recv</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">bottom_halo_recv</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">next_top_halo_width</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bottom_halo_send</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">next_top_halo_width</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_next</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">bottom_halo_send</span><span class="p">,</span> <span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not rank 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_halo_recv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">top_halo_recv</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">prev_bottom_halo_width</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not rank N-1</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_halo_send</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">prev_bottom_halo_width</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_prev</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">top_halo_send</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bottom_halo_recv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">bottom_halo_recv</span><span class="p">,</span> <span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>拼接 halo 区域</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Remove redundancy at the top of the input</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]:,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">top_halo_recv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># concat the halo region to the input tensor </span>
</span></span><span class="line"><span class="cl">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">top_halo_recv</span><span class="p">,</span> <span class="nb">input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">bottom_halo_recv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="nb">input</span><span class="p">,</span> <span class="n">bottom_halo_recv</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="6">
<li>等待发送完成再开始计算</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">to_next</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_next</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">to_prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_prev</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="7">
<li>进行卷积和后处理</li>
</ol>
<p>为了减少 memory spike 一次计算 block_size*block_size 的区域，并将结果拼接起来</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjust_padding_for_patch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">h</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">and</span> <span class="n">w</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                            <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">conv_res</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s2">&#34;zeros&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks_in_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>  <span class="c1"># h 维度的 block 数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks_in_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>  <span class="c1"># w ...</span>
</span></span><span class="line"><span class="cl">    <span class="n">unit_chunk_size_h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">//</span> <span class="n">num_chunks_in_h</span>
</span></span><span class="line"><span class="cl">    <span class="n">unit_chunk_size_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">//</span> <span class="n">num_chunks_in_w</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">idx_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks_in_h</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">inner_output</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx_w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks_in_w</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_w</span> <span class="o">=</span> <span class="n">idx_w</span> <span class="o">*</span> <span class="n">unit_chunk_size_w</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_h</span> <span class="o">=</span> <span class="n">idx_h</span> <span class="o">*</span> <span class="n">unit_chunk_size_h</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">unit_chunk_size_w</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">unit_chunk_size_h</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算每个块的开始和结束索引，调整块的边界</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 对当前块执行卷积操作</span>
</span></span><span class="line"><span class="cl">        <span class="n">inner_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_h</span><span class="p">:</span><span class="n">end_h</span><span class="p">,</span> <span class="n">start_w</span><span class="p">:</span><span class="n">end_w</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">inner_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>VLLM Sourse Code Reading</title>
      <link>http://localhost:57770/blogs/vllm/</link>
      <pubDate>Sat, 07 Jun 2025 18:15:55 +0800</pubDate>
      <guid>http://localhost:57770/blogs/vllm/</guid>
      <description>vllm structure</description>
      <content:encoded><![CDATA[<h1 id="basic">Basic</h1>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample prompts.</span>
</span></span><span class="line"><span class="cl"><span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Hello, my name is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The president of the United States is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The capital of France is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The future of AI is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a sampling params object.</span>
</span></span><span class="line"><span class="cl"><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create an LLM.</span>
</span></span><span class="line"><span class="cl"><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&#34;facebook/opt-125m&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generate texts from the prompts. The output is a list of RequestOutput objects</span>
</span></span><span class="line"><span class="cl"><span class="c1"># that contain the prompt, generated text, and other information.</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Print the outputs.</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
</span></span><span class="line"><span class="cl">    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="architecture">Architecture</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB713ed0d773cac101706cdaa862d71dda?method=download&amp;shareKey=09c7c358d0427427384e027f0ced662a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB713ed0d773cac101706cdaa862d71dda?method=download&amp;shareKey=09c7c358d0427427384e027f0ced662a" alt="VLLM Architecture Overview">
    </a><figcaption>VLLM Architecture Overview</figcaption></figure></p>
<ul>
<li>LLM: 最上层的类，构造函数中会根据传入的参数构建 EngineArgs 然后创建 LLMEngine 对象。</li>
<li>LLMEngine: 包含一些组件 InputPreprocessor, ExecutorBase 负责模型推理的最上层的类</li>
<li>ExecutorBase 会初始化 N 个 WorkerWrapperBase (包装实际的 worker，类比成 GPU)
<ul>
<li>Worker: 在 GPU 上执行 (一部分) 模型推理。每个 worker 与一个 GPU 相关联，负责维护 KV Cache 并在 GPU 上执行模型推理。在分布式推理的情况下，每个 worker 被分配模型的一部分。
<ul>
<li>ModelRunner:  执行模型推理并负责采样新 token.</li>
<li>CacheEngine: 负责初始化和管理 GPU 和 CPU KV Cache. 还提供了对 KV Cache 进行操作的方法。通过 <code>initialize_cache()</code> 初始化。</li>
</ul>
</li>
</ul>
</li>
<li>Scheduler: 负责推理时候对请求的调度。组件包括一个 BlockSpaceManager (KV Cache blocks 管理的核心类) 以及三个队列 waiting, running &amp; swapped.</li>
</ul>
<h1 id="llmengine--initialization">LLMEngine  Initialization</h1>
<ul>
<li>InputPreprocessor: 主要是在 <code>add_request()</code> 方法中将输入的 prompt 放入 tokenizer 进行处理。</li>
<li>InputRegistry: 根据目标模型对 InputPreprocessor 之后的数据进行处理。</li>
</ul>
<h2 id="init-executor">Init Executor</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistributedExecutorBase</span><span class="p">(</span><span class="n">ExecutorBase</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Abstract superclass of distributed executor implementations.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This is non-None when the execute model loop is running</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># in the parallel workers. It&#39;s a coroutine in the AsyncLLMEngine case.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_worker_tasks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Awaitable</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ExecutorBase 的构造函数中会调用 <code>self._init_executor()</code> 对应到具体子类的函数。如果采用 TP 或 PP 的话 对应到的是 RayDistributedExecutor，否则对应到的是 UniProcExecutor. 下面以后者为例。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">UniProcExecutor</span><span class="p">(</span><span class="n">ExecutorBase</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">uses_ray</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_init_executor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Initialize the worker and load the model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">driver_worker</span> <span class="o">=</span> <span class="n">WorkerWrapperBase</span><span class="p">(</span><span class="n">vllm_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                               <span class="n">rpc_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">distributed_init_method</span> <span class="o">=</span> <span class="n">get_distributed_init_method</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_ip</span><span class="p">(),</span> <span class="n">get_open_port</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">local_rank</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># set local rank as the device index if specified</span>
</span></span><span class="line"><span class="cl">        <span class="n">device_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">device_config</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_info</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">device_info</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">vllm_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_rank</span><span class="o">=</span><span class="n">local_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">distributed_init_method</span><span class="o">=</span><span class="n">distributed_init_method</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">is_driver_worker</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="ow">or</span> <span class="p">(</span><span class="n">rank</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;init_worker&#34;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">([</span><span class="n">kwargs</span><span class="p">],</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;init_device&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;load_model&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">collective_rpc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                       <span class="n">method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                       <span class="n">timeout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                       <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(),</span>
</span></span><span class="line"><span class="cl">                       <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="n">answer</span> <span class="o">=</span> <span class="n">run_method</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">driver_worker</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># 初始化 Worker</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="n">answer</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>Executor: 初始化具体的继承自 ExecutorBase 的对象，该对象的初始化过程中会调用 <code>init_worker()</code> 初始化 Worker (被 WorkerWrapperBase 包装)，调用 <code>init_device()</code> 初始化设备，和调用具体 Worker 对象的 model_runner 的 <code>load_model()</code> 将模型加载到设备上。
<ul>
<li>Worker: 构造函数中会初始化 <code>GPUModelRunnerBase</code> 对象，确定计算 attention 使用的 backend 还有 CUDAGraphRunner 用于将模型的计算过程记录为一个静态图，在后续的推理中，通过直接 replay 这个静态图来避免动态调度和重复的内核启动开销。</li>
</ul>
</li>
</ul>
<h2 id="initialize_kv_caches">initialize_kv_caches</h2>
<p>LLMEngine 构造函数在初始化 ExecutorBase 后会调用 <code>initialize_kv_caches()</code> 来初始化 Worker 中的 KV Cache，流程如下:</p>
<ol>
<li>该函数会首先通过 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/neuron_worker.py#L69">Worker.determine_num_available_blocks()</a> 确定 GPU 和 CPU 可用的 block 数量。后者在 <code>memory_profiling</code> 上下文中进行 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/model_runner.py#L1239">profile_run()</a> 模拟模型在最大负载 (max_num_batched_tokens 和 max_num_seqs) 下执行一次推理。测量内存使用并分解为权重、激活张量和非 PyTorch 部分。留给 KV Cache 的内存大小为 <code>total_mem * max_utilization - weight_mem - act_mem - nontorch_mem</code>.  再除以每一个 block 能存储的的 KV Cache 大小 <code>cache_size = Cache_config.block_size * num_attention_layers * 2*num_heads*head_size</code> 即可得到最多能分配多少个 GPU block. 而 CPU block 数量由预设的 <code>swap_size // cache_size</code> 所确定。</li>
<li>确定了 GPU 和 CPU 的 block 数量后会调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/worker.py#L285">Worker.initialize_cache()</a> 方法，里面首先会调用 <code>Worker._init_cache_engine()</code> 根据传入的 GPU block 个数初始化 CacheEngine (初始化 attn_backend，调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/cache_engine.py#L68">CacheEngine._allocate_kv_cache()</a> 为模型的每一层 transformer 开辟 CPU 和 GPU 的 KV Cache 内存)，然后会调用 <a href="https://github.com/vllm-project/vllm/blob/main/vllm/utils.py#L2163">bind_kv_cache()</a> 将 GPU KV Cache Tensor 绑定到对应的模型的注意力层，它筛选需要 KV Cache 的注意力层，按层索引排序并去重后为每个设备绑定对应的 Tensor.</li>
<li>预热之后进行 capture_model 记录计算图。</li>
</ol>
<h2 id="init-scheduler">Init Scheduler</h2>
<p>构造函数中会初始化 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block_manager.py#L61">BlockSpaceManager</a>. 首先会创建一个 <code>CpuGpuBlockAllocator</code>，为 CPU 和 GPU 块维护单独的内存池，并允许在这些内存池中分配、释放、分叉和交换块。它会为 CPU 和 GPU 中的 blocks 分别创建一个 <code>BlockAlloctor</code>. 还会初始化一个空的 <code>Dict[SeqId, BlockTable]</code>， 表示对应 seq 的 KV Cache 所使用的物理内存块。还会初始化一些调度时所需要的数据，后文再谈。</p>
<p>还会初始化 waiting(包含新的或 preempted prefill 请求), running &amp; swapped(被换出的 decoding 请求), 它们是 <code>Deque[SequenceGroup]</code>，其中 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/sequence.py#L633">SequenceGroup(SG)</a> 是一组由同一个 prompt 生成的 Sequences 和对应的采样参数。</p>
<ul>
<li>SequenceGroupOutputProcessor: 抽象基类借接口，会分为 SingleStepOutputProcessor (支持 beam seaching) 和 MultiStepOutputProcessor (支持 speculatice decoding)</li>
</ul>
<h1 id="llm-generate">LLM Generate</h1>
<h2 id="_validate_and_add_requests">_validate_and_add_requests</h2>
<p>里面会调用 <code>_add_request()</code> 给 prompt 分配 reqest_id 后会调用 <code>LLMEngine.add_request()</code> 将其添加到请求池中，并将在调用 <code>LLMEngine.step()</code> 时由调度器处理。确切的调度策略由调度程序确定。主要就是进行 tokenize，然后打包成 SG 后加入 waiting.</p>
<h2 id="__run_engine">__run_engine</h2>
<p>调用 generate 时首先会将 prompt 包装成 SG，它是包含某个 prompt 生成的所有 Sequence，以及一些其他在调度时需要的信息的结构。Scheduler 里面包含三个 <code>Deque[SequenceGroup]</code>: waiting, running &amp; swapped.
generate() &ndash;&gt; _run_engine() &ndash;&gt; step() &ndash;&gt; Scheduler.schedule() &ndash;&gt; Scheduler._schedule()
Scheduler 的一些操作与 BlockManager 息息相关，我们在下面先简要说明逻辑，有关其具体结构和操作流程在后文中解释。</p>
<h2 id="step">step</h2>
<p>执行一次 decoding 迭代并返回新生成的结果。

<figure class="post-figure">
    <a href="https://i.imgur.com/sv2HssD.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://i.imgur.com/sv2HssD.png" alt="Overview of the step function">
    </a><figcaption>Overview of the step function</figcaption></figure>
主要流程如下</p>
<ol>
<li>调度要在下一次迭代中执行的 seq 和要交换入/出/复制的令牌块。根据调度策略，Sequences 可能被抢占/重新排序。</li>
<li>调用分布式执行器来执行模型。</li>
<li>处理模型输出。主要包括： decoding 相关输出，使用 _beam_search 与否的模型输出更新调度 seq 组和释放已完成的 seq 组。</li>
<li>读取上一次调度的元数据和输出</li>
<li>如果没有剩余步骤且，调用 <code>Scheduler.schedule()</code> 执行新调度，生成 seq 组元数据、调度输出和异步标志。</li>
<li>获取并重置已完成请求 ID，清理内存</li>
<li>如果不允许异步且有输出队列，处理模型输出。</li>
<li>从 Cache 获取上一次迭代的 sampled_token_ids，构造 ExecuteModelRequest 后调用 <code>Executor.execute_model()</code> (最后是由 ModelRunner) 执行模型推理，获取输出。</li>
</ol>
<h2 id="_schedule_prefill">_schedule_prefill()</h2>
<ol>
<li>检查 budget 是否耗尽</li>
<li>取出队列head 部的 SequenceGroup (prefill 阶段 SequenceGroup 只有一个初始 prompt Sequence)</li>
<li>计算 uncached 和 cached 的新 token 数</li>
<li>调用 <code>BlockSpaceManager.can_allocate()</code> 检查是否能分配足够内存。</li>
<li>若能满足 budget，从 waiting 中移除 SequenceGroup. 调用 <code>_allocate_and_set_running()</code> 分配内存并设置为 RUNNING 状态。</li>
</ol>
<h2 id="_schedule_running">_schedule_running()</h2>
<ol>
<li>取出队列head 部 SequenceGroup 并计算其包含 seq 的 #uncached_token. 这里不需要 #cached_token 因为若使用 chunked prefill，该信息已经在第一次 prefill 时使用，如果不使用那么他就是进行 decoding 的 seq ，不需要用到这个信息。</li>
<li>从 running 移除该 SequenceGroup. 循环调用 <code>Scheduler._can_append_slots()</code> 检查是否有足够的空间存储该 SequenceGroup 的 KV Cache，若不能，进入抢占逻辑</li>
<li>从 budget 中减去当前 SequenceGroup 的 token 和 seq 数</li>
<li>若 running 有其他 SequenceGroup，抢占最低优先级（队列尾部）的，若该 SequenceGroup 只有一个正在运行的 Sequence 则抢占模式为 RECOMPUTE 加入到 <code>preempted</code>，否则为 SWAP 加入到 <code>swapped_out</code>.</li>
<li>分配 slot 并更新 blocks_to_copy，根据该 Sequence 处于 decoding(生成 1 个 token 的 KV Cache ) 或者 prefill(生成 #uncached_token 的 KV Cache) 加入到 <code>prefill_seq_group</code> 或者 <code>decode_seq_groups</code>，并更新 budget.</li>
<li>返回 decode_seq_groups：存储 decoding  SequenceGroup. prefill_seq_groups：存储分块 prefill  SequenceGroup. preempted：被抢占需重新计算的 SequenceGroup. swapped_out：被交换到 CPU 的 SequenceGroup. keys_to_swap_out 和 keys_to_copy：内存块交换和复制的映射</li>
</ol>
<h2 id="_schedule_swapepd">_schedule_swapepd()</h2>
<ol>
<li>循环遍历 swapped 队列，取出队列head 部的 SequenceGroup，调用 <code>BlockManager.can_swap_in()</code> (实际上是 SWAPPED 状态的 <code>can_swap</code>)</li>
<li>获取 SequenceGroup 中处于 SWAPPED 的 Sequence 个数和 token 个数，是否满足预算。</li>
<li>调用 <code>_swap_in</code>(实际上是 <code>BlockManager.swap_in()</code>) 执行交换，更新 blocks_to_swap_in，将 Sequence 状态由 SWAPPED 变为 RUNNING.</li>
<li>调用 <code>_append_slots</code> 给被换入的 Sequence 分配 block.</li>
<li>根据 SequenceGroup 的状态添加到不同队列。</li>
<li>返回blocks_to_swap_in：记录需要从 CPU 交换到 GPU 的块映射。blocks_to_copy：记录需要复制的块映射（例如写时复制）。decode_seq_groups 和 prefill_seq_groups：分别存储 decoding 和 prefill  SequenceGroup. infeasible_seq_groups：存储无法调度的 SequenceGroup. swapped_queue：引用交换队列。leftover_swapped：暂存无法立即调度的 SequenceGroup.</li>
</ol>
<h2 id="_schedule_chunked_prefill">_schedule_chunked_prefill()</h2>
<p>主要思想是: 1.安排尽可能多的 decoding 请求。2.调度未完成的 prefill 请求。3.调度交换请求。4.安排新的 prefill 请求。</p>
<ol>
<li>初始化 budget，限制最大批处理 token 数和 seq 数。</li>
<li>从 running 和 waiting 生成 <code>PartialPrefillMetadata</code></li>
</ol>
<ul>
<li>prefills: running 和 waiting 中未完成 prefill 的 #SequenceGroup.</li>
<li>long_prefills: running 中需要进行 prefill 的 token 数很多的 #SequenceGroup.</li>
<li>waiting_long_prefills: waiting 中需要进行且能进行的 (未超过 ScheduleConfig 限制) prefill 的 token 数很多的 #SequenceGroup.</li>
</ul>
<ol start="3">
<li>调用 <code>_schedule_running</code>.</li>
<li>在 running 调度返回中无无抢占或交换时(说明有足够空间) 执行 <code>_schedule_swapped</code></li>
<li>调用 <code>_schedule_prefills</code>.</li>
<li>更新 waiting，添加 running 调度中返回的被抢占的 seq  <code>running_scheduled.preempted</code>.</li>
<li>按优先级更新 running.</li>
<li>swapped_in.decode_seq_groups：交换回来的 decoding 请求。</li>
<li>swapped_in.prefill_seq_groups：交换回来的 prefill 请求。</li>
<li>running_scheduled.decode_seq_groups：运行中的 decoding 请求。</li>
<li>running_scheduled.prefill_seq_groups（按完成顺序）：未完成的分块 prefill 。使用 _order_finishing_prefills_first 确保即将完成的 prefill 优先，便于下一轮转为 decoding.</li>
<li>prefills.seq_groups：新 prefill 请求。</li>
<li>将运行队列中交换出去的 <code>running_scheduled.swapped_out</code> 添加到 swapped.</li>
<li>按顺序组合所有调度的 SequenceGroup: prefill 优先（满足注意力机制假设），decoding 次之。</li>
<li>调整 lookahead_slots 数量。若所有被调度的均为 prefill 且未启用多步调度，设置 num_lookahead_slots = 0(避免推测 decoding 路径). 否则，使用 running 计算的 lookaheadh slots 数量。</li>
</ol>
<h2 id="_schedule_default">_schedule_default</h2>
<p>尽可能多地批处理 prefill 请求，然后调度 decoding 请求. 在 GPU 内存压力下，需要 preempt 或 swap out 运行中的 decoding 请求。</p>
<ol>
<li>swapped 为空则进行 <code>_schedule_prefills</code>.</li>
<li>如果没有调度任何 prefill 请求，调用 <code>_schedule_running</code>.</li>
<li>如果 running 调度结果中没有发生抢占或换出时 (否则说明资源不够)，执行 <code>_schedule_swapped</code>.</li>
<li>更新 waiting, running &amp; swapped 三个队列。</li>
</ol>
<h2 id="after-schedule">After schedule</h2>
<p>调度结果返回后，</p>
<ol>
<li>遍历调度结果中的 SequenceGroup</li>
<li>遍历该 SequenceGroup 中状态为 RUNNING 的 Sequence. 获取其数据，对应的 BlockID 列表，并更新其访问时间。若使用 prefix_caching, 则调用 <code>BlockManager.get_common_computed_block_ids()</code> 获取共享的已计算的部分的 BlockID 列表。</li>
<li>如果该 SequenceGroup 处于 prefill 阶段，则判断这次调度后是否能完成 prefill.</li>
<li>构造返回结果，标记所有调度 SequenceGroup 的 blocks 为已计算。</li>
</ol>
<h1 id="blockspacemanager">BlockSpaceManager</h1>
<p>用于将 SequenceGroup 操作映射到其包含的对应组件的操作。</p>
<ul>
<li>CpuGpuBlockAlloctor: 根据是否采用 prefix caching 分别为 CPU 和 GPU 初始化一个 Alloctor
<ul>
<li><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/prefix_caching_block.py#L53">PrefixCachingBlockAlloctor</a>: 基于哈希值维护 block 的Cache)重用具有相同哈希值的 block，以避免冗余的内存分配。
<ul>
<li><code>Dict[PrefixHash, BlockId]</code> 将用于 prefix caching blocks 的哈希值与其 BlockID 对应。</li>
<li><code>Dict[BlockId, BlockTracker]</code> 为每个物理 block 初始化一个 BlockTracker.</li>
<li><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/naive_block.py#L13">NaiveBlockAllocator</a> 用于分配不作为 prefix caching 的 blocks. 有一个 <code>RefCounter</code> 表示某个物理 block 被多少逻辑 block 指向。</li>
<li><code>Evictor</code> 采用 LRU 策略驱逐已经Cache) blocks.</li>
<li><code>CopyOnWriterTracker</code> 用于将原先的 block ID 映射到目的 block ID.</li>
</ul>
</li>
</ul>
</li>
<li>Dict[SeqId, BlockTable]: <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/block_table.py#L11">BlockTable</a> 用于将单个 seq 的 KV Cache 映射到物理内存分配。会在调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block_manager.py#L148">_allocate_sequence()</a> 时被初始化。包含一个 <a href="https://github.com/vllm-project/vllm/blob/main/vllm/core/block/common.py#L231">BlockList</a> (block 列表和一个表示对应 ID 的 int 列表) 和 BlockpaceManager 的 BlockAllocator.</li>
<li>ComputedBlocksTracker: 维护一个 <code>Dict[SeqId, List[int]]</code> ( seq id到 seq 块哈希列表的映射)。Cache)个 seq 的完整块 (块全部被占满) 的哈希值。当一个 seq 进行 decoding 时，也相应更新 seq 的哈希值。还有一个 <code>Dict[int, int]</code> ( seq id到已计算 token 数的映射)</li>
</ul>
<h2 id="can_allocate">can_allocate</h2>
<p>在 <code>_schedule_prefills</code> 中被调用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">can_allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol>
<li>取出该 SequenceGroup 中处于 WAITING 状态的第一个 Sequence (i.e. prompt).</li>
<li>调用 <code>BlockTable.get_num_required_blocks()</code> 计算存储 token 和 lookahead slots 所需的最小 block 数 (假设无 prefix caching), i.e. <code>cdiv(len(token_ids) + num_lookahead_slots, block_size)</code>.</li>
<li>调用 <code>BlockAlloctor.get_num_free_blocks()</code> 获取 GPU 上空闲的 block 数 (非 prefix_caching 中的空闲个数 + 可以被驱逐的个数).</li>
<li>返回分配状态</li>
</ol>
<ul>
<li>NEVER: <code>#total - #required &lt; #watermark</code></li>
<li>OK: <code>#free  - #required &gt;= #watermark</code></li>
<li>LATER: <code>#free  - #required &lt; #watermark</code></li>
</ul>
<h2 id="allocate">allocate</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 <code>_schedule_prefills</code> 中步骤 4 中调用的 <code>_allocate_and_set_running</code> 内部被调用。</p>
<ol>
<li>取出该 SequenceGroup 中处于 WAITING 状态的第一个 Sequence (i.e. prompt).</li>
<li>调用 <code>BlockManager._allocate_sequence()</code> 创建一个 BlockTable，在获取 token_ids 列表后调用 <code>BlockTable.allocate()</code> 为该 Sequence 分配 blocks.</li>
<li>将 token_ids 按 _block_size 大小进行分块。最后一块可能不能占满一个 block.</li>
<li>对于能够占满一个 block 的 token_ids 分块，调用 <code>BlockAlloctor.allocate_immutable_block()</code>. 该函数优先从Cache)查找是否已有相同内容的块，若有则直接复用该块并增加其引用计数；否则调用 <code>BlockAlloctor.allocate_mutable_blocks()</code> 分配一个新的 block，并将 token_ids 添加到该 block 中. 该函数会尝试从非 prefix caching blocks 中分配一个 block_id，若没找到则会驱逐一个。</li>
<li>对于最后一个可能被没占满的 block 调用 <code>BlockAlloctor.allocate_mutable_blocks()</code>.</li>
</ol>
<h2 id="can_append_slots">can_append_slots</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">can_append_slots</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>确定 GPU KV Cache 中是否有足够的空间来继续生成指定的 SequenceGroup. 上层接口为 <code>Scheduler._can_append_slots()</code>，在 <code>_schedule_running</code> 中步骤 2 中确定是否需要进行抢占时被调用。</p>
<ol>
<li>遍历该 Sequence Group 中处于 RUNNING 状态的 Sequence 对应的 BlockTable</li>
<li>调用 <code>BlockTable.get_unseen_token_ids()</code> 获取该 Sequence 还未被Cache) token 部分。</li>
<li>调用 <code>BlockTable.get_num_blocks_touched_by_append_slots()</code> 获取Cache)余部分和 lookahead 部分需要几个 block.</li>
<li>调用 <code>BlockAlloctor.get_num_free_blocks()</code> 获取 GPU 上空闲的 block 数.</li>
<li>需要个数小于空闲个数返回 True.</li>
</ol>
<h2 id="append_slots">append_slots</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">append_slots</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上层接口为 <code>Scheduler._append_slots()</code>. 在 <code>_schedule_running</code> 中检查到有空间添加，<code>_schedule_swapped</code> 中有 budget 进行换入，<code>_schedule_prefills</code> 中允许进行 chunked prefill 时被调用。</p>
<ol>
<li>调用 <code>BlockTable.append_token_ids()</code>. 该方法将 tokens 添加到 BlockTable 中的现有 block 中。会调用 <code>BlockTable.ensure_num_empty_slots()</code>， 它查看当前能够容纳多少个 token. 如果没有足够的空间，则使用 <code>BlockAlloctor.allocate_mutable_block()</code> 方法分配新 block.</li>
<li>调用 <code>BlockAllocator.clear_copy_on_writes()</code> 返回一个映射源 block ID 到当前 COW 的目标 block ID 的元组的列表.</li>
</ol>
<h2 id="_can_swap">_can_swap</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_can_swap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">status</span><span class="p">:</span> <span class="n">SequenceStatus</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>根据 status 区分上层接口: RUNNING/SWAPPED 表示需要把该 SequenceGroup 处于 RUNNING/SWAPPED 状态的 Sequence 对应的 blocks 从 GPU/CPU 换到 CPU/GPU.</p>
<ol>
<li>获取 SequenceGroup 中符合指定状态的 seq  Sequence，然后根据 SeqID 获取对应的 BlockTable.</li>
<li>调用 <code>BlockTable.get_num_blocks_touched_by_append_slots()</code> 计算添加未存储 token 加上 lookahead_slots 所需的 block 数量。</li>
<li>调用 <code>BlockAlloctor.get_num_full_blocks_touched()</code> 获取当前有被使用的 block 数量。</li>
<li>如果总块数小于被使用的加上需要的 block 数量 返回 Never. 如果空闲块减去 被使用的加上需要的 block 数量后仍大于等于 watermark_blocks，返回 OK. 否则为 LATER.</li>
</ol>
<h2 id="swap_in">swap_in</h2>
<p>调用的是  <code>self.block_allocator.swap(blocks=blocks, src_device=Device.CPU, dst_device=Device.GPU)</code>，即 blocks 从原设备的换出，换入到目的设备。
进一步则是 <code>BlockAlloctor.swap_in()</code>，该函数遍历传入的 blocks，若已经被占满调用 <code>BlockAlloctor.allocate_immutable_block()</code>. 否则调用 <code>BlockAlloctor.allocate_mutable_blocks()</code> 分配一个新的 block 后将原 block的 token 数据追加到新 block.</p>
<h2 id="swap_out">swap_out</h2>
<p>同上，最终调用的是 <code>BlockAlloctor.swap_out()</code>. 该函数对传入的每个 block 调用 <code>_free_block_id</code>，逐个处理释放逻辑。若 block 有哈希值，refcount -1，若减去后为 0 则将 block 信息添加到 evictor 中，从跟踪系统中移除，然后设置 BlockId 为 None. 否则就直接设置为 None. 若无哈希值则释放 BlockId，减去对应的 refcount，但保留 block 对象本身.</p>
<h1 id="attention">Attention</h1>
<p><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/attention/backends/xformers.py#L354">XFormersImpl</a> 中使用了 vllm 自己写的 PagedAttention kernel.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">XFormersImpl</span><span class="p">(</span><span class="n">AttentionImpl</span><span class="p">[</span><span class="n">XFormersMetadata</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">head_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">num_kv_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">alibi_slopes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">      <span class="n">sliding_window</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">      <span class="n">kv_cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">blocksparse_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">logits_soft_cap</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">attn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其中 <code>attn_type</code> 分为四种，下面我们主要分析 DECODER 的情况。</p>
<ul>
<li>DECODER: 使用 decoding 器的 self-attention block table 来Cache)KV(GPT).</li>
<li>ENCODER: 不进行 KV Cache)用于 Encoder-Decoder 模编码器分支。编码器通常一次性处理整个输入 seq 。</li>
<li>ENCODER-ONLY: 不进行 KV Cache)BERT).</li>
<li>ENCODER_DECODER: 用于编码器- decoding 器模型中的交叉注意力部分，其中 KV  seq 长度与编码器 seq 长度一致(T5).</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">layer</span><span class="p">:</span> <span class="n">AttentionLayer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [2, num_blocks, block_size * num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/attention/backends/.py#L104">AttentionMetadata</a> 类定义如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@dataclass</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AttentionMetadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Attention metadata for prefill and decode batched together.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_prefills</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># prefill 请求的总数</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_prefill_tokens</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># 所有 prefill 请求中的 token 总数。</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_decode_tokens</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># decodeing token 的数量，等同于 decoding 请求的数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">slot_mapping</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>  <span class="c1"># (num_tokens,)，指定每个输入 token 存储到 KV cache 中的 slot 索引</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># block_idx = x // block_size, block_offset = x % block_size</span>
</span></span><span class="line"><span class="cl">    <span class="n">multi_modal_placeholder_index_maps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="nb">str</span><span class="p">,</span> <span class="n">MultiModalPlaceholderMap</span><span class="o">.</span><span class="n">IndexMap</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">    <span class="n">enable_kv_scales_calculation</span><span class="p">:</span> <span class="nb">bool</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>forward 方法如下，简化了成了 DECODER 情况的逻辑。
主要流程为</p>
<ol>
<li>调用 <code>PagedAttention.split_kv_cache</code> 分离并 reshape KV Cache 张量后 调用 PagedAttention.write_to_paged_cache`
写入当前 key 和 value 到Cache)。</li>
<li>分离 prefill 和 decoding 的 token，初始化输出。对于 prefill 部分根据是否采用了 prefix_caching 调用 <code>self._run_memory_efficient_xformers_forward</code> 或 <code>PagedAttention.forward_prefix</code> 计算注意力。</li>
<li>调用 <code>get_seq_len_block_table_args</code> 获取 decoding Sequence 对应的 BlockTable后调用 <code>PagedAttention.forward_decode</code> 计算注意力。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">layer</span><span class="p">:</span> <span class="n">AttentionLayer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>    <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [2, num_blocks, block_size * num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">    <span class="c1"># 将 query 重塑为 [num_tokens, num_heads, head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># key 和 value 必须非空（自注意力要求），重塑为 [num_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果 KV Cache)空，处理Cache)辑</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kv_cache</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 从 kv_cache 分离出 key_cache 和 value_cache</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># key_cache: [num_blocks, num_kv_heads, head_size/x, block_size, x]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># value_cache: [num_blocks, num_kv_heads, head_size, block_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">split_kv_cache</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv_cache</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新自注意力的 KV Cache)        # 使用 attn_metadata.slot_mapping 指定 token 存储位置</span>
</span></span><span class="line"><span class="cl">        <span class="n">PagedAttention</span><span class="o">.</span><span class="n">write_to_paged_cache</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">slot_mapping</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取 prefill 和 decoding 阶段的 token 数量</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">num_prefill_query_tokens</span><span class="p">,</span> <span class="n">num_prefill_kv_tokens</span><span class="p">,</span> <span class="n">num_decode_query_tokens</span><span class="p">)</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">        <span class="n">get_num_prefill_decode_query_kv_tokens</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 创建输出张量与 query 相同</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 分离 prefill 和 decoding 的 QKV</span>
</span></span><span class="line"><span class="cl">    <span class="n">decode_query</span> <span class="o">=</span> <span class="n">query</span><span class="p">[</span><span class="n">num_prefill_query_tokens</span><span class="p">:]</span>  <span class="c1">#</span>
</span></span><span class="line"><span class="cl">    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span>     
</span></span><span class="line"><span class="cl">    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[:</span><span class="n">num_prefill_kv_tokens</span><span class="p">]</span>             
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[:</span><span class="n">num_prefill_kv_tokens</span><span class="p">]</span>         
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 prefill 阶段（如果存在）</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">prefill_meta</span> <span class="o">:=</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">prefill_metadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">kv_cache</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 普通注意力（无Cache)缀）</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_memory_efficient_xformers_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="p">,</span> <span class="n">attn_type</span><span class="o">=</span><span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 前缀Cache)意力</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">forward_prefix</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">prefill_meta</span><span class="o">.</span><span class="n">block_tables</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">query_start_loc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">prefill_meta</span><span class="o">.</span><span class="n">seq_lens_tensor</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">max_query_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 decoding 阶段（如果存在）</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">decode_meta</span> <span class="o">:=</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">decode_metadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 获取 decoding 所需的 seq 长度和 BlockTable 参数</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq_lens_arg</span><span class="p">,</span> <span class="n">max_seq_len_arg</span><span class="p">,</span> <span class="n">block_tables_arg</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">get_seq_len_block_table_args</span><span class="p">(</span><span class="n">decode_meta</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 运行 decoding 注意力</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">[</span><span class="n">num_prefill_query_tokens</span><span class="p">:]</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">forward_decode</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">decode_query</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">block_tables_arg</span><span class="p">,</span> <span class="n">seq_lens_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_seq_len_arg</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 将输出 reshape 为 [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="write_to_paged_cache">write_to_paged_cache</h2>
<p>调用的是已经注册到 torch.ops 中的 CUDA 函数。其对应的 host 函数为每个 token 分配一个 CUDA block，每个 CUDA block 的线程数被限制在最多 512 个。主要的 kernel 函数如下。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// scalar_t: 输入 key 和 value 的数据类型（如 float、half）
</span></span></span><span class="line"><span class="cl"><span class="c1">// cache_t: Cache)key_cache 和 value_cache 的数据类型（如 half、uint8_t）
</span></span></span><span class="line"><span class="cl"><span class="c1">// kv_dt: KV Cache) FP8 数据类型（如 kAuto 或具体 FP8 格式）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">template</span> <span class="o">&lt;</span><span class="kr">typename</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="kr">typename</span> <span class="kt">cache_t</span><span class="p">,</span> <span class="n">Fp8KVCacheDataType</span> <span class="n">kv_dt</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">reshape_and_cache_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">key</span><span class="p">,</span>    <span class="c1">// [num_tokens, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">value</span><span class="p">,</span>  <span class="c1">// [num_tokens, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">key_cache</span><span class="p">,</span>     <span class="c1">// [num_blocks, num_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">value_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">slot_mapping</span><span class="p">,</span>  <span class="c1">// [num_tokens]，指定每个 token 的Cache)置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">key_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">value_stride</span><span class="p">,</span>  <span class="c1">// key 和 value 在 token 维的步幅
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_heads</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">head_size</span><span class="p">,</span>      <span class="c1">// 注意力head 数和每个head 的维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">block_size</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">x</span><span class="p">,</span>             <span class="c1">// Cache)大小和 key_cache 中 head_size 的拆分因子
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">k_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v_scale</span><span class="p">)</span>    <span class="c1">// key 和 value 的缩放因子，用于数据类型转换
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  <span class="c1">// host 函数定义 block 个数与 token 个数相同
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">slot_idx</span> <span class="o">=</span> <span class="n">slot_mapping</span><span class="p">[</span><span class="n">token_idx</span><span class="p">];</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// Cache Block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">slot_idx</span> <span class="o">/</span> <span class="n">block_size</span><span class="p">;</span>  <span class="c1">// 块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">block_offset</span> <span class="o">=</span> <span class="n">slot_idx</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">;</span>  <span class="c1">// 块内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">;</span>  <span class="c1">// 每个 token 的维度数目
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// CUDA Block 级别并行，每个线程处理token 的一个维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算输入 key 和 value 的源索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">src_key_idx</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">*</span> <span class="n">key_stride</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">src_value_idx</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">*</span> <span class="n">value_stride</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算当前处理的head 索引和head 内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">head_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="n">head_size</span><span class="p">;</span>      <span class="c1">// 第几个head 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">head_offset</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">head_size</span><span class="p">;</span>   <span class="c1">// head 内的第几个元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 将 head_offset 拆分为 x_idx 和 x_offset（仅用于 key_cache）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">x_idx</span> <span class="o">=</span> <span class="n">head_offset</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>       <span class="c1">// head_size/x 维的索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">x_offset</span> <span class="o">=</span> <span class="n">head_offset</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>    <span class="c1">// x 维的偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算 key_cache 的目标索引，按维度逐步偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">tgt_key_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">head_size</span> <span class="o">/</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>  <span class="c1">// 块偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_idx</span> <span class="o">*</span> <span class="p">(</span><span class="n">head_size</span> <span class="o">/</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>               <span class="c1">// head 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">x_idx</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>                                    <span class="c1">// head_size/x 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">block_offset</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">;</span>                                <span class="c1">// 块内和 x 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算 value_cache 的目标索引，按维度逐步偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">tgt_value_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>            <span class="c1">// 块偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_idx</span> <span class="o">*</span> <span class="n">head_size</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>                         <span class="c1">// head 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_offset</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>                                  <span class="c1">// head_size 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">block_offset</span><span class="p">;</span>                                               <span class="c1">// 块内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 从输入张量读取当前元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">scalar_t</span> <span class="n">tgt_key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="n">src_key_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">scalar_t</span> <span class="n">tgt_value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="n">src_value_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 根据 kv_dt 类型决定存储方式
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="nf">constexpr</span> <span class="p">(</span><span class="n">kv_dt</span> <span class="o">==</span> <span class="n">Fp8KVCacheDataType</span><span class="o">::</span><span class="n">kAuto</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 如果是 kAuto，直接存储，不进行类型转换
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">key_cache</span><span class="p">[</span><span class="n">tgt_key_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_key</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">value_cache</span><span class="p">[</span><span class="n">tgt_value_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_value</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 否则，使用 scaled_convert 进行类型转换（如 FP8 量化）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">key_cache</span><span class="p">[</span><span class="n">tgt_key_idx</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">          <span class="n">fp8</span><span class="o">::</span><span class="n">scaled_convert</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="n">kv_dt</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tgt_key</span><span class="p">,</span> <span class="o">*</span><span class="n">k_scale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="n">value_cache</span><span class="p">[</span><span class="n">tgt_value_idx</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">          <span class="n">fp8</span><span class="o">::</span><span class="n">scaled_convert</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="n">kv_dt</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tgt_value</span><span class="p">,</span> <span class="o">*</span><span class="n">v_scale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="_run_memory_efficient_xformers_forward">_run_memory_efficient_xformers_forward</h2>
<p>也同样简化成 DECODER 的逻辑的情况</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_memory_efficient_xformers_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_prefill_tokens, num_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>    <span class="c1"># [num_prefill_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_prefill_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">original_query</span> <span class="o">=</span> <span class="n">query</span>  <span class="c1"># 保存原始 query，用于最后 reshape 输出</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 GQA/MQA</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape Q to [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># expand K to [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># expand V to  [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                            <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取或设置 attention bias</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">_get_attn_bias</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attn_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># 确保 seq 长度信息存在</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 创建 causal mask</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">BlockDiagonalCausalMask</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># 如果有滑动窗口，应用局部注意力</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">make_local_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="p">[</span><span class="n">attn_bias</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 使用 ALiBi 偏置（线性偏置注意力）</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">_make_alibi_bias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_set_attn_bias</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 执行 xFormers 高效注意力计算</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 为 QKV 添加 batch</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ALiBi 模式直接使用 attn_bias</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">original_query</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># xformers 不支持在自定义 bias 的情况下每个 seq 的长度不同</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">):</span> 
</span></span><span class="line"><span class="cl">            <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">key</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">value</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">original_query</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">            <span class="n">start</span> <span class="o">+=</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 将输出 reshape 为原始 query </span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">original_query</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="forward_prefix">forward_prefix</h2>
<p>不考虑 ALiBi 的情况调用的是 triton 编写的 <a href="https://github.com/vllm-project/vllm/blob/d1695758b2f65fd314d1aee71ba2469ceba67a5b/vllm/attention/ops/prefix_prefill.py#L22">_fwd_kernel()</a> 每个线程块独立处理一个 Q 的一部分，对 KV Cache 和 当前 KV 分别采取 flash-attention 的计算策略。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">triton</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">triton.language</span> <span class="k">as</span> <span class="nn">tl</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@triton.jit</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_fwd_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 输入张量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">Q</span><span class="p">,</span>  <span class="c1">#  Query 张量: [total_seq_len, num_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># total_seq_len 是所有 batch  seq 长度的总和，当前块为 [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="n">K</span><span class="p">,</span>  <span class="c1"># 键张量（当前输入）: [total_seq_len, num_kv_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">    <span class="n">V</span><span class="p">,</span>  <span class="c1"># 值张量（当前输入）: [total_seq_len, num_kv_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">    <span class="n">K_cache</span><span class="p">,</span>  <span class="c1"># 键Cache) [num_blocks, num_kv_heads, head_dim, block_size, x]</span>
</span></span><span class="line"><span class="cl">              <span class="c1"># 用于存储上下文部分的 K</span>
</span></span><span class="line"><span class="cl">    <span class="n">V_cache</span><span class="p">,</span>  <span class="c1"># 值Cache) [num_blocks, num_kv_heads, head_dim, block_size]</span>
</span></span><span class="line"><span class="cl">              <span class="c1"># 用于存储上下文部分的 V</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Loc</span><span class="p">,</span>  <span class="c1"># 块索引表: [batch_size, max_seq_len // block_size]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 记录每个 batch 中每个块的块编号</span>
</span></span><span class="line"><span class="cl">    <span class="n">sm_scale</span><span class="p">,</span>  <span class="c1"># softmax 缩放因子，通常为 1/sqrt(head_dim)</span>
</span></span><span class="line"><span class="cl">    <span class="n">k_scale</span><span class="p">,</span>  <span class="c1"># 用于 FP8 精度转换的缩放因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">v_scale</span><span class="p">,</span>  <span class="c1"># 用于 FP8 精度转换的缩放因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Start_Loc</span><span class="p">,</span>  <span class="c1">#  batch 起始位置: [batch_size + 1]</span>
</span></span><span class="line"><span class="cl">                  <span class="c1"># 每个 batch 的全局 seq 起始索引，最后一个元素是总长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Seqlen</span><span class="p">,</span>  <span class="c1">#  batch  seq 长度: [batch_size]</span>
</span></span><span class="line"><span class="cl">               <span class="c1"># 每个 batch 的总 seq 长度（上下文 +  Query ）</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_size</span><span class="p">,</span>  <span class="c1"># 每个Cache)的大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span>  <span class="c1"># K_cache 的额外维度分片因子（通常为 1 或小整数）</span>
</span></span><span class="line"><span class="cl">    <span class="n">Out</span><span class="p">,</span>  <span class="c1"># 输出张量: [total_seq_len, num_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># 存储注意力计算结果</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 步幅参数 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_b_loc_b</span><span class="p">,</span>  <span class="c1"># B_Loc 的 batch 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_b_loc_s</span><span class="p">,</span>  <span class="c1"># B_Loc 的 seq 块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qbs</span><span class="p">,</span>  <span class="c1"># Q 的 batch / seq 步幅，通常为 num_heads * head_dim</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qh</span><span class="p">,</span>   <span class="c1"># Q 的head 步幅，通常为 head_dim</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qd</span><span class="p">,</span>   <span class="c1"># Q 的head_size步幅，通常为 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kbs</span><span class="p">,</span>  <span class="c1"># K 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kh</span><span class="p">,</span>   <span class="c1"># K 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kd</span><span class="p">,</span>   <span class="c1"># K 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vbs</span><span class="p">,</span>  <span class="c1"># V 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vh</span><span class="p">,</span>   <span class="c1"># V 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vd</span><span class="p">,</span>   <span class="c1"># V 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_obs</span><span class="p">,</span>  <span class="c1"># Out 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_oh</span><span class="p">,</span>   <span class="c1"># Out 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_od</span><span class="p">,</span>   <span class="c1"># Out 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_bs</span><span class="p">,</span>  <span class="c1"># K_cache 的块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_h</span><span class="p">,</span>   <span class="c1"># K_cache 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_d</span><span class="p">,</span>   <span class="c1"># K_cache 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_bl</span><span class="p">,</span>  <span class="c1"># K_cache 的块内偏移步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_x</span><span class="p">,</span>   <span class="c1"># K_cache 的额外维度步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_bs</span><span class="p">,</span>  <span class="c1"># V_cache 的块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_h</span><span class="p">,</span>   <span class="c1"># V_cache 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_d</span><span class="p">,</span>   <span class="c1"># V_cache 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_bl</span><span class="p">,</span>  <span class="c1"># V_cache 的块内偏移步幅</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 超参数 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_queries_per_kv</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>  <span class="c1"># 每个 KV head 对应的 Query head 数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">IN_PRECISION</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 输入精度（例如 tl.float32）</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#  Query 块大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_DMODEL</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># head 维度大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># head 维度填充到 2 的幂次</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># KV 块大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">SLIDING_WINDOW</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 滑动窗口大小，0 表示无窗口</span>
</span></span><span class="line"><span class="cl">    <span class="n">SKIP_DECODE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 是否跳过解码（仅处理上下文）</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 网格定义 ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># grid = (batch_size, num_heads, max_seq_len // BLOCK_M)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 当前 batch 索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_head</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># 当前head 索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">start_m</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>    <span class="c1"># 当前 Query 块索引</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 KV head 索引 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_kv_head</span> <span class="o">=</span> <span class="n">cur_head</span> <span class="o">//</span> <span class="n">num_queries_per_kv</span>  <span class="c1"># 当前 KV head 索引</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 加载 batch 信息 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_seq_len</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Seqlen</span> <span class="o">+</span> <span class="n">cur_batch</span><span class="p">)</span>  <span class="c1"># 当前 batch 总 seq 长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_in_all_start_index</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Start_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span><span class="p">)</span>  <span class="c1"># 当前 batch 全局起始索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_in_all_stop_index</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Start_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 下一 batch 起始索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_query_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">cur_batch_in_all_stop_index</span> <span class="o">-</span> 
</span></span><span class="line"><span class="cl">                          <span class="n">cur_batch_in_all_start_index</span><span class="p">)</span>  <span class="c1"># 当前 batch  Query 长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_ctx_len</span> <span class="o">=</span> <span class="n">cur_batch_seq_len</span> <span class="o">-</span> <span class="n">cur_batch_query_len</span>  <span class="c1"># 上下文长度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 Query 块起始位置 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_start_loc</span> <span class="o">=</span> <span class="n">BLOCK_M</span> <span class="o">*</span> <span class="n">start_m</span>  <span class="c1"># 当前 Query 块的起始位置</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 初始化索引范围 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>  <span class="c1"># KV 块内偏移: [0, BLOCK_N)</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_d</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">)</span>  <span class="c1"># head_size 偏移: [0, BLOCK_DMODEL_PADDED)</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">)</span>  <span class="c1">#  Query 块内偏移: [start_m * BLOCK_M, (start_m + 1) * BLOCK_M)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 Q 的偏移量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q: [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 定位当前 Query 块在 Q 张量中的内存地址</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_q</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">stride_qbs</span> <span class="o">+</span>  <span class="c1">#  batch 和 seq 偏移</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_head</span> <span class="o">*</span> <span class="n">stride_qh</span> <span class="o">+</span>  <span class="c1"># head 偏移</span>
</span></span><span class="line"><span class="cl">        <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_qd</span>  <span class="c1"># head_size偏移</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 示例: 假设 Q [100, 4, 64], stride_qbs=256, stride_qh=64, stride_qd=1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># cur_batch_in_all_start_index=20, cur_head=1, start_m=1, BLOCK_M=16</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># offs_m=[16, 17, ..., 31], offs_d=[0, 1, ..., 63]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q[0, 0] = (20 + 16) * 256 + 1 * 64 + 0 * 1 = 9216 + 64 = 9280</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q[0, 1] = (20 + 16) * 256 + 1 * 64 + 1 * 1 = 9281</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 创建head_size维度掩码 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">dim_mask</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BLOCK_DMODEL</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">int1</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 屏蔽填充部分，例如 BLOCK_DMODEL=64, BLOCK_DMODEL_PADDED=128，则后 64 个值为 0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 加载 Q 数据 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">q</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">off_q</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加载当前 Query 块，掩码确保不加载超出 Query 长度和填充维度的数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 初始化online softmax 变量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">m_i</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;inf&#34;</span><span class="p">)</span>  <span class="c1"># 最大值</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_i</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 归一化因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 注意力累加</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算上下文注意力（Q 对 KV Cache) ---</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">start_n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">multiple_of</span><span class="p">(</span><span class="n">start_n</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>  <span class="c1"># 确保 start_n 是 BLOCK_N 的倍数</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 Cache 索引 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">bn</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span> <span class="o">*</span> <span class="n">stride_b_loc_b</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                     <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">)</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_b_loc_s</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">mask</span><span class="o">=</span><span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">other</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># bn 是当前 KV Cache的块编号</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: B_Loc=[0, 1, 2, ...], cur_batch=0, start_n=16, block_size=16, offs_n=[0, 1, 2, 3]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># bn = B_Loc[0, 1]（若 stride_b_loc_b=8, stride_b_loc_s=1，则地址为 0*8 + 1*1 = 1）</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 K_cache 偏移量 ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># off_k: [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">off_k</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">bn</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_k_cache_bs</span> <span class="o">+</span>  <span class="c1"># 块偏移</span>
</span></span><span class="line"><span class="cl">            <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_k_cache_h</span> <span class="o">+</span>   <span class="c1"># head 偏移</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">//</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_d</span> <span class="o">+</span>  <span class="c1"># head_size偏移（分片）</span>
</span></span><span class="line"><span class="cl">            <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_bl</span> <span class="o">+</span>  <span class="c1"># 块内偏移</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">%</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_x</span>  <span class="c1"># 额外维度偏移</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: bn=[1], cur_kv_head=1, stride_k_cache_bs=4096, stride_k_cache_h=1024, stride_k_cache_d=16</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># offs_d=[0, 1, ..., 63], start_n=16, offs_n=[0, 1, 2, 3], block_size=16, x=1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># off_k[0, 0] = 1*4096 + 1*1024 + (0//1)*16 + (16+0)%16*256 + (0%1)*1 = 4096 + 1024 = 5120</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 K_cache 数据 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_load</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">K_cache</span> <span class="o">+</span> <span class="n">off_k</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 处理 FP8 精度</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">k_load</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_fp8</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="n">k_load</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_scale</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">k_load</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 QK 注意力分数 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">qk</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>  <span class="c1"># [BLOCK_M, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">*=</span> <span class="n">sm_scale</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">SLIDING_WINDOW</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">cur_batch_ctx_len</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-</span> 
</span></span><span class="line"><span class="cl">                          <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">SLIDING_WINDOW</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="o">-</span><span class="mi">10000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- online softmax 更新 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [BLOCK_M]</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">qk</span> <span class="o">-</span> <span class="n">m_ij</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>  <span class="c1"># [BLOCK_M, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [BLOCK_M]</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i_new</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">m_i</span><span class="p">,</span> <span class="n">m_ij</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_i</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">beta</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_ij</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l_i</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">l_ij</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 更新累加器 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_scale</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">p_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc_scale</span> <span class="o">=</span> <span class="n">l_i</span> <span class="o">/</span> <span class="n">l_i_new</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">acc</span> <span class="o">*</span> <span class="n">acc_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 加载 V_cache</span>
</span></span><span class="line"><span class="cl">        <span class="n">off_v</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">bn</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_v_cache_bs</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_v_cache_h</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_v_cache_d</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">%</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">stride_v_cache_bl</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_load</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">V_cache</span> <span class="o">+</span> <span class="n">off_v</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">v_load</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_fp8</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">v_load</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_scale</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="n">v_load</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">acc</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新 m_i 和 l_i</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i</span> <span class="o">=</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i</span> <span class="o">=</span> <span class="n">m_i_new</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算自注意力（Q 对当前 K 和 V） ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算 K 和 V 的初始偏移</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_kbs</span> <span class="o">+</span> <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_kh</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">             <span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_kd</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_v</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_vbs</span> <span class="o">+</span> <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_vh</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">             <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_vd</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="n">k_ptrs</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">off_k</span>  <span class="c1"># 初始指针</span>
</span></span><span class="line"><span class="cl">    <span class="n">v_ptrs</span> <span class="o">=</span> <span class="n">V</span> <span class="o">+</span> <span class="n">off_v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 检查当前 Query 块是否有效</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_mask</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">block_start_loc</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 遍历当前输入的 K 和 V</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">start_n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">block_mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">start_m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">multiple_of</span><span class="p">(</span><span class="n">start_n</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 K 数据 ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 全局偏移: (cur_batch_in_all_start_index + start_n) * stride_kbs 定位 batch 和 seq 块</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: K [100, 4, 64], stride_kbs=256, cur_batch_in_all_start_index=20, start_n=8</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 基地址偏移 = (20 + 8) * 256 = 7168</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># k_ptrs[0, 0] = K + 0 + 1*64 + 0*1 + 7168 = K + 7232</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_ptrs</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">start_n</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_kbs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 QK 注意力分数 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">qk</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">*=</span> <span class="n">sm_scale</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 应用因果掩码</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]),</span> <span class="n">qk</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">SLIDING_WINDOW</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">SLIDING_WINDOW</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="o">-</span><span class="mi">10000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- online softmax 更新 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">qk</span> <span class="o">-</span> <span class="n">m_ij</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i_new</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">m_i</span><span class="p">,</span> <span class="n">m_ij</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_i</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">beta</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_ij</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l_i</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">l_ij</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 更新累加器 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_scale</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">p_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc_scale</span> <span class="o">=</span> <span class="n">l_i</span> <span class="o">/</span> <span class="n">l_i_new</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">acc</span> <span class="o">*</span> <span class="n">acc_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_ptrs</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">start_n</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_vbs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">acc</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新 m_i 和 l_i</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i</span> <span class="o">=</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i</span> <span class="o">=</span> <span class="n">m_i_new</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 存储输出 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_o</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">stride_obs</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_head</span> <span class="o">*</span> <span class="n">stride_oh</span> <span class="o">+</span> <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_od</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_ptrs</span> <span class="o">=</span> <span class="n">Out</span> <span class="o">+</span> <span class="n">off_o</span>
</span></span><span class="line"><span class="cl">    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">out_ptrs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="forward_decode">forward_decode</h2>
<p>调用的是 <a href="https://github.com/vllm-project/vllm/blob/400d483e87b71315bbb73edb0da9fd629212ca82/csrc/attention/attention_kernels.cuh#L90">paged_atention_kernel</a>
gridDim = (num_heads, num_seqs, 1). decode 的时候每个 seq 的 Query 的 toekn 数目都是 1，</p>
<ul>
<li>gridDim = (num_heads, num_seqs, 1): 每个线程块负责一个 seq 的 一个 head，函数定义如下</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="n">template</span> <span class="o">&lt;</span><span class="kr">typename</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="kr">typename</span> <span class="kt">cache_t</span><span class="p">,</span> <span class="kt">int</span> <span class="n">HEAD_SIZE</span><span class="p">,</span> <span class="kt">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>  <span class="c1">// default 16
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="kt">int</span> <span class="n">NUM_THREADS</span> <span class="cm">/*=128*/</span><span class="p">,</span> <span class="n">vllm</span><span class="o">::</span><span class="n">Fp8KVCacheDataType</span> <span class="n">KV_DTYPE</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">          <span class="kt">bool</span> <span class="n">IS_BLOCK_SPARSE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="kt">int</span> <span class="n">PARTITION_SIZE</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>  <span class="c1">// Zero means no partitioning.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__device__</span> <span class="kt">void</span> <span class="nf">paged_attention_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">exp_sums</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads, max_num_partitions]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">max_logits</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                     <span class="c1">// max_num_partitions]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">out</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">q</span><span class="p">,</span>       <span class="c1">// [num_seqs, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">k_cache</span><span class="p">,</span>  <span class="c1">// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">v_cache</span><span class="p">,</span>  <span class="c1">// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_kv_heads</span><span class="p">,</span>               <span class="c1">// [num_heads]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">float</span> <span class="n">scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">block_tables</span><span class="p">,</span>  <span class="c1">// [num_seqs, max_num_blocks_per_seq]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">seq_lens</span><span class="p">,</span>      <span class="c1">// [num_seqs]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">max_num_blocks_per_seq</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">alibi_slopes</span><span class="p">,</span>  <span class="c1">// [num_heads]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 矩阵每一维度的 stride，便于移动指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">q_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_block_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_head_stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">k_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">tp_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_local_blocks</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_vert_stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_block_size</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_head_sliding_step</span><span class="p">)</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>首先先计算一下当前线程对应的各种参数，这里根据模板函数定义不使用 PARTITIONING.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// grid = (num_heads, num_seqs, 1) 一个 thread block 处理一个 seq 的 一个 head
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">partition_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">max_num_partitions</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>  <span class="c1">// 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_lens</span><span class="p">[</span><span class="n">seq_idx</span><span class="p">];</span>  <span class="c1">// 该 seq token 数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 计算块范围和 token 范围
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_seq_blocks</span> <span class="o">=</span> <span class="nf">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>  <span class="c1">// seq 要分几块读取
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks_per_partition</span> <span class="o">=</span>  <span class="n">num_seq_blocks</span><span class="p">;</span>  <span class="c1">// 分了几块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">start_block_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>  <span class="c1">// 起始块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">end_block_idx</span> <span class="o">=</span> <span class="n">num_seq_blocks</span><span class="p">;</span>  <span class="c1">// 结束块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">end_block_idx</span> <span class="o">-</span> <span class="n">start_block_idx</span><span class="p">;</span>  <span class="c1">// 当前分区块数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">start_token_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>  <span class="c1">// 起始 token 索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">end_token_idx</span> <span class="o">=</span> <span class="nf">MIN</span><span class="p">(</span><span class="n">start_token_idx</span> <span class="o">+</span> <span class="n">num_blocks</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">);</span>  <span class="c1">// 结束 token 索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_tokens</span> <span class="o">=</span> <span class="n">end_token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">;</span>  <span class="c1">// 当前分区 token 数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 线程组织参数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">THREAD_GROUP_SIZE</span> <span class="o">=</span> <span class="nf">MAX</span><span class="p">(</span><span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// 几个 thread 处理一个 token 32/16=2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_THREAD_GROUPS</span> <span class="o">=</span> <span class="n">NUM_THREADS</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 一个 thread block 被分成几组 128/2=64
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span> <span class="o">=</span> <span class="nf">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>  <span class="c1">// 每线程处理的 token 数 16/32=1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_WARPS</span> <span class="o">=</span> <span class="n">NUM_THREADS</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// warp 个数 128/32=4
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">thread_idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  <span class="c1">// 线程索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_idx</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程位于第几个 warp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">lane</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">%</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程是该 warp 中的第几个
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">head_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 考虑 GQA MQA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_queries_per_kv</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">/</span> <span class="n">num_kv_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_head_idx</span> <span class="o">=</span> <span class="n">head_idx</span> <span class="o">/</span> <span class="n">num_queries_per_kv</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">float</span> <span class="n">alibi_slope</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">alibi_slopes</span> <span class="o">==</span> <span class="n">nullptr</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">alibi_slopes</span><span class="p">[</span><span class="n">head_idx</span><span class="p">];</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>定义 thread group ，保证其一次访问的数据为 16 Bytes，需要计算其中每个 thread 处理几个元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// VEC_SIZE 即为一个 thread group 中每个线程需要处理元素个数，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">VEC_SIZE</span> <span class="o">=</span> <span class="nf">MAX</span><span class="p">(</span><span class="mi">16</span> <span class="o">/</span> <span class="p">(</span><span class="n">THREAD_GROUP_SIZE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">scalar_t</span><span class="p">)),</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// 16/2/2=4 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">using</span> <span class="n">K_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">using</span> <span class="n">Q_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">using</span> <span class="n">Quant_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">=</span> <span class="n">HEAD_SIZE</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 每个 thread 处理几个元素 64/2=32
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_VECS_PER_THREAD</span> <span class="o">=</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">/</span> <span class="n">VEC_SIZE</span><span class="p">;</span>  <span class="c1">// 这几个元素相当于几个向量  32/4=8
</span></span></span><span class="line"><span class="cl"><span class="c1">// thread_idx = thread_group_idx * THREAD_GROUP_SIZE + thread_group_offset
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">thread_group_idx</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程位于第几个 thread group
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">thread_group_offset</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">%</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程是该 thread group 中第几个线程
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>下面将 Q 加载进共享内存。

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7a7b85b64fbddcf13d703135a4bf6d32?method=download&amp;shareKey=6ca032c977b9f14a0864999633e8e08f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7a7b85b64fbddcf13d703135a4bf6d32?method=download&amp;shareKey=6ca032c977b9f14a0864999633e8e08f" alt="loadQ">
    </a><figcaption>loadQ</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">q_ptr</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">q_stride</span> <span class="o">+</span> <span class="n">head_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">__shared__</span> <span class="n">Q_vec</span> <span class="n">q_vecs</span><span class="p">[</span><span class="n">THREAD_GROUP_SIZE</span><span class="p">][</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>  <span class="c1">// HEAD_SIZE * VEC_SIZE * sizeof(scalar_t) 大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_group_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREAD_GROUPS</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// NUM_ELEMS_PER_THREAD / VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 使得每个 thread group 的线程访问相邻的 vec
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Q_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">q_ptr</span> <span class="o">+</span> <span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>假设块不稀疏并且把不采用量化，加载 K 并计算 <a href="mailto:Q@K.T">Q@K.T</a>. 核心思想是一个 thread group 访问 16 Bytes. 一个 thread 访问一个 vec，一个向量包含的元素个数 <code>VEC_SIZE = 16 / sizeof (scalar_t) / THREAD_GROUP_SIZE</code></p>
<ol>
<li>1st for 循环确定的是每次迭代中每个 warp 处理的是哪一个 block，一共要循环 num_seq_blocks / NUM_WARPS 次</li>
<li>2nd for 循环确定的是该 warp 中的每个 thread group 访问的是该 block 的第几个 token. 即每个线程组处理一个 token.</li>
<li>3rd for 循环确定的是该 thread group 中的每个 thread 访问的是第几个 vec. 该循环使得该 thread group 里面的线程读取一个完整的 headsize. 一次迭代读取的大小为 16 Bytes.</li>
</ol>
<p>首先将 block_table 指针移动到存储该 kv cache 的首个 blockID 处，取出实际的物理块 ID，用在第三个 for 循环中将指针移动到该 K cache block 起始处. 由于
k_cache 的 shape 是 <code>[num_blocks, num_kv_heads, head_size/x, block_size, x]</code>，在第三个 for 循环中 k_ptr 被移动到了该 thread_group 要读取的 block 的 token 的 head 处。<code>vec_idx * VEC_SIZE</code> 即为 thread 要读取的元素开始位置，/x 表示对应的是第几个 16Bytes 划分, offset1 移动的是 dim3，offset2 移动的 则是 dim4.</p>
<p>3rd loop 结束后已经读取了一个 K cache 的完整 head_size 到寄存器中，因此 qk 为一个 token 的一个 head 的 Score Matrix. 根据 token_idx 由每个 thread group 里的 第一个线程负责将累加和到 logits 中并更新 qk_max。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="c1">// Memory planning.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">char</span> <span class="n">shared_mem</span><span class="p">[];</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span><span class="o">*</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">shared_mem</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Workspace for reduction.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">red_smem</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">NUM_WARPS</span><span class="p">];</span>  <span class="c1">// 前一半用于存储 qk_max 后一半用于存储 exp_sum
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 每次 thread group 一次取的元素数量 保证为 16 bytes
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">cache_t</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="kt">float</span> <span class="n">qk_max</span> <span class="o">=</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 指针移动到当前 seq 对应的首个 blockID
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">block_table</span> <span class="o">=</span> <span class="n">block_tables</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">max_num_blocks_per_seq</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">    <span class="c1">// 每个 warp 处理一个 block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">physical_block_number</span> <span class="o">=</span> <span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">block_table</span><span class="p">[</span><span class="n">block_idx</span><span class="p">]);</span>  <span class="c1">// 该 warp 当前处理的 block 对应的 id
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Load a key to registers.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// BLOCK_SIZE(16) / WARP_SIZE(32) = 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">thread_group_idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>  <span class="c1">// thread group 处理的是该 block 的第几个 token
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">const</span> <span class="kt">int</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>  <span class="c1">// 该 token 是该 seq 的第几个
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// NUM_ELEMS_PER_THREAD(32) / VEC_SIZE(4) = 8
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">k_ptr</span> <span class="o">=</span> <span class="n">k_cache</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">physical_block_number</span> <span class="o">*</span> <span class="n">kv_block_stride</span> <span class="o">+</span>  <span class="c1">// 移动到该 block 起始处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">kv_head_idx</span> <span class="o">*</span> <span class="n">kv_head_stride</span> <span class="o">+</span>  <span class="c1">// 移动到对应的 head 处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">physical_block_offset</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 移动到对应的 token 处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 该 thread 要读取 head_size 划分成的第几个 vec
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">offset1</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 第几个 16Bytes 划分
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">offset2</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 划分的第几个元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nf">constexpr</span> <span class="p">(</span><span class="n">KV_DTYPE</span> <span class="o">==</span> <span class="n">Fp8KVCacheDataType</span><span class="o">::</span><span class="n">kAuto</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">k_vecs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">*</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">K_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">k_ptr</span> <span class="o">+</span> <span class="n">offset1</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">offset2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// Compute dot product.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// This includes a reduction across the threads in the same thread group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="nf">dot</span><span class="p">(</span><span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// Add the ALiBi bias if slopes are given.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">qk</span> <span class="o">+=</span> <span class="p">(</span><span class="n">alibi_slope</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">alibi_slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// 每个线程组的第一个线程进行更新 max
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">bool</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">&gt;=</span> <span class="n">seq_len</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span><span class="p">[</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">qk</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">qk_max</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="nl">qk_max</span> <span class="p">:</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">qk</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB36d66a13612972c7f567ed8f20600664?method=download&amp;shareKey=9a305814befc64b17e64feb1c8d76b17" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB36d66a13612972c7f567ed8f20600664?method=download&amp;shareKey=9a305814befc64b17e64feb1c8d76b17" alt="load k &amp; QK Mul">
    </a><figcaption>load k &amp; QK Mul</figcaption></figure></p>
<p>上面这一段结束后下面每个 warp 内 thread group 中的第一个线程已经记录了该 group 的 qk_max. 下一步则是在 warp 内进行 qk_max 归约，存储在共享内存 red_smem 中。 由于一个 warp 处理的是一个 block，相当于现在 red_smem 每个元素存储了对应 block 内的 qk_max.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="nf">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">red_smem</span><span class="p">[</span><span class="n">warp_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">qk_max</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下一步则是在 thread block 内对所有 warp 进行规约，得到该 seq 最后的 qk_max. 然后广播到所有线程中。之后每个线程计算 exp 存入 logits，每个 warp 内的 exp 求和结果存储在 red_smem 的后一半中。最后则是计算 softmax 存到 logits.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="n">qk_max</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">&lt;</span> <span class="n">NUM_WARPS</span> <span class="o">?</span> <span class="n">red_smem</span><span class="p">[</span><span class="n">lane</span><span class="p">]</span> <span class="o">:</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_WARPS</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="nf">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Broadcast the max qk value to all threads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">VLLM_SHFL_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Get the sum of the exp values.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="nf">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">qk_max</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">exp_sum</span> <span class="o">=</span> <span class="n">block_sum</span><span class="o">&lt;</span><span class="n">NUM_WARPS</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">red_smem</span><span class="p">[</span><span class="n">NUM_WARPS</span><span class="p">],</span> <span class="n">exp_sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Compute softmax.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">float</span> <span class="n">inv_sum</span> <span class="o">=</span> <span class="nf">__fdividef</span><span class="p">(</span><span class="mf">1.f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">inv_sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>加载 v 的逻辑与 k 相同，但没有使用 thread group 概念，而是让一个 thread 一次加载 16 Bytes.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Functional Test of Hugo</title>
      <link>http://localhost:57770/blogs/functiontest/</link>
      <pubDate>Sat, 07 Jun 2025 16:05:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/functiontest/</guid>
      <description>function test</description>
      <content:encoded><![CDATA[<h1 id="github-card">Github Card</h1>
<div class="github">
    <div class="github_bar">
        <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" viewBox="0 0 50 50"><path d="M17.791,46.836C18.502,46.53,19,45.823,19,45v-5.4c0-0.197,0.016-0.402,0.041-0.61C19.027,38.994,19.014,38.997,19,39 c0,0-3,0-3.6,0c-1.5,0-2.8-0.6-3.4-1.8c-0.7-1.3-1-3.5-2.8-4.7C8.9,32.3,9.1,32,9.7,32c0.6,0.1,1.9,0.9,2.7,2c0.9,1.1,1.8,2,3.4,2 c2.487,0,3.82-0.125,4.622-0.555C21.356,34.056,22.649,33,24,33v-0.025c-5.668-0.182-9.289-2.066-10.975-4.975 c-3.665,0.042-6.856,0.405-8.677,0.707c-0.058-0.327-0.108-0.656-0.151-0.987c1.797-0.296,4.843-0.647,8.345-0.714 c-0.112-0.276-0.209-0.559-0.291-0.849c-3.511-0.178-6.541-0.039-8.187,0.097c-0.02-0.332-0.047-0.663-0.051-0.999 c1.649-0.135,4.597-0.27,8.018-0.111c-0.079-0.5-0.13-1.011-0.13-1.543c0-1.7,0.6-3.5,1.7-5c-0.5-1.7-1.2-5.3,0.2-6.6 c2.7,0,4.6,1.3,5.5,2.1C21,13.4,22.9,13,25,13s4,0.4,5.6,1.1c0.9-0.8,2.8-2.1,5.5-2.1c1.5,1.4,0.7,5,0.2,6.6c1.1,1.5,1.7,3.2,1.6,5 c0,0.484-0.045,0.951-0.11,1.409c3.499-0.172,6.527-0.034,8.204,0.102c-0.002,0.337-0.033,0.666-0.051,0.999 c-1.671-0.138-4.775-0.28-8.359-0.089c-0.089,0.336-0.197,0.663-0.325,0.98c3.546,0.046,6.665,0.389,8.548,0.689 c-0.043,0.332-0.093,0.661-0.151,0.987c-1.912-0.306-5.171-0.664-8.879-0.682C35.112,30.873,31.557,32.75,26,32.969V33 c2.6,0,5,3.9,5,6.6V45c0,0.823,0.498,1.53,1.209,1.836C41.37,43.804,48,35.164,48,25C48,12.318,37.683,2,25,2S2,12.318,2,25 C2,35.164,8.63,43.804,17.791,46.836z"></path></svg>
        <a class="github_name" href="" target="_blank"></a>
    </div>
    <div class="github_description">this is a github card</div>
    <div class="github_language">
        
    </div>
</div>

<h1 id="big-quote">Big Quote</h1>
<blockquote class="quote"><p>Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell</p></blockquote>
<h1 id="margin-note">Margin Note</h1>
<p>这是一段正常的文本，我们正在讨论一个非常重要的概念。<span class="sidenote-number"><small class="sidenote">这就是<a href="https://www.bilibili.com/">bilibili</a>对那个重要概念的解释和补充说明。你甚至可以在这里使用 <strong>Markdown</strong> 语法！</small></span>
 这个概念源于古希腊，对后世影响深远。</p>
<p>继续你的文章&hellip; 另一处需要注解的地方。<span class="sidenote-number"><small class="sidenote">这是第二个旁注，它会自动对齐，不会和第一个重叠。</small></span>
</p>
<h1 id="various-notice">Various Notice</h1>
<p>关于以下notice 请参考 <span class="sidenote-number"><small class="sidenote"><a href="https://github.com/martignoni/hugo-notice?tab=readme-ov-file">hugo_notice</a></small></span>

<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>This is a warning notice. Be warned!</p></div>
</p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>This is a very good tip.</p></div>

<div class="notice info" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="92 59.5 300 300">
  <path d="M292 303.25V272c0-3.516-2.734-6.25-6.25-6.25H267v-100c0-3.516-2.734-6.25-6.25-6.25h-62.5c-3.516 0-6.25 2.734-6.25 6.25V197c0 3.516 2.734 6.25 6.25 6.25H217v62.5h-18.75c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h87.5c3.516 0 6.25-2.734 6.25-6.25Zm-25-175V97c0-3.516-2.734-6.25-6.25-6.25h-37.5c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h37.5c3.516 0 6.25-2.734 6.25-6.25Zm125 81.25c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Info</p><p>This is a use info.</p></div>

<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>This is a note.</p></div>

]]></content:encoded>
    </item>
    <item>
      <title>A Simple Cmake Example</title>
      <link>http://localhost:57770/blogs/simple_cmake/</link>
      <pubDate>Fri, 06 Jun 2025 18:32:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/simple_cmake/</guid>
      <description>A Simple Cmake Example</description>
      <content:encoded><![CDATA[<p>CMake 入门教程：从项目结构到链接库</p>
<ol>
<li>核心理念：源码外构建 (Out-of-Source Builds)</li>
</ol>
<p>在开始之前，最重要的一点是理解 CMake 的核心哲学：源码外构建。这意味着所有由构建过程产生的文件（例如 Makefiles、Visual Studio 项目文件、目标文件 .o、可执行文件 .exe、库文件 .a 或 .so）都应该与你的源代码完全分离开。这样做最大的好处是能保持你的源码目录永远干净整洁。我们将创建一个 build 目录来存放所有这些生成的文件。</p>
<ol start="2">
<li>推荐的项目目录结构 📂</li>
</ol>
<p>一个良好组织的 C++ 项目结构不仅清晰，也让 CMake 的配置工作事半功倍。这是一个推荐的、可扩展的目录结构：my_project/</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">│
</span></span><span class="line"><span class="cl">├── build/                  # 构建目录 (初始为空，所有生成文件都在此)
</span></span><span class="line"><span class="cl">│
</span></span><span class="line"><span class="cl">├── include/                # 存放项目全局头文件
</span></span><span class="line"><span class="cl">│   └── my_app/
</span></span><span class="line"><span class="cl">│       └── my_lib.h
</span></span><span class="line"><span class="cl">│
</span></span><span class="line"><span class="cl">├── src/                    # 存放所有源文件 (.cpp)
</span></span><span class="line"><span class="cl">│   │
</span></span><span class="line"><span class="cl">│   ├── main.cpp            # 主程序入口
</span></span><span class="line"><span class="cl">│   │
</span></span><span class="line"><span class="cl">│   └── my_lib/             # 一个独立的库模块
</span></span><span class="line"><span class="cl">│       ├── CMakeLists.txt  # 这个库自己的 CMake 配置文件
</span></span><span class="line"><span class="cl">│       └── my_lib.cpp
</span></span><span class="line"><span class="cl">│
</span></span><span class="line"><span class="cl">└── CMakeLists.txt          # 整个项目的顶层 CMake 配置文件
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>build/: 这个目录用于执行所有构建命令，源码不会被污染。include/: 存放可以被项目内其他部分（或被其他项目）引用的头文件。按模块组织可以避免头文件名冲突。src/: 存放所有 .cpp 源文件。</li>
<li>src/my_lib/: 将项目按功能模块化是一种好习惯。每个模块（比如一个库）可以有自己的 CMakeLists.txt 文件，负责管理自身的编译。</li>
<li>CMakeLists.txt (顶层): 这是整个项目的入口，负责设置全局配置、找到并构建所有子模块，最后生成主程序。</li>
</ul>
<ol start="3">
<li>编写各层级的 CMakeLists.txt 📝我们将采用“自下而上”的方式来编写配置文件，先从底层的库开始，再到顶层的项目。
第 1 步: 库的 CMakeLists.txt (src/my_lib/CMakeLists.txt</li>
</ol>
<p>)这个文件只负责一件事：将 my_lib.cpp 和相关的头文件编译成一个库。# 文件位置: src/my_lib/CMakeLists.txt</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cmake" data-lang="cmake"><span class="line"><span class="cl"><span class="c"># 使用 add_library 命令创建一个库。
</span></span></span><span class="line"><span class="cl"><span class="c"># 语法: add_library(&lt;库名称&gt; [STATIC | SHARED] &lt;源文件...&gt;)
</span></span></span><span class="line"><span class="cl"><span class="c">#
</span></span></span><span class="line"><span class="cl"><span class="c"># &lt;库名称&gt;: 我们称之为 my_lib，这是其他部分链接此库时使用的名字。
</span></span></span><span class="line"><span class="cl"><span class="c"># STATIC:   生成静态链接库 (.a, .lib)。
</span></span></span><span class="line"><span class="cl"><span class="c"># SHARED:   生成动态/共享链接库 (.so, .dll)。
</span></span></span><span class="line"><span class="cl"><span class="c">#           如果不指定，默认是 STATIC。
</span></span></span><span class="line"><span class="cl"><span class="c"># &lt;源文件&gt;:  用于编译这个库的源文件列表。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">add_library</span><span class="p">(</span><span class="s">my_lib</span> <span class="s">STATIC</span> <span class="s">my_lib.cpp</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 为这个库目标指定它需要包含的头文件目录。
</span></span></span><span class="line"><span class="cl"><span class="c"># 语法: target_include_directories(&lt;目标&gt; &lt;PUBLIC|PRIVATE|INTERFACE&gt; &lt;路径...&gt;)
</span></span></span><span class="line"><span class="cl"><span class="c">#
</span></span></span><span class="line"><span class="cl"><span class="c"># &lt;目标&gt;:    就是我们上面用 add_library 创建的 my_lib。
</span></span></span><span class="line"><span class="cl"><span class="c"># PUBLIC:   表示此头文件路径不仅 my_lib 自己需要，任何链接了 my_lib 的目标也需要。
</span></span></span><span class="line"><span class="cl"><span class="c">#           这是最关键的设置，它实现了依赖的自动传递。
</span></span></span><span class="line"><span class="cl"><span class="c"># PRIVATE:  表示此头文件路径只有 my_lib 内部编译时需要，不会传递给链接它的目标。
</span></span></span><span class="line"><span class="cl"><span class="c"># INTERFACE:表示此头文件路径只有链接它的目标需要，my_lib 自己编译时不需要。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">target_include_directories</span><span class="p">(</span><span class="s">my_lib</span>
</span></span><span class="line"><span class="cl">  <span class="s">PUBLIC</span>
</span></span><span class="line"><span class="cl">    <span class="c"># ${PROJECT_SOURCE_DIR} 是一个非常有用的内置变量，指向顶层 CMakeLists.txt 所在的目录。
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="c"># 我们将项目的全局 include 目录暴露出去。
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="o">${</span><span class="nv">PROJECT_SOURCE_DIR</span><span class="o">}</span><span class="s">/include</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span><span class="err">
</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>add_library()</code> 定义了一个编译目标——一个库。</li>
<li><code>target_include_directories()</code> 为这个目标指定了头文件搜索路径。使用 <code>PUBLIC </code>关键字至关重要使得任何链接到 <code>my_lib</code> 的程序都能自动找到 my_lib.h，无需在链接方再次手动添加头文件路径。</li>
</ul>
<p>第 2 步: 顶层的 CMakeLists.txt 这个文件是整个项目的总指挥，负责设置全局配置、调用子模块，并生成最终的可执行文件。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cmake" data-lang="cmake"><span class="line"><span class="cl"><span class="c"># 文件位置: my_project/CMakeLists.txt
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 1. 指定 CMake 的最低版本要求。这是每个顶层文件都应该有的第一行。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span> <span class="s">3.10</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 2. 定义项目信息。
</span></span></span><span class="line"><span class="cl"><span class="c"># 语法: project(&lt;项目名称&gt; VERSION &lt;版本号&gt; LANGUAGES &lt;语言&gt;)
</span></span></span><span class="line"><span class="cl"><span class="c"># 这会创建一些有用的变量，比如 PROJECT_NAME, PROJECT_SOURCE_DIR。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">project</span><span class="p">(</span><span class="s">MyApp</span> <span class="s">VERSION</span> <span class="s">1.0</span> <span class="s">LANGUAGES</span> <span class="s">CXX</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 3. 设置 C++ 标准 (这是现代 CMake 推荐的方式)。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">set</span><span class="p">(</span><span class="s">CMAKE_CXX_STANDARD</span> <span class="s">17</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">set</span><span class="p">(</span><span class="s">CMAKE_CXX_STANDARD_REQUIRED</span> <span class="s">ON</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">set</span><span class="p">(</span><span class="s">CMAKE_CXX_EXTENSIONS</span> <span class="s">OFF</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 4. 打印一条消息，方便调试时查看变量值 (可选)。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">message</span><span class="p">(</span><span class="s">STATUS</span> <span class="s2">&#34;Project source directory is: ${PROJECT_SOURCE_DIR}&#34;</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 5. 添加子目录。
</span></span></span><span class="line"><span class="cl"><span class="c"># 这个命令会告诉 CMake 去处理 src/my_lib 目录下的 CMakeLists.txt 文件。
</span></span></span><span class="line"><span class="cl"><span class="c"># 当执行到这里时，上面定义的 my_lib 库目标就会被创建出来。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">add_subdirectory</span><span class="p">(</span><span class="s">src/my_lib</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 6. 添加可执行文件。
</span></span></span><span class="line"><span class="cl"><span class="c"># 语法: add_executable(&lt;可执行文件名&gt; &lt;源文件...&gt;)
</span></span></span><span class="line"><span class="cl"><span class="c"># 我们将主程序命名为 app，它由 src/main.cpp 编译而来。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">add_executable</span><span class="p">(</span><span class="s">app</span> <span class="s">src/main.cpp</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># 7. 链接库！这是将所有部分组合在一起的关键步骤。
</span></span></span><span class="line"><span class="cl"><span class="c"># 语法: target_link_libraries(&lt;目标&gt; &lt;PUBLIC|PRIVATE|INTERFACE&gt; &lt;要链接的库...&gt;)
</span></span></span><span class="line"><span class="cl"><span class="c">#
</span></span></span><span class="line"><span class="cl"><span class="c"># &lt;目标&gt;: 我们要链接的目标，即 app。
</span></span></span><span class="line"><span class="cl"><span class="c"># PRIVATE: 表示 app 的编译需要 my_lib，但这个依赖关系不会继续传递。
</span></span></span><span class="line"><span class="cl"><span class="c">#          对于可执行文件，通常使用 PRIVATE。
</span></span></span><span class="line"><span class="cl"><span class="c"># &lt;要链接的库&gt;: 我们在子目录中定义的库目标 my_lib。
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">app</span> <span class="s">PRIVATE</span> <span class="s">my_lib</span><span class="p">)</span><span class="err">
</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>add_subdirectory() 使得顶层文件保持简洁，只负责“指挥”，具体实现则交给各个子模块。</li>
<li>target_link_libraries() 负责将不同的编译目标（库和可执行文件）链接在一起，形成依赖关系。</li>
</ul>
<ol start="4">
<li>如何构建项目 🚀
现在已经写好了所有的 CMakeLists.txt 文件，可以开始构建了。整个过程都在终端中完成。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 1. 确保你位于项目的根目录 (my_project)</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> path/to/my_project
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 2. 创建并进入我们规划好的 build 目录</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">mkdir build
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> build
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 3. 运行 CMake 来生成构建系统。</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;..&#39; 指向上一级目录，也就是 my_project/ 根目录，CMake 会在那里寻找顶层的 CMakeLists.txt。</span>
</span></span><span class="line"><span class="cl"><span class="c1"># -DCMAKE_BUILD_TYPE=Debug 指定了构建类型为 Debug，会包含调试信息。</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">cmake -DCMAKE_BUILD_TYPE<span class="o">=</span>Debug ..
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># CMake 会扫描你的系统，找到 C++ 编译器，然后根据 CMakeLists.txt 的内容</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 生成特定平台的构建文件（在 Linux/macOS 上是 Makefile，在 Windows 上是 Visual Studio sln 文件）。</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 4. 编译项目</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 这个命令会调用底层的构建工具（如 make 或 msbuild）来执行真正的编译和链接工作。</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &#39;--build .&#39; 是一个平台无关的命令，告诉 CMake 在当前目录执行构建。</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">cmake --build .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 或者在 Linux/macOS 上，你可以直接运行:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># make</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 编译完成后，你会在 build 目录（或其子目录）下找到你的可执行文件 `app` 和库文件 `libmy_lib.a`。</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>How to Use git rebase</title>
      <link>http://localhost:57770/blogs/git-rebase-flow/</link>
      <pubDate>Fri, 06 Jun 2025 17:38:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/git-rebase-flow/</guid>
      <description>Use of git rebase</description>
      <content:encoded><![CDATA[<h1 id="what-can-git-rebase-do">What Can git rebase Do</h1>
<p><code>rebase</code> 的字面意思是“变基”——也就是改变一个分支的“基础”提交点。它的主要目标是：将一系列的提交以更整洁、线性的方式应用到另一个分支上，从而创造一个干净、没有多余合并记录的项目历史。</p>
<p>假设你的项目历史是这样的：你在 main 分支上切出了一个 feature 分支，之后 main 分支和你自己的 feature 分支都有了新的 commits.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">      A---B---C   &lt;-- feature
</span></span><span class="line"><span class="cl">     /
</span></span><span class="line"><span class="cl">D---E---F---G   &lt;-- main
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果你在 feature 分支上运行 git rebase main，Git 会做一件非常神奇的事：</p>
<ol>
<li>Git 会暂时“收起” feature 分支上的所有提交 (A, B, C).</li>
<li>将 feature 分支的起点移动到 main 分支的最新提交 G 上。</li>
<li>把刚才收起的提交 (A, B, C) 依次重新应用到新的起点上，形成新的提交 A&rsquo;, B&rsquo;, C'</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">              A&#39;--B&#39;--C&#39;  &lt;-- feature
</span></span><span class="line"><span class="cl">             /
</span></span><span class="line"><span class="cl">D---E---F---G   &lt;-- main
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>A&rsquo; 和 A 的内容虽然一样，但它们的 Commit ID 是不同的，因为它们的父提交变了。rebase 相当于重写了历史。</strong></p>
<p>现在，再切换回 main 分支，执行 <code>git merge feature</code>，由于 main 分支的所有历史现在是 feature 分支历史的子集，Git 只会进行一次 Fast-forward 合并，不会产生新的合并提交。最终结果如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">D---E---F---G---A&#39;--B&#39;--C&#39;  &lt;-- main, feature
</span></span></code></pre></td></tr></table>
</div>
</div><p>最终的项目历史是一条完美的直线，非常清晰，就像所有开发都是按顺序发生的一样。rebase 重写了历史，抹去了分支开发的“并行”痕迹。</p>
<h1 id="compared-to-merge">Compared to merge</h1>
<p>要理解 rebase，最好的方法就是和 merge 对比。如果在 main 分支上运行 <code>git merge feature</code>，结果会是这样</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">      A---B---C
</span></span><span class="line"><span class="cl">     /         \
</span></span><span class="line"><span class="cl">D---E---F---G---H   &lt;-- main (H 是一个合并提交)
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>merge</code> 做的事情是：</p>
<ol>
<li>找到两个分支的共同祖先 E.</li>
<li>将两个分支的修改整合起来，创建一个全新的 Merge Commit，也就是 H. 该提交有两个父提交点 C 和 G.</li>
</ol>
<p>merge 完全全保留了历史的真实性。它清楚地记录了“在某个时间点，我们把一个分支合并了进来”。但如果项目频繁合并，历史记录会充满大量的合并提交，形成一个复杂的“菱形”或“意大利面条”式的网状结构，难以阅读。</p>
<h1 id="how-to-use-rebase">How to use rebase</h1>
<p>假设你正在 feature-login 分支上开发，同时主分支 main 也有了新的更新。</p>
<ol>
<li>确保 main 分支处于最新的状态</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git checkout main
</span></span><span class="line"><span class="cl">git pull origin main
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>切换到你正在开发的分支 <code>git checkout feature-login</code></li>
<li>把 main 分支上的最新修改 rebase 到你当前的 feature-login 分支上 <code>git rebase main</code></li>
<li>解决冲突 (如果有的话). 因为 rebase 是逐个应用提交，所以可能会在某个提交应用时发生冲突。此时，rebase 会暂停。
<ul>
<li>打开冲突文件，手动解决冲突（和 merge 冲突一样）。</li>
<li>解决后，使用 <code>git add &lt;filename&gt;</code> 将文件标记为已解决。</li>
<li>然后，继续 rebase 过程 <code>git rebase --continue</code></li>
<li>如果中途想放弃，可以回到 rebase 开始前的状态 <code>git rebase --abort</code></li>
</ul>
</li>
<li>合并到主分支
rebase 成功后，你的 feature-login 分支就已经包含了 main 的所有更新，并且<strong>你的提交都在最前面</strong>。现在可以进行一次干净的快进合并。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git checkout main
</span></span><span class="line"><span class="cl">git merge feature-login
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="when-not-to-use-rebase">When NOT to Use rebase</h1>
<p>**永远不要对一个已经推送到 remote，并且可能被团队其他人使用的公共分支 (如 main, develop)进行 rebase！**因为 rebase 会重写历史。如果你 rebase 了一个公共分支并强制推送 (<code>git push --force</code>)，那么所有团队成员的本地历史记录都将与远程的“新历史”产生严重分歧。</p>
<p>正确用法是只在你自己的、还未与他人分享的本地分支上使用 rebase，用来整理你自己的提交记录，以便在合并到公共分支前有一个干净的历史。</p>
<h1 id="advanced-use-git-rebase--i">Advanced Use git rebase -i</h1>
<p><code>git rebase -i</code> 允许你在 rebase 的过程中，对你的提交进行编辑、合并、拆分或删除。这常用于在合并到 main 分支前，将自己本地凌乱的提交（如 &ldquo;修复拼写错误&rdquo;, &ldquo;临时提交&rdquo;, &ldquo;又改了一点&rdquo;）整理成几个有意义的提交。</p>
<p>假设你的 feature-login 分支有 3 个凌乱的提交，你想把它们合并成一个。</p>
<ol>
<li>启动交互式 rebase <code>git rebase -i HEAD~3</code>. 其中 <code>HEAD~3</code> 表示从当前提交 (HEAD) 往前数 3 个提交。</li>
<li>编辑 Rebase 脚本
Git 会打开一个文本编辑器，列出这 3 个提交：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">pick a31ab34 complete login UI
</span></span><span class="line"><span class="cl">pick 58c34bb fix a button bug
</span></span><span class="line"><span class="cl">pick 948f2cb add backend verify logic
</span></span></code></pre></td></tr></table>
</div>
</div><p>在文件下方会有指令说明。你可以修改每一行前面的 pick 命令。比如，我们想把后两个提交合并到第一个里面：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">pick a31ab34 complete login UI
</span></span><span class="line"><span class="cl">squash 58c34bb fix a button bug
</span></span><span class="line"><span class="cl">squash 948f2cb add backend verify logic
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>保存并退出编辑器
Git 会开始合并提交，并弹出另一个编辑器，让你为这个合并后的新提交编写一个新的 commit message. 整理好后保存退出。现在再用 <code>git log</code> 查看，你会发现原来 3 个凌乱的提交已经变成了一个干净、完整的提交。</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>All2All Communication Cost</title>
      <link>http://localhost:57770/blogs/all2allcommcost/</link>
      <pubDate>Sun, 12 Jan 2025 16:05:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/all2allcommcost/</guid>
      <description>Introduction of Transformer Family</description>
      <content:encoded><![CDATA[<p>在 All2All 通信中，每个设备给其他设备发送大小为 m 的不同的消息。此操作相当于使用一维数组分区对分布在 p 个进程中的二维数据数组进行转置，因此也被称作全交换 (<strong>total exchange</strong>)</p>
<h2 id="ring--bidirectional-linear-array">Ring / Bidirectional Linear Array</h2>
<p>线性数组拓扑结构的 All2All 通信中，每个设备需要发送 p-1 份大小为 m 的数据。用 {i,j} 表示消息需要从设备 i 发送到设备 j. 首先，每个节点将所有要发送的数据作为一个大小为 m(p-1) 的合并消息发送给它邻居 (假设所有设备通信方向相同)。当邻居收到这个消息后提取他所需要的那一部分，发送剩下的大小为 m(p-2). 每个设备一共发送 p-1 次，每次要发送的消息大小减少 m.</p>
<p>由此可以得出在 p 个设备组成的线性数组拓扑上进行 All2All 每个设备需要向相邻设备通信 p-1 次，第 i 次通信的消息大小为 m(p-i). 如果向两个方向都进行发送，那么每个方向都只用发送原先一半的数据。</p>
$$
\begin{aligned}T_{ring}&=\quad\sum_{i=1}^{p-1}(t_{s}+t_{w}m(p-i))\\&=\quad t_{s}(p-1)+\sum_{i=1}^{p-1}it_{w}m\\&=\quad(t_{s}+t_{w}mp/2)(p-1).\end{aligned}
$$<p>环状网络中每份消息的平均传输跳数是 $\frac{\sum_{d=1}^{p-1}i}{p-1} = p/2$，因此 p 个节点总共的通信量之和为  $p\times m(p-1)\times\frac p2$  环状网络中总的链路数目为 p. 因此负载平均的情况下，最少需要的时间为 $\frac{m(p-1)\times\frac p2\times p}p = m(p-1)\frac p2$ ，因此算法时间为最优的。</p>
<p>跳数为 d 的消息数量对应于相距 d 的节点对 (i, j)，其中 |i-j|=d</p>
<ul>
<li>(0, d),(1, d+1), \ldots,(p-1-d, p-1)，即 i 从 0 到 p-1-d, j=i+d ，共有 p-d 对。</li>
<li>(d, 0),(d+1,1), \ldots,(p-1, p-1-d)，即  i  从  d  到  p-1, ~ j=i-d  ，也有 p-d 对。
总共有 2(p-d) 条消息的跳数为 d</li>
</ul>
<p>总跳数</p>
$$
\begin{aligned}
\text { 总跳数 } & =\sum_{d=1}^{p-1} d \times 2(p-d) \\
& =2 \sum_{d=1}^{p-1} d(p-d)=2\left(p \sum_{d=1}^{p-1} d-\sum_{d=1}^{p-1} d^{2}\right) \\
& = p \cdot \frac{(p-1) p}{2}-\frac{(p-1) p(2 p-1)}{6} \\
& = =\frac{(p-1) p(p+1)}{6}
\end{aligned}
$$<p>因此平均跳数 =$\frac{\text { 总跳数 }}{\text { 总消息数 }}=\frac{\frac{(p-1) p(p+1)}{3}}{p(p-1)}=\frac{p+1}{3}$</p>
<h2 id="mesh">Mesh</h2>
<p>若 p 个设备组成大小为 $\sqrt{p} \times \sqrt{p}$ 的 mesh 进行 All2All 通信，每个设备首先将其 p 个数据按照目的设备的列进行分组，即分成 $\sqrt{p}$ 组，每组包含大小为 $m\sqrt{p}$ 的消息。假设 3x3 的 mesh，则第一组消息的目的节点为 {0,3,6}，第二组消息的目的节点为 {1,4,7}，第三组消息的目的节点为 {2,5,8}</p>
<p>首先同时分别在每一行中进行 All2All 通信，每一份数据大小为 $m\sqrt{p}$. 通信结束后每个设备拥有的是该行目的设备为所在列的所有数据。然后将数据按照目的设备所在的行进行分组。即设备 {0,3,6} 第一组消息的目的节点为 0，第二组消息的目的节点为 3，第三组消息的目的节点为 6. 然后同时分别在每一列中进行 All2All 通信。</p>
<p>我们只需要将 Linear Array 拓扑结构中的公式的 p 换成 $\sqrt{p}$ ，m 换成 $m\sqrt{p}$，再乘以 2 就得到在 mesh 上进行 All2All 的时间</p>
$$
T_{mesh}=(2t_{s}+t_{w}mp)(\sqrt{p}-1).
$$<h2 id="hypercube">Hypercube</h2>
<p>超立方体拓扑在每个维度上都有两个节点，一共有 $\log{p}$ 个维度。在一共有 p 个节点超立方体中，在某个维度 $d$ 上，超立方体可以被划分为两个 (n−1) 维的子立方体，这两个子立方体通过维度 d 上的 p/2 条链路相连。</p>
<p>在 All2All 通信的任何阶段，每个节点都持有 $p$ 个大小为 $m$ 的数据包。当在特定维度上通信时，每个节点发送 $p/2$ 个数据包 (合并为一条消息)。这些数据包的目的地是由当前维度的链路连接的另一个子立方体包含的节点。在上述过程中，节点必须在每个 $\log{p}$ 通信步骤之前在本地重新排列消息。</p>
<p>$\log{p}$ 步中的每一步，每个设备沿当前维度的双向链路交换大小为 mp/2 的数据。因此在 hypercube 上进行 All2All 的时间为</p>
$$
T_{hcube}=(t_{s}+t_{w}mp/2)\log p.
$$<p>值得注意的是与 ring 和 mesh 算法不同，超立方体算法不是最优的。每个设备发送和接收大小为 m(p- 1) 的数据，超立方体上任意两个节点之间的平均距离为 $\log{p}/2$ . 因此，网络上的总数据流量为 $p\times m(p - 1)\times(\log{p})/2$. 每个超立方体一共有 $p\log{p}/2$  条双向链路，如果流量能够被平分，则通信用时下界应该为</p>
$$
\begin{aligned}T_{min}&=\frac{t_{w}pm(p-1)(\log p)/2}{(p\log p)/2}\\&=t_{w}m(p-1).\end{aligned}
$$<h2 id="optimal-algorithm-in-hypercube">Optimal Algorithm in Hypercube</h2>
<p>在超立方体上，执行 All2All 的最佳方法是让每一对节点彼此直接通信。因此，每个节点只需执行 p-1 次通信，每次与不同设备交换大小为 m 的数据。设备必须在每次通信中选择不会出现拥塞的通信对象。在第 j 次通信中，节点 i 与节点 $i \oplus j$ 交换数据。在超立方体上，从节点 i 到节点 j 的消息必须经过至少 l 条链路，其中 l 是 i 和 j 之间的汉明距离 (即 $i \oplus j$ 的二进制表示中的非零比特数). 我们通过 E-cube 路由来选择路径：</p>
<ol>
<li>将当前节点地址 C 与目标节点地址 D 进行 XOR 操作，得到 $R=C\oplus D$.</li>
<li>找到 R 的最低有效非零位，决定下一步跳转的维度。</li>
<li>沿选定维度跳转到下一个节点，更新当前节点地址。</li>
<li>重复上述步骤，直到 R=0， 即到达目标节点。
对于节点i和节点j之间的消息传输，该算法保证每一步的通信时间为 t_s + t_wm，因为在节点 i 和节点 j 之间的链路上沿着同一方向传播的任何其他消息都不存在竞争，切每一步只切换一个维度，通信距离为 1. 整个 All2All 的总通信时间为</li>
</ol>
$$T_{xor}=(t_{s}+t_{w}m)(p-1).$$<h1 id="bruck-algorithm-in-full-connected-network">Bruck Algorithm in Full-connected Network</h1>
<p>Bruck是一种存储-转发 (store-and-forward) 算法，需要 log(P) 次通信步骤。这意味着发送缓冲区 S 和接收缓冲区 R 都用于在中间通信轮次中发送、接收和存储数据。因为某些接收到的数据块必须在后续通信步骤中使用。这种存储-转发的特性对通信轮次的顺序提出了约束。与线性步骤实现不同，Bruck 必须保持明确的通信顺序，其中第 i+1 次迭代必须在第 i 次迭代之后物理时间上发生。

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1b5aaffb71ec91ead2f725d9249728f1?method=download&amp;shareKey=e4eea75f3b72f77982d47b17590c24b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1b5aaffb71ec91ead2f725d9249728f1?method=download&amp;shareKey=e4eea75f3b72f77982d47b17590c24b3" alt="Bruck">
    </a><figcaption>Bruck</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">Algorithm 2 NCCL Bruck algorithm
</span></span><span class="line"><span class="cl">P ← total number of processes.
</span></span><span class="line"><span class="cl">for i ∈ [0, P] do
</span></span><span class="line"><span class="cl">   R[i] = S[(p+i) % P] // S and R are send and receive buffers, and p is rank id of each process;
</span></span><span class="line"><span class="cl">end for
</span></span><span class="line"><span class="cl">allocate temporary buffer T with SC × (P+1) / 2 elements; // SC is number of elements per data-block.
</span></span><span class="line"><span class="cl">for k = 1; k &lt; P; k &lt;&lt;= 1 do
</span></span><span class="line"><span class="cl">   allocate send indexes array SB with (P+1) / 2 integers;
</span></span><span class="line"><span class="cl">   number of send data-blocks NB ← 0;
</span></span><span class="line"><span class="cl">   for i ∈ [k, P] do
</span></span><span class="line"><span class="cl">      if i &amp; k then
</span></span><span class="line"><span class="cl">            SB[NB] ← i;
</span></span><span class="line"><span class="cl">            copy R[i] into T[NB];
</span></span><span class="line"><span class="cl">            NB ← NB + 1;
</span></span><span class="line"><span class="cl">      end if
</span></span><span class="line"><span class="cl">      sendproc ← (p + k) % P;
</span></span><span class="line"><span class="cl">      recvproc ← (p - k + P) % P;
</span></span><span class="line"><span class="cl">      ncclGroupStart()
</span></span><span class="line"><span class="cl">      send data in T to sendproc;
</span></span><span class="line"><span class="cl">      receive data from recvproc into S;
</span></span><span class="line"><span class="cl">      ncclGroupEnd()
</span></span><span class="line"><span class="cl">      for i ∈ [0, SB] do
</span></span><span class="line"><span class="cl">            copy T[i] into R[SB[i]];
</span></span><span class="line"><span class="cl">      end for
</span></span><span class="line"><span class="cl">   end for
</span></span><span class="line"><span class="cl">   for i ∈ [0, P] do
</span></span><span class="line"><span class="cl">      R[i] = R[(p - i + P) % P] // final rotation;
</span></span><span class="line"><span class="cl">   end for
</span></span><span class="line"><span class="cl">end for
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>line(2-4): 将每个设备发送缓冲区 S 中的数据按照 rank 偏移重新排列拷贝到接收缓冲区 R 中。</li>
<li>line(5): 为通信阶段准备一个临时缓冲区 T</li>
<li>line(6): 通信步开始 k 以指数方式增长 (1, 2, 4, &hellip;)，总共执行 logP 次迭代
<ul>
<li>line(7-14): 用索引数组 SB，记录需要发送的数据块位置。遍历 k~P-1 同通过对 i&amp;k 判断哪些数据块需要在此轮发送. (若 P 是 2 的指数幂，因为 k 是 2 的指数幂，因此只有一位为 1，那么就是每轮发送 p/2 个数据块) 将接收缓冲区 R 中满足条件的数据拷贝到临时缓冲区 T，并记录索引。</li>
<li>line(15-16): 确定要接收和发送的目标。</li>
<li>line(17-20): 进行通信操作，将数据发送到目标的发送缓冲区。</li>
<li>line(21-23): 更新接收缓冲区。</li>
<li>line(25-27): 反向调整接收缓冲区数据的位置。</li>
</ul>
</li>
</ul>
<p>总共 log(p) 步骤每步发送 m 消息。</p>
<h1 id="tree-based">Tree-based</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6ed5f5f2681e4f2c3a57bfb7b901515a?method=download&amp;shareKey=7aafd92596dbc981100138525e0f6d09" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6ed5f5f2681e4f2c3a57bfb7b901515a?method=download&amp;shareKey=7aafd92596dbc981100138525e0f6d09" alt="Tree">
    </a><figcaption>Tree</figcaption></figure></p>
<p>采用先在行上进行 All-gather, 再在列上进行 Scatter. 也需要 log(p) 步，其中 gather 阶段第一步通信量为 m(p-1)，一共进行 0.5log(p) 步每一步通信量翻倍，跳数也翻倍；scatter阶段则是相反，因此两步的通信时间相同总共 t_s*log(p) + m(p-1)^2/3</p>
]]></content:encoded>
    </item>
    <item>
      <title>MLIR-Ch9 Dialect Conversion</title>
      <link>http://localhost:57770/blogs/courselearning/mlir/mlir-ch9-dialect-conversion/</link>
      <pubDate>Tue, 12 Nov 2024 15:22:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/mlir/mlir-ch9-dialect-conversion/</guid>
      <description>Personal MLIR learning notes 9.</description>
      <content:encoded><![CDATA[<p>MLIR 的主要原则之一是逐步下降，即存在许多级别的 IR 粒度，并且逐步下降 IR 的不同部分，仅在不再对优化有用时丢弃信息。在本文中，将完成其中的第一步：使用所谓的方言转换基础设施将多方言 lowering 为标准MLIR方言的组合。</p>
<h1 id="the-type-obstacle">The Type Obstacle</h1>
<p>如果不是针对类型，方言转换 (lowering) 本质上与普通 pass 相同：编写一些重写模式并将其应用于 IR. 对于每个需要 lowering 的 OP ，通常会有一个重写模式。</p>
<p>类型使这个问题变得更加复杂，我将通过poly的示例来演示这个问题。</p>
<p>poly.add 对两个多项式进行相加并返回结果多项式。我们想 lowering poly。例如，添加到 arith.addi 算术运算的矢量化循环中。但 arith 并不知道 poly.poly 类型的存在。</p>
<p>如果必须使扩展 arith 以了解poly，需要对 arith 进行上游更改。添加 op 的 operands 以允许实现某种接口的类型，例如 integer-like 或 containers of integer-like.</p>
<p>所以，除了 lowering op，还需要 lowering poly.<code> poly&lt;N&gt;</code> 变成张量 <code>&lt;Nxi32&gt;</code>. 这就是类型障碍发挥作用的地方。一旦更改了特定值的类型，例如，在 lowering 生成该值作为输出的 OP 时，那么该值的所有下游用户仍然期望使用旧类型，并且在 lowering 它们之前在技术上是无效的。在每次传递之间，MLIR运行验证器以确保IR有效，因此如果没有一些特殊处理，这意味着需要在一次传递中转换所有类型和 OP ，否则这些验证器将失败。但是用标准重写规则管理所有这些将是困难的：对于每个重写规则，您都必须不断检查参数和结果是否已经转换。</p>
<p>例如在 lowering 一个生成该值作为输出的 OP 时，所有依赖该值的下游用户仍然期望旧的类型，因此在技术上这些下游用户在未被 lowering 之前是无效的。MLIR 在每次转换 (pass) 之间运行验证器以确保中间表示 (IR) 是有效的，因此如果没有特殊处理，这意味着所有类型和 OP 必须在一个转换中全部转换，否则验证器会失败。但是，使用标准的重写规则来管理这一切会很困难：对于每个 OP 重写规则，你需要不断地检查参数和结果是否已经转换。</p>
<p>MLIR 通过一个围绕标准转换的包装器来处理这种情况，这个包装器被称为<a href="https://mlir.llvm.org/docs/DialectConversion/">方言转换框架(dialect conversion framework)</a>. 使用这个框架需要用户继承不同的类来实现普通的重写，设置一些额外的元数据，并以特定的方式 <code>将类型转换与 OP 转换分开</code>，我们稍后会看到具体方式。但从高层次来看，这个框架通过以某种排序顺序 lowering  OP 、同时转换类型，并让 OP 转换器能够访问每个 OP 的原始类型以及在 OP 被框架访问时的进行中的转换类型。每个基于 OP 的重写模式都期望在访问后使该 OP 的类型合法，但不需要担心下游 OP.</p>
<h2 id="modes-of-conversion">Modes of Conversion</h2>
<p>当对一组 OP 进行转换时，有几种不同的转换模式可供选择：</p>
<ul>
<li>Partial Conversion
<ul>
<li>使尽可能多的对目标的操作合法化，但将允许未显式标记为“非法”的预先存在的操作保持未转换。这允许在存在未知操作的情况下部分降低输入。</li>
<li>可以通过 <code>applyPartialConversion</code> 进行部分转换。</li>
</ul>
</li>
<li>Full Conversion
<ul>
<li>使所有输入操作合法化，并且只有当所有操作都正确地合法化到给定的转换目标时才成功。这确保了在转换过程之后只存在已知的操作。</li>
<li>可以通过 applyFullConversion 进行完整转换。</li>
</ul>
</li>
<li>Analysis Conversion
<ul>
<li>如果要应用转换，<code>Analysis Conversion</code> 将分析哪些操作对给定的转换目标是合法的。这是通过执行 &lsquo;Partial&rsquo; Conversion 并记录哪些操作如果成功将被成功转换来完成的。注意，没有 rewrites 或转换实际应用于输入操作。</li>
<li>可以通过 a <code>pplyAnalysisConversion</code> 应用分析转换。</li>
</ul>
</li>
</ul>
<h2 id="conversion-target">Conversion Target</h2>
<p>转换目标是在转换过程中被认为是合法的内容的正式定义。转换框架生成的最终操作必须在converontarget上标记为合法，这样重写才能成功。根据转换模式的不同，现有操作不一定总是合法的。操作和方言可以标记为下列任何规定的合法性行为：</p>
<ul>
<li>Legal: 表明给定操作的每个实例都是合法的，即属性、操作数、类型等的任何组合都是有效的。</li>
<li>Dynamic: 此操作表示给定操作的某些实例是合法的。这允许定义微调约束，例如，<code>arith.addi</code> 仅在操作32位整数时合- Illegal: 此操作表示给定操作的实例不合法。为使转换成功，必须始终转换标记为“非法”的操作。此操作还允许有选择地将特定操作标记为非法，否则将是合法的方言。</li>
</ul>
<p>未明确标记为合法或非法的操作和方言与上述（“未知”操作）分开，并被区别对待，例如，出于上述部分转换的目的。</p>
<p>最后，方言转换框架会跟踪任何未解决的类型冲突。如果在转换结束时仍存在类型冲突，会发生以下两种情况之一。转换框架允许用户可选地实现一个称为类型物化器 (type materializer) 的功能，它会插入新的中间 OP 来解决类型冲突。因此，第一种可能是方言转换框架使用你的类型物化器钩子来修补 IR，转换成功结束。如果这些钩子失败，或者你没有定义任何钩子，那么转换会失败。</p>
<p>这种基础设施的复杂性部分还与上游 MLIR 中一个更困难的 lowering 流水线有关：缓冲区化流水线 (bufferization pipeline). 这个流水线本质上将使用 value semantics 的操作的 IR 转换为使用 pointer semantics 的中间表示。例如，张量类型 (tensor type) 及其相关操作具有 value semantics，这意味着每个操作在语义上都会生成一个全新的张量作为输出，并且所有操作都是 pure 的 (有一些例外情况) 。另一方面， memref 具有 pointer semantics，意味着它更接近于对物理硬件的建模，需要显式的内存分配，并支持对内存位置进行变动的操作。</p>
<p>由于缓冲区化过程复杂，它被拆分为 sub-passes，分别处理与上游 MLIR 各相关方言特定的缓冲区化问题 (参见文档，例如 arith-bufferize、func-bufferize 等) 。每个缓冲区化转换都会产生一些内部无法解决的类型冲突，这些冲突需要自定义的类型物化 (type materializations) 来解决。为了在所有相关方言中处理这些问题，MLIR 团队构建了一个专门的方言，称为缓冲区化方言 (bufferization dialect) ，用来存放中间操作。你会注意到像 to_memref 和 to_tensor 这样的操作，它们扮演了这一角色。然后还有一个最终缓冲区化转换 (finalizing-bufferize pass) ，其作用是清理任何残留的缓冲区化或物化操作。</p>
<h1 id="lowering-poly-with-type-materializations">Lowering Poly with Type Materializations</h1>
<p>跟之前写 Pass tablegen 的时候大同小异，主要是需要定义 dependent dialects. Lowering 必须以这种方式依赖于包含将创建的操作或类型的任何方言，以确保 MLIR 在尝试运行 pass 之前加载这些方言。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// include/Conversion/PolyToStandard/PolyToStandard.td
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#ifndef LIB_CONVERSION_POLYTOSTANDARD_POLYTOSTANDARD_TD_
</span></span></span><span class="line"><span class="cl"><span class="cp">#define LIB_CONVERSION_POLYTOSTANDARD_POLYTOSTANDARD_TD_
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="n">include</span> <span class="s">&#34;mlir/Pass/PassBase.td&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">def</span> <span class="nl">PolyToStandard</span> <span class="p">:</span> <span class="n">Pass</span><span class="o">&lt;</span><span class="s">&#34;poly-to-standard&#34;</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">let</span> <span class="n">summary</span> <span class="o">=</span> <span class="s">&#34;Lower `poly` to standard MLIR dialects.&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">let</span> <span class="n">description</span> <span class="o">=</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="n">This</span> <span class="n">pass</span> <span class="n">lowers</span> <span class="n">the</span> <span class="err">`</span><span class="n">poly</span><span class="err">`</span> <span class="n">dialect</span> <span class="n">to</span> <span class="n">standard</span> <span class="n">MLIR</span><span class="p">,</span> <span class="n">a</span> <span class="n">mixture</span> <span class="n">of</span> <span class="n">affine</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">tensor</span><span class="p">,</span> <span class="n">and</span> <span class="n">arith</span><span class="p">.</span>
</span></span><span class="line"><span class="cl">  <span class="p">}];</span>
</span></span><span class="line"><span class="cl">  <span class="n">let</span> <span class="n">dependentDialects</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s">&#34;mlir::arith::ArithDialect&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s">&#34;mlir::tutorial::poly::PolyDialect&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s">&#34;mlir::tensor::TensorDialect&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#endif  </span><span class="c1">// LIB_CONVERSION_POLYTOSTANDARD_POLYTOSTANDARD_TD_
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>下一步需要定义 ConversionTarget，告诉 MLIR 哪些 OP 需要进行 lowering，可以定义整个需要下降的 dialect 为 illegal，确保在转换完成后没有该 dialect. 这里使用 <code>applyPartialConversion</code> 而不是 <code>applyFullConversion</code> 的原因是报错消息更直观。Partial Conversion 可以看到步骤以及最后无法修补的冲突类型。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// lib/Conversion/PolyToStandard/PolyToStandard.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">PolyToStandard</span> <span class="o">:</span> <span class="n">impl</span><span class="o">::</span><span class="n">PolyToStandardBase</span><span class="o">&lt;</span><span class="n">PolyToStandard</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="n">PolyToStandardBase</span><span class="o">::</span><span class="n">PolyToStandardBase</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">void</span> <span class="nf">runOnOperation</span><span class="p">()</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="o">*</span><span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// TODO: implement pass
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">ConversionTarget</span> <span class="n">target</span><span class="p">(</span><span class="o">*</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">target</span><span class="p">.</span><span class="n">addIllegalDialect</span><span class="o">&lt;</span><span class="n">PolyDialect</span><span class="o">&gt;</span><span class="p">();</span>  <span class="c1">//  declare an entire dialect as “illegal”
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">RewritePatternSet</span> <span class="n">patterns</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">failed</span><span class="p">(</span><span class="n">applyPartialConversion</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">patterns</span><span class="p">))))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>接下来需要定义一个 <a href="https://github.com/llvm/llvm-project/blob/11b9ec5f240ebb32013c33b0c2c80cb7f05ba213/mlir/include/mlir/Transforms/DialectConversion.h#L38">TypeConverter</a> 的子类将 poly dialect 下的 type 转换成其他类型. 其中类型转换和 materialization 是分别通过 <code>addConversion</code> 和 <code>addMaterialization</code> 完成的。这里我们将属于 poly.poly 类型的 degreBound 转换成 Tensor.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PolyToStandardTypeConverter</span> <span class="o">:</span> <span class="k">public</span> <span class="n">TypeConverter</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">PolyToStandardTypeConverter</span><span class="p">(</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">ctx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">addConversion</span><span class="p">([](</span><span class="n">Type</span> <span class="n">type</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="n">type</span><span class="p">;</span> <span class="p">});</span>
</span></span><span class="line"><span class="cl">        <span class="n">addConversion</span><span class="p">([</span><span class="n">ctx</span><span class="p">](</span><span class="n">PolynomialType</span> <span class="n">type</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Type</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">degreeBound</span> <span class="o">=</span> <span class="n">type</span><span class="p">.</span><span class="n">getDegreeBound</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">            <span class="n">IntegerType</span> <span class="n">elementType</span> <span class="o">=</span> <span class="n">IntegerType</span><span class="o">::</span><span class="n">get</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">ctx</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">IntegerType</span><span class="o">::</span><span class="n">SignednessSemantics</span><span class="o">::</span><span class="n">Signless</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">RankedTensorType</span><span class="o">::</span><span class="n">get</span><span class="p">({</span><span class="n">degreeBound</span><span class="p">},</span> <span class="n">elementType</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">});</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>接下来就是要转换 Poly 中的各种 op，需要继承 <a href="https://github.com/llvm/llvm-project/blob/11b9ec5f240ebb32013c33b0c2c80cb7f05ba213/mlir/include/mlir/Transforms/DialectConversion.h#L511">OpConversionPattern</a>，重写里面的 <code>matchAndRewrtite</code> 方法. 以 poly.add 为例，根据父类里的定义，这里 <code>OpAdaptor</code> 即为 <code>AddOp:OpAdaptor</code>，它使用 tablegen 定义的名称作为 op 的参数和方法名称的结果，而不是之前的的getOperand. <code>AddOp</code> 参数包含原始的、未类型转换的操作数和结果。ConversionPatternRewriter类 似于PatternRewriter，但有与方言转换相关的其他方法，例如 convertRegionTypes，用于为嵌套区域的操作应用类型转换。对IR</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ConvertAdd</span> <span class="o">:</span> <span class="k">public</span> <span class="n">OpConversionPattern</span><span class="o">&lt;</span><span class="n">AddOp</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ConvertAdd</span><span class="p">(</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span> <span class="o">:</span> <span class="n">OpConversionPattern</span><span class="o">&lt;</span><span class="n">AddOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">using</span> <span class="n">OpConversionPattern</span><span class="o">::</span><span class="n">OpConversionPattern</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">LogicalResult</span> <span class="nf">matchAndRewrite</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">AddOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">OpAdaptor</span> <span class="n">adaptor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">ConversionPatternRewriter</span><span class="o">&amp;</span> <span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">addOp</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">arith</span><span class="o">::</span><span class="n">AddIOp</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">op</span><span class="o">-&gt;</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">adaptor</span><span class="p">.</span><span class="n">getLhs</span><span class="p">(),</span> <span class="n">adaptor</span><span class="p">.</span><span class="n">getRhs</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">        <span class="n">rewriter</span><span class="p">.</span><span class="n">replaceOp</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="n">getOperation</span><span class="p">(),</span> <span class="n">addOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面我们需要将 ConvertAdd 添加进 <code>PolyToStandard::runOnOperation</code> 中定义的 RewriterPatternSet 中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl">  <span class="n">RewritePatternSet</span> <span class="n">patterns</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">PolyToStandardTypeConverter</span> <span class="n">typeConverter</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">patterns</span><span class="p">.</span><span class="n">add</span><span class="o">&lt;</span><span class="n">ConvertAdd</span><span class="o">&gt;</span><span class="p">(</span><span class="n">typeConverter</span><span class="p">,</span> <span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>MLIR-Ch8 Canonicalizers and Declarative Rewrite Patterns</title>
      <link>http://localhost:57770/blogs/courselearning/mlir/mlir-ch8-canonicalizers-and-declarative-rewrite-patterns/</link>
      <pubDate>Mon, 11 Nov 2024 13:48:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/mlir/mlir-ch8-canonicalizers-and-declarative-rewrite-patterns/</guid>
      <description>Personal MLIR learning notes 8.</description>
      <content:encoded><![CDATA[<h1 id="why-is-canonicalization-needed">Why is Canonicalization Needed?</h1>
<p>规范化器可以用标准的方式编写：在 tablegen 中声明 op 具有规范化器，然后实现生成的 C++函数声明。<a href="https://mlir.llvm.org/docs/Canonicalization/#canonicalizing-with-rewritepatterns">官网例子如下</a></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">def</span> <span class="nl">MyOp</span> <span class="p">:</span> <span class="p">...</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// I want to define a fully general set of patterns for this op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">let</span> <span class="n">hasCanonicalizer</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">def</span> <span class="nl">OtherOp</span> <span class="p">:</span> <span class="p">...</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// A single &#34;matchAndRewrite&#34; style RewritePattern implemented as a method
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// is good enough for me.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">let</span> <span class="n">hasCanonicalizeMethod</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Canonicalization 模式可以通过如下方式定义</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MyOp</span><span class="o">::</span><span class="n">getCanonicalizationPatterns</span><span class="p">(</span><span class="n">RewritePatternSet</span> <span class="o">&amp;</span><span class="n">patterns</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                       <span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">patterns</span><span class="p">.</span><span class="n">add</span><span class="o">&lt;</span><span class="p">...</span><span class="o">&gt;</span><span class="p">(...);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">LogicalResult</span> <span class="n">OtherOp</span><span class="o">::</span><span class="n">canonicalize</span><span class="p">(</span><span class="n">OtherOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// patterns and rewrites go here.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">return</span> <span class="nf">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="canonicalizers-in-c">Canonicalizers in C++</h1>
<p>在 Op 定义中添加 <code>let hasCanonicalizeMethod = 1;</code> 后会为该 Op 生成如下的函数声明。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">static</span> <span class="kt">void</span> <span class="nf">getCanonicalizationPatterns</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">RewritePatternSet</span><span class="o">&amp;</span> <span class="n">results</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span>
</span></span><span class="line"><span class="cl"><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这个函数需要对 results 加入自定义的 <code>OpRewritePattern</code>. 例如可以重写 x^2 - y^2 这个 SubOp 为 (x+y)(x-y)，当 x^2 和 y^2 在后续没有被使用时。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">DifferenceOfSquares</span> <span class="o">:</span> <span class="k">public</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">SubOp</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">DifferenceOfSquares</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">SubOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">LogicalResult</span> <span class="nf">matchAndRewrite</span><span class="p">(</span><span class="n">SubOp</span> <span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">PatternRewriter</span><span class="o">&amp;</span> <span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">Value</span> <span class="n">lhs</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>  <span class="c1">// x^2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">Value</span> <span class="n">rhs</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>  <span class="c1">// y^2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// If either arg has another use, then this rewrite is probably less
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// efficient, because it cannot delete the mul ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">lhs</span><span class="p">.</span><span class="n">hasOneUse</span><span class="p">()</span> <span class="o">||</span> <span class="o">!</span><span class="n">rhs</span><span class="p">.</span><span class="n">hasOneUse</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">rhsMul</span> <span class="o">=</span> <span class="n">rhs</span><span class="p">.</span><span class="n">getDefiningOp</span><span class="o">&lt;</span><span class="n">SubOp</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">lhsMul</span> <span class="o">=</span> <span class="n">rhs</span><span class="p">.</span><span class="n">getDefiningOp</span><span class="o">&lt;</span><span class="n">SubOp</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rhsMul</span> <span class="o">||</span> <span class="o">!</span><span class="n">lhsMul</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// check if lhsMul &amp;&amp; rhsMul is squre operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">bool</span> <span class="n">rhsMulOpsAgree</span> <span class="o">=</span> <span class="n">rhsMul</span><span class="p">.</span><span class="n">getLhs</span><span class="p">()</span> <span class="o">==</span> <span class="n">rhsMul</span><span class="p">.</span><span class="n">getRhs</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="kt">bool</span> <span class="n">lhsMulOpsAgree</span> <span class="o">=</span> <span class="n">lhsMul</span><span class="p">.</span><span class="n">getLhs</span><span class="p">()</span> <span class="o">==</span> <span class="n">lhsMul</span><span class="p">.</span><span class="n">getRhs</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rhsMulOpsAgree</span> <span class="o">||</span> <span class="o">!</span><span class="n">lhsMulOpsAgree</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">x</span> <span class="o">=</span> <span class="n">lhsMul</span><span class="p">.</span><span class="n">getLhs</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">y</span> <span class="o">=</span> <span class="n">rhsMul</span><span class="p">.</span><span class="n">getLhs</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">newAdd</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">AddOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">newSub</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">AddOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">newMul</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">AddOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">newAdd</span><span class="p">,</span> <span class="n">newSub</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rewriter</span><span class="p">.</span><span class="n">replaceOp</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">newMul</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// We don&#39;t need to remove the original ops because MLIR already has
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// canonicalization patterns that remove unused ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">SubOp</span><span class="o">::</span><span class="n">getCanonicalizationPatterns</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">RewritePatternSet</span><span class="o">&amp;</span> <span class="n">results</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">results</span><span class="p">.</span><span class="n">add</span><span class="o">&lt;</span><span class="n">DifferenceOfSquares</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="canonicalizers-in-tablegen">Canonicalizers in Tablegen</h1>
<p>下面利用 tablegen 实现一个多项式共轭的 canonicalizer，f(conj(z)) = conj(f(z)).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// PolyPatterns.td
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">def</span> <span class="nl">LiftConjThroughEval</span> <span class="p">:</span> <span class="n">Pat</span><span class="o">&lt;</span><span class="p">(</span><span class="n">Poly_EvalOp</span> <span class="err">$</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ConjOp</span> <span class="err">$</span><span class="n">z</span><span class="p">,</span> <span class="err">$</span><span class="n">fastmath</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">                                <span class="p">(</span><span class="n">ConjOp</span> <span class="p">(</span><span class="n">Poly_EvalOp</span> <span class="err">$</span><span class="n">f</span><span class="p">,</span> <span class="err">$</span><span class="n">z</span><span class="p">),</span> <span class="err">$</span><span class="n">fastmath</span><span class="p">)</span><span class="o">&gt;</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里的义了重写模式的 <a href="https://github.com/llvm/llvm-project/blob/d8873df4dc74cdcbbfd3334657daf9fedfaab951/mlir/include/mlir/IR/PatternBase.td#L120">Pat</a> 类和定义要匹配和重写的 IR tree 的括号. Pattern 和 Pat 的定义如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Pattern</span><span class="o">&lt;</span><span class="n">dag</span> <span class="n">source</span><span class="p">,</span> <span class="n">list</span><span class="o">&lt;</span><span class="n">dag</span><span class="o">&gt;</span> <span class="n">results</span><span class="p">,</span> <span class="n">list</span><span class="o">&lt;</span><span class="n">dag</span><span class="o">&gt;</span> <span class="n">preds</span> <span class="o">=</span> <span class="p">[],</span>
</span></span><span class="line"><span class="cl">              <span class="n">list</span><span class="o">&lt;</span><span class="n">dag</span><span class="o">&gt;</span> <span class="n">supplemental_results</span> <span class="o">=</span> <span class="p">[],</span>
</span></span><span class="line"><span class="cl">              <span class="n">dag</span> <span class="n">benefitAdded</span> <span class="o">=</span> <span class="p">(</span><span class="n">addBenefit</span> <span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">dag</span> <span class="n">sourcePattern</span> <span class="o">=</span> <span class="n">source</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">list</span><span class="o">&lt;</span><span class="n">dag</span><span class="o">&gt;</span> <span class="n">resultPatterns</span> <span class="o">=</span> <span class="n">results</span><span class="p">;</span> <span class="c1">// 注意这里是 list&lt;dag&gt;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">list</span><span class="o">&lt;</span><span class="n">dag</span><span class="o">&gt;</span> <span class="n">constraints</span> <span class="o">=</span> <span class="n">preds</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">list</span><span class="o">&lt;</span><span class="n">dag</span><span class="o">&gt;</span> <span class="n">supplementalPatterns</span> <span class="o">=</span> <span class="n">supplemental_results</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">dag</span> <span class="n">benefitDelta</span> <span class="o">=</span> <span class="n">benefitAdded</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Pat</span><span class="o">&lt;</span><span class="n">dag</span> <span class="n">pattern</span><span class="p">,</span> <span class="n">dag</span> <span class="n">result</span><span class="p">,</span> <span class="n">list</span><span class="o">&lt;</span><span class="n">dag</span><span class="o">&gt;</span> <span class="n">preds</span> <span class="o">=</span> <span class="p">[],</span>
</span></span><span class="line"><span class="cl">          <span class="n">list</span><span class="o">&lt;</span><span class="n">dag</span><span class="o">&gt;</span> <span class="n">supplemental_results</span> <span class="o">=</span> <span class="p">[],</span>
</span></span><span class="line"><span class="cl">          <span class="n">dag</span> <span class="n">benefitAdded</span> <span class="o">=</span> <span class="p">(</span><span class="n">addBenefit</span> <span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">Pattern</span><span class="o">&lt;</span><span class="n">pattern</span><span class="p">,</span> <span class="p">[</span><span class="n">result</span><span class="p">],</span> <span class="n">preds</span><span class="p">,</span> <span class="n">supplemental_results</span><span class="p">,</span> <span class="n">benefitAdded</span><span class="o">&gt;</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Pattern 类接受一个名为 results 的模板参数，它是一个 <code>list&lt;dag&gt;</code> 类型，可以定义一个或多个结果模式。这使得 Pattern 非常灵活，可以用于处理以下情况：</p>
<ul>
<li>源操作产生多个结果，并且每个结果都需要被不同的新操作替换。</li>
<li>重写过程需要生成一些辅助操作，这些辅助操作本身不直接替换源操作的结果，但有助于构建最终的替换结果。</li>
</ul>
<p>Pat 类继承自 Pattern 类。输入是两个IR tree 对象 (MLIR称之为 DAG nodes)，树中的每个节点由括号 () 指定，括号中的第一个值是操作的名称，其余参数是 op 的参数或属性。当节点可以嵌套，这对应于应用于参数的匹配。它将这个单一的 result DAG 包装成一个只包含一个元素的列表 <code>[result]</code> ，然后传递给父类 Pattern 的 results 参数。因此 Pat 实际上是 Pattern 的一个特例，专门用于定义那些只产生单一结果模式的重写规则。</p>
<p>生成的代码如下所示</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/* Generated from:
</span></span></span><span class="line"><span class="cl"><span class="cm">     /code/sac_mlir_learning/Ch8-DialectConversion/include/mlir-tutorial/Dialect/Poly/PolyPatterns.td:8
</span></span></span><span class="line"><span class="cl"><span class="cm">*/</span>
</span></span><span class="line"><span class="cl"><span class="c1">// 定义一个名为 LiftConjThroughEval 的重写模式结构体，继承自 mlir::RewritePattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">LiftConjThroughEval</span> <span class="o">:</span> <span class="k">public</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">RewritePattern</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 构造函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">LiftConjThroughEval</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">:</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">RewritePattern</span><span class="p">(</span><span class="s">&#34;poly.eval&#34;</span><span class="p">,</span> <span class="c1">// 此模式匹配的根操作名
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                 <span class="mi">2</span><span class="p">,</span>           <span class="c1">// 此模式的收益 (benefit)，用于解决多个模式匹配时的优先级
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                 <span class="n">context</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                 <span class="p">{</span><span class="s">&#34;complex.conj&#34;</span><span class="p">,</span> <span class="s">&#34;poly.eval&#34;</span><span class="p">}</span> <span class="cm">/* 依赖或生成的其他操作名列表 */</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 核心的匹配与重写逻辑
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">matchAndRewrite</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op0</span><span class="p">,</span> <span class="c1">// 当前尝试匹配的操作 (op0 预期为 poly.eval)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span><span class="o">&amp;</span> <span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 用于捕获匹配过程中操作数和属性的变量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">::</span><span class="n">operand_range</span> <span class="n">z</span><span class="p">;</span> <span class="c1">// 将捕获 complex.conj 的操作数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">arith</span><span class="o">::</span><span class="n">FastMathFlagsAttr</span> <span class="n">fastmath</span><span class="p">;</span> <span class="c1">// 将捕获 complex.conj 的 fastmath 属性
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">::</span><span class="n">operand_range</span> <span class="n">f</span><span class="p">;</span> <span class="c1">// 将捕获 poly.eval 的第一个操作数 (多项式)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 用于存储匹配到的操作，方便后续统一获取位置信息
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="mi">4</span><span class="o">&gt;</span> <span class="n">tblgen_ops</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// --- 开始匹配 ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">tblgen_ops</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">op0</span><span class="p">);</span> <span class="c1">// 将根操作 op0 (poly.eval) 加入列表
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 尝试将 op0 动态转换为 poly.eval 类型
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">castedOp0</span> <span class="o">=</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">EvalOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="n">castedOp0</span><span class="p">;</span> <span class="c1">// 避免未使用警告 (如果后续不直接使用 castedOp0 的某些特性)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 获取 poly.eval 的第一个操作数 (多项式 f)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">f</span> <span class="o">=</span> <span class="n">castedOp0</span><span class="p">.</span><span class="n">getODSOperands</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="p">{</span> <span class="c1">// 内嵌作用域，用于匹配 poly.eval 的第二个操作数 (求值点 point)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// 获取定义 poly.eval 第二个操作数 (point) 的那个操作 (op1)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">auto</span><span class="o">*</span> <span class="n">op1</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">castedOp0</span><span class="p">.</span><span class="n">getODSOperands</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">begin</span><span class="p">()).</span><span class="n">getDefiningOp</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">op1</span><span class="p">))</span> <span class="p">{</span> <span class="c1">// 如果 point 不是由某个操作定义的 (例如，它是块参数)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="k">return</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">notifyMatchFailure</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">castedOp0</span><span class="p">,</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Diagnostic</span><span class="o">&amp;</span> <span class="n">diag</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                        <span class="n">diag</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;There&#39;s no operation that defines operand 1 &#34;</span>
</span></span><span class="line"><span class="cl">                                <span class="s">&#34;of castedOp0 (the point operand)&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                    <span class="p">});</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// 尝试将 op1 动态转换为 complex.conj 类型
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">auto</span> <span class="n">castedOp1</span> <span class="o">=</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">complex</span><span class="o">::</span><span class="n">ConjOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="n">castedOp1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">castedOp1</span><span class="p">))</span> <span class="p">{</span> <span class="c1">// 如果 op1 不是 complex.conj 操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="k">return</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">notifyMatchFailure</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">op1</span><span class="p">,</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Diagnostic</span><span class="o">&amp;</span> <span class="n">diag</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                        <span class="n">diag</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Operand 1 of poly.eval is not defined by mlir::complex::ConjOp&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                    <span class="p">});</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// 获取 complex.conj 的操作数 (z)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">z</span> <span class="o">=</span> <span class="n">castedOp1</span><span class="p">.</span><span class="n">getODSOperands</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span> <span class="c1">// 内嵌作用域，用于提取 complex.conj 的 fastmath 属性
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="na">[[maybe_unused]] auto tblgen_attr = // [[maybe_unused]]</span> <span class="err">避免未使用警告</span>
</span></span><span class="line"><span class="cl">                    <span class="n">castedOp1</span><span class="p">.</span><span class="n">getProperties</span><span class="p">().</span><span class="n">getFastmath</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">tblgen_attr</span><span class="p">)</span> <span class="c1">// 如果没有显式设置 fastmath，则默认为 none
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                    <span class="n">tblgen_attr</span> <span class="o">=</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">arith</span><span class="o">::</span><span class="n">FastMathFlagsAttr</span><span class="o">::</span><span class="n">get</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="n">rewriter</span><span class="p">.</span><span class="n">getContext</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">arith</span><span class="o">::</span><span class="n">FastMathFlags</span><span class="o">::</span><span class="n">none</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                <span class="n">fastmath</span> <span class="o">=</span> <span class="n">tblgen_attr</span><span class="p">;</span> <span class="c1">// 保存 fastmath 属性
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="n">tblgen_ops</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">op1</span><span class="p">);</span> <span class="c1">// 将匹配到的 complex.conj 操作 (op1) 加入列表
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// --- 匹配结束 ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// --- 开始重写 ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// 为新生成的操作创建一个融合的位置信息，源自所有匹配到的操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">odsLoc</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">getFusedLoc</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span><span class="n">tblgen_ops</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">tblgen_ops</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-&gt;</span><span class="n">getLoc</span><span class="p">()});</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="n">odsLoc</span><span class="p">;</span> <span class="c1">// 避免未使用警告
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// 用于存储替换原操作 op0 的新值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Value</span><span class="p">,</span> <span class="mi">4</span><span class="o">&gt;</span> <span class="n">tblgen_repl_values</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 声明新的 poly.eval 操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">EvalOp</span> <span class="n">tblgen_EvalOp_0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span> <span class="c1">// 创建新的 poly.eval 操作: eval(f, z)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Value</span> <span class="n">tblgen_value_0</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">f</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span> <span class="c1">// poly.eval 的第一个操作数 (多项式 f)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Value</span> <span class="n">tblgen_value_1</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">z</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span> <span class="c1">// poly.eval 的第二个操作数 (原 conj 的操作数 z)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">tblgen_EvalOp_0</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">EvalOp</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">odsLoc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="cm">/*input=*/</span><span class="n">tblgen_value_0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="cm">/*point=*/</span><span class="n">tblgen_value_1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 声明新的 complex.conj 操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">complex</span><span class="o">::</span><span class="n">ConjOp</span> <span class="n">tblgen_ConjOp_1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span> <span class="c1">// 创建新的 complex.conj 操作: conj(result of new eval)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Value</span><span class="p">,</span> <span class="mi">4</span><span class="o">&gt;</span> <span class="n">tblgen_values</span><span class="p">;</span> <span class="c1">// 新 conj 的操作数列表
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="n">tblgen_values</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">complex</span><span class="o">::</span><span class="n">ConjOp</span><span class="o">::</span><span class="n">Properties</span> <span class="n">tblgen_props</span><span class="p">;</span> <span class="c1">// 新 conj 的属性
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="n">tblgen_props</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">// 新 conj 的操作数是新创建的 poly.eval 的结果
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">tblgen_values</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="p">(</span><span class="o">*</span><span class="n">tblgen_EvalOp_0</span><span class="p">.</span><span class="n">getODSResults</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">begin</span><span class="p">()));</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// 设置新 conj 的 fastmath 属性，与原 conj 保持一致
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">tblgen_props</span><span class="p">.</span><span class="n">fastmath</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">                <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast_if_present</span><span class="o">&lt;</span><span class="k">decltype</span><span class="p">(</span><span class="n">tblgen_props</span><span class="p">.</span><span class="n">fastmath</span><span class="p">)</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">fastmath</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="n">tblgen_ConjOp_1</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">complex</span><span class="o">::</span><span class="n">ConjOp</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">odsLoc</span><span class="p">,</span> <span class="n">tblgen_values</span><span class="p">,</span> <span class="n">tblgen_props</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 将新创建的 complex.conj 操作的结果作为替换值
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">v</span> <span class="p">:</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Value</span><span class="p">,</span> <span class="mi">4</span><span class="o">&gt;</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">                 <span class="n">tblgen_ConjOp_1</span><span class="p">.</span><span class="n">getODSResults</span><span class="p">(</span><span class="mi">0</span><span class="p">)})</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">tblgen_repl_values</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">v</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// 用新的值替换原始操作 op0
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">rewriter</span><span class="p">.</span><span class="n">replaceOp</span><span class="p">(</span><span class="n">op0</span><span class="p">,</span> <span class="n">tblgen_repl_values</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span> <span class="c1">// 表示匹配和重写成功
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">LLVM_ATTRIBUTE_UNUSED</span>
</span></span><span class="line"><span class="cl"><span class="nf">populateWithGenerated</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">RewritePatternSet</span><span class="o">&amp;</span> <span class="n">patterns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">patterns</span><span class="p">.</span><span class="n">add</span><span class="o">&lt;</span><span class="n">LiftConjThroughEval</span><span class="o">&gt;</span><span class="p">(</span><span class="n">patterns</span><span class="p">.</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后跟上一个方法一样，需要添加这个 canonicalizer.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">EvalOp</span><span class="o">::</span><span class="n">getCanonicalizationPatterns</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">RewritePatternSet</span><span class="o">&amp;</span> <span class="n">results</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">populateWithGenerated</span><span class="p">(</span><span class="n">results</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>同样我们可以通过 tablegen 的方式编写 DifferenceOfSquares，但由于将一个 SubOp 替换成了 3 个 Op，需要继承 <code>Pattern</code> 而不是 <code>Pat</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// PolyPatterns.td
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">def</span> <span class="nl">HasOneUse</span><span class="p">:</span> <span class="n">Constraint</span><span class="o">&lt;</span><span class="n">CPred</span><span class="o">&lt;</span><span class="s">&#34;$_self.hasOneUse()&#34;</span><span class="o">&gt;</span><span class="p">,</span> <span class="s">&#34;has one use&#34;</span><span class="o">&gt;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Rewrites (x^2 - y^2) as (x+y)(x-y) if x^2 and y^2 have no other uses.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">def</span> <span class="nl">DifferenceOfSquares</span> <span class="p">:</span> <span class="n">Pattern</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="n">Poly_SubOp</span> <span class="p">(</span><span class="nl">Poly_MulOp</span><span class="p">:</span><span class="err">$</span><span class="n">lhs</span> <span class="err">$</span><span class="n">x</span><span class="p">,</span> <span class="err">$</span><span class="n">x</span><span class="p">),</span> <span class="p">(</span><span class="nl">Poly_MulOp</span><span class="p">:</span><span class="err">$</span><span class="n">rhs</span> <span class="err">$</span><span class="n">y</span><span class="p">,</span> <span class="err">$</span><span class="n">y</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="nl">Poly_AddOp</span><span class="p">:</span><span class="err">$</span><span class="n">sum</span> <span class="err">$</span><span class="n">x</span><span class="p">,</span> <span class="err">$</span><span class="n">y</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="nl">Poly_SubOp</span><span class="p">:</span><span class="err">$</span><span class="n">diff</span> <span class="err">$</span><span class="n">x</span><span class="p">,</span> <span class="err">$</span><span class="n">y</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="nl">Poly_MulOp</span><span class="p">:</span><span class="err">$</span><span class="n">res</span> <span class="err">$</span><span class="n">sum</span><span class="p">,</span> <span class="err">$</span><span class="n">diff</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">  <span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="p">[(</span><span class="nl">HasOneUse</span><span class="p">:</span><span class="err">$</span><span class="n">lhs</span><span class="p">),</span> <span class="p">(</span><span class="nl">HasOneUse</span><span class="p">:</span><span class="err">$</span><span class="n">rhs</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>MLIR-Ch7 Verifiers</title>
      <link>http://localhost:57770/blogs/courselearning/mlir/mlir-ch7-verifiers/</link>
      <pubDate>Sun, 10 Nov 2024 23:51:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/mlir/mlir-ch7-verifiers/</guid>
      <description>Personal MLIR learning notes 7.</description>
      <content:encoded><![CDATA[<h1 id="purposes-of-a-verifier">Purposes of a Verifier</h1>
<p>Verifiers 确保具体的 MLIR 程序中的类型和操作格式正确。验证器会在每次优化 pass 之前和之后运行，帮助确保单个 pass, folders, rewrite patterns 等都能生成正确的 IR. 这使得每个操作的约束条件（invariants）能够得到强制执行，同时简化了传递的实现，因为它们可以依赖这些约束条件，从而避免检查边界情况。多数情况下验证代码是用 Traits 来实现的。</p>
<h1 id="trait-based-verifiers">Trait-based Verifiers</h1>
<p>上一章我们加入了 <code>SameOperandsAndResultElementType</code> 从而让 <code>poly.add</code> 的输入可以既是 poly 或者张量类型的 poly. 从技术上讲，这向 IR 添加了一个验证器，但是为了更清楚地演示这一点，这一章将限制该行为，我们将 Trait 改成 <code>SameOperandsAndResultType</code> 以断言输入和输出类型必须全部一致。</p>
<p>这样会自动生成一些新功能。首先，验证引擎会使用 <code>verifyTrait</code> 来检查类型是否一致。在这里，<code>verifyInvariants</code> 是 <code>Operation</code> 基类中的一个方法，当某些 Traits 注入验证逻辑时，生成的代码会覆盖这个方法，用于检查操作类型上的类型约束。(如果是自定义验证器，则会使用名为 <code>verify</code> 的方法，以与 <code>verifyInvariants</code> 区分开来) 由于 <code>SameOperandsAndResultType</code> 是一个通用检查，因此它不会影响生成的代码。</p>
<p>下面展示了 AddOp 的 inferReturnTypes 方法</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">AddOp</span><span class="o">::</span><span class="nf">inferReturnTypes</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">,</span> <span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Location</span><span class="o">&gt;</span> <span class="n">location</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">ValueRange</span> <span class="n">operands</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">DictionaryAttr</span> <span class="n">attributes</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpaqueProperties</span> <span class="n">properties</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">RegionRange</span> <span class="n">regions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SmallVectorImpl</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span><span class="o">&gt;&amp;</span> <span class="n">inferredReturnTypes</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">inferredReturnTypes</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>  <span class="c1">// Represent AddOp&#39;s output as a single type.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Builder</span> <span class="nf">odsBuilder</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">operands</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1">// Check that there is at least one operand.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="nf">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">odsInferredType0</span> <span class="o">=</span> <span class="n">operands</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">getType</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">inferredReturnTypes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">odsInferredType0</span><span class="p">;</span>  <span class="c1">// Set the output type to the first operand&#39;s type.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>有了类型推导钩子，我们可以简化操作的汇编格式，类型只需要指定一次，而不是三次 (<code>(type, type) -&gt; type</code>). 同时也需要更新所有测试的 mlir 以启用这个新的 assemblyFormat.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-tablegen" data-lang="tablegen"><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">assemblyFormat</span> <span class="p">=</span> <span class="s">&#34;$lhs `,` $rhs attr-dict `:` qualified(type($output))&#34;</span><span class="p">;</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以从 AddOp 的 build 方法中看到现在不需要指定返回值，而是通过 <code>inferReturnTypes</code> 来推导。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">AddOp</span><span class="o">::</span><span class="nf">build</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpBuilder</span><span class="o">&amp;</span> <span class="n">odsBuilder</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OperationState</span><span class="o">&amp;</span> <span class="n">odsState</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Value</span> <span class="n">lhs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Value</span> <span class="n">rhs</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">odsState</span><span class="p">.</span><span class="nf">addOperands</span><span class="p">(</span><span class="n">lhs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">odsState</span><span class="p">.</span><span class="nf">addOperands</span><span class="p">(</span><span class="n">rhs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span><span class="p">,</span> <span class="mi">2</span><span class="o">&gt;</span> <span class="n">inferredReturnTypes</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="nf">succeeded</span><span class="p">(</span><span class="n">AddOp</span><span class="o">::</span><span class="nf">inferReturnTypes</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">odsBuilder</span><span class="p">.</span><span class="nf">getContext</span><span class="p">(),</span> <span class="n">odsState</span><span class="p">.</span><span class="n">location</span><span class="p">,</span> <span class="n">odsState</span><span class="p">.</span><span class="n">operands</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">odsState</span><span class="p">.</span><span class="n">attributes</span><span class="p">.</span><span class="nf">getDictionary</span><span class="p">(</span><span class="n">odsState</span><span class="p">.</span><span class="nf">getContext</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">            <span class="n">odsState</span><span class="p">.</span><span class="nf">getRawProperties</span><span class="p">(),</span> <span class="n">odsState</span><span class="p">.</span><span class="n">regions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">inferredReturnTypes</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="n">odsState</span><span class="p">.</span><span class="nf">addTypes</span><span class="p">(</span><span class="n">inferredReturnTypes</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">detail</span><span class="o">::</span><span class="nf">reportFatalInferReturnTypesError</span><span class="p">(</span><span class="n">odsState</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>EvalOp</code> 无法使用 <code>SameOperandsAndResultType</code>，因为它的操作数需要不同的类型。然而，我们可以使用 <code>AllTypesMatch</code>，它会生成类似的代码，但将验证限制在某些特定类型的子集上。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-td" data-lang="td"><span class="line"><span class="cl"><span class="k">def</span> <span class="nv">Poly_EvalOp</span> <span class="p">:</span> <span class="nv">Op</span><span class="p">&lt;</span><span class="nv">Poly_Dialect</span><span class="p">,</span> <span class="s">&#34;eval&#34;</span><span class="p">,</span> <span class="p">[</span><span class="nv">AllTypesMatch</span><span class="p">&lt;[</span><span class="s">&#34;point&#34;</span><span class="p">,</span> <span class="s">&#34;output&#34;</span><span class="p">]&gt;]&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">summary</span> <span class="p">=</span> <span class="s">&#34;Evaluates a Polynomial at a given input value.&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">arguments</span> <span class="p">=</span> <span class="p">(</span><span class="nv">ins</span> <span class="nv">Polynomial</span><span class="p">:</span><span class="nv">$input</span><span class="p">,</span> <span class="nv">AnyInteger</span><span class="p">:</span><span class="nv">$point</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">results</span> <span class="p">=</span> <span class="p">(</span><span class="nv">outs</span> <span class="nv">AnyInteger</span><span class="p">:</span><span class="nv">$output</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到相似的 <code>inferReturnTypes</code> 方法，由于 EvalOp 是返回多项式在某个整数点上的值，因此推断的返回值类型需要与第二个操作数类型一致。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">EvalOp</span><span class="o">::</span><span class="nf">inferReturnTypes</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">,</span> <span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Location</span><span class="o">&gt;</span> <span class="n">location</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">ValueRange</span> <span class="n">operands</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">DictionaryAttr</span> <span class="n">attributes</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpaqueProperties</span> <span class="n">properties</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">RegionRange</span> <span class="n">regions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SmallVectorImpl</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span><span class="o">&gt;&amp;</span> <span class="n">inferredReturnTypes</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">inferredReturnTypes</span><span class="p">.</span><span class="nf">resize</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Builder</span> <span class="nf">odsBuilder</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">operands</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="nf">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">odsInferredType0</span> <span class="o">=</span> <span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">getType</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">inferredReturnTypes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">odsInferredType0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="a-custom-verifier">A Custom Verifier</h1>
<p>如果需要添加自定义的 verifier 我们需要在 def 的时候添加 <code>let hasVerifier = 1</code>. 我们会发现生成的类里面定义了 verify 方法。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">class</span> <span class="n">EvalOp</span> <span class="p">...</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="p">...</span>
</span></span><span class="line"><span class="cl">  <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="nf">verify</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>因此我们需要在 PolyOps.cpp 中实现它。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="c1">// lib/Dialect/Poly/PolyOps.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">LogicalResult</span> <span class="n">EvalOp</span><span class="o">::</span><span class="nf">verify</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">getPoint</span><span class="p">().</span><span class="nf">getType</span><span class="p">().</span><span class="nf">isSignlessInteger</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">               <span class="o">?</span> <span class="nf">success</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">               <span class="o">:</span> <span class="nf">emitError</span><span class="p">(</span><span class="s">&#34;argument point must be a 32-bit integer&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="a-trait-based-custom-verifier">A Trait-based Custom Verifier</h1>
<p>在 MLIR 中，每个 Trait 都有一个可选的 <code>verifyTrait</code> 钩子，这个钩子会在通过 <code>hasVerifier</code> 创建的自定义验证器之前执行。我们可以利用这个钩子定义通用的验证器，使其适用于多个操作。比如，我们可以通过扩展上一节的内容，创建一个通用的验证器，用于断言所有整数类型的操作数必须是 32 位。</p>
<p>因此我们先需要 def 一个新的 Trait，然后将它加入到 <code>EvalOp</code> 中.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-tablegen" data-lang="tablegen"><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">cppNamespace</span> <span class="p">=</span> <span class="s">&#34;::mlir::tutorial::poly&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以看到生成的代码里有一个新类需要我们实现</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">class</span> <span class="nl">EvalOp</span> <span class="p">:</span> <span class="n">public</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Op</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">    <span class="n">EvalOp</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpTrait</span><span class="o">::</span><span class="n">ZeroRegions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//...,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">Has32BitArguments</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1">//...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们需要新建一个 PolyTraits.h 文件并且让 PolyOps.h 包含它</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="c1">// 
</span></span></span><span class="line"><span class="cl"><span class="c1">// /include/mlir-learning/Dialect/Poly/PolyOps.h
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#ifndef LIB_DIALECT_POLY_POLYTRAITS_H_
</span></span></span><span class="line"><span class="cl"><span class="cp">#define LIB_DIALECT_POLY_POLYTRAITS_H_
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/include/mlir/IR/OpDefinition.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="n">namespace</span> <span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">template</span> <span class="o">&lt;</span><span class="kr">typename</span> <span class="n">ConcreteType</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">class</span> <span class="nl">Has32BitArguments</span> <span class="p">:</span> <span class="n">public</span> <span class="n">OpTrait</span><span class="o">::</span><span class="n">TraitBase</span><span class="o">&lt;</span><span class="n">ConcreteType</span><span class="p">,</span> <span class="n">Has32BitArguments</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="nl">public</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="k">static</span> <span class="n">LogicalResult</span> <span class="nf">verifyTrait</span><span class="p">(</span><span class="n">Operation</span> <span class="o">*</span><span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">type</span> <span class="p">:</span> <span class="n">op</span><span class="o">-&gt;</span><span class="nf">getOperandTypes</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// OK to skip non-integer operand types
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">type</span><span class="p">.</span><span class="nf">isIntOrIndex</span><span class="p">())</span> <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">type</span><span class="p">.</span><span class="nf">isInteger</span><span class="p">(</span><span class="mi">32</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">op</span><span class="o">-&gt;</span><span class="nf">emitOpError</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">               <span class="o">&lt;&lt;</span> <span class="s">&#34;requires each numeric operand to be a 32-bit integer&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#endif  </span><span class="c1">// LIB_DIALECT_POLY_POLYTRAITS_H_
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这样做的优点是具有更强的通用性，但缺点是需要进行繁琐的类型转换来支持特定的操作及其命名参数。例如，这里我们无法直接调用 <code>getPoint</code>，除非对操作进行动态转换为 <code>EvalOp</code>.</p>
]]></content:encoded>
    </item>
    <item>
      <title>MLIR-Ch6 Folders and Constant Propagation</title>
      <link>http://localhost:57770/blogs/courselearning/mlir/mlir-ch6-folders-and-constant-propagation/</link>
      <pubDate>Sat, 09 Nov 2024 20:51:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/mlir/mlir-ch6-folders-and-constant-propagation/</guid>
      <description>Personal MLIR learning notes 6.</description>
      <content:encoded><![CDATA[<h1 id="constant-propagation-vs-canonicalization">Constant Propagation vs Canonicalization</h1>
<p><code>-sccp</code> Sparse Conditional Constant Propagation 是稀疏条件常数传播，它试图推断 op 何时具有常量输出，然后用常量值替换 op 。重复这个过程，它在程序中尽可能地“传播”这些常量。</p>
<p>例如对于如下的函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@test_arith_sccp</span><span class="p">()</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%0</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">7</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%1</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">8</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%2</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%0</span><span class="p">,</span> <span class="nv">%0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%3</span> <span class="p">=</span> arith<span class="p">.</span>muli <span class="nv">%0</span><span class="p">,</span> <span class="nv">%0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%4</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%2</span><span class="p">,</span> <span class="nv">%3</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="kt">return</span> <span class="nv">%2</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>-sccp</code> 优化后的结果如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@test_arith_sccp</span><span class="p">()</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%c63_i32</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">63</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%c49_i32</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">49</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%c14_i32</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">14</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%c8_i32</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">8</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%c7_i32</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">7</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="kt">return</span> <span class="nv">%c14_i32</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>需要注意的是：sccp 不会删除死代码；这里没有展示的是 sccp 的主要作用，它可以通过控制流 (if 或者 loop) 传播常量。</p>
<p>一个相关的概念是 canonicalization，<code>--canonicalize</code> pass 隐藏了 MLIR 中的许多繁重工作。它与 sccp 有一点重叠，因为它也计算常量并在 IR 中具体化它们。例如，在上面的 IR 上使用 <code>——canonicalize</code> pass 的结果如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@test_arith_sccp</span><span class="p">()</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%c14_i32</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">14</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="kt">return</span> <span class="nv">%c14_i32</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>中间的常量都被修剪掉了，剩下的只是返回值，没有任何 op. <strong>规范化不能通过控制流传播常量</strong>。</p>
<p>这两者都是通过折叠 (folding) 来支持的，折叠是采取一系列 op 并将它们合并在一起为更简单的 op 的过程。它还要求我们的方言具有某种常量 op ，该 op 与折叠的结果一起插入。</p>
<p>以这种方式支持折叠所需的大致步骤是：</p>
<ol>
<li>添加一个常量 op.</li>
<li>添加实例化钩子。</li>
<li>为每个 op 添加 folders.</li>
</ol>
<h1 id="making-a-constant-operation">Making a Constant Operation</h1>
<p>我们目前只支持通过 <code>from_tensor</code>  op 从 <code>arith.constant</code> 创建常量。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="nv">%0</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> dense<span class="p">&lt;[</span><span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">]&gt;</span> <span class="p">:</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">3x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nv">%p0</span> <span class="p">=</span> poly<span class="p">.</span>from_tensor <span class="nv">%0</span> <span class="p">:</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">3x</span><span class="k">i32</span><span class="p">&gt;</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>一个常量 op 可以将上述两个操作简化成一个 op. <code>from_tensor</code> op 还可以用于根据数据 (而不仅仅是常数) 构建一个多项函数，因此即使在我们实现了 <code>poly.constant</code> 之后，它也应该保留。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="nv">%0</span> <span class="p">=</span> poly<span class="p">.</span><span class="kt">constant</span> dense<span class="p">&lt;[</span><span class="m">2</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="m">20</span><span class="p">,</span> <span class="m">24</span><span class="p">,</span> <span class="m">18</span><span class="p">]&gt;</span> <span class="p">:</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://mlir.llvm.org/docs/Canonicalization/#canonicalizing-with-the-fold-method">fold</a> 可以用于向 sccp 等 pass 传递信号，表明 op 的结果是常量，或者它可以用于说 op 的结果等效于由不同 op 创建的预先存在的值。对于常量的情况，还需要一个 <code>materializeConstant</code> 钩子来告诉 MLIR 如何获取常量结果并将其转化为适当的 IR  op. 常量 op 的定义如下</p>
<pre tabindex="0"><code>def Poly_ConstantOp: Op&lt;Poly_Dialect, &#34;constant&#34;, [Pure, ConstantLike]&gt; {
  let summary = &#34;Define a constant polynomial via an attribute.&#34;;
  let arguments = (ins AnyIntElementsAttr:$coefficients);
  let results = (outs Polynomial:$output);
  let assemblyFormat = &#34;$coefficients attr-dict `:` type($output)&#34;;
}
</code></pre><p><code>ConstantLike</code> trait 标记的 op 被视为常量值生成 op ，可以在编译时进行常量折叠等优化。<code>arguments</code> 定义 op 的输入是一个具有 <code>AnyIntElementsAttr</code> 的值，使得 op 可以处理任意包含整数的集合，而不仅仅是特定位宽的整数。</p>
<h1 id="adding-folders">Adding Folders</h1>
<p>我们为定义的 op 都加上 <code>let hasFolder = 1;</code> 它在 .hpp.inc 中添加了如下形式的声明。<code>FoldAdaptor</code> 定义为 <code>GenericAdaptor</code> 类型的别名，而 <code>GenericAdaptor</code> 包含了一个 <code>Attribute</code> 数组的引用，这个数组提供了对 op 属性的访问接口。</p>
<p>Attribute 类的核心作用是：</p>
<ul>
<li>表示常量值：Attribute 用于表示操作的静态、不可变的常量值，例如整数、浮点数、字符串、类型信息等。这些值在编译期已知且不可更改。</li>
<li>支持编译器优化：通过提供常量值的表示，Attribute 支持 MLIR 的优化流程，如折叠 (folding) 、规范化 (canonicalization), 常量传播 (constant propagation) 等。</li>
<li>跨方言的通用接口：Attribute 是一个抽象接口，允许不同方言 (dialects) 定义自己的常量表示，同时通过统一的 API 进行操作。</li>
<li>轻量级和高效：Attribute 是一个值类型 (passed by value) ，内部仅存储指向底层存储的指针，依赖 MLIRContext 的唯一化机制 (uniquing) 确保内存效率和一致性。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">using</span> <span class="n">FoldAdaptor</span> <span class="o">=</span> <span class="n">GenericAdaptor</span><span class="o">&lt;::</span><span class="n">llvm</span><span class="o">::</span><span class="n">ArrayRef</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Attribute</span><span class="o">&gt;&gt;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpFoldResult</span> <span class="nf">fold</span><span class="p">(</span><span class="n">FoldAdaptor</span> <span class="n">adaptor</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们需要在 <code>PolyOps.cpp</code> 中实现这个函数。如果 <code>fold</code> 方法决定 op 应被替换为一个常量，则必须返回一个表示该常量的 <code>Attribute</code>，该属性可以作为 <code>poly.constant</code> 操作的输入。<code>FoldAdaptor</code> 是一个适配器，它具有与操作的 C++ 类实例相同的方法名称，但对于那些已经被折叠的参数，会用表示其折叠结果常量的 <code>Attribute</code> 实例替换。这在折叠加法和乘法操作时尤为重要，因为折叠的实现需要立即计算结果，并且需要访问实际的数值来完成计算。</p>
<p>对于 <code>poly.constant</code> 我们只需要返回输入的 attribute.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">OpFoldResult</span> <span class="n">ConstantOp</span><span class="o">::</span><span class="nf">fold</span><span class="p">(</span><span class="n">ConstantOp</span><span class="o">::</span><span class="n">FoldAdaptor</span> <span class="n">adaptor</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">adaptor</span><span class="p">.</span><span class="nf">getCoefficients</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于 from_tensor 我们需要有一个额外的强制转换作为断言，因为张量可能是用我们不希望作为输入的奇怪类型构造的。如果 <code>dyn_cast</code> 结果是 <code>nullptr</code>， MLIR 将其强制转换为失败的 <code>OpFoldResult</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">OpFoldResult</span> <span class="n">FromTensorOp</span><span class="o">::</span><span class="nf">fold</span><span class="p">(</span><span class="n">FromTensorOp</span><span class="o">::</span><span class="n">FoldAdaptor</span> <span class="n">adaptor</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Returns null if the cast failed, which corresponds to a failed fold.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">return</span> <span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">DenseIntElementsAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">adaptor</span><span class="p">.</span><span class="nf">getInput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>BinOp 稍微复杂一些，因为这些 fold 方法中的每一个 op 都接受两个 <code>DenseIntElementsAttr</code> 作为输入，并期望我们为结果返回另一个 <code>DenseIntElementsAttr</code>.</p>
<p>对于 elementwise op 的 add/sub，我们可以使用现有的方法 <code>constFoldBinaryOp</code>，它通过一些模板元编程技巧，允许我们只指定元素 op 本身。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">OpFoldResult</span> <span class="n">AddOp</span><span class="o">::</span><span class="nf">fold</span><span class="p">(</span><span class="n">AddOp</span><span class="o">::</span><span class="n">FoldAdaptor</span> <span class="n">adaptor</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">constFoldBinaryOp</span><span class="o">&lt;</span><span class="n">IntegerAttr</span><span class="p">,</span> <span class="n">APInt</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="n">adaptor</span><span class="p">.</span><span class="nf">getOperands</span><span class="p">(),</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">APInt</span> <span class="n">a</span><span class="p">,</span> <span class="n">APInt</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span> <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于 mul，我们手动的通过循环计算每个系数。<code>getResult()</code> 方法来自于 <code>OneTypedResult</code> 类模板及其内部类 <code>Impl</code> 是一个 MLIR Trait，它主要用于那些返回单一特定类型结果的 op 。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">OpFoldResult</span> <span class="n">MulOp</span><span class="o">::</span><span class="nf">fold</span><span class="p">(</span><span class="n">MulOp</span><span class="o">::</span><span class="n">FoldAdaptor</span> <span class="n">adaptor</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">lhs</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">DenseIntElementsAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">adaptor</span><span class="p">.</span><span class="nf">getOperands</span><span class="p">()[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">rhs</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">DenseIntElementsAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">adaptor</span><span class="p">.</span><span class="nf">getOperands</span><span class="p">()[</span><span class="mi">1</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">lhs</span> <span class="o">||</span> <span class="o">!</span><span class="n">rhs</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">degree</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">mlir</span><span class="o">::</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">PolynomialType</span><span class="o">&gt;</span><span class="p">(</span><span class="nf">getResult</span><span class="p">().</span><span class="nf">getType</span><span class="p">()).</span><span class="nf">getDegreeBound</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">maxIndex</span> <span class="o">=</span> <span class="n">lhs</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span> <span class="o">+</span> <span class="n">rhs</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">llvm</span><span class="o">::</span><span class="n">APInt</span><span class="p">,</span> <span class="mi">8</span><span class="o">&gt;</span> <span class="n">results</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">results</span><span class="p">.</span><span class="nf">reserve</span><span class="p">(</span><span class="n">maxIndex</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">maxIndex</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">results</span><span class="p">.</span><span class="nf">push_back</span><span class="p">(</span><span class="nf">APInt</span><span class="p">((</span><span class="o">*</span><span class="n">lhs</span><span class="p">.</span><span class="nf">begin</span><span class="p">()).</span><span class="nf">getBitWidth</span><span class="p">(),</span> <span class="mi">0</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int64_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">lhsIt</span> <span class="o">=</span> <span class="n">lhs</span><span class="p">.</span><span class="n">value_begin</span><span class="o">&lt;</span><span class="n">APInt</span><span class="o">&gt;</span><span class="p">();</span> <span class="n">lhsIt</span> <span class="o">!=</span> <span class="n">lhs</span><span class="p">.</span><span class="n">value_end</span><span class="o">&lt;</span><span class="n">APInt</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">         <span class="n">lhsIt</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int64_t</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="n">rhsIt</span> <span class="o">=</span> <span class="n">rhs</span><span class="p">.</span><span class="n">value_begin</span><span class="o">&lt;</span><span class="n">APInt</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">             <span class="n">rhsIt</span> <span class="o">!=</span> <span class="n">rhs</span><span class="p">.</span><span class="n">value_end</span><span class="o">&lt;</span><span class="n">APInt</span><span class="o">&gt;</span><span class="p">();</span> <span class="n">rhsIt</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">results</span><span class="p">[(</span><span class="n">i</span> <span class="o">+</span> <span class="n">j</span><span class="p">)</span> <span class="o">%</span> <span class="n">degree</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="o">*</span><span class="n">lhsIt</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">*</span><span class="n">rhsIt</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="n">j</span><span class="o">++</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">i</span><span class="o">++</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">DenseIntElementsAttr</span><span class="o">::</span><span class="nf">get</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">RankedTensorType</span><span class="o">::</span><span class="nf">get</span><span class="p">(</span><span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">results</span><span class="p">.</span><span class="nf">size</span><span class="p">()),</span>
</span></span><span class="line"><span class="cl">                              <span class="n">mlir</span><span class="o">::</span><span class="n">IntegerType</span><span class="o">::</span><span class="nf">get</span><span class="p">(</span><span class="nf">getContext</span><span class="p">(),</span> <span class="mi">32</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">        <span class="n">results</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="adding-a-constant-materializer">Adding a Constant Materializer</h1>
<p>最后我们添加常量实例化函数，这是一个 dialect 级别的特性，我们在 <code>PolyDialect.td</code> 中添加 <code>let hasConstantMaterializer = 1;</code> 则会在 .hpp.inc 中添加如下形式的声明。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span> <span class="o">*</span><span class="nf">materializeConstant</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpBuilder</span> <span class="o">&amp;</span><span class="n">builder</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Attribute</span> <span class="n">value</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">type</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                         <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Location</span> <span class="n">loc</span><span class="p">)</span> <span class="n">override</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该函数作用是将给定 Attribute (上面每个折叠步骤的结果) 的单个常量 op 实例化为所需的结果 Type.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="n">Operation</span> <span class="o">*</span><span class="n">PolyDialect</span><span class="o">::</span><span class="nf">materializeConstant</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">OpBuilder</span> <span class="o">&amp;</span><span class="n">builder</span><span class="p">,</span> <span class="n">Attribute</span> <span class="n">value</span><span class="p">,</span> <span class="n">Type</span> <span class="n">type</span><span class="p">,</span> <span class="n">Location</span> <span class="n">loc</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">coeffs</span> <span class="o">=</span> <span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">DenseIntElementsAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">value</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">coeffs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">nullptr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">builder</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">type</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>MLIR-Ch5 Using Traits</title>
      <link>http://localhost:57770/blogs/courselearning/mlir/mlir-ch5-using-traits/</link>
      <pubDate>Fri, 08 Nov 2024 23:06:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/mlir/mlir-ch5-using-traits/</guid>
      <description>Personal MLIR learning notes 5.</description>
      <content:encoded><![CDATA[<h1 id="traits-and-loop-invariant-code-motion">Traits and Loop Invariant Code Motion</h1>
<p>为了提高代码重用性，MLIR 提供了 <a href="https://mlir.llvm.org/docs/Traits/">Traits</a> 和 <a href="https://mlir.llvm.org/docs/Interfaces/">Interfaces</a> Traits，用于增强 op  (Operation) 或类型的功能，提供结构化的约束和功能接口，方便在编译优化和生成过程中进行更强大和灵活的 op 。</p>
<blockquote>
<p><a href="https://mlir.llvm.org/docs/Traits/">Traits</a> 是一种机制，用于抽象出多个不同属性、 op 或类型之间共同的实现细节和特性。可用于指定对象的特殊属性和约束，例如 op 是否具有副作用，或其输出类型是否与输入类型相同。Traits 将特定的行为或限制抽象出来，使这些行为可以复用在不同的对象上，而不需要在每个对象中重复实现相同的逻辑。</p></blockquote>
<blockquote>
<p><a href="https://mlir.llvm.org/docs/Interfaces/">Interfaces</a> 是一种通用的机制，用于与 IR 进行交互。它们的目标是使转换或分析可以基于这些接口进行，而无需了解具体的 op 或 dialect 的内部实现。通过这种方法，编译器可以在实现转换和分析时不依赖于特定 dialect 或 op ，从而更轻松地扩展编译器的功能。</p></blockquote>
<p>Loop Invariant Code Motion 是 MLIR 提供的 <a href="">General Transform Passes</a> 之一。它会检查循环体中的 op ，如果发现某些 op 在循环内部执行没有必要（即它们的结果在每次循环中保持不变），就会将这些 op 移出循环体。这可以减少循环中的重复计算，提高效率。</p>
<p>要让某个自定义 op 可以被这种 pass 识别并移出循环体，需要添加两个关键的 Traits 来表明该 op 在循环外执行是安全的：</p>
<ul>
<li><a href="https://github.com/llvm/llvm-project/blob/71be020dda2c97c2733e45f4b1003d1c135b3b43/mlir/include/mlir/Interfaces/SideEffectInterfaces.td#L81">NoMemoryEffect</a>: 是 MemoryEffect 的一个 empty 实现，表示该 op 不会产生任何与内存写入相关的副作用。</li>
<li><a href="https://github.com/llvm/llvm-project/blob/71be020dda2c97c2733e45f4b1003d1c135b3b43/mlir/include/mlir/Interfaces/SideEffectInterfaces.td#L123">AlwaysSpeculatable</a>: 是一个包含两个 Traits 的 列表，告诉编译器该 op 可以在不影响程序逻辑的前提下，将其提前计算或移动到其他位置。</li>
</ul>
<p>在 MLIR 中，Loop Invariant Code Motion (LICM) 会将具有 <code>NoMemoryEffect</code> 和 <code>AlwaysSpeculatable</code> 这两个 Traits 的 op 移动到循环体外部，但前提是该 op 的 operands 在整个循环体中保持不变。这样可以避免循环内部的重复计算，从而优化代码执行效率。MLIR 提供了一个方便的组合 Trait <a href="https://github.com/llvm/llvm-project/blob/71be020dda2c97c2733e45f4b1003d1c135b3b43/mlir/include/mlir/Interfaces/SideEffectInterfaces.td#L133">Pure</a>，它包含了 <code>NoMemoryEffect</code> 和 <code>AlwaysSpeculatable</code> 这两个 Traits. 因此，直接添加 <code>Pure</code> Trait 到 op 的定义中就能让编译器自动识别它为可移动到循环外部的 op 。</p>
<p><code>TypeOrContainer</code> 是一个用于处理 op 输入和输出类型的机制，它可以匹配单个类型 (如 <code>f32</code> 或 <code>i32</code>) 以及容器类型(如 <code>vector&lt;f32&gt;</code> 或 <code>tensor&lt;i32&gt;</code>)，使得一个 op 可以被设计为同时支持标量类型和集合类型。</p>
<pre tabindex="0"><code>include &#34;mlir/Interfaces/SideEffectInterfaces.td&#34;

def PolyOrContainer: TypeOrContainer&lt;Polynomial, &#34;poly-or-container&#34;&gt;;

class Poly_BinOp&lt;string mnemonic&gt;: Op&lt;Poly_Dialect, mnemonic, [Pure]&gt; {
    let arguments = (ins PolyOrContainer:$lhs, PolyOrContainer:$rhs);
    let results = (outs PolyOrContainer:$output);
    let assemblyFormat = &#34;$lhs `,` $rhs attr-dict `:` `(` type($lhs) `,` type($rhs) `)` `-&gt;` type($output)&#34;;
}
</code></pre><p>加入 <code>Pure</code> trait 后生成的 .hpp.inc 中关于 op 的定义继承了新的内容</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AddOp</span>
</span></span><span class="line"><span class="cl">    <span class="o">:</span> <span class="k">public</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Op</span><span class="o">&lt;</span> <span class="n">AddOp</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpTrait</span><span class="o">::</span><span class="n">ZeroRegions</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpTrait</span><span class="o">::</span><span class="n">OneResult</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpTrait</span><span class="o">::</span><span class="n">OneTypedResult</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="o">&gt;::</span><span class="n">Impl</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpTrait</span><span class="o">::</span><span class="n">ZeroSuccessors</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpTrait</span><span class="o">::</span><span class="n">NOperands</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;::</span><span class="n">Impl</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpTrait</span><span class="o">::</span><span class="n">OpInvariants</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">ConditionallySpeculatable</span><span class="o">::</span><span class="n">Trait</span><span class="p">,</span>            <span class="c1">// &lt;-- new
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OpTrait</span><span class="o">::</span><span class="n">AlwaysSpeculatableImplTrait</span><span class="p">,</span>   <span class="c1">// &lt;-- new
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MemoryEffectOpInterface</span><span class="o">::</span><span class="n">Trait</span><span class="o">&gt;</span>          <span class="c1">// &lt;--- new
</span></span></span></code></pre></td></tr></table>
</div>
</div><p><code>NoMemoryEffect</code> interface 则在生成的 .cpp.inc 中添加了一个简单的函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">AddOp</span><span class="o">::</span><span class="n">getEffects</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SmallVectorImpl</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">        <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">SideEffects</span><span class="o">::</span><span class="n">EffectInstance</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MemoryEffects</span><span class="o">::</span><span class="n">Effect</span><span class="o">&gt;&gt;&amp;</span>
</span></span><span class="line"><span class="cl">        <span class="n">effects</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以写一个 .mlir 来测试 <code>%2</code> 的计算是否能优化到循环外：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="c">// RUN: build/Ch4-UsingTraits/tools/ch4-tutorial-opt %s --loop-invariant-code-motion &gt; %t
</span></span></span><span class="line"><span class="cl"><span class="c">// RUN: FileCheck %s &lt; %t
</span></span></span><span class="line"><span class="cl"><span class="c"></span>
</span></span><span class="line"><span class="cl">module <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c">// CHECK-LABEL: func.func @test_loop_invariant_code_motion
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@test_loop_invariant_code_motion</span><span class="p">()</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nv">%0</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> dense<span class="p">&lt;[</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">]&gt;</span> <span class="p">:</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">3x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">        <span class="nv">%p0</span> <span class="p">=</span> poly<span class="p">.</span>from_tensor <span class="nv">%0</span> <span class="p">:</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">3x</span><span class="k">i32</span><span class="p">&gt;</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nv">%1</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> dense<span class="p">&lt;[</span><span class="m">9</span><span class="p">,</span><span class="m">8</span><span class="p">,</span><span class="m">16</span><span class="p">]&gt;</span> <span class="p">:</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">3x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">        <span class="nv">%p1</span> <span class="p">=</span> poly<span class="p">.</span>from_tensor <span class="nv">%0</span> <span class="p">:</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">3x</span><span class="k">i32</span><span class="p">&gt;</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">        <span class="c">// CHECK: poly.mul
</span></span></span><span class="line"><span class="cl"><span class="c"></span>
</span></span><span class="line"><span class="cl">        <span class="c">// CHECK: affine.for
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="nv">%ret_val</span> <span class="p">=</span> affine<span class="p">.</span>for <span class="nv">%i</span> <span class="p">=</span> <span class="m">0</span> to <span class="m">100</span> iter_args<span class="p">(</span><span class="nv">%sum_iter</span> <span class="p">=</span> <span class="nv">%p0</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c">// The polt.mul should be hoisted out of the loop.
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="c">// CHECK-NOT: poly.mul
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="nv">%2</span> <span class="p">=</span> poly<span class="p">.</span>mul <span class="nv">%p0</span><span class="p">,</span> <span class="nv">%p1</span> <span class="p">:</span> <span class="p">(!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;,</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">            <span class="nv">%sum_next</span> <span class="p">=</span> poly<span class="p">.</span>add <span class="nv">%sum_iter</span><span class="p">,</span> <span class="nv">%2</span> <span class="p">:</span>  <span class="p">(!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;,</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">            affine<span class="p">.</span>yield <span class="nv">%sum_next</span> <span class="p">:</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kt">return</span> <span class="nv">%ret_val</span><span class="p">:</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="passes-already-handled-by-pure">Passes Already Handled by Pure</h1>
<p>给某个 op 加上 <code>Pure</code> Trait 后，下列 Pass 就会自动识别并优化该 op ：</p>
<ul>
<li><code>--control-flow-sink</code>: 将只在条件语句的某一个分支中使用的 op 移动到对应的分支中，以减少无效代码的执行。需要 op 无内存副作用 (memory-effect free)，通常可以通过 <code>Pure</code> Trait 来满足。</li>
<li><code>--cse</code> (Constant Subexpression Elimination): 常量子表达式消除。当某些重复的计算结果已经存在时，消除不必要的重复计算，提高效率。需要 op 没有内存副作用（memory-effect free），因此 <code>Pure</code> Trait 也可以满足这一要求。</li>
<li><code>--inline</code>: 将函数调用“内联”到调用位置，以减少函数调用的开销。在某些情况下，这可以减少调用栈的深度或优化代码执行的性能。</li>
<li><code>--mem2reg</code>: 将内存中的存储/加载 op 转换为对实际值的直接使用，从而减少内存访问，提高运行效率。</li>
<li><code>--remove-dead-values</code>: 移除未使用的函数参数或返回值，以减少不必要的数据传递或内存占用。</li>
<li><code>--sroa</code> (Scalar Replacement of Aggregates): 将聚合类型（例如数组或结构体）拆分为标量值，通常会对内存布局进行重排，以便更好地利用内存。</li>
<li><code>--symbol-dce</code> (Symbol Dead Code Elimination): 消除不再使用的私有函数 (死代码)，减少不必要的代码量。</li>
</ul>
<h1 id="elementwise-mappings">Elementwise Mappings</h1>
<p>有四种 traits 可以把标量运算扩展到张量运算或者反过来</p>
<ul>
<li>
<p><code>Elemntwise</code>: 标记逐元素的 op ，仅适用于向量或张量，不允许广播。</p>
<ul>
<li>如果任何结果是向量或张量，至少有一个 operand 必须是向量或张量。</li>
<li>如果任何 operand 是向量或张量，至少有一个结果并且所有结果必须是向量或张量。</li>
<li>所有 operand 和结果的向量或张量类型必须具有相同的形状。形状可以是动态的，但对于不匹配的形状，行为是未定义的。</li>
<li>该 op 必须在 operand 和结果上逐元素进行，即在单元素向量或张量上应用时，每个元素的结果应相同。</li>
</ul>
</li>
<li>
<p><code>Scalarizable</code>: 标记和验证某些操作是否可以被系统性地标量化，即将其基于向量或张量的操作转化为基于标量的操作。只要操作是 Elementwise 的，Scalarizable 就可以使用。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="nv">%tensor_select</span> <span class="p">=</span> <span class="s">&#34;arith.select&#34;</span><span class="p">(</span><span class="nv">%pred_tensor</span><span class="p">,</span> <span class="nv">%true_val</span><span class="p">,</span> <span class="nv">%false_val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="p">:</span> <span class="p">(</span><span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">i1</span><span class="p">&gt;,</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;,</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;)</span>
</span></span><span class="line"><span class="cl">                <span class="p">-&gt;</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="c">// Can be scalarized to
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nv">%scalar_select</span> <span class="p">=</span> <span class="s">&#34;arith.select&#34;</span><span class="p">(</span><span class="nv">%pred</span><span class="p">,</span> <span class="nv">%true_val_scalar</span><span class="p">,</span> <span class="nv">%false_val_scalar</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="p">:</span> <span class="p">(</span><span class="k">i1</span><span class="p">,</span> <span class="k">f32</span><span class="p">,</span> <span class="k">f32</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="k">f32</span>
</span></span></code></pre></div></li>
<li>
<p><code>Vectorizable</code>: 提供了与 <code>Scalarizable</code> 相反的 op 。所有的标量 operand 和结果将被替换为相应的向量类型。即，该 op 表示同时作用于多个元素。允许通过广播将标量提升为向量，再进行向量化操作。</p>
</li>
<li>
<p><code>Tensorizable</code>: 提供了与 <code>Scalarizable</code> 相反的 op ，允许在张量和标量之间进行推理。允许通过广播将标量提升为张量，以便在张量 op 中保持一致的 op 结构。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="nv">%scalar</span> <span class="p">=</span> <span class="s">&#34;arith.addf&#34;</span><span class="p">(</span><span class="nv">%a</span><span class="p">,</span> <span class="nv">%b</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="k">f32</span><span class="p">,</span> <span class="k">f32</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="k">f32</span>
</span></span><span class="line"><span class="cl"><span class="c">// Can be tensorized to
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nv">%tensor</span> <span class="p">=</span> <span class="s">&#34;arith.addf&#34;</span><span class="p">(</span><span class="nv">%a</span><span class="p">,</span> <span class="nv">%b</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;,</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;)</span>
</span></span><span class="line"><span class="cl">            <span class="p">-&gt;</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="c">// Also supports broadcasting
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nv">%scalar_pred</span> <span class="p">=</span> <span class="s">&#34;arith.select&#34;</span><span class="p">(</span><span class="nv">%pred</span><span class="p">,</span> <span class="nv">%true_val</span><span class="p">,</span> <span class="nv">%false_val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="p">:</span> <span class="p">(</span><span class="k">i1</span><span class="p">,</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;,</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="c">// Can be tensorized to
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nv">%tensor_pred</span> <span class="p">=</span> <span class="s">&#34;arith.select&#34;</span><span class="p">(</span><span class="nv">%pred</span><span class="p">,</span> <span class="nv">%true_val</span><span class="p">,</span> <span class="nv">%false_val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="p">:</span> <span class="p">(</span><span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">i1</span><span class="p">&gt;,</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;,</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;)</span>
</span></span><span class="line"><span class="cl">            <span class="p">-&gt;</span> <span class="kt">tensor</span><span class="p">&lt;</span><span class="m">?x</span><span class="k">f32</span><span class="p">&gt;</span>
</span></span></code></pre></div></li>
</ul>
<p><code>ElementwiseMappable</code> Trait 包含了以上所有的 Traits. 我们可以修改 <code>Poly_BinOp</code> 定义如下：</p>
<pre tabindex="0"><code>// PolyOps.td
def PolyOrContainer : TypeOrContainer&lt;Polynomial, &#34;poly-or-container&#34;&gt;;

class Poly_BinOp&lt;string mnemonic&gt; : Op&lt;Poly_Dialect, mnemonic, [Pure, ElementwiseMappable]&gt; {
  let arguments = (ins PolyOrContainer:$lhs, PolyOrContainer:$rhs);
  let results = (outs PolyOrContainer:$output);
  ...
}
</code></pre><p>添加这个 Trait 后，生成的 .cpp.inc 文件定义了许多检查 op 数类型的函数，下面是其中一个：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">static</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">__mlir_ods_local_type_constraint_PolyOps1</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">type</span><span class="p">,</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringRef</span> <span class="n">valueKind</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="n">valueIndex</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(((</span><span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">isa</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="o">&gt;</span><span class="p">(</span><span class="n">type</span><span class="p">)))</span> <span class="o">||</span>
</span></span><span class="line"><span class="cl">            <span class="p">(((</span><span class="n">type</span><span class="p">.</span><span class="n">hasTrait</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">ValueSemantics</span><span class="o">&gt;</span><span class="p">()))</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">            <span class="p">([](</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">elementType</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="p">(</span><span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">isa</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">elementType</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">            <span class="p">}(</span><span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">cast</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">(</span><span class="n">type</span><span class="p">).</span><span class="n">getElementType</span><span class="p">())))))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">emitOpError</span><span class="p">(</span><span class="n">valueKind</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="o">&lt;&lt;</span> <span class="s">&#34; #&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">valueIndex</span>
</span></span><span class="line"><span class="cl">                <span class="o">&lt;&lt;</span> <span class="s">&#34; must be poly-or-container, but got &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该函数首先检查 <code>type</code> 是否为 <code>PolynomialType</code>；如果不是，则进一步检查它是否具有 <code>ValueSemantics</code> Trait，并且是一个 <code>ShapedType</code>（即容器类型，如 <code>vector</code> 或 <code>tensor</code>），其中包含的元素类型是 <code>PolynomialType</code>.</p>
]]></content:encoded>
    </item>
    <item>
      <title>MLIR-Ch4 Defining a New Dialect</title>
      <link>http://localhost:57770/blogs/courselearning/mlir/mlir-ch4-defining-a-new-dialect/</link>
      <pubDate>Thu, 07 Nov 2024 18:16:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/mlir/mlir-ch4-defining-a-new-dialect/</guid>
      <description>Personal MLIR learning notes 4.</description>
      <content:encoded><![CDATA[<h1 id="sketching-out-a-dseign">Sketching Out a Dseign</h1>
<p>TableGen 也可以用来定义 dialect. 本文将定义一个单未知数多项式运算的 dialect，系数用 uint32_t 类型表示。，并提供通过从标准 MLIR 类型指定多项式系数来定义多项式的操作，提取关于多项式的数据以将结果存储在标准MLIR类型中，以及对多项式进行算术运算。</p>
<h1 id="an-empty-dialect">An Empty Dialect</h1>
<p>我们首先用 TableGen 定义一个空的 dialect. 它和上一章定义 Pass 没什么不同，只不过 include 的是 DialectBase.td 文件。同时也定义了命名空间为 <code>::mlir::tutorial::poly</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-tablegen" data-lang="tablegen"><span class="line"><span class="cl"><span class="nv">include</span> <span class="s">&#34;mlir/IR/DialectBase.td&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nv">Poly_Dialect</span> <span class="p">:</span> <span class="nv">Dialect</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">name</span> <span class="p">=</span> <span class="s">&#34;poly&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">summary</span> <span class="p">=</span> <span class="s">&#34;A dialect for polynomial math&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">description</span> <span class="p">=</span> <span class="s">[{
</span></span></span><span class="line"><span class="cl"><span class="s">    The poly dialect defines types and operations for single-variable
</span></span></span><span class="line"><span class="cl"><span class="s">    polynomials over integers.
</span></span></span><span class="line"><span class="cl"><span class="s">  }]</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">cppNamespace</span> <span class="p">=</span> <span class="s">&#34;::mlir::tutorial::poly&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们需要在 include 目录下的 CMakeLists.txt 文件中添加</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cmake" data-lang="cmake"><span class="line"><span class="cl"><span class="nb">set</span><span class="p">(</span><span class="s">TARGET_NAME</span> <span class="s2">&#34;${PROJECT_TARGET_PREFIX}-Dialect-PolyDialect-IncGen&#34;</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">set</span><span class="p">(</span><span class="s">LLVM_TARGET_DEFINITIONS</span> <span class="s">mlir-learning/Dialect/Poly/PolyDialect.td</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">mlir_tablegen</span><span class="p">(</span><span class="s">mlir-learning/Dialect/Poly/PolyDialect.hpp.inc</span> <span class="s">--gen-dialect-decls</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">mlir_tablegen</span><span class="p">(</span><span class="s">mlir-learning/Dialect/Poly/PolyDialect.cpp.inc</span> <span class="s">--gen-dialect-defs</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">add_public_tablegen_target</span><span class="p">(</span><span class="o">${</span><span class="nv">TARGET_NAME</span><span class="o">}</span><span class="p">)</span><span class="err">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>然后在 tutorial-opt.cpp 中注册所有 mlir 自带的所有 dialect 后进行构建，我们可以查看生成的 .hpp.inc 和.cpp.inc 文件。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">mlir</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">tutorial</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PolyDialect</span> <span class="o">:</span> <span class="k">public</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Dialect</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">explicit</span> <span class="nf">PolyDialect</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">void</span> <span class="nf">initialize</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="k">friend</span> <span class="k">class</span> <span class="err">::</span><span class="nc">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="o">~</span><span class="n">PolyDialect</span><span class="p">()</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">static</span> <span class="k">constexpr</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringLiteral</span> <span class="n">getDialectNamespace</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringLiteral</span><span class="p">(</span><span class="s">&#34;poly&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="c1">// namespace tutorial
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace mlir
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">MLIR_DECLARE_EXPLICIT_TYPE_ID</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">PolyDialect</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>编译器会报错，因为 inc 不会包含 Dialect 等类所在的头文件。这需要我们自己在 PolyDialect.h 文件中进行 include，这样 当重新构建的时候该文件注入变不会报错</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// include/mlir-learning/Dialect/Poly/PolyDialect.h
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#ifndef LIB_DIALECT_POLY_POLYDIALECT_H
</span></span></span><span class="line"><span class="cl"><span class="cp">#define LIB_DIALECT_POLY_POLYDIALECT_H
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/IR/DialectImplementation.h&#34;</span><span class="cp">  </span><span class="c1">// include mannually
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Dialect/Poly/PolyDialect.hpp.inc&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>生成的 .cpp.inc 如下，他只包含了该类基本的构造函数和析构函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">MLIR_DEFINE_EXPLICIT_TYPE_ID</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolyDialect</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">mlir</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">tutorial</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">poly</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">PolyDialect</span><span class="o">::</span><span class="n">PolyDialect</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="o">:</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Dialect</span><span class="p">(</span><span class="n">getDialectNamespace</span><span class="p">(),</span> <span class="n">context</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">TypeID</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="n">PolyDialect</span><span class="o">&gt;</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">     <span class="p">{</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">initialize</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">PolyDialect</span><span class="o">::~</span><span class="n">PolyDialect</span><span class="p">()</span> <span class="o">=</span> <span class="k">default</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="c1">// namespace poly
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace tutorial
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace mlir
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>然后我们可以在 tutorial-opt.cpp 中注册该 dialect.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/* other includes */</span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Dialect/Poly/PolyDialect.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Register all built-in MLIR dialects
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mlir</span><span class="o">::</span><span class="n">DialectRegistry</span> <span class="n">registry</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Register our Dialect
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">registry</span><span class="p">.</span><span class="n">insert</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolyDialect</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">registerAllDialects</span><span class="p">(</span><span class="n">registry</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">asMainReturnCode</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">mlir</span><span class="o">::</span><span class="n">MlirOptMain</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">,</span> <span class="s">&#34;Tutorial Pass Driver&#34;</span><span class="p">,</span> <span class="n">registry</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="adding-a-trival-type">Adding a Trival Type</h1>
<p>下面我们需要定义自己的 poly.poly 类型.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-tablegen" data-lang="tablegen"><span class="line"><span class="cl"><span class="c">// poly_types.td
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="cp">#ifndef LIB_DIALECT_POLY_POLYTYPES_TD_</span>
</span></span><span class="line"><span class="cl"><span class="cp">#define LIB_DIALECT_POLY_POLYTYPES_TD_</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">include</span> <span class="s">&#34;mlir-learning/Dialect/Poly/PolyDialect.td&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nv">include</span> <span class="s">&#34;mlir/IR/AttrTypeBase.td&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c">// a base class for all types in the dialect
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="k">class</span> <span class="nv">Poly_Type</span><span class="p">&lt;</span><span class="k">string</span> <span class="nv">name</span><span class="p">,</span> <span class="k">string</span> <span class="nv">typeMnemonic</span><span class="p">&gt;</span> <span class="p">:</span> <span class="nv">TypeDef</span><span class="p">&lt;</span><span class="nv">Poly_Dialect</span><span class="p">,</span> <span class="nv">name</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">let</span> <span class="nv">mnemonic</span> <span class="p">=</span> <span class="nv">typeMnemonic</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nv">Polynomial</span><span class="p">:</span> <span class="nv">Poly_Type</span><span class="p">&lt;</span><span class="s">&#34;Polynomial&#34;</span><span class="p">,</span> <span class="s">&#34;poly&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">summary</span> <span class="p">=</span> <span class="s">&#34;A polynomial with u32 coefficients&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">let</span> <span class="nv">description</span> <span class="p">=</span> <span class="s">[{
</span></span></span><span class="line"><span class="cl"><span class="s">    A type for polynomials with integer coefficients in a single-variable polynomial ring.
</span></span></span><span class="line"><span class="cl"><span class="s">  }]</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 MLIR 的 TableGen 文件中，class 和 def 的用法和含义有所不同</p>
<ul>
<li><code>class</code> 用于定义一个模板或基类，可以被其他类型或定义继承和重用。它本身不会创建实际的对象或具体类型，它只是一种结构，可以包含参数和默认属性。其他定义可以通过继承该类来获得其功能。</li>
<li><code>def</code> 用于创建一个具体的实例，比如一个类型、操作或属性。它会将所定义的内容应用到 TableGen 中，使其成为可用的具体类型或功能。</li>
</ul>
<p>这里我们定义了一个名为 <code>Poly_Type</code> 的类，参数为 <code>name</code>（类型的名称）和 <code>typeMnemonic</code>（类型的简写或助记符）。这个类继承自 <code>TypeDef&lt;Poly_Dialect, name&gt;</code>. 然后 <code>def</code> 特定的多项式类型 <code>Polynomial</code>，继承自 <code>Poly_Type</code>.</p>
<p>在 MLIR 的 TableGen 中，<a href="https://github.com/llvm/llvm-project/blob/630ba7d705fa1d55096dbbf88c6886d64033a780/mlir/include/mlir/IR/AttrTypeBase.td#L281">TypeDef</a> 本身也是一个类，它接受模板参数，用于指定该类型所属的 dialect 和名称字段。其作用包括将生成的C++类与该 dialect 的命名空间相关联。</p>
<p>生成的 .hpp.inc 文件如下。生成的类 <code>PolynomialType</code> 就是在我们的 TableGen 文件中定义的 <code>Polynomial</code> 类型后面加上了 Type.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#ifdef GET_TYPEDEF_CLASSES
</span></span></span><span class="line"><span class="cl"><span class="cp">#undef GET_TYPEDEF_CLASSES
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">mlir</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AsmParser</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AsmPrinter</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="c1">// namespace mlir
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">namespace</span> <span class="n">mlir</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">tutorial</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">poly</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PolynomialType</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PolynomialType</span> <span class="o">:</span> <span class="k">public</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span><span class="o">::</span><span class="n">TypeBase</span><span class="o">&lt;</span><span class="n">PolynomialType</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">TypeStorage</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="n">Base</span><span class="o">::</span><span class="n">Base</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">static</span> <span class="k">constexpr</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringLiteral</span> <span class="n">name</span> <span class="o">=</span> <span class="s">&#34;poly.poly&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">static</span> <span class="k">constexpr</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringLiteral</span> <span class="n">dialectName</span> <span class="o">=</span> <span class="s">&#34;poly&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">static</span> <span class="k">constexpr</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringLiteral</span> <span class="n">getMnemonic</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">{</span><span class="s">&#34;poly&#34;</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="c1">// namespace poly
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace tutorial
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace mlir
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">MLIR_DECLARE_EXPLICIT_TYPE_ID</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#endif  </span><span class="c1">// GET_TYPEDEF_CLASSES
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>生成的 .cpp.inc 文件如下。TableGen 试图为 dialect 中的 <code>PolynomialType</code> 自动生成一个 类型解析器 (type parser) 和类型打印器 (type printer). 不过此时这些功能还不可用，构建项目时会看到一些编译警告。</p>
<p>代码中使用了 头文件保护 (header guards) 来将 <code>cpp</code> 文件分隔为两个受保护的部分。这样可以分别管理类型声明和函数实现。</p>
<p><code>GET_TYPEDEF_LIST</code> 只包含类名的逗号分隔列表。原因在于 <code>PolyDialect.cpp</code> 文件需要负责将类型注册到 dialect 中，而该注册过程通过在方言初始化函数中将这些 C++ 类名作为模板参数来实现。换句话说，<code>GET_TYPEDEF_LIST</code> 提供了一种简化机制，使得 <code>PolyDialect.cpp</code> 可以自动获取所有类名称列表，便于统一注册，而不需要手动添加每一个类型。</p>
<ul>
<li><strong><code>generatedTypeParser</code></strong> 函数是为 <code>PolynomialType</code> 定义的解析器。当解析器遇到 <code>PolynomialType</code> 的助记符（<code>poly</code>）时，会将 <code>PolynomialType</code> 类型实例化。<code>KeywordSwitch</code> 使用 <code>getMnemonic()</code> 来匹配 <code>PolynomialType</code> 的助记符（<code>poly</code>）。如果匹配成功，则调用 <code>PolynomialType::get()</code> 来获取类型实例。<code>Default</code> 子句在助记符不匹配时执行，记录未知的助记符，并返回 <code>std::nullopt</code> 表示解析失败。</li>
<li><strong><code>generatedTypePrinter</code></strong> 函数为 <code>PolynomialType</code> 提供了打印功能。当类型为 <code>PolynomialType</code> 时，打印其助记符（<code>poly</code>），否则返回失败。<code>TypeSwitch</code> 用于检查 <code>def</code> 类型是否是 <code>PolynomialType</code>。如果是，打印助记符；否则返回失败，表示该类型不属于此方言。</li>
<li><code>PolyDialect::parseType</code> 和 <code>PolyDialect::printType</code> 作为方言接口调用这两个函数，从而实现类型的解析和打印功能。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#ifdef GET_TYPEDEF_LIST
</span></span></span><span class="line"><span class="cl"><span class="cp">#undef GET_TYPEDEF_LIST
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#endif  </span><span class="c1">// GET_TYPEDEF_LIST
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#ifdef GET_TYPEDEF_CLASSES
</span></span></span><span class="line"><span class="cl"><span class="cp">#undef GET_TYPEDEF_CLASSES
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="k">static</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OptionalParseResult</span> <span class="n">generatedTypeParser</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">AsmParser</span> <span class="o">&amp;</span><span class="n">parser</span><span class="p">,</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringRef</span> <span class="o">*</span><span class="n">mnemonic</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="o">&amp;</span><span class="n">value</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">AsmParser</span><span class="o">::</span><span class="n">KeywordSwitch</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OptionalParseResult</span><span class="o">&gt;</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">.</span><span class="n">Case</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="o">::</span><span class="n">getMnemonic</span><span class="p">(),</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringRef</span><span class="p">,</span> <span class="n">llvm</span><span class="o">::</span><span class="n">SMLoc</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">value</span> <span class="o">=</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="n">parser</span><span class="p">.</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">(</span><span class="o">!!</span><span class="n">value</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">})</span>
</span></span><span class="line"><span class="cl">    <span class="p">.</span><span class="n">Default</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringRef</span> <span class="n">keyword</span><span class="p">,</span> <span class="n">llvm</span><span class="o">::</span><span class="n">SMLoc</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="o">*</span><span class="n">mnemonic</span> <span class="o">=</span> <span class="n">keyword</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">static</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">generatedTypePrinter</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">def</span><span class="p">,</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">AsmPrinter</span> <span class="o">&amp;</span><span class="n">printer</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">TypeSwitch</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span><span class="p">,</span> <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">LogicalResult</span><span class="o">&gt;</span><span class="p">(</span><span class="n">def</span><span class="p">)</span>    <span class="p">.</span><span class="n">Case</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="o">&gt;</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="k">auto</span> <span class="n">t</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">printer</span> <span class="o">&lt;&lt;</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="o">::</span><span class="n">getMnemonic</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">})</span>
</span></span><span class="line"><span class="cl">    <span class="p">.</span><span class="n">Default</span><span class="p">([](</span><span class="k">auto</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span> <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">mlir</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">tutorial</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">poly</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="c1">// namespace poly
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace tutorial
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace mlir
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">MLIR_DEFINE_EXPLICIT_TYPE_ID</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">poly</span><span class="o">::</span><span class="n">PolynomialType</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">mlir</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">tutorial</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">poly</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">/// Parse a type registered to this dialect.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">PolyDialect</span><span class="o">::</span><span class="n">parseType</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">DialectAsmParser</span> <span class="o">&amp;</span><span class="n">parser</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">SMLoc</span> <span class="n">typeLoc</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="n">getCurrentLocation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="o">::</span><span class="n">llvm</span><span class="o">::</span><span class="n">StringRef</span> <span class="n">mnemonic</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">genType</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">parseResult</span> <span class="o">=</span> <span class="n">generatedTypeParser</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">mnemonic</span><span class="p">,</span> <span class="n">genType</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">parseResult</span><span class="p">.</span><span class="n">has_value</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">genType</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">parser</span><span class="p">.</span><span class="n">emitError</span><span class="p">(</span><span class="n">typeLoc</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;unknown  type `&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="o">&lt;&lt;</span> <span class="n">mnemonic</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;` in dialect `&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">getNamespace</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;`&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="p">{};</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">/// Print a type registered to this dialect.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="n">PolyDialect</span><span class="o">::</span><span class="n">printType</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">type</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">DialectAsmPrinter</span> <span class="o">&amp;</span><span class="n">printer</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">succeeded</span><span class="p">(</span><span class="n">generatedTypePrinter</span><span class="p">(</span><span class="n">type</span><span class="p">,</span> <span class="n">printer</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="c1">// namespace poly
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace tutorial
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="c1">// namespace mlir
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#endif  </span><span class="c1">// GET_TYPEDEF_CLASSES
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>在设置 C++ 接口以使用 TableGen 文件时，通常会按照以下步骤来组织代码文件和包含关系。</p>
<ul>
<li><code>PolyTypes.h</code> 是唯一被允许包含 <code>PolyTypes.h.inc</code> 的文件。</li>
<li><code>PolyTypes.cpp.inc</code> 文件包含了 TableGen 为 <code>PolyDialect</code> 中的类型生成的实现。我们需要在 <code>PolyDialect.cpp</code> 中将其包含进去，以确保所有实现都能在该方言的主文件中使用。</li>
<li><code>PolyTypes.cpp</code> 文件应该包含 <code>PolyTypes.h</code>，以便访问类型声明，并在该文件中实现所有需要的额外功能。</li>
</ul>
<pre tabindex="0"><code class="language-plaintexxt" data-lang="plaintexxt">./Ch3-DefiningANewDialect/
├── CMakeLists.txt
├── include
│   ├── CMakeLists.txt
│   └── mlir-tutorial
│       └── Dialect
│           └── Poly
│               ├── PolyDialect.hpp
│               ├── PolyDialect.td
│               ├── PolyOps.hpp
│               ├── PolyOps.td
│               ├── PolyTypes.hpp
│               └── PolyTypes.td
├── lib
│   ├── CMakeLists.txt
│   └── Dialect
│       └── Poly
│           └── PolyDialect.cpp
</code></pre><p>为了让类型解析器和打印器能够正确编译和运行，需要最后在方言的 TableGen 文件中添加 <code>let useDefaultTypePrinterParser = 1</code>;，这个指令告诉 TableGen 使用默认的类型解析和打印器。当这个选项启用后，TableGen 会生成相应的解析和打印代码，并将这些实现作为 <code>PolyDialect</code> 类的成员函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">/// Parse a type registered to this dialect.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">parseType</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">DialectAsmParser</span> <span class="o">&amp;</span><span class="n">parser</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">/// Print a type registered to this dialect.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">void</span> <span class="nf">printType</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">type</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">DialectAsmPrinter</span> <span class="o">&amp;</span><span class="n">os</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以写一个 .mlir 来测试属性是是否获取正确。在 MLIR 中自定义的 dialect 前都需要加上 <code>!</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">    <span class="c">// CHECK-LABEL: test_type_syntax
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@test_type_syntax</span><span class="p">(</span><span class="nv">%arg0</span><span class="p">:</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c">// CHECK: poly.poly
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="kt">return</span> <span class="nv">%arg0</span><span class="p">:</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="add-a-poly-type-parameter">Add a Poly Type Parameter</h1>
<p>我们需要为多项式类型添加一个属性，表示它的次数上限。</p>
<pre tabindex="0"><code>// include/mlir-tutorial/Dialect/Poly/PolyTypes.td
let parameters = (ins &#34;int&#34;:$degreeBound);
let assemblyFormat = &#34;`&lt;` $degreeBound `&gt;`&#34;;
</code></pre><p>第一行定义了类型的一个参数 <code>degreeBound</code>，类型为 <code>int</code>. 表示在实例化该类型时，用户可以指定一个整数值作为类型的参数。<code>parameters</code> 中的 (<code>ins &quot;int&quot;:$degreeBound</code>) 指定了输入参数的类型和名称，其中 int 是数据类型，<code>$degreeBound</code> 是参数的占位符。<code>assemblyFormat</code> 用于定义该类型在 MLIR 文本格式中的打印和解析格式。<code>&quot;&lt;&quot; $degreeBound &quot;&gt;&quot;</code> 表示该类型的参数会用尖括号包裹。第二行是必需的，因为现在一个 Poly 类型有了这个关联的数据，我们需要能够将它打印出来并从文本 IR 表示中解析它。</p>
<p>加上这两行代码后进行 build 会发现多了一些新的内容。</p>
<ul>
<li><code>PolynomialType</code> 有一个新的 <code>int getDegreeBound()</code> 方法，以及一个静态 <code>get</code> 工厂方法。</li>
<li><code>parse</code> 和 <code>print</code> 升级为新格式。</li>
<li>有一个名为 <code>typestorage</code> 的新类，它包含 int 形参，并隐藏在内部细节名称空间中。</li>
</ul>
<p>MLIR会自动生成简单类型的 storage 类，因为它们不需要复杂的内存管理。如果参数更复杂，就需要开发者手动编写 storage 类来定义构造、析构和其他语义。复杂的 storage 类需要实现更多细节，以确保类型能够在 MLIR 的 dialect 系统中顺利运行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// include/mlir-learning/Dialect/Poly/PolyTypes.hpp.inc
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">static</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">parse</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">AsmParser</span> <span class="o">&amp;</span><span class="n">odsParser</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="kt">void</span> <span class="nf">print</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">AsmPrinter</span> <span class="o">&amp;</span><span class="n">odsPrinter</span><span class="p">)</span> <span class="k">const</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="nf">getDegreeBound</span><span class="p">()</span> <span class="k">const</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// include/mlir-learning/Dialect/Poly/PolyTypes.cpp.inc
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">PolynomialTypeStorage</span> <span class="o">:</span> <span class="k">public</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">TypeStorage</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="cm">/* lots of code */</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">PolynomialType</span> <span class="n">PolynomialType</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">,</span> <span class="kt">int</span> <span class="n">degreeBound</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">Base</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">degreeBound</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Type</span> <span class="n">PolynomialType</span><span class="o">::</span><span class="n">parse</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">AsmParser</span> <span class="o">&amp;</span><span class="n">odsParser</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="cm">/* code to parse the type */</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">PolynomialType</span><span class="o">::</span><span class="n">print</span><span class="p">(</span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">AsmPrinter</span> <span class="o">&amp;</span><span class="n">odsPrinter</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Builder</span> <span class="n">odsBuilder</span><span class="p">(</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">odsPrinter</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;&lt;&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">odsPrinter</span><span class="p">.</span><span class="n">printStrippedAttrOrType</span><span class="p">(</span><span class="n">getDegreeBound</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="n">odsPrinter</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;&gt;&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">PolynomialType</span><span class="o">::</span><span class="n">getDegreeBound</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="nf">getImpl</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">degreeBound</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="adding-some-simple-operations">Adding Some Simple Operations</h1>
<p>下面我们定义一个简单的多项式加法操作</p>
<pre tabindex="0"><code>// include/mlir-tutorial/Dialect/Poly/PolyOps.td
include &#34;PolyDialect.td&#34;
include &#34;PolyTypes.td&#34;

def Poly_AddOp : Op&lt;Poly_Dialect, &#34;add&#34;&gt; {
  let summary = &#34;Addition operation between polynomials.&#34;;
  let arguments = (ins Polynomial:$lhs, Polynomial:$rhs);
  let results = (outs Polynomial:$output);
  let assemblyFormat = &#34;$lhs `,` $rhs attr-dict `:` `(` type($lhs) `,` type($rhs) `)` `-&gt;` type($output)&#34;;
}
</code></pre><p>和刚才定义 types 非常相近，但基类是 Op，arguments 对应于操作的输入，assemblyFormat 更复杂。生成的 .hpp.inc 和 .cpp.inc 非常复杂。我们可以编写一个 .mlir 来测试。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">  <span class="c">// CHECK-LABEL: test_add_syntax
</span></span></span><span class="line"><span class="cl"><span class="c"></span>  <span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@test_add_syntax</span><span class="p">(</span><span class="nv">%arg0</span><span class="p">:</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;,</span> <span class="nv">%arg1</span><span class="p">:</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c">// CHECK: poly.add
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">%0</span> <span class="p">=</span> poly<span class="p">.</span>add <span class="nv">%arg0</span><span class="p">,</span> <span class="nv">%arg1</span> <span class="p">:</span> <span class="p">(!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;,</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">return</span> <span class="nv">%0</span> <span class="p">:</span> <span class="p">!</span>poly<span class="p">.</span>poly<span class="p">&lt;</span><span class="m">10</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>生成的代码定义了以下几个方面：</p>
<ol>
<li>
<p>Adaptor Classes:</p>
<ul>
<li>AddOpGenericAdaptorBase 和 AddOpAdaptor: 提供了便捷的方式来访问操作的操作数 (operands) 和属性 (attributes)。它们在编写转换和重写模式时特别有用。</li>
</ul>
</li>
<li>
<p>Properties Handling:</p>
<ul>
<li>诸如 setPropertiesFromAttr , getPropertiesAsAttr , computePropertiesHash 等函数是 MLIR 操作属性系统的接口。虽然在这个特定的 AddOp 实现中，有些函数可能是空实现或返回默认值，但它们是操作定义结构的一部分。</li>
</ul>
</li>
<li>
<p>Builder Methods:</p>
<ul>
<li>多个重载的 AddOp::build 静态方法。这些方法用于在代码中以编程方式创建 AddOp 的实例。</li>
</ul>
</li>
<li>
<p>Verification:</p>
<ul>
<li>AddOp::verifyInvariantsImpl() 和 AddOp::verifyInvariants() : 这些方法用于检查一个 AddOp 实例是否符合其定义。例如，它们会验证操作数的数量和类型是否正确，结果类型是否符合预期。代码中调用了像 __mlir_ods_local_type_constraint_PolyOps2 这样的辅助函数来进行类型约束检查。</li>
</ul>
</li>
<li>
<p>Assembly Format Parsing and Printing:</p>
<ul>
<li>AddOp::parse(::mlir::OpAsmParser&amp; parser, ::mlir::OperationState&amp; result) : 这个方法定义了如何从 MLIR 的文本汇编格式中解析出 AddOp 。当 MLIR 工具读取 .mlir 文件时，会调用此方法。</li>
<li>AddOp::print(::mlir::OpAsmPrinter&amp; _odsPrinter) : 这个方法定义了如何将 AddOp 实例打印成 MLIR 的文本汇编格式。</li>
</ul>
</li>
<li>
<p>Type ID Definition:</p>
<ul>
<li>MLIR_DEFINE_EXPLICIT_TYPE_ID(::mlir::tutorial::poly::AddOp) : 这个宏用于 MLIR 的运行时类型信息 (RTTI) 系统，为 AddOp 类型生成一个唯一的标识符。</li>
</ul>
</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>MLIR-Ch3 Using Tablegen for Passes</title>
      <link>http://localhost:57770/blogs/courselearning/mlir/mlir-ch3-using-tablegen-for-passes/</link>
      <pubDate>Wed, 06 Nov 2024 09:37:32 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/mlir/mlir-ch3-using-tablegen-for-passes/</guid>
      <description>Personal MLIR learning notes 3.</description>
      <content:encoded><![CDATA[<h1 id="what-is-tablegen">What is Tablegen?</h1>
<p>TableGen 是一个用于生成代码和描述结构的 DSL 和工具，最初由 LLVM 开发，后来被 MLIR 继承并扩展。它主要用于以声明式的方式定义和生成 MLIR 的各种组件，例如 Dialects、Operations、Attributes、Types 和 Passes，从而减少手动编写重复性 C++ 代码的工作量。</p>
<p>mlir-tablegen 并没有清楚地告诉你哪些函数没有实现，也没有解释必须编写的函数。确定缺失内容的主要方法是尝试用一些使用它的代码来构建生成的代码，然后筛选数百行 c++ 编译器错误，这反过来又需要了解生成代码中的各种模板操作。生成的代码将使用必须知道的符号，以便在正确的位置导入或提前声明，并且它要求管理生成的代码所在的名称空间。</p>
<h1 id="tablegen-files-and-the-mlir-tblgen-binary">Tablegen Files and the mlir-tblgen Binary</h1>
<p>TableGen 允许你定义变量，并且这些变量可以在多个定义中重复使用。</p>
<p>TableGen允许你在定义中嵌入C++代码片段。这些代码片段会被插入到TableGen生成的C++类中，并且这些C++代码片段可以访问前面定义的变量。这使得TableGen能够生成高度定制化的C++代码。如果需要为你的 pass 编写特殊的构造函数，就可以在 <code>PassBase.td</code>中用 TableGen 的语法写下相应的 C++ 代码。</p>
<p>下面给出了一个以 tablegen 语法重写上一章的 <code>AffineFullUnroll </code>pass 的例子</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// mlir-learning/Transform/Affine/Pass.td
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">include</span> <span class="s">&#34;mlir/Pass/PassBase.td&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">def</span> <span class="nl">AffineFullUnroll</span> <span class="p">:</span> <span class="n">Pass</span><span class="o">&lt;</span><span class="s">&#34;affine-full-unroll&#34;</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">let</span> <span class="n">summary</span> <span class="o">=</span> <span class="s">&#34;Fully unroll all affine loops&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">let</span> <span class="n">description</span> <span class="o">=</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Fully</span> <span class="n">unroll</span> <span class="n">all</span> <span class="n">affine</span> <span class="n">loops</span><span class="p">.</span> <span class="p">(</span><span class="n">could</span> <span class="n">add</span> <span class="n">more</span> <span class="n">docs</span> <span class="n">here</span> <span class="n">like</span> <span class="n">code</span> <span class="n">examples</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">}];</span>
</span></span><span class="line"><span class="cl">  <span class="n">let</span> <span class="n">dependentDialects</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#34;mlir::affine::AffineDialect&#34;</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>TableGen 拥有类似的类和继承的概念。<code>: Pass&lt;...&gt;</code> 表示一个类继承自 <a href="https://github.com/llvm/llvm-project/blob/1b74459df8a6d960f7387f0c8379047e42811f58/mlir/include/mlir/Pass/PassBase.td#L95">PassBase.td</a> 文件中定义的 <code>Pass</code> 基类</p>
<p><code>def</code> 用于定义一个具体实例，它会生成对应的 C++ 代码。 也就是说，使用 <code>def</code> 定义的类实例会被 TableGen 处理，最终转换成实际的代码，而仅仅使用 <code>class</code> 定义的类则不会直接生成代码，只作为模板或基类存在。</p>
<p>上面代码说明 TableGen 允许定义字符串变量和列表。 TableGen 还有一个重要功能：它允许定义变量并在多个定义中复用这些变量，还可以定义 C++ 代码片段，并将这些片段插入到生成的类中。 这些 C++ 代码片段可以使用前面定义的变量。例如 <a href="https://github.com/llvm/llvm-project/blob/1b74459df8a6d960f7387f0c8379047e42811f58/mlir/include/mlir/Pass/PassBase.td#L82">PassBase.td</a> 类定义了一个代码构造函数变量。 如果需要为你的 Pass 类编写特殊的构造函数，可以在 PassBase.td 中编写相应的 C++ 代码。 这意味着 TableGen 不仅仅是简单的文本替换，它能够处理更复杂的代码生成逻辑，包括变量的跨定义使用和 C++ 代码的嵌入。</p>
<p>和上一章不同的是，这次我们也需要在 include 目录下写一个 CMakeLists.txt</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cmake" data-lang="cmake"><span class="line"><span class="cl"><span class="nb">set</span><span class="p">(</span><span class="s">TARGET_NAME</span> <span class="s2">&#34;${PROJECT_TARGET_PREFIX}-Transform-Affine-Passes-IncGen&#34;</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">set</span><span class="p">(</span><span class="s">LLVM_TARGET_DEFINITIONS</span> <span class="s">mlir-learning/Transform/Affine/Pass.td</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">mlir_tablegen</span><span class="p">(</span><span class="s">mlir-learning/Transform/Affine/Pass.h.inc</span> <span class="s">-gen-pass-decls</span> <span class="s">-name=Affine</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">mlir_tablegen</span><span class="p">(</span><span class="s">mlir-learning/Transform/Affine/Pass.md</span> <span class="s">-gen-pass-doc</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">add_public_tablegen_target</span><span class="p">(</span><span class="o">${</span><span class="nv">TARGET_NAME</span><span class="o">}</span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="nb">set</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s">ALL_TABLEGEN_TARGETS</span>
</span></span><span class="line"><span class="cl">    <span class="o">${</span><span class="nv">PROJECT_TARGET_PREFIX</span><span class="o">}</span><span class="s">-Transform-Affine-Passes-IncGen</span>
</span></span><span class="line"><span class="cl">    <span class="c">#${PROJECT_TARGET_PREFIX}-Transform-Arith-Passes-IncGen
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="p">)</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Add the generated files to a global property, so they can be used in the library
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="nb">set_property</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s">GLOBAL</span> <span class="s">PROPERTY</span> <span class="o">${</span><span class="nv">PROJECT_TARGET_PREFIX</span><span class="o">}</span><span class="s">-TABLEGEN-TARGETS</span>
</span></span><span class="line"><span class="cl">    <span class="o">${</span><span class="nv">ALL_TABLEGEN_TARGETS</span><span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span><span class="err">
</span></span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>set(LLVM_TARGET_DEFINITIONS mlir-learning/Transform/Affine/Pass.td)</code>: 这行代码设置了 TableGen 的输入文件。</li>
<li><code>mlir_tablegen(mlir-learning/Transform/Affine/Pass.h.inc -gen-pass-decls -name=Affine)</code>: 这行调用了 <code>mlir_tablegen</code> 命令，它将 Pass.td 文件作为输入，生成一个名为 Pass.h.inc 的头文件，其中包含 Pass 的声明 (<code>-gen-pass-decls</code>)，并且命名空间为 Affine (<code>-name=Affine</code>).</li>
<li><code>mlir_tablegen(mlir-learning/Transform/Affine/Pass.md -gen-pass-doc)</code>: 这行同样调用 mlir_tablegen，生成一个名为 Pass.md 的文件，包含 Pass 的文档信息 (<code>-gen-pass-doc</code>).</li>
<li><code>add_public_tablegen_target(${TARGET_NAME})</code>: 这行代码将 TableGen 生成的目标添加到 CMake 项目中，使其成为一个公共目标，其他部分可以依赖它。</li>
<li><code>set(ALL_TABLEGEN_TARGETS ...)</code>: 这行代码定义了一个列表 <code>ALL_TABLEGEN_TARGETS</code>，包含所有 TableGen 生成的目标。</li>
<li><code>set_property(GLOBAL PROPERTY ...)</code>: 这行代码将所有 TableGen 生成的目标添加到全局属性 <code>${PROJECT_TARGET_PREFIX}-TABLEGEN-TARGETS}</code> 中。 使得构建系统能够跟踪和管理所有由 TableGen 生成的文件，确保它们被正确地包含在库或可执行文件中。</li>
</ul>
<h1 id="inc-files">.inc Files</h1>
<p>我们同样创建和上一章相同的文件 (可以先不写)，需要注意的是由于 TableGen 生成的 .inc 文件位于构建目录下，在 lib 的 CMakeLists.txt 中我们需要在 <code>target_include_directories</code> 命令中加入 <code>${CMAKE_OUTPUT_DIR}/include</code></p>
<p>下面我们来逐段看生成的 .inc 文件</p>
<ol>
<li>头部保护和条件编译</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="c1">//===----------------------------------------------------------------------===//
</span></span></span><span class="line"><span class="cl"><span class="c1">// AffineFullUnroll
</span></span></span><span class="line"><span class="cl"><span class="c1">//===----------------------------------------------------------------------===//
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#ifdef GEN_PASS_DECL_AFFINEFULLUNROLL
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Pass</span><span class="o">&gt;</span> <span class="nf">createAffineFullUnroll</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="cp">#undef GEN_PASS_DECL_AFFINEFULLUNROLL
</span></span></span><span class="line"><span class="cl"><span class="cp">#endif </span><span class="c1">// GEN_PASS_DECL_AFFINEFULLUNROLL
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这部分代码使用了预处理宏 <code>GEN_PASS_DECL_AFFINEFULLUNROLL</code>。  如果这个宏被定义，则编译器会生成 <code>createAffineFullUnroll()</code> 函数的声明。</p>
<ol start="2">
<li>Pass 的实现</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#ifdef GEN_PASS_DEF_AFFINEFULLUNROLL
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">namespace</span> <span class="n">impl</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Pass</span><span class="o">&gt;</span> <span class="nf">createAffineFullUnroll</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="c1">// namespace impl
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">namespace</span> <span class="n">impl</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">template</span> <span class="o">&lt;</span><span class="kr">typename</span> <span class="n">DerivedT</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl">  <span class="n">class</span> <span class="nl">AffineFullUnrollBase</span> <span class="p">:</span> <span class="n">public</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">OperationPass</span><span class="o">&lt;&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ... (Pass 的方法定义) ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="c1">// namespace impl
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Pass</span><span class="o">&gt;</span> <span class="nf">createAffineFullUnroll</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">impl</span><span class="o">::</span><span class="nf">createAffineFullUnroll</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#undef GEN_PASS_DEF_AFFINEFULLUNROLL
</span></span></span><span class="line"><span class="cl"><span class="cp">#endif </span><span class="c1">// GEN_PASS_DEF_AFFINEFULLUNROLL
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>这部分是 Pass 的主要实现。它使用了 <code>GEN_PASS_DEF_AFFINEFULLUNROLL</code> 宏来控制编译。如果该宏被定义，则编译器会编译 AffineFullUnrollBase 类以及 <code>createAffineFullUnroll</code> 函数。</p>
<ul>
<li><code>AffineFullUnrollBase</code> 是一个基类模板，使用 CRTP (Curiously Recurring Template Pattern) 技术，允许派生类通过 DerivedT 获取自身的类型信息。 这是一种常见的 C++ 设计模式，用于实现静态多态。它定义了 Pass 的基本信息，例如名称、描述、命令行参数、依赖的 Dialect (这里是 <code>mlir::affine::AffineDialect</code>).</li>
<li><code>createAffineFullUnroll</code> 函数负责创建 <code>AffineFullUnroll</code> Pass 的实例。 它使用了 <code>impl</code> 命名空间，这是一种常见的 C++ 代码组织方式，用于隐藏实现细节。</li>
</ul>
<ol start="3">
<li>Pass 注册</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#ifdef GEN_PASS_REGISTRATION
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="c1">//===----------------------------------------------------------------------===//
</span></span></span><span class="line"><span class="cl"><span class="c1">// AffineFullUnroll Registration
</span></span></span><span class="line"><span class="cl"><span class="c1">//===----------------------------------------------------------------------===//
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="kr">inline</span> <span class="kt">void</span> <span class="nf">registerAffineFullUnroll</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="nf">registerPass</span><span class="p">([]()</span> <span class="o">-&gt;</span> <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Pass</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="nf">createAffineFullUnroll</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Old registration code, kept for temporary backwards compatibility.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kr">inline</span> <span class="kt">void</span> <span class="nf">registerAffineFullUnrollPass</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="nf">registerPass</span><span class="p">([]()</span> <span class="o">-&gt;</span> <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;::</span><span class="n">mlir</span><span class="o">::</span><span class="n">Pass</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="nf">createAffineFullUnroll</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">//===----------------------------------------------------------------------===//
</span></span></span><span class="line"><span class="cl"><span class="c1">// Affine Registration
</span></span></span><span class="line"><span class="cl"><span class="c1">//===----------------------------------------------------------------------===//
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="kr">inline</span> <span class="kt">void</span> <span class="nf">registerAffinePasses</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="nf">registerAffineFullUnroll</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="cp">#undef GEN_PASS_REGISTRATION
</span></span></span><span class="line"><span class="cl"><span class="cp">#endif </span><span class="c1">// GEN_PASS_REGISTRATION
</span></span></span></code></pre></td></tr></table>
</div>
</div><h1 id="complete-hpp--cpp">Complete .hpp &amp; .cpp</h1>
<p>TableGen根据 <code>.td</code>文件生成Pass的代码，生成的代码包含注册函数，这些注册函数最终会被调用，将Pass注册到MLIR系统中。 我们可以通过写一个 <code>Passes.h</code>文件集中管理所有Pass的注册，简化构建过程。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="c1">// include/mlir-learning/Transform/Affine/Pass.h
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Transform/Affine/AffineFullUnroll.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="n">namespace</span> <span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="cp">#define GEN_PASS_REGISTRION
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Transform/Affine/Pass.h.inc&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后再对应的 AffineFullUnroll.hpp 中定义 <code>GEN_PASS_DECL_AFFINEFULLUNROLL</code> 宏，以实现创建 Pass 函数的声明。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#pragma once 
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Pass/Pass.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="cp">#define GEN_PASS_DECL_AFFINEFULLUNROLL
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Transform/Affine/Pass.h.inc&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="p">}</span>  <span class="c1">// namespace mlir::tutorial
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>同样在 cpp 中需要定义 <code>GEN_PASS_DEF_AFFINEFULLUNROLL</code> 宏，然后写你对应的实现 (与上一章相同). 问题是仅仅查看生成的代码并不能直接看出还需要实现哪些函数，需要通过其他方法来确定。</p>
<ul>
<li><strong>编译并查看编译器错误信息:</strong>  最直接的方法是尝试编译代码。编译器会指出哪些函数没有实现，从而告诉你需要实现哪些函数。</li>
<li><strong>与基类进行比较:</strong>  可以将生成的代码与基类（<code>OperationPass</code>和 <code>Pass</code>）进行比较。通过比较，可以发现唯一需要实现的函数是 <code>runOnOperation()</code>。  这需要你熟悉MLIR Pass的继承结构和各个函数的作用。</li>
<li><strong>观察缺失的函数:</strong>  如果之前已经从原始API手动实现过类似的Pass，可以观察生成的代码中哪些函数已经存在（例如 <code>getArgument</code>），哪些函数缺失（例如 <code>runOnOperation</code>）。 通过对比，可以确定还需要实现哪些函数。</li>
</ul>
<p>具体的实现与上一章相同，这里我们要继承 .inc 文件中生成的类</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Transform/Affine/AffineFullUnroll.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Dialect/Affine/IR/AffineOps.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Dialect/Affine/LoopUtils.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Pass/Pass.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="k">namespace</span> <span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#define GEN_PASS_DEF_AFFINEFULLUNROLL
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Transform/Affine/Pass.h.inc&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">mlir</span><span class="o">::</span><span class="n">affine</span><span class="o">::</span><span class="n">AffineForOp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">using</span> <span class="n">mlir</span><span class="o">::</span><span class="n">affine</span><span class="o">::</span><span class="n">loopUnrollFull</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AffineFullUnroll</span> <span class="o">:</span> <span class="k">public</span> <span class="n">impl</span><span class="o">::</span><span class="n">AffineFullUnrollBase</span><span class="o">&lt;</span><span class="n">AffineFullUnroll</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">using</span> <span class="n">AffineFullUnrollBase</span><span class="o">::</span><span class="n">AffineFullUnrollBase</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">runOnOperation</span><span class="p">()</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">getOperation</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">AffineForOp</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">failed</span><span class="p">(</span><span class="n">loopUnrollFull</span><span class="p">(</span><span class="n">op</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="n">op</span><span class="p">.</span><span class="n">emitError</span><span class="p">(</span><span class="s">&#34;unrolling failed&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                <span class="n">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">});</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>  <span class="c1">// namespace mlir::tutorial
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>最后在 <code>tutorial.cpp</code> 中使用 .inc 文件生成的 <code>registerAffinePasses</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/IR/DialectRegistry.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/InitAllDialects.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Pass/PassManager.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Pass/PassRegistry.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Tools/mlir-opt/MlirOptMain.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Transform/Affine/Pass.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Register all built-in MLIR dialects
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mlir</span><span class="o">::</span><span class="n">DialectRegistry</span> <span class="n">registry</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">registerAllDialects</span><span class="p">(</span><span class="n">registry</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">registerAffinePasses</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">asMainReturnCode</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">mlir</span><span class="o">::</span><span class="n">MlirOptMain</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">,</span> <span class="s">&#34;Tutorial Pass Driver&#34;</span><span class="p">,</span> <span class="n">registry</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>MLIR-Ch2 Writing Our First Pass</title>
      <link>http://localhost:57770/blogs/courselearning/mlir/mlir-ch2-writing-our-first-pass/</link>
      <pubDate>Wed, 30 Oct 2024 11:42:34 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/mlir/mlir-ch2-writing-our-first-pass/</guid>
      <description>Personal MLIR learning notes 2.</description>
      <content:encoded><![CDATA[<h1 id="tutorial-opt-and-project-organization">Tutorial-opt and Project Organization</h1>
<p>编译器可能将 mlir-opt 作为子例程在前端 (c++ -&gt; 某些MLIR方言) 和后端 (MLIR 的 LLVM 方言 -&gt; LLVM -&gt; 机器码) 之间运行。
(我将它命名为 tutorial-opt).</p>
<p>典型的 MLIR 代码库将代码分成具有大致相同层次结构的目录：</p>
<ul>
<li><code>include/</code> 目录用于存放头文件和tablegen 文件，</li>
<li><code>lib/</code> 目录用于存放实现代码。可能会有 <code>Transform/</code> 子目录用于存储在方言中转换代码的 pass，<code>Conversion/</code> 子目录用于在方言之间转换的 pass ，<code>Analysis/</code> 子目录用于分析 pass，等等。这些目录中的每一个都可能有它们所操作的特定方言的子目录。</li>
<li><code>test/</code> 用于存放需要测试的 mlir 文件。</li>
<li><code>tools/</code> 存放用于注册 pass 的主文件</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl"> ./Ch1-WritingOurFirstPass/
</span></span><span class="line"><span class="cl">├── CMakeLists.txt
</span></span><span class="line"><span class="cl">├── include
</span></span><span class="line"><span class="cl">│   └── mlir-tutorial
</span></span><span class="line"><span class="cl">├── lib
</span></span><span class="line"><span class="cl">│   ├── CMakeLists.txt
</span></span><span class="line"><span class="cl">│   └── Transform
</span></span><span class="line"><span class="cl">├── tests
</span></span><span class="line"><span class="cl">│   ├── Output
</span></span><span class="line"><span class="cl">│   ├── affine_loop_unroll.mlir
</span></span><span class="line"><span class="cl">│   ├── lit.cfg.py
</span></span><span class="line"><span class="cl">│   └── mul_to_add.mlir
</span></span><span class="line"><span class="cl">└── tools
</span></span><span class="line"><span class="cl">    ├── CMakeLists.txt
</span></span><span class="line"><span class="cl">    └── tutorial-opt.cpp
</span></span></code></pre></td></tr></table>
</div>
</div><p>尽管 MLIR 提供了许多定义循环和控制流的机制，最高级的是 affine dialect. 它被设计用来进行多面体循环分析 (polyhedral loop analysis).</p>
<details class="custom-details">
    <summary class="custom-summary">Polyhedral Loop Analysis</summary>
    <div><p>多面体循环分析的核心思想是将程序中的循环和数组访问抽象为数学形式，使得可以应用几何变换来优化代码。这种数学形式通常表示为 <strong>整数线性不等式的集合</strong> ，这些不等式定义了循环迭代空间和数组访问的范围。</p>
<ol>
<li><strong>迭代空间（Iteration Space）</strong> ：程序中的循环嵌套可以被表示为一个多维的迭代空间。例如，对于一个双层嵌套循环：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里的迭代空间是二维的，由 <code>(i, j)</code> 构成。</p>
<ol start="2">
<li><strong>访问关系（Access Relations）</strong> ：每个数组的访问模式（例如 <code>A[i][j]</code>）也可以被表示为几何关系。这种关系定义了哪些迭代变量访问哪些数组元素。</li>
<li><strong>多面体表示（Polyhedral Representation）</strong> ：在多面体循环分析中，循环的迭代空间和数组访问模式可以用整数线性不等式来表示，从而形成一个多面体。例如，<code>0&lt;=i&lt;N</code> 和 <code>0&lt;=j&lt;M</code> 是两个简单的线性不等式，它们表示循环的边界。</li>
</ol>
</div>
</details><br>
<p>一个简单的对数组求和的函数如下: <code>affine.for</code> 定义一个循环，迭代变量为 <code>%i</code>，范围 <code>[0,4)</code>，即循环 4 次。
<code>iter_args(%sum_iter = %sum_0)</code> 表示循环维护一个迭代变量 <code>%sum_iter</code>，初始值为 <code>%sum_0</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@sum_buffer</span><span class="p">(</span><span class="nv">%buffer</span><span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%sum_0</span> <span class="p">=</span> arigh<span class="p">.</span><span class="kt">constant</span> <span class="m">0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%sum</span> <span class="p">=</span> affine<span class="p">.</span>for <span class="nv">%i</span> <span class="p">=</span> <span class="m">0</span> to <span class="m">4</span> iter_args<span class="p">(</span><span class="nv">%sum_iter</span> <span class="p">=</span> <span class="nv">%sum_0</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="p">(</span><span class="k">i32</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nv">%t</span> <span class="p">=</span> affine<span class="p">.</span>load <span class="nv">%buffer</span><span class="p">[</span><span class="nv">%i</span><span class="p">]</span> <span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">        <span class="nv">%sum_next</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%sum_iter</span><span class="p">,</span> <span class="nv">%t</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">        affine<span class="p">.</span>yield <span class="nv">%sum_next</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="kt">return</span> <span class="nv">%sum</span><span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://mlir.llvm.org/docs/LangRef/#high-level-structure">MLIR 高级结构</a> 基于图数据结构，其节点称为 Operations，边称为 Values。每个 Value 都是一个 Operation 或 Block Argument 的结果，并具有由类型系统定义的 Value Type。Operations 包含在 Blocks 中，Blocks 包含在 Regions 中。Operations 在其所在的 Block 中是有序的，Blocks 在其所在的 Region 中也是有序的，尽管这种顺序在特定类型的 Region 中可能具有或不具有语义意义。Operations 还可以包含 Regions，从而能够表示层次化的结构。</p>
<p>Operations 可以表示多种不同的概念，从高级概念如函数定义、函数调用、缓冲区分配、缓冲区的视图或切片、进程创建，到低级概念如目标无关的算术运算、目标特定的指令、配置寄存器和逻辑门。这些不同的概念在 MLIR 中由不同的 Operations 表示，并且 MLIR 中可用的 Operations 集可以任意扩展。</p>
<p>MLIR 还提供了一个可扩展的框架，用于对 Operations 进行转换，使用熟悉的编译器 Passes 概念。在任意 Operations 集上启用任意 Passes 集会带来显著的扩展性挑战，因为每个转换可能需要考虑任何 Operation 的语义。MLIR 通过允许使用 Traits 和 Interfaces 抽象地描述 Operation 的语义来解决这种复杂性，从而使转换能够更通用地操作 Operations。Traits 通常描述对有效 IR 的验证约束，能够捕获和检查复杂的不变性。（参见 Op vs Operation）</p>
<p>MLIR 的表示基于 SSA 的 IR，例如 LLVM core IR，通过适当选择 Operation 类型来定义 Modules、Functions、Branches、Memory Allocation，以及验证约束以确保 SSA Dominance 属性。MLIR 包含一组 Dialects，定义了此类结构。</p>
<h1 id="affine-full-unroll-pass">Affine Full Unroll Pass</h1>
<p>MLIR 提供了一个方法 <a href="https://github.com/llvm/llvm-project/blob/dea01f5e00e45dec4319475a001024c6ee882283/mlir/include/mlir/Dialect/Affine/LoopUtils.h#L46">loopUnrollFull</a> 来进行循环展开，因此我们的 pass 将是对这个函数调用的一个包装，直接调用 C++ API 实现。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// include/mlir-learning/Transform/Affine/AffineFullUnroll.h
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">class</span>  <span class="nl">AffineFullUnrollPass</span> 
</span></span><span class="line"><span class="cl">    <span class="p">:</span> <span class="n">public</span> <span class="n">PassWrapper</span><span class="o">&lt;</span><span class="n">AffineFullUnrollPass</span><span class="p">,</span> <span class="n">OperationPass</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">FuncOp</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nl">private</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">runOnOperation</span><span class="p">()</span> <span class="n">override</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">StringRef</span> <span class="nf">getArgument</span><span class="p">()</span> <span class="k">const</span> <span class="n">final</span> <span class="p">{</span><span class="k">return</span> <span class="s">&#34;affine-full-unroll&#34;</span><span class="p">;}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">StringRef</span> <span class="nf">getDescription</span><span class="p">()</span> <span class="k">const</span> <span class="n">final</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="s">&#34;Perform full unrolling of all affine.for loops&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// lib/Transform/Affine/AffineFullUnroll.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">using</span> <span class="n">mlir</span><span class="o">::</span><span class="n">affine</span><span class="o">::</span><span class="n">AffineForOp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">using</span> <span class="n">mlir</span><span class="o">::</span><span class="n">affine</span><span class="o">::</span><span class="n">loopUnrollFull</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">AffineFullUnrollPass</span><span class="o">::</span><span class="nf">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nf">getOperation</span><span class="p">().</span><span class="nf">walk</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">AffineForOp</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="nf">failed</span><span class="p">(</span><span class="nf">loopUnrollFull</span><span class="p">(</span><span class="n">op</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="n">op</span><span class="p">.</span><span class="nf">emitError</span><span class="p">(</span><span class="s">&#34;unrolling failed&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                <span class="nf">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该类的定义使用了奇异递归模板模式 (Curiously Recurring Template Pattern, CRTP). <a href="https://github.com/llvm/llvm-project/blob/9ab34689b08ee66f0868d38456fccc8b80d1b656/mlir/include/mlir/Pass/Pass.h#L441">PassWrapper</a> 是 MLIR 框架中的一个模板类，为定义的 Pass 提供通用功能 (如类型检查、名称获取、克隆)。开发者只需专注于 Pass 的核心逻辑（如 runOnOperation），而无需手动实现类型标识、克隆等辅助功能。</p>
<ul>
<li><code>runOnOperation</code> 中调用了 <code>getOperation</code> 方法，它是 MLIR 中 <code>Pass</code> 类提供的一个方法，返回当前操 <code>Operation</code>.  <code>walk</code> 方法是 MLIR 提供的一个遍历方法，用来遍历操作树中的每个节点。它会递归地遍历操作树中的所有子操作，并对每个操作应用传入的回调函数 (lambda func). 当运行这个 Pass 时，它会在每一个 <code>AffineForOp</code> 类型的操作上执行 <code>runOnOperation</code> 函数。</li>
<li><code>getArgument</code> 方法返回 Pass 的命令行参数。这个返回值 <code>affine-full-unroll</code> 表示这个 Pass 的名称，可以在运行时通过命令行参数指定是否启用该 Pass.</li>
<li><code>getDescription</code> 方法会在调用像 <code>mlir-opt</code> 这样的工具时若有 <code>--help</code> 参数则返回 Pass 的描述信息。</li>
</ul>
<details class="custom-details">
    <summary class="custom-summary">Callback Function</summary>
    <div><p>回调函数 (Callback Function) 是一种通过将函数作为参数传递给另一个函数，来实现某些特定操作的机制。回调函数通常在某个事件发生或某个特定条件满足时被调用。简而言之，回调函数就是<strong>被调用的函数</strong>，它会在特定的时机被执行。</p>
<p>在这个例子中，<code>invokeCallback</code> 函数接收到 <code>printMessage</code> 函数的地址，并在 <code>main</code> 函数中调用它。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// 回调函数的定义
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">printMessage</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Hello, World!&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// 接受回调函数作为参数的函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">invokeCallback</span><span class="p">(</span><span class="kt">void</span> <span class="p">(</span><span class="o">*</span><span class="n">callback</span><span class="p">)())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 调用回调函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nf">callback</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 将回调函数传递给另一个函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nf">invokeCallback</span><span class="p">(</span><span class="n">printMessage</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在现代 C++ 中，回调函数通常通过 lambda 表达式传递。下面的例子中 <code>invokeCallback</code> 函数接受一个 <code>std::function&lt;void()&gt;</code> 类型的回调函数参数。在 <code>main</code> 函数中，传入了一个 Lambda 表达式作为回调函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">invokeCallback</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="kt">void</span><span class="p">()</span><span class="o">&gt;</span> <span class="n">callback</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nf">callback</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 使用 Lambda 表达式作为回调函数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="nf">invokeCallback</span><span class="p">([](){</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Hello from Lambda!&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<h1 id="registering-the-pass">Registering the Pass</h1>
<p>接下来我们需要在 tutorial.cpp 中注册这个 Pass。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir-learning/Transform/Affine/AffineFullUnroll.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/InitAllDialects.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Pass/PassManager.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Pass/PassRegistry.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&#34;mlir/Tools/mlir-opt/MlirOptMain.h&#34;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">DialectRegistry</span> <span class="n">registry</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="nf">registerAllDialects</span><span class="p">(</span><span class="n">registry</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">PassRegistration</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">tutorial</span><span class="o">::</span><span class="n">AffineFullUnrollPass</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="nf">asMainReturnCode</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">mlir</span><span class="o">::</span><span class="nf">MlirOptMain</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span> <span class="n">argv</span><span class="p">,</span> <span class="s">&#34;Tutorial Pass Driver&#34;</span><span class="p">,</span> <span class="n">registry</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>mlir::registerAllDialects(registry);</code> 会调用 MLIR 库的函数，将所有可用的方言注册到 <code>registry</code> 中。方言是 MLIR 中用来定义各种中间表示的抽象，可以理解为不同类型的 IR.</li>
<li><code>mlir::PassRegistration&lt;mlir::tutorial::AffineFullUnrollPass&gt;();</code> 将自定义的 <code>AffineFullUnrollPass</code> 注册到 MLIR 的 Pass 系统中。</li>
<li><code>MlirOptMain</code> 是 MLIR 提供的一个函数，处理命令行参数，并执行相应的 Pass.
<ul>
<li>argc 和 argv：来自命令行的参数。</li>
<li>&ldquo;Tutorial Pass Driver&rdquo;：这是一个程序描述字符串，通常是给用户的信息。</li>
<li>registry：之前创建的 DialectRegistry，它包含了所有已注册的方言。</li>
</ul>
</li>
<li><code>mlir::asMainReturnCode(...)</code> 将 <code>MlirOptMain</code> 的返回值转换为标准的退出代码 (0 表示成功，非零值表示失败).</li>
</ul>
<h1 id="test-the-pass">Test the Pass</h1>
<p>我们写一个 .mlir 来测试我们的 Pass，这是一个对数组进行累加的函数。FileCheck 检查经过 Pass 后函数中不会存在 <code>affine.for</code> 指令。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="c">// RUN: /leaning/build/chapter2/tools/02-tutorial-opt %s --affine-full-unroll &gt; %t
</span></span></span><span class="line"><span class="cl"><span class="c">// RUN: FileCheck %s &lt; %t
</span></span></span><span class="line"><span class="cl"><span class="c"></span>
</span></span><span class="line"><span class="cl"><span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@test_single_nested_loop</span><span class="p">(</span><span class="nv">%buffer</span><span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="p">(</span><span class="k">i32</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%sum_0</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="c">// CHECK-NOT: affine.for
</span></span></span><span class="line"><span class="cl"><span class="c"></span>  <span class="nv">%sum</span> <span class="p">=</span> affine<span class="p">.</span>for <span class="nv">%i</span> <span class="p">=</span> <span class="m">0</span> to <span class="m">4</span> iter_args<span class="p">(</span><span class="nv">%sum_iter</span> <span class="p">=</span> <span class="nv">%sum_0</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%t</span> <span class="p">=</span> affine<span class="p">.</span>load <span class="nv">%buffer</span><span class="p">[</span><span class="nv">%i</span><span class="p">]</span> <span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%sum_next</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%sum_iter</span><span class="p">,</span> <span class="nv">%t</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    affine<span class="p">.</span>yield <span class="nv">%sum_next</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="kt">return</span> <span class="nv">%sum</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>经过优化后的函数如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="nv">#map</span> <span class="p">=</span> affine_map<span class="p">&lt;(</span>d0<span class="p">)</span> <span class="p">-&gt;</span> <span class="p">(</span>d0 <span class="err">+</span> <span class="m">1</span><span class="p">)&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nv">#map1</span> <span class="p">=</span> affine_map<span class="p">&lt;(</span>d0<span class="p">)</span> <span class="p">-&gt;</span> <span class="p">(</span>d0 <span class="err">+</span> <span class="m">2</span><span class="p">)&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nv">#map2</span> <span class="p">=</span> affine_map<span class="p">&lt;(</span>d0<span class="p">)</span> <span class="p">-&gt;</span> <span class="p">(</span>d0 <span class="err">+</span> <span class="m">3</span><span class="p">)&gt;</span>
</span></span><span class="line"><span class="cl">module <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@test_single_nested_loop</span><span class="p">(</span><span class="nv">%arg0</span><span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;)</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%c0</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">0</span> <span class="p">:</span> <span class="k">index</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%c0_i32</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%0</span> <span class="p">=</span> affine<span class="p">.</span>load <span class="nv">%arg0</span><span class="p">[</span><span class="nv">%c0</span><span class="p">]</span> <span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%1</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%c0_i32</span><span class="p">,</span> <span class="nv">%0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%2</span> <span class="p">=</span> affine<span class="p">.</span>apply <span class="nv">#map</span><span class="p">(</span><span class="nv">%c0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%3</span> <span class="p">=</span> affine<span class="p">.</span>load <span class="nv">%arg0</span><span class="p">[</span><span class="nv">%2</span><span class="p">]</span> <span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%4</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%1</span><span class="p">,</span> <span class="nv">%3</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%5</span> <span class="p">=</span> affine<span class="p">.</span>apply <span class="nv">#map1</span><span class="p">(</span><span class="nv">%c0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%6</span> <span class="p">=</span> affine<span class="p">.</span>load <span class="nv">%arg0</span><span class="p">[</span><span class="nv">%5</span><span class="p">]</span> <span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%7</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%4</span><span class="p">,</span> <span class="nv">%6</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%8</span> <span class="p">=</span> affine<span class="p">.</span>apply <span class="nv">#map2</span><span class="p">(</span><span class="nv">%c0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%9</span> <span class="p">=</span> affine<span class="p">.</span>load <span class="nv">%arg0</span><span class="p">[</span><span class="nv">%8</span><span class="p">]</span> <span class="p">:</span> <span class="kt">memref</span><span class="p">&lt;</span><span class="m">4x</span><span class="k">i32</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%10</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%7</span><span class="p">,</span> <span class="nv">%9</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="kt">return</span> <span class="nv">%10</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="a-rewrite-pattern-version">A Rewrite Pattern Version</h1>
<p>当想要对一个给定的 IR 子结构重复应用相同的变换子集，直到该子结构被完全去除时，需要写一个重写模式引擎。重写模式是 <code>OpRewritePattern</code> 的子类，它有一个名为 <code>matchAndRewrite</code> 的方法来执行转换。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="c1">// chapter2/lib/Transform/Affine/AffineFullUnroll.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nl">AffineFullUnrollPattern</span> <span class="p">:</span> <span class="n">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">AffineForOp</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nf">AffineFullUnrollPattern</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">:</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">AffineForOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 一般在 OpRewritePattern 中，IR 的更改要通过 PatternRewriter
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// PatternRewriter 处理 OpRewritePattern中发生的突变的原子性
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">LogicalResult</span> <span class="nf">matchAndRewrite</span><span class="p">(</span><span class="n">AffineForOp</span> <span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                   <span class="n">PatternRewriter</span><span class="o">&amp;</span> <span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="n">override</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nf">loopUnrollFull</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>AffineFullUnrollPattern</code> 继承自 <code>OpRewritePattern&lt;AffineForOp&gt;</code>，<a href="https://github.com/llvm/llvm-project/blob/9654bc3960c460bd9d8b06cfa4cfe0e52c6582bd/mlir/include/mlir/IR/PatternMatch.h#L356">OpRewritePattern</a> 是 MLIR 中用于对特定操作类型 (在这里是 <code>AffineForOp</code>) 进行模式匹配和重写的基类。模板参数 <code>AffineForOp</code> 表示我们要为 <code>AffineForOp</code> 这个操作创建一个模式。</li>
<li>构造函数初始化了基类 <code>OpRewritePattern&lt;AffineForOp&gt;</code>，并传递了两个参数
<ul>
<li><code>context</code>：<code>MLIRContext</code> 是 MLIR 的上下文，保存着所有的操作、方言和类型等信息。在这里，<code>context</code> 用来初始化模式对象。</li>
<li><code>benefit</code> 是一个表示模式匹配优先级的整数值，优先级越高的模式越先应用。</li>
</ul>
</li>
<li><code>matchAndRewrite</code> 是在 MLIR 中进行模式重写的核心方法。它的目的是：检查某个操作是否符合当前模式的要求。如果操作匹配模式，则执行重写操作，通常会用新的 IR 替换原来的 IR。
<ul>
<li><code>AffineForOp op</code> 表示要进行模式匹配的 <code>AffineForOp</code> 操作。</li>
<li><code>PatternRewriter &amp;rewriter</code> 是一个用于生成新的 MLIR 操作的工具，它可以修改 IR.</li>
</ul>
</li>
</ul>
<p>我们同样要像上一节一样在头文件中声明一个 <code>AffineFullUnrollPassAsPatternRewrite</code> 类，然后实现其 <code>runOnOperation</code> 方法。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="c1">// chapter2/lib/Transform/Affine/AffineFullUnroll.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="n">AffineFullUnrollPassAsPatternRewrite</span><span class="o">::</span><span class="nf">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">RewritePatternSet</span> <span class="nf">patterns</span><span class="p">(</span><span class="o">&amp;</span><span class="nf">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">patterns</span><span class="p">.</span><span class="n">add</span><span class="o">&lt;</span><span class="n">AffineFullUnrollPattern</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="nf">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="nf">applyPatternsGreedily</span><span class="p">(</span><span class="nf">getOperation</span><span class="p">(),</span> <span class="n">std</span><span class="o">::</span><span class="nf">move</span><span class="p">(</span><span class="n">patterns</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>RewritePatternSet</code> 是 MLIR 中一个容器，用于存储多个 <code>Rewrite Pattern</code>. 每个模式都是针对某种特定操作进行的优化规则。<code>RewritePatternSet</code> 会把所有这些规则聚合在一起，方便在后续的步骤中批量应用。</li>
<li>然后通过 <code>patterns.add&lt;AffineFullUnrollPattern&gt;</code>，将一个 Rewrite Pattern (这里是上面定义的 AffineFullUnrollPattern) 添加到 patterns 集合中。</li>
<li><code>applyPatternsGreedily</code>是 MLIR 提供的一个函数，用于将定义的模式应用到给定的操作 (getOperation()) 上。这个函数使用贪心策略，在一次遍历中尽可能多地应用模式，直到无法再应用为止。</li>
</ul>
<details class="custom-details">
    <summary class="custom-summary">std::move</summary>
    <div><p><code>std::move</code> 是 C++11 引入的一个标准库函数，它的主要作用是将一个对象转换为右值引用，以便启用<strong>移动语义</strong> (Move Semantics). 简单来说，<code>std::move</code> 本身并不实际移动对象，而是为对象提供一个指示，告诉编译器该对象可以被<strong>移动</strong>而不是<strong>复制</strong>。</p>
<p>在 C++ 中，有两种主要的值类别:</p>
<ul>
<li><strong>左值 (Lvalue)</strong> ：表示可以取地址的对象，可以理解为拥有持久生命周期的对象。它通常是变量、数组元素、对象成员等。</li>
<li><strong>右值 (Rvalue)</strong> ：表示临时对象、非持久生命周期的对象，通常是返回值、字面常量等。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;utility&gt;  // std::move</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="n">class</span> <span class="n">MyClass</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="nl">public</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nf">MyClass</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Constructor</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="nf">MyClass</span><span class="p">(</span><span class="k">const</span> <span class="n">MyClass</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Copy Constructor</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="nf">MyClass</span><span class="p">(</span><span class="n">MyClass</span><span class="o">&amp;&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="n">noexcept</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Move Constructor</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">MyClass</span><span class="o">&amp;</span> <span class="n">operator</span><span class="o">=</span><span class="p">(</span><span class="k">const</span> <span class="n">MyClass</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Copy Assignment</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">*</span><span class="n">this</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">MyClass</span><span class="o">&amp;</span> <span class="n">operator</span><span class="o">=</span><span class="p">(</span><span class="n">MyClass</span><span class="o">&amp;&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="n">noexcept</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Move Assignment</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">*</span><span class="n">this</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">MyClass</span> <span class="n">obj1</span><span class="p">;</span>  <span class="c1">// Constructor
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">MyClass</span> <span class="n">obj2</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="nf">move</span><span class="p">(</span><span class="n">obj1</span><span class="p">);</span>  <span class="c1">// Move Constructor
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">MyClass</span> <span class="n">obj3</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">obj3</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="nf">move</span><span class="p">(</span><span class="n">obj2</span><span class="p">);</span>  <span class="c1">// Move Assignment
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<h1 id="a-proper-greedy-rewritepattern">A proper greedy RewritePattern</h1>
<p>接下来写一个用重写模式定义的 <code>MulToAddPass</code>，它会将 <code>y=C*x</code> 形式的乘法转换为 <code>y=C/2*x+C/2*x</code> 形式的加法当 C 是偶数。否则转换成 <code>y=1+(C-1)/2*x+(C-1)/2*x</code> 形式的加法。</p>
<h2 id="poweroftwoexpand">PowerOfTwoExpand</h2>
<ul>
<li>获取了 <code>rhs</code> 的定义操作（<code>rhs.getDefiningOp&lt;arith::ConstantIntOp&gt;()</code>），以确保右操作数是一个常数。</li>
<li>如果右操作数的值是 2 的幂，即 <code>(value &amp; (value - 1)) == 0</code>，则进行优化。
<ul>
<li>将 <code>value</code> 除以 2 然后生成新的常数 <code>newConstant</code>。</li>
<li>计算新的乘法 <code>lhs * newConstant</code>，并将其加倍（通过 <code>AddIOp</code> 来实现 <code>lhs * value</code>）。</li>
<li>最终用新的加法替代原来的乘法。</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nl">PowerOfTwoExpand</span> <span class="p">:</span> <span class="n">public</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">MulIOp</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nf">PowerOfTwoExpand</span><span class="p">(</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">MulIOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">LogicalResult</span> <span class="nf">matchAndRewrite</span><span class="p">(</span><span class="n">MulIOp</span> <span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">PatternRewriter</span><span class="o">&amp;</span> <span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="n">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Value represents an instance of an SSA value in the MLIR system
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">Value</span> <span class="n">lhs</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="nf">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">Value</span> <span class="n">rhs</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="nf">getOperand</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">rhsDefiningOp</span> <span class="o">=</span> <span class="n">rhs</span><span class="p">.</span><span class="n">getDefiningOp</span><span class="o">&lt;</span><span class="n">arith</span><span class="o">::</span><span class="n">ConstantIntOp</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rhsDefiningOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="nf">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kt">int64_t</span> <span class="n">value</span> <span class="o">=</span> <span class="n">rhsDefiningOp</span><span class="p">.</span><span class="nf">value</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="kt">bool</span> <span class="n">is_power_of_two</span> <span class="o">=</span> <span class="p">(</span><span class="n">value</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_power_of_two</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="nf">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">newConstant</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">rhsDefiningOp</span><span class="o">-&gt;</span><span class="nf">getLoc</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">rewriter</span><span class="p">.</span><span class="nf">getIntegerAttr</span><span class="p">(</span><span class="n">rhs</span><span class="p">.</span><span class="nf">getType</span><span class="p">(),</span> <span class="n">value</span> <span class="o">/</span> <span class="mi">2</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">newMul</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">MulIOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="nf">getLoc</span><span class="p">(),</span> <span class="n">lhs</span><span class="p">,</span> <span class="n">newConstant</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">newAdd</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">AddIOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="nf">getLoc</span><span class="p">(),</span> <span class="n">newMul</span><span class="p">,</span> <span class="n">newMul</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rewriter</span><span class="p">.</span><span class="nf">replaceOp</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">newAdd</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">rewriter</span><span class="p">.</span><span class="nf">eraseOp</span><span class="p">(</span><span class="n">rhsDefiningOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="peelfrommul"><strong>PeelFromMul</strong></h2>
<p>这个 Pass 的目标是将一个常数乘法转化为加法形式，适用于常数值 <code>rhs</code> 不为 2 的幂时。</p>
<ul>
<li>将 <code>rhs</code> 减去 1，然后生成一个新的常数 <code>newConstant</code>（即 <code>value - 1</code>）。</li>
<li>用 <code>lhs * newConstant</code> 进行计算，并将结果加上 <code>lhs</code>（即 <code>lhs * value</code> 转化为 <code>(lhs * (value - 1)) + lhs</code>）。</li>
<li>最终用新的加法替代原来的乘法。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nl">PeelFromMul</span> <span class="p">:</span> <span class="n">public</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">MulIOp</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nf">PeelFromMul</span><span class="p">(</span><span class="n">MLIRContext</span><span class="o">*</span> <span class="n">context</span><span class="p">)</span> <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">MulIOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">LogicalResult</span> <span class="nf">matchAndRewrtite</span><span class="p">(</span><span class="n">MulIOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">PatternRewriter</span><span class="o">&amp;</span> <span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">Value</span> <span class="n">lhs</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="nf">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">Value</span> <span class="n">rhs</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="nf">getOperand</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">rhsDefiningOp</span> <span class="o">=</span> <span class="n">rhs</span><span class="p">.</span><span class="n">getDefiningOp</span><span class="o">&lt;</span><span class="n">arith</span><span class="o">::</span><span class="n">ConstantIntOp</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rhsDefiningOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="nf">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="kt">int64_t</span> <span class="n">value</span> <span class="o">=</span> <span class="n">rhsDefiningOp</span><span class="p">.</span><span class="nf">value</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Beacause PowerOfTwoExpand has higher benefit,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// value must not be power of 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">newConstant</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">rhsDefiningOp</span><span class="o">-&gt;</span><span class="nf">getLoc</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">rewriter</span><span class="p">.</span><span class="nf">getIntegerAttr</span><span class="p">(</span><span class="n">rhs</span><span class="p">.</span><span class="nf">getType</span><span class="p">(),</span> <span class="n">value</span> <span class="o">-</span> <span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">newMul</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">MulIOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="nf">getLoc</span><span class="p">(),</span> <span class="n">lhs</span><span class="p">,</span> <span class="n">newConstant</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">newAdd</span> <span class="o">=</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">AddIOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="nf">getLoc</span><span class="p">(),</span> <span class="n">newMul</span><span class="p">,</span> <span class="n">lhs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">rewriter</span><span class="p">.</span><span class="nf">replaceOp</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">newAdd</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">rewriter</span><span class="p">.</span><span class="nf">eraseOp</span><span class="p">(</span><span class="n">rhsDefiningOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="add-the-pass">Add the Pass</h2>
<p>之后我们同样在 <code>runOnOperation</code> 方法中注册 <code>PowerOfTwoExpand</code> 和 <code>PeelFromMul</code> 两个模式。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MulToAddPass</span><span class="o">::</span><span class="nf">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">mlir</span><span class="o">::</span><span class="n">RewritePatternSet</span> <span class="nf">patterns</span><span class="p">(</span><span class="o">&amp;</span><span class="nf">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">patterns</span><span class="p">.</span><span class="n">add</span><span class="o">&lt;</span><span class="n">PowerOfTwoExpand</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="nf">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">patterns</span><span class="p">.</span><span class="n">add</span><span class="o">&lt;</span><span class="n">PeelFromMul</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="nf">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="nf">applyPatternsAndFoldGreedily</span><span class="p">(</span><span class="nf">getOperation</span><span class="p">(),</span> <span class="n">std</span><span class="o">::</span><span class="nf">move</span><span class="p">(</span><span class="n">patterns</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="lit-filecheck">Lit, FileCheck</h1>
<p>LLVM 和 MLIR 使用的是同一个测试框架，分为两个测试步骤。</p>
<ol>
<li><a href="https://llvm.org/docs/CommandGuide/lit.html">lit</a> (LLVM Integratesd Tester) 负责发现、组织和运行测试，并报告测试结果。测试文件中通常包含 RUN: 指令，告诉 lit 如何运行测试。</li>
<li><a href="https://llvm.org/docs/CommandGuide/FileCheck.html">FileCheck</a> 通过模式匹配的方式，验证输出是否包含特定的字符串或结构。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># lit.cfg.py</span>
</span></span><span class="line"><span class="cl"><span class="c1"># CMD: llvm-lit -v path/to/test_files</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">path</span> <span class="k">as</span> <span class="n">osp</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">lit.formats</span> <span class="kn">import</span> <span class="n">ShTest</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">config</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&#34;MLIR-LEARN&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">config</span><span class="o">.</span><span class="n">test_format</span> <span class="o">=</span> <span class="n">ShTest</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">config</span><span class="o">.</span><span class="n">suffixes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;.mlir&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">current_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">tool_path</span> <span class="o">=</span> <span class="s2">&#34;path/to/build/opt_executable&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">[</span><span class="s2">&#34;PATH&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">osp</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">current_path</span><span class="p">,</span> <span class="n">tool_path</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&#34;:&#34;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;PATH&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="test-the-pass-1">Test the Pass</h2>
<p>我们同样创建一个 .mlir 文件来测试我们的 Pass. 我们希望 Pass 能够将递归地将乘法转化为加法形式，</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl"><span class="c">// RUN: /leaning/build/chapter2/tools/02-tutorial-opt %s --mul-to-add &gt; %t
</span></span></span><span class="line"><span class="cl"><span class="c">// RUN: FileCheck %s &lt; %t
</span></span></span><span class="line"><span class="cl"><span class="c"></span>
</span></span><span class="line"><span class="cl"><span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@just_power_of_two</span><span class="p">(</span><span class="nv">%arg0</span><span class="p">:</span> <span class="k">i32</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%0</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">8</span><span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%1</span> <span class="p">=</span> arith<span class="p">.</span>muli <span class="nv">%arg0</span><span class="p">,</span> <span class="nv">%0</span><span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="kt">func</span><span class="p">.</span><span class="kt">return</span> <span class="nv">%1</span><span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c">// CHECK-LABEL: func.func @just_power_of_two(
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK-SAME:    %[[ARG:.*]]: i32
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK-SAME:  ) -&gt; i32 {
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   %[[SUM_0:.*]] = arith.addi %[[ARG]], %[[ARG]]
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   %[[SUM_1:.*]] = arith.addi %[[SUM_0]], %[[SUM_0]]
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   %[[SUM_2:.*]] = arith.addi %[[SUM_1]], %[[SUM_1]]
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   return %[[SUM_2]] : i32
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK: }
</span></span></span><span class="line"><span class="cl"><span class="c"></span>
</span></span><span class="line"><span class="cl"><span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@power_of_two_plus_one</span><span class="p">(</span><span class="nv">%arg</span><span class="p">:</span> <span class="k">i32</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%0</span> <span class="p">=</span> arith<span class="p">.</span><span class="kt">constant</span> <span class="m">9</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="nv">%1</span> <span class="p">=</span> arith<span class="p">.</span>muli <span class="nv">%arg</span><span class="p">,</span> <span class="nv">%0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="kt">func</span><span class="p">.</span><span class="kt">return</span> <span class="nv">%1</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c">// CHECK-LABEL: func.func @power_of_two_plus_one(
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK-SAME:    %[[ARG:.*]]: i32
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK-SAME:  ) -&gt; i32 {
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   %[[SUM_0:.*]] = arith.addi %[[ARG]], %[[ARG]]
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   %[[SUM_1:.*]] = arith.addi %[[SUM_0]], %[[SUM_0]]
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   %[[SUM_2:.*]] = arith.addi %[[SUM_1]], %[[SUM_1]]
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   %[[SUM_3:.*]] = arith.addi %[[SUM_2]], %[[ARG]]
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK:   return %[[SUM_3]] : i32
</span></span></span><span class="line"><span class="cl"><span class="c">// CHECK: }
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>经过优化后的代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">module <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@just_power_of_two</span><span class="p">(</span><span class="nv">%arg0</span><span class="p">:</span> <span class="k">i32</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%0</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%arg0</span><span class="p">,</span> <span class="nv">%arg0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%1</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%0</span><span class="p">,</span> <span class="nv">%0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%2</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%1</span><span class="p">,</span> <span class="nv">%1</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="kt">return</span> <span class="nv">%2</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="kt">func</span><span class="p">.</span><span class="kt">func</span> <span class="nf">@power_of_two_plus_one</span><span class="p">(</span><span class="nv">%arg0</span><span class="p">:</span> <span class="k">i32</span><span class="p">)</span> <span class="p">-&gt;</span> <span class="k">i32</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%0</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%arg0</span><span class="p">,</span> <span class="nv">%arg0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%1</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%0</span><span class="p">,</span> <span class="nv">%0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%2</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%1</span><span class="p">,</span> <span class="nv">%1</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="nv">%3</span> <span class="p">=</span> arith<span class="p">.</span>addi <span class="nv">%2</span><span class="p">,</span> <span class="nv">%arg0</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">    <span class="kt">return</span> <span class="nv">%3</span> <span class="p">:</span> <span class="k">i32</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="summary">Summary</h1>
<p>使用模式重写引擎通常比编写遍历AST的代码更容易。不需要大型 case/switch 语句来处理 IR 中可能出现的所有内容。因此可以单独编写模式，并相信引擎会适当地组合它们。</p>
]]></content:encoded>
    </item>
    <item>
      <title>DistriFusion</title>
      <link>http://localhost:57770/blogs/distrifusion/</link>
      <pubDate>Wed, 23 Oct 2024 14:28:37 +0800</pubDate>
      <guid>http://localhost:57770/blogs/distrifusion/</guid>
      <description>Paper reading about DistriFusion.</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>DistriFusion 将模型输入分割成多个 patch 后分配给 GPU。但是直接实现这样的算法会破坏 patch 之间的交互并失去保真度，而同步 GPU 之间的激活将产生巨大的通信开销。为了克服这一困境，根据观察到的相邻扩散步输入之间的高度相似性提出了 <strong>displaced patch parallelism</strong>，该方法通过重用前一个时间步骤中预先计算的 feature map 来利用扩散过程的顺序性，为当前步提供 context. 该方法支持异步通信，可以通过计算实现流水线化。</p>
<h1 id="introduction">Introduction</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" alt="Original, Navie Patch &amp; DistriFusion">
    </a><figcaption>Original, Navie Patch &amp; DistriFusion</figcaption></figure></p>
<p>加速扩散模型推理主要集中在两种方法上：减少采样步骤和优化网络推理。随着计算资源的快速增长，利用多个 GPU 来加速推理是很有吸引力的。例如在 NLP 中， LLM 已经成功地利用了 GPU 之间的张量并行性，从而显著降低了延迟。然而，对于扩散模型，由于激活尺寸大，张量并行这样的技术不太适合扩散模型。多个 GPU 通常只用于 batch 推理，当生成单个图像时，通常只涉及一个GPU.</p>
<blockquote>
<p>Techniques like tensor parallelism are less suitable for diffusion models due to the large activation size, as communication costs outweigh savings from distributed computation.</p></blockquote>
<p>自然而然的一种方法是将图像分成几个 patch 后分配给不同的设备进行生成。由于各个 patch 之间缺乏相互作用，它在每个 patch 的边界处都有一个清晰可见的分界线。</p>
<p>DistriFusion 也是基于 patch parallelism. 关键在于扩散模型中相邻去噪步骤的输入是相似的，因此，只在第一步采用同步通信。后续步骤重用前一步中预先计算的激活，为当前步骤提供全局上下文和 patch 交互。通过异步通信有效地隐藏了计算中的通信开销。并且还稀疏地在指定的区域上进行卷积和注意力计算，从而按比例减少每个设备的计算量。</p>
<h1 id="method">Method</h1>
<h2 id="displaced-patch-parallelism">Displaced Patch Parallelism.</h2>
<p>在预测 $\epsilon_{\theta}(\mathbf{x}_{t})$ 时 (忽略条件 c 和时间步 t 的输入) ，首先将 $\mathbf{x}_{t}$ 分割成多个 patch $\mathbf{x}_t^{(1)},\mathbf{x}_t^{(2)},\ldots,\mathbf{x}_t^{(N)}$ ，对于每一层 l 和设备 i，在获得输入激活 patch $\mathbf{A}_{t}^{l,(i)}$ 后异步处理两个操作：首先，对于设备i， 激活 $\mathbf{A}_{t}^{l,(i)}$ 首先 scatter 到上一步旧的激活 $\mathbf{A}_{t+1}^{l}$ 中。然后将此分散操作的输出送入稀疏算子 Fl (线性、卷积或注意层)，该算子专门对新区域执行计算并产生相应的输出。同时，对 $\mathbf{A}_{t}^{l,(i)}$ 执行 AllGather 操作，为下一步的全尺寸激活 $\mathbf{A}_{t}^{l}$ 做准备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" alt="Overview of DistriFusion">
    </a><figcaption>Overview of DistriFusion</figcaption></figure></p>
<p>我们对除第一层 (采用同步通信获得其他设备上的输入) 外的每一层重复这个过程。然后将最终输出 Gather 在一起以近似 $\epsilon_{\theta}(\mathbf{x}_{t})$，用于计算 $\mathbf{x}_{t-1}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" alt="Timeline Visualization on Each Device">
    </a><figcaption>Timeline Visualization on Each Device</figcaption></figure></p>
<h2 id="sparse-operations">Sparse Operations</h2>
<p>对于每一层 l，如果原始算子 Fl 是一个卷积层、线性层或交叉注意层，调整使其专门作用于新激活的区域。这可以通过从 scatter 输出中提取最新部分并将其输入到 Fl 中来实现。对于 self-attention，将其转换为 cross-attention，仅在设备上保留来自新激活的 Q，而 KV 仍然包含整个特征图。</p>
<h2 id="corrected-asynchronous-groupnorm">Corrected Asynchronous GroupNorm</h2>
<p>仅对新 patch 进行归一化或重用旧特征都会降低图像质量。同步 AllGather 所有均值和方差将产生相当大的开销。为了解决这一困境，DistriFusion 在陈旧的统计数据中引入了一个校正项。计算公式如下</p>
$$
\mathbb{E}[\mathbf{A}_t]\approx\underbrace{\mathbb{E}[\mathbf{A}_{t+1}]}_{\text{stale global mean}}+\underbrace{\mathbb{E}[\mathbf{A}_t^{(i)}]-\mathbb{E}[\mathbf{A}_{t+1}^{(i)}]}_{\text{correction}}
$$<p>同样对二阶矩 $\mathbb{E}[\mathbf{A}^2_t]$ 也采用这种计算方式，然后通过 $\mathbb{E}[\mathbf{A}^2_t] - \mathbb{E}[\mathbf{A}_t]^2$ 来计算方差。对于方差结果为负的部分，将使用新鲜 patch 的局部方差代替。</p>
<h1 id="code-implementation">Code Implementation</h1>
<p>Distrifusion 中主要就是将 <a href="https://github.com/huggingface/diffusers/blob/9366c8f84bfe47099ff047272661786ebb54721d/src/diffusers/models/unets/unet_2d_condition.py#L71">UNet2DConditionModel</a> 中的 Conv2d, Attention 和 GroupNorm 替换成对应的 patch 实现的网络结构 <a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L15">DistriUNetPP</a>. 这里继承的 BaseModel 类为集成了 PatchParallelismCommManager 类 (介绍见后文) 的网络。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" alt="UNet2DConditionModel">
    </a><figcaption>UNet2DConditionModel</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriUNetPP</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>  <span class="c1"># for Patch Parallelism</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">UNet2DConditionModel</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">UNet2DConditionModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39; 
</span></span></span><span class="line"><span class="cl"><span class="s1">                Substitute Conv2d, Attention, GroupNorm with DistriConv2dPP, DistriSelfAttentionPP, DistriCrossAttentionPP, DistriGroupNorm 
</span></span></span><span class="line"><span class="cl"><span class="s1">                &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">subname</span><span class="p">,</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>  
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">kernel_size</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriConv2dPP</span><span class="p">(</span>  
</span></span><span class="line"><span class="cl">                            <span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="o">=</span><span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;conv_in&#34;</span>
</span></span><span class="line"><span class="cl">                        <span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">Attention</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn1&#34;</span><span class="p">:</span>  <span class="c1"># self attention</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="k">else</span><span class="p">:</span>  <span class="c1"># cross attention</span>
</span></span><span class="line"><span class="cl">                            <span class="k">assert</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn2&#34;</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriCrossAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriGroupNorm</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriUNetPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchparallelismcommmanager">PatchParallelismCommManager</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/utils.py#L112">PatchParallelismCommManager</a> 类主要处理异步通信的部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchParallelismCommManager</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span> <span class="o">=</span> <span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 已经注册的张量的累计总元素数量</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel_dict</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># 记录每个 layer_type 所注册的张量的累计元素数量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 在每个设备上存储所有注册张量的数据，通信所用的 buffer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">starts</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的起始位置 (在 buffer_list 中的起始索引)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ends</span> <span class="o">=</span> <span class="p">[]</span>    <span class="c1">#                 结束                       结束</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shapes</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的 shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">idx_queue</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 需要进行通信的张量索引的队列</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 存储每个设备通信操作的句柄的 list, 用于检查通信是否完成</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>成员函数功能介绍如下</p>
<ol>
<li>
<p><code>register_tensor(self, shape: tuple[int, ...] or list[int], torch_dtype: torch.dtype, layer_type: str = None) -&gt; int</code>: 用于注册张量的形状和数据类型，同时计算并记录张量在缓冲区中的起始位置和结束位置。</p>
<ul>
<li>如果尚未指定 <code>torch_dtype</code>，则将传入的 <code>torch_dtype</code> 设为类成员的默认数据类型。</li>
<li>计算传入张量形状的总元素数 <code>numel</code>，并更新 <code>starts</code>、<code>ends</code> 和 <code>shapes</code> 列表。</li>
<li>如果指定了 <code>layer_type</code>，更新 <code>numel_dict</code> 中该层类型对应的元素数目。</li>
</ul>
</li>
<li>
<p><code>create_buffer(self)</code> : 每个设备上为所有注册的张量创建一个统一的缓冲区。</p>
<ul>
<li>为每个设备创建一个形状为 <code>(numel,)</code> 的张量，并将其放入 <code>buffer_list</code> 中。</li>
<li>输出在各设备上创建的缓冲区总参数量。</li>
</ul>
</li>
<li>
<p><code>get_buffer_list(self, idx: int) -&gt; list[torch.Tensor]</code>: 返回每个设备上对应于指定索引 <code>idx</code> 的缓冲区张量。</p>
<ul>
<li>根据 <code>starts</code> 和 <code>ends</code> 信息，从 <code>buffer_list</code> 中提取指定索引 <code>idx</code> 的张量片段并调整其形状。</li>
</ul>
</li>
<li>
<p><code>communicate(self)</code>: 调用 <code>dist.all_gather</code> 将缓冲区中的张量在不同设备间进行广播。</p>
<ul>
<li>确定当前需要通信的张量范围 (根据 <code>idx_queue</code> 中的索引).</li>
<li>调用 <code>dist.all_gather</code> 在设备组内进行异步广播通信，并将句柄存储在 <code>handles</code> 中。</li>
</ul>
</li>
<li>
<p><code>enqueue(self, idx: int, tensor: torch.Tensor)</code>: 将指定索引 <code>idx</code> 处的张量数据复制到 <code>buffer_list</code> 中，并将索引添加到通信队列 <code>idx_queue</code>。</p>
<ul>
<li>如果通信队列不为空且索引为 0，则先执行一次通信操作。</li>
<li>将张量数据复制到 <code>buffer_list</code> 中的对应位置。</li>
<li>当通信队列长度达到 <code>distri_config</code> 中设定的通信检查点值时，进行通信。</li>
</ul>
</li>
<li>
<p><code>clear(self)</code>: 执行一次所有待通信张量的通信，并等待所有异步操作完成。</p>
<ul>
<li>如果通信队列不为空，则进行通信操作。</li>
<li>遍历所有句柄，等待所有异步操作完成后，将句柄设为 <code>None</code>.</li>
</ul>
</li>
</ol>
<h2 id="districonv2dpp">DistriConv2dPP</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L10">DistriConv2dPP</a> 计算自己负责 patch 部分的卷积，需要通信其他设备需要自己负责 patch 的上下 padding 部分。</p>
<ul>
<li><code>__init__</code>：构造函数，初始化成员变量，设置是否为第一层卷积。</li>
<li><code>naive_forward</code>：执行标准的前向传播，不进行任何切片操作。这是单个设备处理时的普通卷积操作。</li>
<li><code>sliced_forward</code>：处理输入张量的切片操作。根据当前设备索引 (<code>split_idx</code>) 计算输入张量在高度方向的起始和结束位置，并在必要时为切片后的张量添加 padding 后进行卷积操作。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriConv2dPP</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriConv2dPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_layer</span> <span class="o">=</span> <span class="n">is_first_layer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">naive_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#  x: [B, C, H, W]</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sliced_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;...&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 等待上一步通信完成</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">boundary_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># buffer_list 存储的是每个 devive 进行卷积所需要的其他 devive 的数据</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;conv2d&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">            <span class="k">def</span> <span class="nf">create_padded_x</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39;拼接接收到的数据&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># rank 0</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">elif</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># rank n-1</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>  <span class="c1"># other ranks</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="p">[</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                            <span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">padded_x</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 提取当前输入张量需要发送给其他设备的部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">boundary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">boundary_size</span><span class="p">,</span> <span class="p">:],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">boundary_size</span><span class="p">:,</span> <span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 直接用上一步的 buffer 拼接</span>
</span></span><span class="line"><span class="cl">            <span class="n">padded_x</span> <span class="o">=</span> <span class="n">create_padded_x</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">padded_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">boundary</span><span class="p">)</span>  <span class="c1"># 插入自己要发送的数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distriselfattentionpp">DistriSelfAttentionPP</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/attn.py#L107">DistriSelfAttentionPP</a> 只负责计算自己 patch 的输出，需要完整的 KV，将 self attention 运算变成 cross-attention 计算。需要通信自己的 KV.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">DistriAttentionPP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriSelfAttentionPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>  <span class="c1"># 获取 Attention 模块</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>  <span class="c1"># 残差连接</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">args</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">USE_PEFT_BACKEND</span> <span class="k">else</span> <span class="p">(</span><span class="n">scale</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Q Projection</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_kv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>  <span class="c1"># KV Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># 如果缓冲区未创建</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">buffer</span> <span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span> <span class="o">=</span> <span class="n">kv</span>
</span></span><span class="line"><span class="cl">        <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_buffer_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将 full_kv 分割为 key 和 value</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">full_kv</span><span class="p">,</span> <span class="n">full_kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># multi-head attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># O Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># Dropout</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">hidden_states</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distrigroupnorm">DistriGroupNorm</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/groupnorm.py#L9">DistriGroupNorm</a> 根据上一步全特征图的以及当前步 patch 的均值和二阶矩近似当前步的全特征图均值和方差。需要通信 patch 均值和二阶矩。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriGroupNorm</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriGroupNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl">        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_groups</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="n">group_size</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>  <span class="c1"># register for E[x], E[x^2]</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;gn&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算 patch 均值和二阶矩</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x2_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x2_mean</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="n">slice_mean</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Equation 2 in the paper E[A_t] = E[A_(t+1)] + (E[A^i_t] - E[A^i_(t+1)]), same for E[A^2_t]</span>
</span></span><span class="line"><span class="cl">            <span class="n">correction</span> <span class="o">=</span> <span class="n">slice_mean</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">)</span> <span class="o">/</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">+</span> <span class="n">correction</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">slice_mean</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">full_x_mean</span><span class="p">,</span> <span class="n">full_x2_mean</span> <span class="o">=</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">full_x2_mean</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算方差</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_x_mean</span><span class="p">,</span> <span class="n">slice_x2_mean</span> <span class="o">=</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_var</span> <span class="o">=</span> <span class="n">slice_x2_mean</span> <span class="o">-</span> <span class="n">slice_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">var</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">slice_var</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>  <span class="c1"># Correct negative variance</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">num_elements</span> <span class="o">=</span> <span class="n">group_size</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scale and shift</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>DeepSpeedUlysses</title>
      <link>http://localhost:57770/blogs/deepspeedulysses/</link>
      <pubDate>Mon, 21 Oct 2024 11:09:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/deepspeedulysses/</guid>
      <description>Paper reading of Deepseed Ulysses.</description>
      <content:encoded><![CDATA[<h1 id="deepspeed-ulysses-core-design">DeepSpeed-Ulysses Core Design</h1>
<h2 id="system-design">System Design</h2>
<p>原理如下图所示，假设设备数 P 等于多头注意力的头数 hc. 输入 <code>x[N,d]</code> 被切分到每个设备上 <code>[N/p, d]</code>，之后进行 QKV Projection，随后将 K 进行转置后进行一次 all-to-all 通信，这样每个设备上就有 <code>Q[N, d/P], K[d/P, N], V[N, d/P]</code>, 再执行标准的 attention 计算 $Outputcontext=Softmax((QK^T)/\sqrt{d})V$. 再进行一次 all-to-all 通信使得每个设备上有 <code>[N, d/P]</code> 结果再进行后续操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB06300727bd2f239239db47091e81223c?method=download&amp;shareKey=6abbc645b3fa3a039b464dd405f96d4a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB06300727bd2f239239db47091e81223c?method=download&amp;shareKey=6abbc645b3fa3a039b464dd405f96d4a" alt="DeepSpeed Sequence Parallelism (DeepSpeed-Ulysses) Design">
    </a><figcaption>DeepSpeed Sequence Parallelism (DeepSpeed-Ulysses) Design</figcaption></figure></p>
<h2 id="communication-analysis">Communication Analysis</h2>
<p>在采用节点内 NVSwitch 互连和节点间 fat tree IB 拓扑的集群中，对于总消息大小为 M 的 all-to-all 通信，每条链路通过 P 个 gpu 传输的通信量为 M/P。对于隐藏层大小为 h、序列长度为 N、并行度为 P 的 transform 模型，DS-Sequence 对注意力计算前总消息大小为 3Nh 的 QKV Projection 执行 all-to-all 通信，对每个 transformer block 的输出执行 all-to-all 通信，大小为 Nh. 因此，DeepSpeed 序列下每条链路的总通信量为 4Nh/P (或复杂度为 O(N/P)). 也就是说当 N 和 P 按比例增加时，该通信量是恒定的。</p>
<h2 id="comparison-of-other-works">Comparison of Other Works</h2>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBff8d584feabe45900c3a57eea94a78a0?method=download&amp;shareKey=7bae2e87b18707dabcd5e5ae7976e644" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBff8d584feabe45900c3a57eea94a78a0?method=download&amp;shareKey=7bae2e87b18707dabcd5e5ae7976e644" alt="Comparison of DS-Ulysses to Other Sequence Parallelism Methods">
    </a><figcaption>Comparison of DS-Ulysses to Other Sequence Parallelism Methods</figcaption></figure></p>
<ul>
<li>ColAI-SP 发明了 Ring-Attention，Q 存储在本地 而 KV 以环形方式传输以计算全局注意力，导致通信复杂度与消息大小 M 呈线性关系。</li>
<li>Megatron-LM 序列并行方法与 Megatron 张量并行紧密集成。Megatron-LM 沿着序列维度划分序列，并应用 all gather 和 reduce scatter 来聚合 QKV 注意力计算的投影。并行通信量随消息大小 M 线性增加。</li>
<li>DeepSpeed-Ulysses 通过增加与序列长度成比例的设备数来保持通信量恒定。同时将 Zero3 扩展到数据并行和序列并行的组合。ZeRO 跨序列和数据并行组划分模型状态，并在需要时使用 allgather 收集每个 rank 的部分。</li>
</ul>
<h2 id="general-and-attention-agnostic-solution">General and Attention Agnostic Solution</h2>
<p>DeepSpeed-Ulysses 的优势在于一种以注意力为中心的序列并行设计。在注意力计算是 N/P 划分的序列并行之前，注意力计算是头并行，每个头的注意力都是完整的，但只有较少的头，因此注意力计算可以被任何类型的注意机制所取代，例如 dense attention 和各种形式的 sparse attention.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Efficient Large-Scale Language Model Training on GPU</title>
      <link>http://localhost:57770/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/</link>
      <pubDate>Sat, 05 Oct 2024 10:09:35 +0800</pubDate>
      <guid>http://localhost:57770/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/</guid>
      <description>Paper reading about Efficient Large-Scale Language Model Training on GPU Clusters.</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>本文展示了如何将张量、流水线和数据并行性组合起来以扩展到数千个gpu。我们提出了一种新的交错流水线调度，可以在内存占用与现有方法相当的同时将吞吐量提高 10%.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb0e290b7248b1bb11233e55661bbb736?method=download&amp;shareKey=c1cd9d017d6f99b06b1fc2635f48fc19" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb0e290b7248b1bb11233e55661bbb736?method=download&amp;shareKey=c1cd9d017d6f99b06b1fc2635f48fc19" alt="Trend of Sizes of SOTA NLP Models">
    </a><figcaption>Trend of Sizes of SOTA NLP Models</figcaption></figure></p>
<h1 id="introduction">Introduction</h1>
<p>张量（层内）模型并行对于较大的模型会崩溃。较大的模型在多个多 GPU 服务器上进行切分会导致两个问题：</p>
<ol>
<li>张量并行所需的 all-reduce 通信需要通过服务器间链路进行，这比多 GPU 服务器内可用的高带宽 NVLink 要慢</li>
<li>高度模型并行会产生小规模的矩阵乘法（GEMM），从而可能降低 GPU 利用率。</li>
</ol>
<p>流水线模型并行化是指模型的各层在多个 GPU 上进行条带化处理。batch 被拆分成更小的 microbatch ，并在这些 microbatch 之间流水线执行。无论进度如何，为了保持严格的优化器语义，优化器步骤需要跨设备同步，从而在每个 batch 结束时进行流水线刷新 (<em>pipeline flush</em>)，允许 microbatch 完成执行 (不再注入新的 microbatch). microbatch 数量与流水线级数的比例越大，流水线刷新所花费的时间就越少。</p>
<p>我们展示了如何结合流水线、张量和数据并行性，我们称之为PTD-P. 配置分布式训练的指导原则如下:</p>
<ul>
<li>不同形式的并行性以不同的方式相互作用: 并行策略对通信量、执行内核的计算效率以及由于流水线冲洗 (流水线气泡) 而花费的等待计算的空闲时间有影响。</li>
<li>用于流水线并行性的调度对通信量、流水线气泡大小和用于存储激活的内存有影响。</li>
<li>超参数 (如 microbatch 大小) 的值会影响内存占用、在工作线程上执行的内核的算术效率和流水线气泡大小。</li>
<li>随着规模扩展分布式训练是通信密集型的。使用较慢的节点间互连或更密集的通信分区会影响扩展性能。</li>
</ul>
<h1 id="model-parallelism">Model Parallelism</h1>
<p>本节中将讨论有助于不适合单个 GPU 内存的大模型的并行训练方法。我们将流水线模型并行和张量模型并行 (如图 2 所示的组合) 与数据并行结合起来，简称为PTD-P.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB09f440d2c0b5bd3b923d2d09dbf47eb5?method=download&amp;shareKey=59701400f477d8dd03c4ea43138c933f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB09f440d2c0b5bd3b923d2d09dbf47eb5?method=download&amp;shareKey=59701400f477d8dd03c4ea43138c933f" alt="Combination of Tensor and Pipeline Model Parallelism (MP)">
    </a><figcaption>Combination of Tensor and Pipeline Model Parallelism (MP)</figcaption></figure></p>
<h2 id="data-parallelism">Data Parallelism</h2>
<p>使用数据并行时，每个 worker 都有一个完整模型的副本，输入数据集被分片， worker 定期汇总他们的梯度，以确保所有 worker 看到一个一致的权重版本。</p>
<h2 id="pipeline-parallelism">Pipeline Parallelism</h2>
<p>通过流水线并行，模型的层被分散到多个设备上。一个 batch 被分成更小的 microbatch. 在 microbatch 之间进行流水线执行。为了准确地保持优化器语义，我们引入了定期的流水线刷新，以便在设备之间同步优化器步骤。在每个 batch 处理的开始和结束时，设备都是空闲的。我们称这段空闲时间为流水线气泡 (<em>pipeline bubble</em>).</p>
<h3 id="default-schedule">Default Schedule</h3>
<p>GPipe 提出了一个调度方案，如图 3 所示 (假设<strong>反向传播的时间是前向传播的两倍</strong>，管道调度的效率并不取决于这个因素)，首先执行一个 batch 中所有 microbatch 的前向传播，然后执行所有 microbatch 的反向传播。设 GPipe 流水线气泡的大小为 t_pb，microbatch 的数量为 m，流水线阶段数量 (用于流水线并行的设备数量) 表示为 p，每次迭代的理想时间表示为 t_id (假设理想缩放)，执行单个 microbatch 的向前和反向传播的时间表示为 t_f 和 t_b. 在该调度中，流水线气泡由批处理开始时的 p−1 个前向传播和 p−1 个反向传播组成。则流水线气泡总时间为 t_pb=(p−1)·(t_f+t_b). batch 的理想执行时间为 t_id=m·(t_f+t_b)。因此，在流水线气泡中花费与理想计算时间的比例为:</p>
<p>流水线气泡占比 = t_pb / t_id = (p−1) / m.</p>
<p>为了使流水线气泡占比小，我们需要 m 远大于 p. 然而 m 非常大时这种方法的内存占用很高，因为它需要在训练一次迭代时间内为所有 m 个 microbatch 保存中间激活.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB04e4031b886573061f614e73854d1f43?method=download&amp;shareKey=97391411ed3d06fc2c6de5de5f20d1d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB04e4031b886573061f614e73854d1f43?method=download&amp;shareKey=97391411ed3d06fc2c6de5de5f20d1d0" alt="GPipe Pipeline Schedule">
    </a><figcaption>GPipe Pipeline Schedule</figcaption></figure></p>
<h3 id="schedule-with-interleaved-stages">Schedule with Interleaved Stages</h3>
<p>为了缩小流水线气泡的大小，每个设备都可以对多个层的子集（称为模型块）进行计算，流水线中的每个设备都被分配了多个流水线阶段（与之前相比，每个流水线阶段的计算量更少），而不是单个连续的层。</p>
<details class="custom-details">
    <summary class="custom-summary">An Example</summary>
    <div>例如，如果每个设备之前被分配 4 层 (即设备 1 有 1 - 4 层，设备 2 有 5 - 8层&hellip;)，我们可以让每个设备为两个模型块执行计算 (每个模型块被分配 2 层)，即设备 1 有 1、2、9、10 层; 设备 2 具有第3、4、11、12层&hellip;</div>
</details><br>
<p>和上一小节一样，我们可以执行完所有 microbatch 的前向传播然后执行所有反向传播 (all-forward, all-backward)，但这将占用大量内存 (与 m 成正比). 因此如图 4 所示，我们设计了一个适配于之前的内存高效 1F1B 的交错调度。它要求 <strong>microbatch 数量是流水线并行度 (流水线中的设备数量) 的整数倍</strong>。</p>
<p>如果每个设备都有 v 个阶段 (模型块)，那么每个阶段 microbatch 的前向和反向传播的时间分别为 t_f/v 和 t_b/v. 流水线气泡时间因此减少到 𝑡^int_pb=(p−1)·(tf+tb)/v，</p>
<p>流水线气泡占比为 𝑡^int_pb / t_id = (p−1) / (m·v).</p>
<p>这意味着该调度减少气泡时间到原先的 1/v，但该计划需要额外的通信，因此通信量也为原来的 v 倍。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb74cfaaf752e14cf2e44f4abd7e3e7bf?method=download&amp;shareKey=4558481494c1d4b22574e739743b123d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb74cfaaf752e14cf2e44f4abd7e3e7bf?method=download&amp;shareKey=4558481494c1d4b22574e739743b123d" alt="Default and Interleaved 1F1B Pipeline Schedules">
    </a><figcaption>Default and Interleaved 1F1B Pipeline Schedules</figcaption></figure></p>
<h2 id="tensor-model-parallelism">Tensor Model Parallelism</h2>
<p>详情见 Megatron-LM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB02b1b15f4d736bfee41738f3c3ee72b3?method=download&amp;shareKey=b1a7e4a6171585f4f0a8a39fb4b2d8b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB02b1b15f4d736bfee41738f3c3ee72b3?method=download&amp;shareKey=b1a7e4a6171585f4f0a8a39fb4b2d8b3" alt="Blocks of Transformer Model Partitioned with Tensor Model Parallelsim">
    </a><figcaption>Blocks of Transformer Model Partitioned with Tensor Model Parallelsim</figcaption></figure></p>
<h1 id="performance-analysis-of-parallelization-configurations">Performance Analysis of Parallelization Configurations</h1>
<p>首先定义下符号含义</p>
<ul>
<li>(p,t,d): 并行化维度。p 表示流水线模型并行大小，t 表示张量模型并行大小，d 表示数据并行大小。</li>
<li>n: GPU 数量，要求 ptd=n.</li>
<li>B: 全局批大小 (作为输入提供)</li>
<li>b: microbatch 大小。</li>
<li>m = B/(db): 一个 batch 中每个流水线中的 microbatch 的数量。</li>
</ul>
<h2 id="tensor-and-pipeline-model-parallelism">Tensor and Pipeline Model Parallelism</h2>
<p>如前所述，使用带有周期性冲洗的流水线并行会产生大小为 (p−1)/m 的流水线气泡. 固定 d=1，则 tp=n，气泡大小可以用 t 表示为</p>
<p>(p−1)/m=(n/t-1)/m.</p>
<p>GPU 之间的通信量也受 p 和 t 大小的影响。流水线模型并行的特点是更便宜的点对点通信，每个 microbatch 的每对连续设备之间 (前向或后向传递) 需要执行的通信总量为 bsh. 张量模型并行则使用 all-reduce 通信，总大小为 bsh 的张量需要在每层的前向和后向传递中，在 t 个模型副本之间进行两次 all-reduce，因此每个 microbatch 每层每个设备的总通信量为 4bsh(t-1)/t. 每个设备通常有多个层，则每个设备上每个 microbatch 的张量并行通信总量为 l^stage4bsh(t-1)/t, 其中 l^stage 为流水线阶段的层数。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 1: 当 t 大于单个节点中的 GPU 数量时，在较慢的节点间链路上执行张量模型并行的开销非常大。在考虑不同形式的模型并行时，使用 g-GPU 服务器时张量模型并行度一般为 g (all-reduce 通信量大，NVLink 带宽高)，然后可以使用流水线模型并行来扩展到跨服务器的更大模型 (P2P 通信量小，PCIe 带宽低).</p></div>

<h2 id="data-and-model-parallelism">Data and Model Parallelism</h2>
<p>管道模型并行性。设 t=1，每个管道的 microbatches 数量 m=𝐵/(db)=b&rsquo;/d, b&rsquo;:=B/b. 设 GPU 总数为 n ，流水线阶段数为 p=n/d，气泡大小为</p>
<p>(p−1)/m=(n/d-1)/(b&rsquo;/d)=(n-d)/b'</p>
<p>管道气泡随着 d 变大而变小。如果数据并行所需的 all-reduce 通信不会随着 d 的变大而急剧增加，那么总体吞吐量将会增加，因为基于环的实现的通信时间随着 d 的变化为 (d−1)/d=1−1/d.同样对于给定的并行配置，随着批量大小的增加，b&rsquo; = B/b 增加，因此吞吐量上升。同时数据并行所需的 all-reduce 通信频率也下降，进一步提高了吞吐量。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB27b82e198a75ad564ac917d6d560dec1?method=download&amp;shareKey=cfef60f8f216007803bdafe8b5a5e64c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB27b82e198a75ad564ac917d6d560dec1?method=download&amp;shareKey=cfef60f8f216007803bdafe8b5a5e64c" alt="Fraction of Time Spent Idling due to Pipeline Flush">
    </a><figcaption>Fraction of Time Spent Idling due to Pipeline Flush</figcaption></figure></p>
<p>在张量模型并行下，每个 microbatch 都需要进行 all-reduce 通信，这在多 GPU 服务器上开销很大；而数据并行只需要在每个 batch 中执行一次的 all-reduce通信。此外，使用张量模型并行，每个设备计算每层的一部分，因此对于不够大的层， GPU 可能无法以峰值效率执行这些子矩阵计算。</p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 2：在使用数据和模型并行时，应使用 M=tp 的总模型并行大小，以便模型参数和中间数据满足 GPU 内存限制；数据并行可用于将训练扩展到更多 GPU.</p></div>

<h2 id="microbatch-size">Microbatch Size</h2>
<p>给定函数 t_f(b) 和 t_b(b)，将 microbatch 大小映射为单个 microbatch 的前向和反向计算时间，计算一个 batch 所花费的总时间 (忽略通信成本) 为</p>
<p>(b&rsquo;/b+p-1)·(t_f(b)+t_b(b)).</p>
<p>microbatch 大小因此既影响运算的算术强度，也影响管道气泡大小。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe3b16b74c3ca1aedb0f81939501da9de?method=download&amp;shareKey=cd1c360e2597b37f705bd5f4906d64b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe3b16b74c3ca1aedb0f81939501da9de?method=download&amp;shareKey=cd1c360e2597b37f705bd5f4906d64b3" alt="Per-GPU Throughput versus Microbatch Size for a GPT Model">
    </a><figcaption>Per-GPU Throughput versus Microbatch Size for a GPT Model</figcaption></figure></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3de398053a7040a405722f3b8c929bf1?method=download&amp;shareKey=7014b2696bffc97d5646b4b2614bd3fb" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3de398053a7040a405722f3b8c929bf1?method=download&amp;shareKey=7014b2696bffc97d5646b4b2614bd3fb" alt="">
    </a><figcaption>Behavior of Throughput for the same GPT Model</figcaption></figure></p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 3：最佳 microbatch 大小 b 取决于模型的吞吐量和内存占用特征，以及流水线深度 p、数据并行大小 d 和批量大小 B.</p></div>

<h2 id="activation-recomputation">Activation Recomputation</h2>
<p>激活重计算通过在向后传递之前运行第二次正向传播 (并且仅存储给定流水线阶段的输入激活)，来权衡所执行的计算操作数量的增加对额外内存占用的影响。设 A^input 为一层的输入激活的大小，A^intermediate 为每层的中间激活的大小，一个模型阶段有 l 层， 激活保存点的数量为 c，那么总内存占用为 c·A^input + l/c·A^intermediate. 因此取 c = \sqrt(l·A^input·A^intermediate) 时内存占用最小。</p>
<h1 id="implementation">Implementation</h1>
<h2 id="communication-optimizations">Communication Optimizations</h2>
<p>使用流水线并行时，我们希望在正向和反向并行发送和接收张量。每台 DGX A100 都配备了 8 个 InfiniBand（IB）网卡。然而发送和接收都是点对点的，只发生在两台服务器上的一对 GPU 之间，因此很难充分利用所有网卡。对于流水线内的单次通信，每个 transformer 层的输出都会在张量并行的设备中复制。为了减少这种冗余，我们可以在发送端将张量分割成大小相等的块，然后使用每个 rank 自己的 InfiniBand 发送. 在接收端通过比 InfiniBand 互连快得多的 NVLink 执行 all-gather，重新组装整个张量。通过 scatter-gather 通信优化，将每对连续流水线阶段之间需要执行的通信总量减少为 bsh/t.</p>
<h2 id="computation-optimizations">Computation Optimizations</h2>
<p>将数据布局从 (b,s,a,h) 更改为 (s,b,a,h). 其次，使用 PyTorch JIT 为一系列元素操作 (bias+GeLU 和 bias+dropout+add) 生成融合算子。</p>
<h1 id="evaluation">Evaluation</h1>
<p>在 Selene 超级计算机上以混合精度运行。每个集群节点有</p>
<ul>
<li>8 个 NVIDIA 80GB A100 GPU，通过 NVLink 和 NVSwitch 互连。</li>
<li>8 个 NVIDIA Mellanox 200Gbps HDR Infiniband HCA 用于应用程序通信</li>
<li>额外有 2 个 HCA 用于专用存储。
节点以三级 (leaf, spine, core) 胖树拓扑结构连接，一共有 850个交换机。集群使用 all-NVME 共享并行文件系统进行高性能数据访问和存储。16 位精度的 A100 GPU 的峰值设备吞吐量为 312 teraFLOP/s.</li>
</ul>
<p>QKV 变换的线性层权重参数量均为 h^2, attention 后的线性层权重参数量为 h^2, 两层前馈网络每个线性层的权重参数量为 4h^2，因此每一个 transformer block 的所有线性层的参数量为 12h^2. 词嵌入的参数量为 Vh，位置编码的参数量为 sh.</p>
<p>一个 $A_{m\times k}\times X_{k\times n}$ 矩阵乘法需要 2mkn FLOPs( 2 是因为乘法和加法). transformer block 包含一个注意力块和一个两层前馈网络组成。对于注意力块，主要的 FLOP 来源于 QKV 转换 (6Bsh^2 次操作)、注意力矩阵计算 (2Bhs^2 次操作)、注意力乘 Value (2Bhs^2 次操作) 和 attention 后的线性层 (2Bsh^2 次操作). 前馈网络将隐藏维度放大到 4h，然后再减小到 1h，需要 16Bsh^2 次操作。将这些加在一起，每个 transformer block 一共有 24Bsh^2+4Bhs^2 FLOPs. 反向传播需要两倍的计算量，因为需要计算关于输入张量和权重张量的梯度。此外，使用激活重计算需要在反向传播之前进行额外的正向传播。因此，每一层的总计算量为 FLOPs 为 4*(24Bsh^2+4Bhs^2).</p>
<p>计算量另一方面来源于 head 的 logit 层，它将维度的特征 h 转换为词汇表维度的特征 V. 该操作所需的计算量为正向传播的 2BshV 和反向传播的 4BshV，总共 6BshV FLOPs.</p>
<h2 id="result">Result</h2>
<p>Pipeline-parallel 并行度增加降低 GPU 的计算效率，因为 bubble 变多了。
Batchsize 的增大可以减少 pipeline-parallel 并行度大小带来的影响。</p>
<p>Batch size增加有助于提高GPU的计算效率。
Interleaved schedules 能显著提高GPU的计算效率。</p>
<p>不使用激活重计算的话单位时间内的训练的吞吐是要高于使用重计算的，因为重计算在反向传播中引入额外的计算量。
由于重计算可以节省显存，batchsize 可以相应提高不少。由于 batchsize 的提高，训练吞吐量也得到了提高，从而达到了优化的效果。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Megatron-LM</title>
      <link>http://localhost:57770/blogs/megatronlm/</link>
      <pubDate>Wed, 02 Oct 2024 15:51:50 +0800</pubDate>
      <guid>http://localhost:57770/blogs/megatronlm/</guid>
      <description>Paper reading about Megatron-LM</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>我们的方法不需要新的编译器或更改库，与流水线模型并行 (<em>pipeline model parallelism</em>) 正交互补，并且可以通过在原生 PyTorch 中插入一些通信操作来实现。为了阐述我们的方法，使用 512 个 GPU 将基于 transformer 的模型扩展到 83 亿个参数。与可保持 39 TeraFLOPs (峰值 FLOPs 的 30%) 的强大单 GPU 基准相比，我们在整个应用中保持了 15.1 PetaFLOPs，扩展效率高达 76%.</p>
<h1 id="introduction">Introduction</h1>
<p>随着 LLM 变得越来越大，它们会超出现代处理器的内存限制，并需要如激活检查点 (activation checkpoint) 等额外的内存管理技术。广泛使用的优化算法 (如ADAM) 需要每个参数额外的内存来存储动量和其他优化器状态。这减少了可以有效训练的模型的大小。模型并行性的几种方法克服了这一限制，它们对模型进行分区，使权重及其相关的优化器状态不需要并发地驻留在处理器上。</p>
<details class="custom-details">
    <summary class="custom-summary">Activation Checkpoint</summary>
    <div>在深度学习模型的训练过程中，前向传播会计算并存储每一层的激活值，这些激活值在后向传播时被用来计算梯度。然而，对于深度很大的模型因为需要存储大量的激活值，可能会导致内存溢出。激活检查点技术通过在前向传播过程中只存储一部分的激活值来解决内存占用问题，如果在后向传播过程中需要没有存储的激活值就进行重新计算。</div>
</details><br>
<p>为了证明方法的可扩展性，通过在单个英伟达 V100 32GB GPU 上训练一个包含 12 亿个参数的模型来建立基准。训练该模型可维持 39 TeraFLOPs 的算力，是在 DGX-2H 服务器中配置的单个 GPU 理论峰值 FLOPS 的 30%. 在 512 个 GPU 上将模型扩展到 83 亿个参数，并采用 8 路模型并行，在整个应用中实现了高达 15.1 PetaFLOPs 的持续运行速度。与单 GPU 情况相比，扩展效率提高了 76%. 下图展示了更详细的扩展结果。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" alt="Model (blue) and model&#43;data (green) parallel FLOPS">
    </a><figcaption>Model (blue) and model&#43;data (green) parallel FLOPS</figcaption></figure></p>
<h1 id="background--chanllenges">Background &amp; Chanllenges</h1>
<h2 id="neural-language-model-pretraining">Neural Language Model Pretraining</h2>
<p>早期的预训练和传递语言神经表示的例子表明，与从头开始学习的词嵌入表相比，预训练的词嵌入表改善了下游任务的结果。目前的技术水平已经从传输单词嵌入表发展到传输整个数十亿参数的语言模型。这种方法的进步要求硬件、系统技术和框架能够高效地大规模运行。</p>
<h2 id="transformer-language-models-and-multi-head-attention">Transformer Language Models and Multi-Head Attention</h2>
<p>下图展示了使用的 transformer 模型的示意图。最近利用 transformer 进行语言建模的工作，如 BERT 和 GPT-2 根据需要分别只使用编码器和解码器。</p>
<blockquote>
<p>GPT-2 和 BERT 都对多头注意和 FFN 的输入使用 GeLU 非线性和层归一化，而原始 transformer 使用 ReLU 非线性并对输出进行层归一化。</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" alt="Transformer Architecture">
    </a><figcaption>Transformer Architecture</figcaption></figure></p>
<h2 id="data-and-model-parallelism-in-deep-learning">Data and Model Parallelism in Deep Learning</h2>
<p>将深度神经网络训练扩展到多硬件加速器有两种范式:</p>
<ul>
<li>Data Parallelism (DP): 将 batch 拆分到多个 worker</li>
<li>Model Parallelism (MP): 将模型的内存使用和计算分布在多个 worker 中。
<ul>
<li>Pipeline Parallelism (PP): 一组操作在一个设备上执行，然后将输出传递到流水线中的下一个设备执行另一组操作。</li>
<li>Distributed Tensor Computation: 将张量运算分割到多个设备上，以加速计算或增加模型大小。</li>
</ul>
</li>
</ul>
<p>然而，这些技术有一个基本的限制: 模型权重必须能加载进 worker. 我们的方法是利用模型并行性在多个加速器之间分割模型。</p>
<h1 id="model-parallel-transformers">Model Parallel Transformers</h1>
<p>我们利用 transformer 网络的结构 (self-attention 和 FFN (2*MLP) 组成)，通过添加一些同步原语，创建了一个简单的并行计算模型。下面分别阐述对 FFN 和 self-attention 的并行化。</p>
<p>FFN 第一个 MLP 由一个 GEMM，后跟一个 GeLU 非线性组成:</p>
$$
Y=\text{GeLU}(XA)
$$<p>并行化 GEMM 的一种选择是将权重矩阵 A 沿着行切分，并将 X 沿着其列切分:</p>
$$
X=[X_1,X_2], A=\begin{bmatrix}A_1\\A_2\end{bmatrix}
$$<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" alt="Row Split of Weight">
    </a><figcaption>Row Split of Weight</figcaption></figure></p>
<p>可以得出 $Y = X_1A_1+X_2A_2$. 由于 GeLU 是非线性函数，因此这种方法需要在 GeLU 函数之前进行同步。</p>
<p>另一个选择是沿着列切分 $A=\begin{bmatrix}A_1,A_2\end{bmatrix}$. 这样可以让 GeLU 独立地应用于每个 GEMM 的输出</p>
<p>$[Y_1, Y_2]=\begin{bmatrix}\text{GeLU}(XA_1),\text{GeLU}(XA_2)\end{bmatrix}$.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" alt="Column Split of Weight">
    </a><figcaption>Column Split of Weight</figcaption></figure></p>
<p>这种切分方式的优点是不需要进行同步操作。</p>
<p>如下图所示，以列并行方式切分第一个 GEMM，并沿着行切分第二个GEMM。然后，在将输出传递给 dropout 层之前，第二个GEMM 的输出在 GPU 之间进行 all-reduce 操作。这种方法将 FFN 中的两个 GEMM 拆分到多个 GPU 上执行，并且只需要在正向传播 (g 操作符) 和反向传播 (f 操作符) 中分别执行一次 all-reduce 操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" alt="Parallelism of MLP">
    </a><figcaption>Parallelism of MLP</figcaption></figure></p>
<p>如下图所示，利用多头注意力操作中本身存在的并行性，以列并行的方式划分与 QKV 相关的 GEMM，以便每个注意力头对应的矩阵乘法在一个 GPU 上独立完成。输出线性层的 GEMM 沿着其行并行化，并直接获取并行 attention 的输出。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" alt="Parallelism of Self-Attention">
    </a><figcaption>Parallelism of Self-Attention</figcaption></figure></p>
<p>如下图所示，这使能够仅在正向传播和反向传播中分别中使用两个 all-reduce 操作执行 transformer 中所有的 GEMM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" alt="Parallelism of Transformer Layer">
    </a><figcaption>Parallelism of Transformer Layer</figcaption></figure></p>
<p>基于 transformer 的语言模型的输出嵌入维度为隐藏层大小 (H) 乘以词汇表大小 (v). 我们沿着词汇表维度 $E = \begin{bmatrix}E_1,E_2\end{bmatrix}$ 并行化权重矩阵。每一块现在只包含嵌入表的一部分，输入嵌入后需要一个 all-reduce (g 算子).</p>
<p>对于输出嵌入，一种方法是通过并行 $\mathrm{GEMM} [Y_{1},Y_{2}]=[XE_{1},XE_{2}]$ 来获得 logits，并对结果 all-gather 后送入交叉熵损失函数。在这种情况下，all-gather 通信量为 bsv 个元素 (b 是批处理大小，s 是序列长度). 为了减小通信规模，我们将输出与交叉熵损失融合，这样通信量降为 bs.</p>
<p>我们在每个 GPU 上维护层归一化参数的副本，并在将这些张量作为输入送到下一个模型并行区域之前，在本地输出上进行 dropout 和残差连接。为了优化模型，我们允许每个模型并行 worker 优化自己的一组参数。因为所有的值要么是本地的，要么是在 GPU上 重复的，所以在这个公式中不需要通信更新的参数值。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Ring Attention Principle</title>
      <link>http://localhost:57770/blogs/ringattention/</link>
      <pubDate>Thu, 26 Sep 2024 22:59:35 +0800</pubDate>
      <guid>http://localhost:57770/blogs/ringattention/</guid>
      <description>This is a brief introduction to the Ring Attention Principle.</description>
      <content:encoded><![CDATA[<h1 id="background">Background</h1>
<p>如今 LLM 的 token 长度显著增加，从 GPT-3.5 的 16k 到 Claude 2 的 200k，现在 Gemini 1.5 Pro 甚至有 1M 的 token 长度。如此长的 token 在计算 attention 时对显存的需求非常大。<a href="https://arxiv.org/abs/2310.01889">Ring Attention</a> 便是为了并行计算 attention 而提出的一种方法<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<blockquote>
<p>Ring Attention 和 Flash Attention 可以同时使用。</p></blockquote>
<h1 id="attention-and-memory">Attention and Memory</h1>
<p>要计算 attention， 我们需要三个大小为 (s, d) 的矩阵：Q (query)、K (key)、V (value)，其中 s 为序列长度，d 为模型维度。attention 的计算公式为</p>
$$
Attention(Q, K, V) = softmax(QK^T / \sqrt{d})V
$$<p>忽略 sqrt(d) 项，我们记 Score Matrix 为 S = QK^T / \sqrt{d}，然后对 S 进行 softmax 归一化，得到 Attention Matrix. 可以发现它们占用显存大小是 O(s*s) 数量级。即使使用 <a href="https://arxiv.org/abs/2205.14135">Flash Attention</a>，显存占用量也是 O(s) 数量级。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe66e94e161b89a4ba25d05b67a47e393?method=download&amp;shareKey=742185dd412edbdb3266fa16ab91d787" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe66e94e161b89a4ba25d05b67a47e393?method=download&amp;shareKey=742185dd412edbdb3266fa16ab91d787" alt="Attention Compute Process">
    </a><figcaption>Attention Compute Process</figcaption></figure></p>
<p>我们希望如果在 N 个设备上并行计算 attention，每个设备的显存占用量为整个的 1/N, 因此就需要对 Q、K、V 的 sequence 长度进行切分。但是如果得到的最终 attention 矩阵需要在设备间进行集合通信组装每个的计算结果，通信量也和 sequence 长度成正比。Ring Attention 提出了一个巧妙的解决方案：在设备之间进行轮转，并行化所有计算而且完全隐藏通信的开销。</p>
<blockquote>
<p>We will rotate between devices to parallelize all computation and hide the communication overhead completely.</p></blockquote>
<h1 id="splitting-the-query">Splitting the Query</h1>
<p>假设我们有 N 个设备，我们将 Q 沿着 sequence 维度切分为 N 份，每份大小为 (s/N, d). 由于计算 Score 和 Attention 需要完整的 K 和 V，这样它们也被切分成 N 份，每份大小为 (s/N, d). 计算示意图如下。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB170087e68345309f813b8edee9487b92?method=download&amp;shareKey=f848ff8adb5676443347921c65a3b104" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB170087e68345309f813b8edee9487b92?method=download&amp;shareKey=f848ff8adb5676443347921c65a3b104" alt="Split Q">
    </a><figcaption>Split Q</figcaption></figure></p>
<h1 id="splitting-the-key-and-value">Splitting the Key and Value</h1>
<p>对 K 和 V 的切分并不能像 Q 那样直接。因为 softmax 的计算公式如下，要得到分母的值意味着我们需要对每一行进行计算。</p>
$$
softmax(s_i) = \frac{\exp(s_i)}{\sum_{j=i}^d{\exp(s_j)}}
$$<p>如果我们能对 K 和 V 进行切分并正确计算 softmax，那么计算过程可以由下图所示的那样完成 (忽略 softmax). 外循环遍历 Q 的所有分块，内循环遍历 K 和 V 的所有分块，一次计算一部分的 attention. Ring Attention 示意图如下所示，顾名思义所有设备组成一个环状，每个设备存储 Q 的一部分，每次迭代过程会传递 K 和 V 到下一个设备，最终每个设备将得到计算自己 Q 部分的 attention 矩阵所需要的 K 和 V. 每个设备被分配 Q 的一部分 (即一个外层循环索引)，并迭代计算每个 K 和 V 的分块 (内循环)。每个设备只需要跟踪形状为 (s/N, s/N) 的累积和 A_j。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbc9ef7d01431fe639ecf44842bce0e1a?method=download&amp;shareKey=03d587a38ca574ed1547f2594a45ab4c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbc9ef7d01431fe639ecf44842bce0e1a?method=download&amp;shareKey=03d587a38ca574ed1547f2594a45ab4c" alt="Attention Parallel Computation">
    </a><figcaption>Attention Parallel Computation</figcaption></figure></p>
<h1 id="online-softmax">Online Softmax</h1>
<p>在内循环的每次迭代中我们可以更新部分和为 $l^j = l^{j-1} + \sum_{k_t\in K_j}{\exp(Q_ik_t^T)}$. 在内循环结束后我们就可以获得每一行的指数和。归一化和与 V 的相乘顺序不会影响结果，我们可以先累加总和，并在所有其他计算完成后再执行实际的归一化操作。</p>
<p>因此，设备 i 除了计算当前的累计和 $A^j = A^{j-1} + \exp(Q_i K_j^T) V_j$ 外，还需要在内循环每次迭代中更新部分和 $l^j \in \mathbb{R}^{B_q}$ ，其中 $B_q$ 为 Q 的分块大小。</p>
<h1 id="safe-softmax">Safe softmax</h1>
<p>由于指数运算经常容易出现溢出，我们通常减去 max(s_i) 后进行指数运算，公式如下，这样并不会影响结果。</p>
$$
\mathrm{softmax}(s_{1:N})=\frac{\exp(s_{1:N})}{\sum_i\exp(s_i)}\cdot\frac{\exp(-s_{max})}{\exp(-s_{max})}=\frac{\exp(s_{1:N}-s_{max})}{\sum_i\exp(s_i-s_{max})}
$$<p>所以我们在内循环每次迭代中需要先更新当前的最大值 $m^{j+1}=\max(m^j,\max(Q_iK_{j+1}^T))$，然后更新之前迭代的计算结果 A_j 和 部分和 l_j. 最后再计算本次迭代的结果。</p>
$$
A^{j+1}=A^j\cdot\exp(m^j-m^{j+1})+\exp(Q_iK_{j+1}^T-m^{j+1})\cdot V_j
$$<p>更新部分和</p>
$$
l^{j+1}=l^j\cdot\exp(m^j-m^{j+1})+\exp(Q_iK_{j+1}^T-m^{j+1})
$$<h1 id="putting-it-together">Putting it Together</h1>
<p>Ring Attention 计算步骤如下：</p>
<ol>
<li>沿着 Q 的 sequence 长度拆分为一个独立的外循环。</li>
<li>应用 Online Safe Softmax，以便沿着 K 和 V 的sequence 长度拆分，从而在内层循环中累积计算注意力。</li>
</ol>
<p>这种并行化的方式是通过将每个设备分配一个 Q_i 块来实现的。因此，我们需要将 Q 拆分为 N 个相等的部分 (B_Q=N). 每个设备将分别计算它的输出块 $\text{Output}(Qi,K,V)= \text{softmax}(Q_i K^T)V ，通过在 K 和 V 块上执行内循环来迭代计算。难点挑战在于设备无法一次存储完整的 K 和 V 矩阵。</p>
<p>如果我们有 4 个 GPU，那么我们将把每个设备的 Q 按序列维度分成 4 个块，K 和 V 被分割成 B_K=B_Q=N 个块，并对设备进行初始化，使每个设备都持有一个 Qi 块、 一个 Kj 块和 一个 Vj 块。为简单起见，我们可以假设设备 i 在开始时持有 Qi, Ki 和 Vj 块。在设备计算完与其当前 vj kj 相对应的一个内循环步骤后，每个设备都需要接收下一个 Key 和 Value 块，以继续内循环。 我们将 N 个设备围成一个环，其中设备 i 可以向设备 i+1 以此类推，如图所示：</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5d0930a41cedf1d4e46af9baa5071f78?method=download&amp;shareKey=0e898be310f92f54f0b065a38771eb5f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5d0930a41cedf1d4e46af9baa5071f78?method=download&amp;shareKey=0e898be310f92f54f0b065a38771eb5f" alt="KV-overlap">
    </a><figcaption>KV-overlap</figcaption></figure></p>
<p>如果在设备 i 上计算内循环的一个步骤 Qi,Vj,Kj 的这段时间内，设备 i 还能向设备 i+1 发送其当前 Kj Vj，并同时从设备 i-1 接收 V_j-1,K_j-1，那么只要发送和接收密钥和值块的时间低于计算时间，那么发送和接收 Key 和 Value 块的延迟就会隐藏在执行实际计算时间之内。一个例子如下图所示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB935e1d2c0eb43c35c5c828abe8a44612?method=download&amp;shareKey=9ec41a1c178534620d9f7274ff2ce9d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB935e1d2c0eb43c35c5c828abe8a44612?method=download&amp;shareKey=9ec41a1c178534620d9f7274ff2ce9d0" alt="KV-rotate">
    </a><figcaption>KV-rotate</figcaption></figure></p>
<h1 id="memory-and-arithmetic-complexity">Memory and Arithmetic Complexity</h1>
<p>以深度学习中常用的 bfloat16 数据类型为例。GPU 或 TPU 等并行处理加速器通常以 FLOP:=F 来衡量，即设备理论上每秒可执行的浮点运算次数。我们假设硬件被完全利用。此外，我们设不同设备之间的连接带宽为:=B (Bytes/sec).</p>
<p>内存复杂度: 为了同时进行接收发送和计算，我们需要有用于接收新 KV 块的寄存器器。存储当前 KV  值块需要 2dc 浮点数或 4dc 字节。用于接收新的 KV 块的内存大小也是 2dc 浮点数或 4dc 字节。假设计算本身不需要更多内存 (利用 Flash Attention 或 Blockwise Attention)，计算当前步骤的输出需要 dc 个浮点数或 2dc 字节。此外，每个设备还需要存储其 Qi 块，这也需要 dc 个浮点数或 2dc 字节。总共需要 6dc 个浮点或 12dc 字节。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>Ring Attention 与 Flash Attention 是正交的，可以一起使用 (Flash Attention 实际上用于 Ring Attention 的内循环). Flash Attention 目标是不将整个 Score Matrix 加载到全局内存中，从而在序列长度上获得线性内存复杂度。Ring Attention 将 原始注意力方法和 Flash Attention 的内存复杂度至少降低了 N 倍，使用 N 个设备的内存复杂度至少降低 N 倍，因为它将所有矩阵都拆分为至少 N 个或更多部分 (将 QKV 分别分成 N 份，并将 Score Matrix 分成 N^2 分). 无论内存复杂度是由 QKV，还是由 Score Matrix 主导，Ring Attention 都能将内存成本降低至少 N 倍。</p></div>

<p>通信开销: 在内循环每一步中，每个设备需要通过带宽为 B 的信道向下一个设备发送 2⋅c_Q⋅d 浮点数。每个 bf16 大小为 2字节，因此，所需的时间约为 4⋅c⋅d/B.</p>
<p>运算强度： 一个内循环步骤，计算局部注意力需要 2⋅d⋅c^2 次浮点计算，计算 softmax，归一化向量和最大值向量需要 2⋅c⋅d 次浮点计算，计算局部注意力与 Vj 块的乘积需 2⋅d⋅c^2 次浮点计算。因此，总计算所需时间≈4⋅d⋅c^2/F.</p>
<p>为了重叠通信和计算 (隐藏通信开销)，我们需要 KV 块的传输时间小于等于计算本地 QKV 所需的时间：</p>
$$
4\cdot c\cdot d/B\leq4\cdot d\cdot c^2/F\iff B\geq F/c\iff s/N\geq F/B 
$$<h1 id="futher-optimization">Futher Optimization</h1>
<p>Ring Attention 的一个应用是用于因果 Transformal 模型时，加上三角形掩码用于注意力计算。这意味着有些 GPU 不需要对整个序列进行计算，导致它们大部分时间处于闲置状态。作为 Ring Attention 的扩展，<a href="https://arxiv.org/pdf/2311.09431.pdf">Stripe Attention</a> 解决了这一问题，并提供了一种分配计算更均匀的方案，从而使 Ring Attention 的计算速度更快。</p>
<p>除了 Ring Attention 和 Flash Attention 等使标准 Transformer 架构能有更长的上下文长度的技术外，人们还尝试使用 <a href="https://arxiv.org/abs/2312.00752">Mamba</a> 等具有线性注意力的状态空间模型（SSM）等模型架构。</p>
<h1 id="references">References</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://coconut-mode.com/posts/ring-attention/">https://coconut-mode.com/posts/ring-attention/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 15 Graph traversal</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch15/</link>
      <pubDate>Wed, 18 Sep 2024 16:05:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch15/</guid>
      <description>Personal notebook 15 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="15-graph-traversal">15 Graph Traversal</h1>
<p>图是一种表示实体之间关系的数据结构。所涉及的实体表示为顶点，关系表示为边。图的遍历是指从一个顶点出发，依次访问图中所有与之相邻的顶点，直到所有顶点都被访问过为止。</p>
<h2 id="151-background">15.1 Background</h2>
<p>下图展示了一个有向的简单图的例子。我们为每个顶点分配一个唯一的数字，称为顶点编号 (<em>vertex id</em>).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe6e0bafe63910bccfb3577f88479dffb?method=download&amp;shareKey=84d69b480eb0abc7389b157e34df8def" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe6e0bafe63910bccfb3577f88479dffb?method=download&amp;shareKey=84d69b480eb0abc7389b157e34df8def" alt="A Simple Graph Example with 9 Vertices and 15 Directional Edges">
    </a><figcaption>A Simple Graph Example with 9 Vertices and 15 Directional Edges</figcaption></figure></p>
<p>图的直观表示是邻接矩阵 (<em>adjacency matrix</em>). 如果存在一条从源顶点 i 到目的顶点 j 的边，则邻接矩阵元素 <code>a[i][j]</code> 的值为 1，否则为 0. 下图展示了对应的邻接矩阵。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB50516a130f544bbffa7beab784efb84a?method=download&amp;shareKey=f6419b90e7b997b86ba1213964c6672d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB50516a130f544bbffa7beab784efb84a?method=download&amp;shareKey=f6419b90e7b997b86ba1213964c6672d" alt="Adjacent Matrix Representation of the Example Graph">
    </a><figcaption>Adjacent Matrix Representation of the Example Graph</figcaption></figure></p>
<p>稀疏连接的图可以用稀疏矩阵表示，下图展示了用三种不同存储格式的邻接矩阵: CSR, CSC 和 COO. 我们将行下标和指针数组分别称为 <code>src</code> 和 <code>srcPtrs</code> 数组，列下标和指针数组分别称为 <code>dst</code> 和 <code>dstPtrs</code> 数组。在图的 CSR 表示中，每个源顶点指针(<code>srcPtrs</code>) 给出顶点出边的起始位置。在图的 CSC 表示中，每个目的顶点指针 (<code>dstPtrs</code>) 给出顶点入边的起始位置。在图的 COO 表示中，<code>src</code> 和 <code>dst</code> 数组分别存储源顶点和目的顶点的编号。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB2d0fabda0f28ddd99a4173aeba461b7d?method=download&amp;shareKey=8204d08f33f6447a23ca5a1c595ea505" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB2d0fabda0f28ddd99a4173aeba461b7d?method=download&amp;shareKey=8204d08f33f6447a23ca5a1c595ea505" alt="Three Sparse Matrix Representations of the Adjacency Matrix">
    </a><figcaption>Three Sparse Matrix Representations of the Adjacency Matrix</figcaption></figure></p>
<h2 id="152-breadth-first-search-bfs">15.2 Breadth-first Search (BFS)</h2>
<p>BFS 通常用于找到从图的一个顶点到另一个顶点所需遍历的最短边数。一种方法是，给定一个被称为根的顶点，用从根到某个顶点所需要遍历的最小边数来标记每个顶点。</p>
<p>下图(A)展示示了以顶点 0 为根的 BFS 结果。如果另一个顶点作为根，BFS 的结果将完全不同。下图(B)是为以顶点 2 为根的 BFS 的结果。可以将 BFS 的标记操作看作是构建一个搜索根节点的 BFS 树。树由所有标记的顶点和在搜索过程中从一个顶点到下一个顶点的遍历的边组成。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB86988e0b8a20689df2c463bc1a37c06a?method=download&amp;shareKey=1439c86c169aaab86a80e4bef46363e3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB86988e0b8a20689df2c463bc1a37c06a?method=download&amp;shareKey=1439c86c169aaab86a80e4bef46363e3" alt="(A and B) Two Examples of BFS Results for Two Different Root Vertices">
    </a><figcaption>(A and B) Two Examples of BFS Results for Two Different Root Vertices</figcaption></figure></p>
<p>下图展示了 BFS 在计算机辅助设计 (Computer-Aided Design, CAD) 中的一个重要应用。迷宫路由 (maze routing) 将芯片表示为图。路由块是顶点。从顶点 i 到顶点 j 的边表示可以将一条线从块 i 延伸到块 j.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB95b0ba416ed35e18d378b0a2cab1841b?method=download&amp;shareKey=83ec350ed8c87f47c217d4b32ffc5d0d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB95b0ba416ed35e18d378b0a2cab1841b?method=download&amp;shareKey=83ec350ed8c87f47c217d4b32ffc5d0d" alt="Maze Routing in Integrated Circuits">
    </a><figcaption>Maze Routing in Integrated Circuits</figcaption></figure></p>
<h2 id="153-vertex-centric-parallelization-of-bfs">15.3 Vertex-centric Parallelization of BFS</h2>
<p>以顶点为中心的并行实现将线程分配给顶点，并让每个线程对其顶点执行操作，这通常涉及迭代该顶点的邻居。当处理不同层级的迭代时，并行实现遵循相同的策略。为每一层调用一个单独的内核的原因是，我们需要等待前一层的所有顶点都被标记，然后再继续标记下一层的顶点。下面实现了一个 BFS 内核，根据前一个层级的顶点标签来标记属于该层级的所有顶点。该内核将每个线程分配给一个顶点，检查其顶点是否属于前一层。如果是，线程将遍历出边，将所有未访问的邻居标记为属于当前级别。这种以顶点为中心的实现通常被称为自顶向下或 push 实现，因为其需要访问给定源顶点的出边。多个线程可以将该标志赋值为 1，代码仍然可以正确执行。这个性质称为幂等性 (<em>idempotence</em>).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">CSRGRAPH</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">numVertices</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span><span class="o">*</span> <span class="n">scrPtrs</span><span class="p">;</span>  <span class="c1">// Strating outgoing edge index of each vertex
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span><span class="o">*</span> <span class="n">dstList</span><span class="p">;</span>  <span class="c1">// Destination vertex index of each edge
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">bfs_kernel_csr</span><span class="p">(</span><span class="n">CSRGRAPH</span> <span class="n">graph</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">level</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">visited</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">currLevel</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="n">vertexId</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">vertexId</span> <span class="o">&lt;</span> <span class="n">graph</span><span class="p">.</span><span class="n">numVertices</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">level</span><span class="p">[</span><span class="n">vertexId</span><span class="p">]</span> <span class="o">==</span> <span class="n">currLevel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">scrPtrs</span><span class="p">[</span><span class="n">vertexId</span><span class="p">];</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">graph</span><span class="p">.</span><span class="n">scrPtrs</span><span class="p">[</span><span class="n">vertexId</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">neighbor</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">dstList</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="n">level</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">==</span> <span class="mh">0xFFFFFFFF</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// unvisited neighbor
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                    <span class="n">level</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">=</span> <span class="n">currLevel</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                    <span class="n">visited</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                    <span class="o">*</span><span class="n">visited</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// flag to indicate whether reached the end of the graph
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下图展示了该内核如何执行从第 1 层 (<code>currLevel-1</code>) 到第 2 层 (<code>currLevel</code>) 的遍历。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB631a2ec2144a24f5b99cda39d3f0da53?method=download&amp;shareKey=30e9a4215568e79524f1e082a1d4c65b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB631a2ec2144a24f5b99cda39d3f0da53?method=download&amp;shareKey=30e9a4215568e79524f1e082a1d4c65b" alt="Example of a Vertex-centric Push BFS Traversal from Level 1 to Level 2">
    </a><figcaption>Example of a Vertex-centric Push BFS Traversal from Level 1 to Level 2</figcaption></figure></p>
<p>第二个以顶点为中心的并行实现将每个线程分配给一个顶点，迭代顶点的入边。每个线程首先检查其顶点是否已被访问。如果没被访问，线程将遍历入边，如果线程找到一个属于前一层的邻居，线程将把它的顶点标记为属于当前层。这种以顶点为中心的实现通常被称为自底向上或 pull 实现。实现要求能访问给定目标顶点的入边，因此要采用 CSC 表示。
以顶点为中心的 pull 实现的内核代码如下，对于一个线程来说，要确定它的顶点处于当前层，只需要该顶点有一个邻居s属于前一层中就足够了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">CSCGRAPH</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">numVertices</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span><span class="o">*</span> <span class="n">dstPtrs</span><span class="p">;</span>  <span class="c1">// Starting incoming edge index of each vertex
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span><span class="o">*</span> <span class="n">scrList</span><span class="p">;</span>  <span class="c1">// Source vertex index of each edge
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">bfs_kernel_csc</span><span class="p">(</span><span class="n">CSCGRAPH</span> <span class="n">graph</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">level</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">visited</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">currLevel</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="n">vertexId</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">vertexId</span> <span class="o">&lt;</span> <span class="n">graph</span><span class="p">.</span><span class="n">numVertices</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">level</span><span class="p">[</span><span class="n">vertexId</span><span class="p">]</span> <span class="o">==</span> <span class="mh">0xFFFFFFF</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// loop through its incoming edges if not visited
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">dstPtrs</span><span class="p">[</span><span class="n">vertexId</span><span class="p">];</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">graph</span><span class="p">.</span><span class="n">dstPtrs</span><span class="p">[</span><span class="n">vertexId</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">neighbor</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">scrList</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="n">level</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">==</span> <span class="n">currLevel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="n">level</span><span class="p">[</span><span class="n">vertexId</span><span class="p">]</span> <span class="o">=</span> <span class="n">currLevel</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                    <span class="o">*</span><span class="n">visited</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// flag to indicate whether reached the end of the graph
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                    <span class="k">break</span><span class="p">;</span>  <span class="c1">// Only need 1 neighbor in previous level to identify the vetex is currLevel
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下图展示了这个内核如何执行从第 1 层到第 2 层的遍历。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB316aa0df65a0a6249ead5e5ecc6290e9?method=download&amp;shareKey=d2f16ad21467d1bedd052c11f45ee5b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB316aa0df65a0a6249ead5e5ecc6290e9?method=download&amp;shareKey=d2f16ad21467d1bedd052c11f45ee5b3" alt="Example of a Vertex-centric Pull (bottom-up) Traversal from Level 1 to Level 2">
    </a><figcaption>Example of a Vertex-centric Pull (bottom-up) Traversal from Level 1 to Level 2</figcaption></figure></p>
<p>在比较推和拉以顶点为中心的并行实现时，需要考虑两个对性能有重要影响的关键差异。</p>
<ol>
<li>在 push 实现中，线程在其顶点的循环遍历所有邻居；而在 pull 实现中，线程可能会提前跳出循环。</li>
<li>在 push 实现中，只有被标记为前一层的顶点的线程在遍历其邻居列表；而在 pull 实现中，任何被标记为未访问顶点的线程会遍历其邻居列表。
基于两种实现的差异，常见的优化方法是对低层级使用 push 实现，然后对较高层级使用 pull 实现。这种方法通常被称为方向优化 (<em>directional optimization</em>) 实现。选择何时切换通常取决于图的类型。低度图通常有很多层；高度图中，从任何顶点到任何其他顶点只需要很少的层。因此对于高度图来说从 push 实现切换到 pull 实现通常比低度图要早得多。
如果要使用方向优化的实现，则图的 CSR 和 CSC 表示都需要储存。但对于无向图来说，其邻接矩阵是对称的，因此 CSR 和 CSC 表示是相同的的，只需要存储其中一个，就可以被两个实现使用。</li>
</ol>
<h2 id="154-edge-centric-parallelization-of-bfs">15.4 Edge-centric Parallelization of BFS</h2>
<p>在这个实现中，每个线程被分配到一条边。它检查边的源顶点是否属于前一层以及边的目标顶点是否未被访问。
以边为中心的并行实现的内核代码如下。每个线程使用 COO <code>src</code> 数组找到其边缘的源顶点，并检查顶点是否属于前一级。通过此检查的线程将使用 COO <code>dst</code> 数组确定边的目的顶点，并检查其是否未被访问过。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">COOGRAPH</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">numVertices</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">numEdges</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span><span class="o">*</span> <span class="n">srcList</span><span class="p">;</span>  <span class="c1">// Source vertex index of each edge
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span><span class="o">*</span> <span class="n">dstList</span><span class="p">;</span>  <span class="c1">// Destination vertex index of each edge
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">bfs_kernel_coo</span><span class="p">(</span><span class="n">COOGRAPH</span> <span class="n">graph</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">level</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">visited</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">currLevel</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="n">edgeId</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">edgeId</span> <span class="o">&lt;</span> <span class="n">graph</span><span class="p">.</span><span class="n">numEdges</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">src</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">srcList</span><span class="p">[</span><span class="n">edgeId</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">level</span><span class="p">[</span><span class="n">src</span><span class="p">]</span> <span class="o">==</span> <span class="n">currLevel</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">neighbor</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">dstList</span><span class="p">[</span><span class="n">edgeId</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">level</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">==</span> <span class="mh">0xFFFFFFFF</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// unvisited neighbor
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="n">level</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">=</span> <span class="n">currLevel</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="n">visited</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="o">*</span><span class="n">visited</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// flag to indicate whether reached the end of the graph
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下图展示了该内核如何执行从从第 1 层到第 2 层的遍历。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1a12c34603381779af3db3e8ff96ca11?method=download&amp;shareKey=835bfaff2b7020013da3dcc13912a00c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1a12c34603381779af3db3e8ff96ca11?method=download&amp;shareKey=835bfaff2b7020013da3dcc13912a00c" alt="Example of an Edge-centric Traversal from Level 1 to Level 2">
    </a><figcaption>Example of an Edge-centric Traversal from Level 1 to Level 2</figcaption></figure></p>
<p>与以顶点为中心的并行实现相比，以边为中心的并行实现的优点如下</p>
<ol>
<li>有更多的并行性。在以顶点为中心的实现中，如果顶点的数量很少，可能不会启动足够的线程来完全占用设备。因为一个图通常有比顶点更多的边，以边为中心的实现可以启动更多的线程。</li>
<li>具有较小的负载不平衡和控制发散。在以顶点为中心的实现中，每个线程迭代不同数量的边。相反，在以边为中心的实现中，每个线程只遍历一个边。
以边为中心的实现的缺点如下</li>
<li>需要检查图中的每条边。相反，以顶点为中心的实现中，如果确定顶点与当前层级无关，则会跳过整个边列表。</li>
<li>使用 COO 格式存储图，与以顶点为中心的实现使用的 CSR 和 CSC 相比，它需要更多的存储空间来存储边。</li>
</ol>
<h2 id="155-improving-efficiency-with-frontiers">15.5 Improving efficiency with frontiers</h2>
<p>在前两节中的方法中，我们会检查每个顶点或每条边是否属和当前层有关。这种策略的优点是内核是高度并行的，并且不需要跨线程进行任何同步。缺点是启动了许多不必要的线程，并执行了大量无用的工作。我们可以让处理前一层顶点的线程将它们访问的顶点作为 frontier. 因此，对于当前层级，只需要为该 frontier 中的顶点启动线程。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB8c15c5b4a331ade6411685dd9c0d1f1a?method=download&amp;shareKey=bd7f5e6781a9073f31fdb3005f541380" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB8c15c5b4a331ade6411685dd9c0d1f1a?method=download&amp;shareKey=bd7f5e6781a9073f31fdb3005f541380" alt="Example of a Vertex-centric Push (top-down) BFS Traversal from Level 1 to Level 2 with Frontiers">
    </a><figcaption>Example of a Vertex-centric Push (top-down) BFS Traversal from Level 1 to Level 2 with Frontiers</figcaption></figure></p>
<p>对应的内核代码如下。首先为 frontier 的每个元素分配一个线程，使用 CSR <code>srcPtrs</code> 数组来定位顶点的出边并进行迭代。对于每个出边，线程使用 CSR <code>dst</code> 数组确定其目的顶点，若未被访问过，并将其标记为属于当前层级。为了避免多个线程将邻居视为未访问，应该以原子方式执行邻居标签的检查和更新。<code>atomicCAS</code> 内置函数提供 compare-and-swap 的原子操作。如果比较成功,与其他原子操作一样，<code>atomicCAS</code> 返回存储的旧值。因此，我们可以通过比较返回值与被比较的值来检查该顶点是否被访问过。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">frontier_bfs_kernel</span><span class="p">(</span><span class="n">CSRGRAPH</span> <span class="n">graph</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">level</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">prevFroniter</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">currFroniter</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">numPrevFroniter</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">numCurrFroniter</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">currLevel</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Each thread processes a node in prevFroniter.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">numPrevFroniter</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">vertexId</span> <span class="o">=</span> <span class="n">prevFroniter</span><span class="p">[</span><span class="n">i</span><span class="p">];</span> 
</span></span><span class="line"><span class="cl">        <span class="c1">// All its neighbouring nodes are traversed.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">edge</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">scrPtrs</span><span class="p">[</span><span class="n">vertexId</span><span class="p">];</span> <span class="n">edge</span> <span class="o">&lt;</span> <span class="n">graph</span><span class="p">.</span><span class="n">scrPtrs</span><span class="p">[</span><span class="n">vertexId</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span> <span class="n">edge</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">neighbor</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">dstList</span><span class="p">[</span><span class="n">edge</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">atomicCAS</span><span class="p">(</span><span class="n">level</span> <span class="o">+</span> <span class="n">neighbor</span><span class="p">,</span> <span class="mh">0xFFFFFFFF</span><span class="p">,</span> <span class="n">currLevel</span><span class="p">)</span> <span class="o">==</span> <span class="mh">0xFFFFFFFF</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// check if neighbor is unvisited
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">currFroniterIndex</span> <span class="o">=</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="n">numCurrFroniter</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                <span class="n">currFroniter</span><span class="p">[</span><span class="n">currFroniterIndex</span><span class="p">]</span> <span class="o">=</span> <span class="n">neighbor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这种基于 frontier 的方法的优势在于，它通过只启动处理相关顶点的线程减少了冗余工作。缺点是长延迟原子操作的开销，特别是当这些操作竞争访问相同的地址时。对于 atomicAdd 操作争用会很高，因为所有线程都增加同一个计数器。</p>
<h2 id="156-reducing-contention-with-privatization">15.6 Reducing Contention with Privatization</h2>
<p>私有化可以应用于对 numCurrFrontier 的增加，以减少插入 frontier 时的争用。我们可以让每个线程块在整个计算过程中维护自己的本地 frontier，并在完成后更新全局 frontier. 本地 frontier 及其计数器可以存储在共享内存中，从而支持对计数器和存储到本地边界的低延迟原子操作。此外，当将共享内存中的 frontier 存储到全局内存中的公共 frontier 时，访问可以合并。</p>
<p>下图说明了 frontier 私有化的执行情况。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB34a0e090d131236cb469365789ba6a21?method=download&amp;shareKey=cd8a9dcd79e858b456d2bcd1d99c16c3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB34a0e090d131236cb469365789ba6a21?method=download&amp;shareKey=cd8a9dcd79e858b456d2bcd1d99c16c3" alt="Privatization of Frontiers Example">
    </a><figcaption>Privatization of Frontiers Example</figcaption></figure></p>
<p>对应的内核代码如下。注意到公共 frontiner 的索引 <code>currFrontierIdx</code> 是用 <code>currFrontierIdx_s</code> 表示的，而 <code>currFrontierIdx_s</code> 是用 <code>threadIdx.x</code> 表示的。因此，相邻线程存储到连续的全局内存位置，这意味着内存访问是合并的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define LOCAL_FRONTIER_SIZE 4
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">private_frontier_bfs_kernel</span><span class="p">(</span><span class="n">CSRGRAPH</span> <span class="n">graph</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">level</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">prevFroniter</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">currFroniter</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">numPrevFroniter</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">numCurrFroniter</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">currLevel</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Initialize privative frontier
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">currFrontier_s</span><span class="p">[</span><span class="n">LOCAL_FRONTIER_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">numCurrFrontier_s</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">numCurrFrontier_s</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Perform BFS on private frontier
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">numPrevFroniter</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">vertexId</span> <span class="o">=</span> <span class="n">prevFroniter</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">edge</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">scrPtrs</span><span class="p">[</span><span class="n">vertexId</span><span class="p">];</span> <span class="n">edge</span> <span class="o">&lt;</span> <span class="n">graph</span><span class="p">.</span><span class="n">scrPtrs</span><span class="p">[</span><span class="n">vertexId</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span> <span class="n">edge</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">neighbor</span> <span class="o">=</span> <span class="n">graph</span><span class="p">.</span><span class="n">dstList</span><span class="p">[</span><span class="n">edge</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">atomicCAS</span><span class="p">(</span><span class="n">level</span> <span class="o">+</span> <span class="n">neighbor</span><span class="p">,</span> <span class="mh">0xFFFFFFFF</span><span class="p">,</span> <span class="n">currLevel</span><span class="p">)</span> <span class="o">==</span> <span class="mh">0xFFFFFFFF</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Once a new frontier node is found,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="kt">unsigned</span> <span class="n">currFroniterIndex</span> <span class="o">=</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">numCurrFrontier_s</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="n">currFroniterIndex</span> <span class="o">&lt;</span> <span class="n">LOCAL_FRONTIER_SIZE</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Try to add it to the private frontier (currFrontier_s)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                    <span class="n">currFrontier_s</span><span class="p">[</span><span class="n">currFroniterIndex</span><span class="p">]</span> <span class="o">=</span> <span class="n">neighbor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="n">numCurrFrontier_s</span> <span class="o">=</span> <span class="n">LOCAL_FRONTIER_SIZE</span><span class="p">;</span>  <span class="c1">// frontier is full, stop adding new elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">currFrontierIdx</span> <span class="o">=</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="n">numCurrFroniter</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                    <span class="n">currFroniter</span><span class="p">[</span><span class="n">currFrontierIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">neighbor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Copy private frontier to global frontier
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">currFrontierStartIdx</span><span class="p">;</span>  <span class="c1">// Start index of private frontier in global frontier
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">currFrontierStartIdx</span> <span class="o">=</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="n">numCurrFroniter</span><span class="p">,</span> <span class="n">numCurrFrontier_s</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Commit private frontier to global frontier
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">numCurrFrontier_s</span><span class="p">;</span> <span class="n">j</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">currFroniterIdx</span> <span class="o">=</span> <span class="n">currFrontierStartIdx</span> <span class="o">+</span> <span class="n">j</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">currFroniter</span><span class="p">[</span><span class="n">currFroniterIdx</span><span class="p">]</span> <span class="o">=</span> <span class="n">currFrontier_s</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 14 Sparse Matrix Computation</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch14/</link>
      <pubDate>Wed, 18 Sep 2024 11:43:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch14/</guid>
      <description>Personal notebook 14 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="14-sparse-matrix-computation">14 Sparse Matrix Computation</h1>
<p>在稀疏矩阵中，大多数元素是零。存储和处理这些零元素在内存容量、内存带宽、时间和能量方面是浪费的。</p>
<h2 id="141-background">14.1 Background</h2>
<p>矩阵常用于求解 N 个未知数 N 个方程的线性系统，其形式为 AX+Y = 0，其中A是一个 NxN 矩阵，X 是一个 N 维的未知数向量，Y 是一个 N 维的常数向量。求解线性方程组的迭代方法中最耗时的部分是对计算 AX+Y，这是一个稀疏矩阵向量的乘法和累加。
删除所有的零元素不仅节省了存储空间，而且消除了从内存中获取这些零元素并对它们执行无用的乘法或加法操作的冗余步骤。
以下是一些在稀疏矩阵存储格式的结构中的关键考虑因素如下:</p>
<ul>
<li>空间效率 (<em>Space efficiency</em>): 使用存储格式表示矩阵所需的内存容量。</li>
<li>灵活性 (<em>Flexibility</em>): 通过添加或删除非零来修改矩阵的存储格式的方便程度•</li>
<li>可访问性 (<em>Accessibility</em>): 存储格式是否易于访问数据。</li>
<li>内存访问效率 (<em>Memory access efficiency</em>): 存储格式在多大程度上为特定计算实现了有效的内存访问模式 (正则化的一个方面).</li>
<li>负载平衡 (<em>Load balancing</em>): 存储格式在多大程度上为特定计算在不同线程之间平衡负载 (正则化的另一个方面).</li>
</ul>
<h2 id="142-a-simple-spmv-kernel-with-the-coo-format">14.2 A simple SpMV kernel with the COO format</h2>
<p>如下图所示， COO (<em>COOrdinate</em>) 格式是一种稀疏矩阵的存储格式，其中矩阵元素以三元组的形式存储，即 <code>(i, j, a_ij)</code>. 、</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBef2abaed055d77396d3fb9ef77660515?method=download&amp;shareKey=7c73fa2fae1de40aff87876e3c37e6f6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBef2abaed055d77396d3fb9ef77660515?method=download&amp;shareKey=7c73fa2fae1de40aff87876e3c37e6f6" alt="Example of the Coordinate List (COO) Format">
    </a><figcaption>Example of the Coordinate List (COO) Format</figcaption></figure></p>
<p>使用以 COO 格式表示的稀疏矩阵并行执行 SpMV (<em>Sparse Matrix Vector Multiplication</em>) 的一种方法是为矩阵中的每个非零元素分配一个线程，下图是其示意图。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBba4140a13706e68918e8a9fc953e764a?method=download&amp;shareKey=21a3ec827ee9d185f7c0c0e124b9379b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBba4140a13706e68918e8a9fc953e764a?method=download&amp;shareKey=21a3ec827ee9d185f7c0c0e124b9379b" alt="Example of Parallelizing SpMV with the COO Format">
    </a><figcaption>Example of Parallelizing SpMV with the COO Format</figcaption></figure></p>
<p>对应的内核代码如下所示，它在列索引对应的位置查找输入向量值，将其乘以非零值，然后将结果累加到对应的行索引处的输出值。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">COOMATRIX</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span><span class="o">*</span> <span class="n">rowIdx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span><span class="o">*</span> <span class="n">colIdx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span><span class="o">*</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">numNonZeros</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">spmv_coo_kernel</span><span class="p">(</span><span class="n">COOMATRIX</span> <span class="n">m</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Assign a thread to each nonzero element
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">numNonZeros</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">rowIdx</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">colIdx</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">val</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">val</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">y</span><span class="p">[</span><span class="n">row</span><span class="p">],</span> <span class="n">val</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">]);</span>  <span class="c1">// Perform the matrix-vector multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面来分析 COO 格式在几个性能指标上的表现。</p>
<ul>
<li>空间效率：COO 需要三个数组，<code>rowIdx</code>, <code>colIdx</code> 和 <code>value</code>，每个数组的元素数量与非零元素的数量相同。</li>
<li>灵活性：只要以相同的方式重新排序 <code>rowIdx</code>, <code>colIdx</code> 和 <code>value</code> 数组，就可以在不丢失任何信息的情况下任意地以 COO 格式重新排序元素。</li>
<li>可访问性方面：COO 不易访问某一行或某一列中的所有非零元素。</li>
<li>内存访问效率：相邻线程访问 COO 格式的每个数组中的相邻元素。因此，通过 SpMV/COO 对矩阵的访问是内存合并的。</li>
<li>负载平衡：由于每个线程负责计算一个非零元素，所有线程负责相同数量的工作。
<em>SpMV/COO 的主要缺点是需要使用原子操作</em>，非常耗时。</li>
</ul>
<h2 id="143-grouping-row-nonzeros-with-the-csr-format">14.3 Grouping Row Nonzeros with the CSR Format</h2>
<p>如果将同一行中的所有非零都分配给同一个线程，那么该线程将是唯一更新相应输出值的线程，则可以避免原子操作。这种可访问性可以通过 CSR (Compressed Sparse Row ) 存储格式实现。下图说明了如何使用 CSR 格式存储 14.1 节中的矩阵。CSR 也将非零值存储在一维数组中，但这些非零值是按行分组的。COO 格式和 CSR 格式之间的关键区别在于，CSR 格式用 rowPtrs 数组替换了 rowIdx 数组，rowPtrs 数组存储了 colIdx 和 value 数组中每行非零的起始偏移量，每行中的非零元素不一定按列索引排序。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB58c586fae078693686b726fe92eca4d5?method=download&amp;shareKey=011f87e0d08ea4e961006f353bf06fa7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB58c586fae078693686b726fe92eca4d5?method=download&amp;shareKey=011f87e0d08ea4e961006f353bf06fa7" alt="Example of Compressed Sparse Row (CSR) Format">
    </a><figcaption>Example of Compressed Sparse Row (CSR) Format</figcaption></figure></p>
<p>如下图所示，要使用以 CSR 格式表示的稀疏矩阵并行执行 SpMV，可以为矩阵的每一行分配一个线程。由于一个线程遍历一行，所以每个线程将输出写入不同的内存位置。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5af8c1b2e91b7e460d54f44a9fa3baaf?method=download&amp;shareKey=434cc8c983d44ceaa2c52d237d6e3c1c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5af8c1b2e91b7e460d54f44a9fa3baaf?method=download&amp;shareKey=434cc8c983d44ceaa2c52d237d6e3c1c" alt="Example of Parallelizing SpMV with the CSR Format">
    </a><figcaption>Example of Parallelizing SpMV with the CSR Format</figcaption></figure></p>
<p>对应的内核代码如下，每个线程确定它负责的行，循环遍历该行的非零元素来执行点积。线程在 <code>rowPtrs</code> 数组中确定它们的起始索引 (<code>rowPtrs[row]</code>)和通过下一行非零的起始索引 (<code>rowPtrs[row+1]</code>) 来确定结束位置。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">CSRMatrix</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span><span class="o">*</span> <span class="n">rowPtrs</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span><span class="o">*</span> <span class="n">colIdx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span><span class="o">*</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">numRows</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">spmv_csr_kernel</span><span class="p">(</span><span class="n">CSRMatrix</span> <span class="n">m</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Assign a thread to each row
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">numRows</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">start</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">rowPtrs</span><span class="p">[</span><span class="n">row</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">end</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">rowPtrs</span><span class="p">[</span><span class="n">row</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">end</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">colIdx</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">val</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">y</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>  <span class="c1">// Perform the matrix-vector multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面来分析 CSR 格式在几个性能指标上的表现。</p>
<ul>
<li>空间效率：CSR 需要三个数组，其中 <code>colIdx</code> 和 <code>value</code> 的维度和非零元素的数量一样。<code>rowPtrs</code> 维度等于行数加 1.</li>
<li>灵活性：CSR 格式中要添加的非零必须添加到它所属的特定行中。这意味着后面行的非零元素都需要移动，后面行的行指针都需要相应增加。</li>
<li>可访问性：CSR 可以很容易地访问给定行的非零元素，允许在 SpMV/CSR 中跨行并行。</li>
<li>内存访问效率：CSR 访问模式使得连相邻程访问的数据相距很远，并不能进行内存合并。</li>
<li>负载平衡：线程在点积循环中进行的迭代次数取决于分配给线程的行中非零元素的数量，因此大多数甚至所有线程中都存在控制发散。</li>
</ul>
<h2 id="144-improving-memory-coalescing-with-the-ell-format">14.4 Improving Memory Coalescing with the ELL Format</h2>
<p>ELL 存储格式通过对稀疏矩阵数据进行填充和转置，可以解决非合并内存访问的问题。它的名字来源于 ELLPACK 中的稀疏矩阵包，一个用于求解椭圆边值问题的包。
一个用 ELL 格式存储的例子如下图所示。从按行分组非零的 CSR 格式中确定具有最大非零元素数量的行。然后在所有其他行的非零元素之后的添加填充元素，使它们与最大行长度相同。最后按列主元素顺序存储填充矩阵。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB8edc4d051905f48396eed329afa0c448?method=download&amp;shareKey=88bf3affb57e04c78c662c0670721e0c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB8edc4d051905f48396eed329afa0c448?method=download&amp;shareKey=88bf3affb57e04c78c662c0670721e0c" alt="Example of ELL Storage Format">
    </a><figcaption>Example of ELL Storage Format</figcaption></figure></p>
<p>下图使用 ELL 格式并行化 SpMV。与 CSR 一样，每个线程被分配到矩阵的不同行。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBacb970fc57de19c7e45ab63353ecfdce?method=download&amp;shareKey=8b7e2e9723776c11dbf40de54b6a6075" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBacb970fc57de19c7e45ab63353ecfdce?method=download&amp;shareKey=8b7e2e9723776c11dbf40de54b6a6075" alt="Example of Parallelizing SpMV with the ELL Format">
    </a><figcaption>Example of Parallelizing SpMV with the ELL Format</figcaption></figure></p>
<p>对应的内核代码如下，点积循环遍历每行的非零元素。SpMV/ELL 内核假设输入矩阵有一个向量 <code>ellMatrix.nnzPerRow</code> 记录每行中非零的数量，每个线程只迭代其分配的行中的非零元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ELLMATRIX</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span><span class="o">*</span> <span class="n">nnzPerRow</span><span class="p">;</span>  <span class="c1">// Number of nonzeros per row
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span><span class="o">*</span> <span class="n">colIdx</span><span class="p">;</span>  <span class="c1">// Column indices of nonzeros
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">float</span><span class="o">*</span> <span class="n">val</span><span class="p">;</span>  <span class="c1">// Nonzero values
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">numRows</span><span class="p">;</span>  <span class="c1">// Number of rows
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">spmv_ell_kernel</span><span class="p">(</span><span class="n">ELLMATRIX</span> <span class="n">m</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">numRows</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">nnzPerRow</span><span class="p">[</span><span class="n">row</span><span class="p">];</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="c1">// ell matrix stores values in column-major order
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">colIdx</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">m</span><span class="p">.</span><span class="n">numRows</span> <span class="o">+</span> <span class="n">row</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">val</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">m</span><span class="p">.</span><span class="n">numRows</span> <span class="o">+</span> <span class="n">row</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">y</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>  <span class="c1">// Perform the matrix-vector multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面来分析 CSR 格式在几个性能指标上的表现。</p>
<ul>
<li>空间效率：由于填充元素的空间开销，ELL格式的空间效率低于CSR格式。</li>
<li>灵活性：ELL 格式的比 CSR 格式有更高的灵活性。只要一行没有达到矩阵中非零的最大数目，就可以通过简单地用实际值替换填充元素来向该行添加非零。</li>
<li>可访问性：ELL 可以访问某一行的非零元素。ELL 还允许在给定非零元素的索引后得到该元素的行和列索引，因为 <code>i = col*m.numRows + row</code>, 通过 <code>i % m.numRows</code> 就可以得到所在的行。</li>
<li>内存访问效率：由于元素按列主序排列，所有相邻的线程现在都访问相邻的内存位置。</li>
<li>负载平衡：SpMV/ELL 仍然和 SpMV/CSR 具有相同的负载不平衡问题，因为每个线程循环次数仍取决它负责的行中的非零元素数量。</li>
</ul>
<h2 id="145-regulating-padding-with-the-hybrid-ell-coo-format">14.5 Regulating Padding with the Hybrid ELL-COO Format</h2>
<p>在 ELL 格式中，当一行或少数行具有非常多的非零元素时，空间效率低和控制发散的问题最为明显。COO 格式可用于限制 ELL 格式中的行长度。在将稀疏矩阵转换为 ELL 之前，我们可以从具有大量非零元素的行中取出一些元素，并将这些元素用单独的 COO 格式存储。
下图展示了如何使用混合 ELL-COO 格式存储图中矩阵。从 ELL 格式中删除第二行的最后 3 个非零元素和第六行的最后 2 个非零元素，并将它们移动到单独的 COO 格式中。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa993b596c97e6f2c8d465a7d2eefee9a?method=download&amp;shareKey=558b288f71efe76be4032f0848e44ebd" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa993b596c97e6f2c8d465a7d2eefee9a?method=download&amp;shareKey=558b288f71efe76be4032f0848e44ebd" alt="Hybrid ELL-COO Example">
    </a><figcaption>Hybrid ELL-COO Example</figcaption></figure></p>
<p>对应的内核代码如下，点积将被划分为两部分处理，一部分负责处理 ELL 格式的非零元素，另一部分负责处理 COO 格式中 rowIdx 与 row 相同的非零元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">spmv_hybrid_ell_coo_kernel</span><span class="p">(</span><span class="n">ELLMATRIX</span> <span class="n">ell</span><span class="p">,</span> <span class="n">COOMATRIX</span> <span class="n">coo</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// ELL part
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">ell</span><span class="p">.</span><span class="n">numRows</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ell</span><span class="p">.</span><span class="n">nnzPerRow</span><span class="p">[</span><span class="n">row</span><span class="p">];</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">ell</span><span class="p">.</span><span class="n">colIdx</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">ell</span><span class="p">.</span><span class="n">numRows</span> <span class="o">+</span> <span class="n">row</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">ell</span><span class="p">.</span><span class="n">val</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">y</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>  <span class="c1">// Perform the matrix-vector multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">	<span class="c1">// COO part
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">coo</span><span class="p">.</span><span class="n">numNonZeros</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">coo</span><span class="p">.</span><span class="n">colIdx</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">coo</span><span class="p">.</span><span class="n">val</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">y</span><span class="p">[</span><span class="n">row</span><span class="p">],</span> <span class="n">val</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">col</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面来分析混合 ELL-COO 格式在几个性能指标上的表现。</p>
<ul>
<li>空间效率：因为减少了填充元素，混合 ELL-COO 格式比单独使用 ELL 格式的空间效率更高。</li>
<li>灵活性：混合 COO-ELL 既可以通过替换填充元素来添加非零。如果该行没有任何可以在 ELL 部分中替换的填充元素，也可以在格式的 COO 部分添加。</li>
<li>可访问性：访问给定行中所有的非零元素只能用于适合用 ELL 格式存储的部分行。</li>
<li>内存访问效率：SpMV/ELL 和 SpMV/COO 都能对稀疏矩阵进行合并内存访问。因此，它们的组合也将是合并访问模式。</li>
<li>负载平衡：从ELL 格式部分移除一些非零元素可以减少 SpMV/ELL 内核的控制发散。这些非零元素被放在 COO 格式部分，不会出现控制发散。</li>
</ul>
<h2 id="146-reducing-control-divergence-with-the-jds-format">14.6 Reducing Control Divergence with the JDS Format</h2>
<p>根据矩阵中行的非零元素夺少进行降序排序之后矩阵在很大程度上看起来像三角形矩阵，因此这种格式通常被称为 JDS (<em>Jagged Diagonal Storage</em>) 格式。
下图展示了如何使用 JDS 格式存储矩阵。首先，与 CSR 和 ELL 格式一样将非零元素按行分组。接下来，按每行中非零的个数从大到小排序。<code>value</code> 数组中的非零值及其存储其对应列索引的 <code>colIdx</code> 数组按列主元素顺序存储。在每次迭代中添加一个 <code>iterPtr</code> 数组来跟踪非零元素的开始位置。并且维护一个保留原始行索引的 <code>rowIdx</code> 数组。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb894f6329a43fbdd14f93fe7572ffaa5?method=download&amp;shareKey=c451117b781a978aa9f937f2bb65f097" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb894f6329a43fbdd14f93fe7572ffaa5?method=download&amp;shareKey=c451117b781a978aa9f937f2bb65f097" alt="Example of JDS Storage Format">
    </a><figcaption>Example of JDS Storage Format</figcaption></figure></p>
<p>对应的内核代码如下，我们一共要迭代 <code>maxNumNonZerosPerRow</code> 次，每次迭代中每个线程判断自己负责的行是否还存在非零元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">JDSMATRIX</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span><span class="o">*</span> <span class="n">iterPtr</span><span class="p">;</span>  <span class="c1">// Pointer to the start of each row in the JDS format
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span><span class="o">*</span> <span class="n">colIdx</span><span class="p">;</span>  <span class="c1">// Column indices of nonzeros
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">float</span><span class="o">*</span> <span class="n">val</span><span class="p">;</span>  <span class="c1">// Nonzero values
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span><span class="o">*</span> <span class="n">rowIdx</span><span class="p">;</span> <span class="c1">// Original row indices
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">numRows</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">maxNumNonZerosPerRow</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">spmv_jds_kernel</span><span class="p">(</span><span class="n">JDSMATRIX</span> <span class="n">m</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">numRows</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">.</span><span class="n">maxNumNonZerosPerRow</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">start</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">iterPtr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">end</span> <span class="o">=</span> <span class="n">m</span><span class="p">.</span><span class="n">iterPtr</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">end</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">sum</span> <span class="o">+=</span> <span class="n">m</span><span class="p">.</span><span class="n">val</span><span class="p">[</span><span class="n">row</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">y</span><span class="p">[</span><span class="n">m</span><span class="p">.</span><span class="n">rowIdx</span><span class="p">[</span><span class="n">row</span><span class="p">]]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>  <span class="c1">// Perform the matrix-vector multiplication
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面来分析 JDS 格式在几个性能指标上的表现。</p>
<ul>
<li>空间效率：因为避免了填充 JDS 格式比 ELL 格式效率更高。</li>
<li>灵活性：JDS 格式的灵活性较差，因为添加非零会改变行大小，这可能需要重新对行进行排序。</li>
<li>可访问性：JDS 格式类似于CSR格式，允许在给定行索引的情况下访问该行的非零元素。</li>
<li>内存访问效率：JDS 格式的内存访问效率比 ELL 格式高，因为它可以对稀疏矩阵进行合并访问。</li>
<li>负载平衡：JDS 格式对矩阵的行进行排序，使得相邻线程遍长度接近的行。因此，JDS 格式能减少控制发散。</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 13 Sorting</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch13/</link>
      <pubDate>Sat, 14 Sep 2024 13:36:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch13/</guid>
      <description>Personal notebook 13 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="13-sorting">13 Sorting</h1>
<p>排序算法将列表中的数据元素按一定的顺序排列。</p>
<h2 id="131-background">13.1 Background</h2>
<p>任何排序算法都必须满足以下两个条件:</p>
<ul>
<li>输出是非递减顺序或非递增顺序。</li>
<li>输出是输入的一种排列 (permutation).</li>
</ul>
<p>排序算法可以分为稳定算法和不稳定算法。当两个元素具有相同的键值时，稳定的排序算法保留了原始的出现顺序。
排序算法也可以分为基于比较的算法和非基于比较的算法。基于比较的排序算法无法达到比 O(NlogN) 更好的复杂度，因为它们必须在元素之间执行最少次数的比较。</p>
<h2 id="132-radix-sort">13.2 Radix Sort</h2>
<p>基数排序是一种基于非比较的排序算法，其工作原理是根据基数值将要排序的键分布到桶 (bucket) 中。如果键由多个数字组成，则重复对每个数字重复分配桶，直到覆盖所有数字。
下图展示了如何使用 1 位基数对 4 位整数列表进行基数排序。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB25b04d30be5a63b12bfdbb3093994f44?method=download&amp;shareKey=92a8bec7bdc022c628a42bd27a54086b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB25b04d30be5a63b12bfdbb3093994f44?method=download&amp;shareKey=92a8bec7bdc022c628a42bd27a54086b" alt="A Radix Sort Example">
    </a><figcaption>A Radix Sort Example</figcaption></figure></p>
<h2 id="133-parallel-radix-sort">13.3 Parallel Radix Sort</h2>
<p>基数排序的每次迭代都依赖于前一次迭代的整个结果。因此，迭代是相对于彼此顺序执行的。我们将重点关注执行单个基数排序迭代的内核的实现，并假设主机代码每次迭代调用该内核一次。
在 GPU 上并行化基数排序迭代的一种直接方法是让每个线程负责输入列表中的一个键。线程必须确定键在输出列表中的位置，然后将键存储到该位置。
下图展示了这种并行化方法第一次迭代的执行情况。对于映射到 0 桶的键，目标索引可以通过如下公式计算：
</p>
$$
\begin{align*} 
\text{destination of a zero} &=  \text{\#zeros before} \\
&=\text{\#keys before} - \text{\#ones before} \\
&=\text{key index}-\text{\#ones before}
\end{align*}
$$<p>对于映射到 1 桶的键，目标索引如下所示:</p>
$$
\begin{align*}
\text{destination of a one}&=\text{\#zeros in total}+\text{\#ones before} \\
&=(\text{\#keys in total}-\text{\#ones in total})+\text{\#ones before} \\
&=\text{input size}-\text{\#ones in total}+\text{\#ones before}
\end{align*}
$$<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa8ef27e147df1b0c4de80ee951d9f79d?method=download&amp;shareKey=c3797389c4f43d2d3ec67d06eef347f3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa8ef27e147df1b0c4de80ee951d9f79d?method=download&amp;shareKey=c3797389c4f43d2d3ec67d06eef347f3" alt="Parallelizing a Radix Sort Iteration by Assigning One Input Key to Each Thread">
    </a><figcaption>Parallelizing a Radix Sort Iteration by Assigning One Input Key to Each Thread</figcaption></figure></p>
<p>下图展示了每个线程查找其键的目标索引所执行的操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb99f77f6611178972cecd41b56175600?method=download&amp;shareKey=3c54c366645f7a1dadda9f884843f70a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb99f77f6611178972cecd41b56175600?method=download&amp;shareKey=3c54c366645f7a1dadda9f884843f70a" alt="Finding the Destination of Each Input Key">
    </a><figcaption>Finding the Destination of Each Input Key</figcaption></figure></p>
<p>对应的内核代码如下所示。在每个线程确定自己的索引并提取出对应的 bit 后，因为这些位不是 0 就是 1，所以排除扫描的结果就等于索引前面 1 的个数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">exclusiveScan</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">bits</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">temp</span><span class="p">[];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">thid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Load input into shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">temp</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="o">?</span> <span class="n">bits</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">temp</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="o">?</span> <span class="n">bits</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Build sum in place up the tree
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">d</span> <span class="o">=</span> <span class="n">N</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="n">d</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">d</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">thid</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">ai</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">bi</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">temp</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="o">+=</span> <span class="n">temp</span><span class="p">[</span><span class="n">ai</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">offset</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Clear the last element
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">thid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">temp</span><span class="p">[</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Traverse down the tree
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">d</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">offset</span> <span class="o">&gt;&gt;=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">thid</span> <span class="o">&lt;</span> <span class="n">d</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="kt">int</span> <span class="n">ai</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// left child index of the thread
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="kt">int</span> <span class="n">bi</span> <span class="o">=</span> <span class="n">offset</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// right
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="n">ai</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="n">temp</span><span class="p">[</span><span class="n">ai</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="n">bi</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">            <span class="n">temp</span><span class="p">[</span><span class="n">bi</span><span class="p">]</span> <span class="o">+=</span> <span class="n">t</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Write results to output array
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="n">bits</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="n">bits</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">thid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">radix_sort_iter</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">bits</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">iter</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">key</span><span class="p">,</span> <span class="n">bit</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">key</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="n">bit</span> <span class="o">=</span> <span class="p">(</span><span class="n">key</span> <span class="o">&gt;&gt;</span> <span class="n">iter</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="n">bits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">bit</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">exclusiveScan</span><span class="p">(</span><span class="n">bits</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>  <span class="c1">// # ones before
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">numberOnesBefore</span> <span class="o">=</span> <span class="n">bits</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">numberOnesTotal</span> <span class="o">=</span> <span class="n">bits</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">dst</span> <span class="o">=</span> <span class="p">(</span><span class="n">bit</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">numberOnesBefore</span><span class="p">)</span> <span class="o">:</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">numberOnesTotal</span> <span class="o">-</span> <span class="n">numberOnesBefore</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="n">output</span><span class="p">[</span><span class="n">dst</span><span class="p">]</span> <span class="o">=</span> <span class="n">key</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="134-optimizing-for-memory-coalescing">13.4 Optimizing for Memory Coalescing</h2>
<p>上面方法效率低下的一个主要原因是，对输出数组的写入显示出不能以内存合并的模式访问。改进后的算法如下图所示，每个块中的线程将首先执行块级别的局部排序，以分离共享内存中映射到 0 bucket 的键和映射到 1 bucket 的键。此优化中的主要挑战是每个线程块在全局 bucket 中确定其位置。线程块的 0 桶的位置在前面线程块的所有 0 桶之后。另一方面，线程块的 1 桶的位置在所有线程块的 0 桶和之前线程块的所有 1 桶之后。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf574d6d0c92edf4dc69026e429cc9f87?method=download&amp;shareKey=37e73d2ec0d91af6e330c10db179d93a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf574d6d0c92edf4dc69026e429cc9f87?method=download&amp;shareKey=37e73d2ec0d91af6e330c10db179d93a" alt="Optimizing for Memory Coalescing by Sorting Locally in Shared Memory">
    </a><figcaption>Optimizing for Memory Coalescing by Sorting Locally in Shared Memory</figcaption></figure></p>
<p>下图展示了如何使用排除扫描来查找每个线程块的本地桶的位置的。在完成局部基数排序之后，每个线程块标识其每个自己桶中键的数量。然后每个块将结果记录在如图中所示的表中，该表按行主顺序存储，对线性化的表执行排除扫描，结果表示线程块的本地 bucket 的起始位置。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBea305ebe970559e5fae76374c79ee2f7?method=download&amp;shareKey=1a6535297a6a2fbccbeed23f7a25eeb6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBea305ebe970559e5fae76374c79ee2f7?method=download&amp;shareKey=1a6535297a6a2fbccbeed23f7a25eeb6" alt="Finding the Destination of Each Thread Block&rsquo;s Local Buckets">
    </a><figcaption>Finding the Destination of Each Thread Block&#39;s Local Buckets</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define SECTION_SIZE 32
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">memory_coalescing_radix_sort</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">bits</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">table</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">iter</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">input_s</span><span class="p">[</span><span class="n">SECTION_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">output_s</span><span class="p">[</span><span class="n">SECTION_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Load input into shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">globalIdx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">globalIdx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">globalIdx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Sort each section
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">radix_sort_iter</span><span class="p">(</span><span class="n">input_s</span><span class="p">,</span> <span class="n">output_s</span><span class="p">,</span> <span class="n">bits</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">SECTION_SIZE</span><span class="p">,</span> <span class="n">SECTION_SIZE</span><span class="p">,</span> <span class="n">iter</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Store local bucket num
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">numberOnesTotal</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">numberZerosTotal</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SECTION_SIZE</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">numberOnesTotal</span> <span class="o">+=</span> <span class="n">bits</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">SECTION_SIZE</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">numberZerosTotal</span> <span class="o">=</span> <span class="n">SECTION_SIZE</span> <span class="o">-</span> <span class="n">numberOnesTotal</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">table</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">numberZerosTotal</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">table</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">numberOnesTotal</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Exclusive prefix sum to determine output index
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">exclusiveScan</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Write results to output array
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">globalIdx</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">zeroOffset</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">oneOffset</span> <span class="o">=</span> <span class="n">table</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bit</span> <span class="o">=</span> <span class="n">bits</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">SECTION_SIZE</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">dst</span> <span class="o">=</span> <span class="p">(</span><span class="n">bit</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="p">(</span><span class="n">globalIdx</span> <span class="o">-</span> <span class="n">zeroOffset</span><span class="p">)</span> <span class="o">:</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">oneOffset</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">[</span><span class="n">dst</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">globalIdx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="135-choice-of-radix-value">13.5 Choice of Radix Value</h2>
<p>使用 2 bit 的基数时，如下图所示，每次迭代使用两个比特将键分发到存储桶。因此，两次迭代就可以对 4 bit 键进行完全排序。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBaa1313e289f139eb041b02b86260874b?method=download&amp;shareKey=6a9fbd43b0fe2c1926459a7a6ba87d92" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBaa1313e289f139eb041b02b86260874b?method=download&amp;shareKey=6a9fbd43b0fe2c1926459a7a6ba87d92" alt="Radix Sort Example with 2-bit Radix">
    </a><figcaption>Radix Sort Example with 2-bit Radix</figcaption></figure></p>
<p>为了内存合并访问，如下图所示，每个线程块可以在共享内存中对其键进行本地排序，然后将每个本地桶中的键的数量写入表中。和 13.4 节一样，对于 r 位基数，对具有 2^r 行的表执行排除扫描操作。最后以合并的方式将本地 bucket 写入全局内存。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7943d691aa80d8cf1d4cc9d9055dad8c?method=download&amp;shareKey=b1c724f7996b46a5b15f81c91d6a88d3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7943d691aa80d8cf1d4cc9d9055dad8c?method=download&amp;shareKey=b1c724f7996b46a5b15f81c91d6a88d3" alt="Optimizing 2-bit Radix Sorting for Memory Coalescing Using the Shared Memory">
    </a><figcaption>Optimizing 2-bit Radix Sorting for Memory Coalescing Using the Shared Memory</figcaption></figure></p>
<p>使用更大的基数也有缺点</p>
<ol>
<li>每个线程块有更多的本地桶，每个桶有更少的键。这样就会向多个全局内存块进行写入，但每一部分写入的数据变少，不利于内存合并。</li>
<li>进行排除扫描的表会随着基数的增大而变大，扫描的开销随着基数的增加而增加。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3df7c81b854884404a6b583c1fbb99fa?method=download&amp;shareKey=82b507cb68c828d45eb81bf4b8be6e5a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3df7c81b854884404a6b583c1fbb99fa?method=download&amp;shareKey=82b507cb68c828d45eb81bf4b8be6e5a" alt="Finding the Destination of Each Block&rsquo;s Local Buckets for a 2-bit Radix">
    </a><figcaption>Finding the Destination of Each Block&#39;s Local Buckets for a 2-bit Radix</figcaption></figure></p>
<h2 id="136-thread-coarsening-to-improve-coalescing">13.6 Thread Coarsening to Improve Coalescing</h2>
<p>跨多个线程块并行化基数排序的一个代价是对全局内存的写的访问合并很差。每个线程块都有自己的本地桶，并将其写入全局内存。拥有更多的线程块意味着每个线程块拥有更少的键，这意味着本地存储桶将更小，从而在将它们写入全局内存时合并机会更少。另一个代价是执行全局排除扫描以识别每个线程块的本地桶的存储位置的开销。通过应用线程粗化，可以减少块的数量，从而减少表的大小和排除扫描操作的开销。
下图展示了如何将线程粗化应用于 2 位基数排序。每个线程被分配给输入列表中的多个键。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc83cef230dd49ea34fafa2da625e2181?method=download&amp;shareKey=551310570976f664cdbd714653b982b2" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc83cef230dd49ea34fafa2da625e2181?method=download&amp;shareKey=551310570976f664cdbd714653b982b2" alt="Radix Sort for a 2-bit Radix with Thread Coarsening">
    </a><figcaption>Radix Sort for a 2-bit Radix with Thread Coarsening</figcaption></figure></p>
<h2 id="137-parallel-merge-sort">13.7 Parallel Merge Sort</h2>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 12 Merge-An Introduction to Dynamic Input Data Identification</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch12/</link>
      <pubDate>Fri, 13 Sep 2024 22:58:03 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch12/</guid>
      <description>Personal notebook 12 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="12-merge-an-introduction-to-dynamic-input-data-identification">12 Merge-An Introduction to Dynamic Input Data Identification</h1>
<p>有序归并操作接受两个有序列表并生成一个合并后的有序列表。</p>
<h2 id="121-background">12.1 Background</h2>
<p>假设数组中的每个元素都有一个键并且键定义了一个用 ≤ 表示的顺序关系。下图展示了基于数字排序关系的简单归并函数的操作。一般来说，如果键值相等的元素在输出中的顺序与其在输入中的顺序相同，则称排序操作是稳定的。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB412d47f606b473f210d022c42a5bfb5a?method=download&amp;shareKey=615ec66182adab314c2bd9f9dbe85bc1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB412d47f606b473f210d022c42a5bfb5a?method=download&amp;shareKey=615ec66182adab314c2bd9f9dbe85bc1" alt="Example of a Merge Operation">
    </a><figcaption>Example of a Merge Operation</figcaption></figure></p>
<h2 id="122-a-sequential-merge-algorithm">12.2 A Sequential Merge Algorithm</h2>
<p>归并操作可以用如下一个简单的顺序算法来实现。顺序归并函数访问 A 和 B 的每个输入元素一次，并向 C 中每个位置写入一次。其算法复杂度为 O(m+n).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">merge_sequential</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>  <span class="c1">// Indices for A, B, and C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">while</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">++</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="o">++</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Done with A[], handling remaining B
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="k">while</span> <span class="p">(</span><span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="o">++</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="k">else</span> <span class="p">{</span>  <span class="c1">// Done with B[], handling remaining A
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="k">while</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">C</span><span class="p">[</span><span class="n">k</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">++</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="123-a-parallelization-approach">12.3 A Parallelization Approach</h2>
<p>每个线程首先确定它将要负责的输出位置范围，并使用该输出范围作为 <code>co-rank</code> 函数的输入，以确定所负责 C 输出范围的对应的 A 和 B 输入范围。这样每个线程在它们的子数组上执行顺序合并函数，从而并行地进行合并。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBcd07196d1e74562f5e44926357511c97?method=download&amp;shareKey=cdae7d4e37b42cde506885c860e9fb5b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBcd07196d1e74562f5e44926357511c97?method=download&amp;shareKey=cdae7d4e37b42cde506885c860e9fb5b" alt="Examples of Observations">
    </a><figcaption>Examples of Observations</figcaption></figure></p>
<ul>
<li><strong>Observation 1</strong>：子数组 <code>C[0]-C[k-1]</code> (k 个元素) 是 <code>A[0]-A[i-1]</code> (i 个元素) 和 <code>B[0]-B[k-i-1]</code> (k-i 个元素) 的归并结果。</li>
<li><strong>Observation 2</strong>：对于任意满足 0≤k≤m+n 的 k，我们可以找到唯一的 i 和 j 使得 k=i+j, 0≤i≤m, 0≤j≤n，并且子数组 <code>C[0]-C[k-1]</code> 是子数组 <code>A[0]-A[i-1]</code> 和子数组 <code>B[0]-B[j-1]</code> 合并的结果。唯一的索引 i 和 j 被称 <code>C[k]</code> 的 co-rank.</li>
</ul>
<p>我们可以通过将输出数组划分为子数组，并让每个线程负责一个子数组的生成来划分工作。由于并行归并算法中每个线程使用的输入元素的范围取决于实际的输入值使得我们需要辅助函数来完成。</p>
<h2 id="124-co-rank-function-implementation">12.4 Co-rank Function Implementation</h2>
<p>将 co-rank 函数定义为接受输出数组 C 中元素的位置 k 和两个输入数组 A 和 B的信息，并返回输入数组 A 对应的 co-rank 值 i.
以下图为例，假设线程 1 的 co-rank 函数的目标是为其秩 k1=4 确定 co-rank值 i1=3 和 j1=1. 也就是说，从 <code>C[4]</code> 开始的子数组将由从 <code>A[3]</code> 和  <code>B[1]</code> 开始的子数组合并生成。我们可以发现线程 t 使用的输入子数组由线程 t 和线程 t+1 的 co-rank 确定。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc24a7460cb3237cd2404a21d2f9984f9?method=download&amp;shareKey=09ff2b105b2d3b266478ce1b840d92bb" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc24a7460cb3237cd2404a21d2f9984f9?method=download&amp;shareKey=09ff2b105b2d3b266478ce1b840d92bb" alt="Example of co-rank Function Execution">
    </a><figcaption>Example of co-rank Function Execution</figcaption></figure></p>
<p>目标是找到使得 <code>A[i - 1] &lt;= B[j]</code> 并且 <code>B[j - 1] &lt;= A[i]</code> 的索引。</p>
<ul>
<li>如果 <code>A[i-1] &gt; B[j]</code>，说明 <code>A[i]</code> 太大，需要减少 i，并增加 j。</li>
<li>如果 <code>B[j-1] &gt; A[i]</code>，说明 <code>B[j]</code> 太大，需要减少 j，并增加 i。
每次调整时，i 和 j 都按照二分方式调整，即调整的步长是 delta / 2. i 和 i_low 确定了当前正在搜索的数组 A 的范围。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">co_rank</span><span class="p">(</span><span class="kt">int</span> <span class="n">k</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// C[k] comes from A[i] of B[j]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="c1">// k = i + j
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">?</span> <span class="nl">k</span> <span class="p">:</span> <span class="n">m</span><span class="p">;</span>  <span class="c1">// max starting search value for A, i.e. A[k-1] &lt; B[0]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">i_low</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">k</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">?</span> <span class="mi">0</span> <span class="o">:</span> <span class="n">k</span> <span class="o">-</span> <span class="n">n</span><span class="p">;</span>  <span class="c1">// when B is done, min starting search value for A is k-n
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">k</span> <span class="o">-</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">j_low</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">k</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">?</span> <span class="mi">0</span> <span class="o">:</span> <span class="p">(</span><span class="n">k</span> <span class="o">-</span> <span class="n">m</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">delta</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">bool</span> <span class="n">active</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">while</span> <span class="p">(</span><span class="n">active</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Binary search for C[k]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">-</span> <span class="n">i_low</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="n">j_low</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="n">j</span> <span class="o">+=</span> <span class="n">delta</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="n">i</span> <span class="o">-=</span> <span class="n">delta</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">j</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="n">j_low</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="n">i_low</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="n">i</span> <span class="o">+=</span> <span class="n">delta</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="n">j</span> <span class="o">-=</span> <span class="n">delta</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>  <span class="c1">// Found the correct position for C[k]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="n">active</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="125-a-basic-parallel-merge-kernel">12.5 A Basic Parallel Merge Kernel</h2>
<p>在剩下的小节里，我们假设输入数组 A 和 B 存储在全局内存中，一个内核被启动用来合并两个输入数组，输出一个同样位于全局内存中的数组 C.
下面内核是并行归并的直接实现。它首先通过计算当前线程 (<code>k_curr</code>) 和下一个线程 (<code>k_next</code>) 产生的输出子数组的起点来确定负责输出的范围。然后分别调用自己和后一个线程的 co_rank 函数来确定对应的 A 和 B 输入子数组的范围。最后调用顺序合并函数来合并两个输入子数组，并将结果写入输出子数组。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">mergre_basic_kernel</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Each thread handles a section of C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">elementsPerThread</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">start</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">*</span> <span class="n">elementsPerThread</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">end</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">elementsPerThread</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="n">n</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// Determin the range of A and B to be merged for this thread
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">i_curr</span> <span class="o">=</span> <span class="n">co_rank</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">i_next</span> <span class="o">=</span> <span class="n">co_rank</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">j_curr</span> <span class="o">=</span> <span class="n">start</span> <span class="o">-</span> <span class="n">i_curr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">j_next</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">i_next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="n">merge_sequential</span><span class="p">(</span><span class="n">A</span> <span class="o">+</span> <span class="n">i_curr</span><span class="p">,</span> <span class="n">B</span> <span class="o">+</span> <span class="n">j_curr</span><span class="p">,</span> <span class="n">C</span> <span class="o">+</span> <span class="n">start</span><span class="p">,</span> <span class="n">i_next</span> <span class="o">-</span> <span class="n">i_curr</span><span class="p">,</span> <span class="n">j_next</span> <span class="o">-</span> <span class="n">j_curr</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上面的基本归并内核有 2 个问题：</p>
<ol>
<li>warp 中的相邻线程在读写输入和输出子数组元素时不会访问相邻的内存位置。</li>
<li>线程在执行 co-rank 函数时还需要从全局内存访问 A 和 B 的元素。</li>
</ol>
<h2 id="126-a-tiled-merge-kernel-to-improve-coalescing">12.6 A Tiled Merge Kernel to Improve Coalescing</h2>
<p>注意到相邻线程使用的 A 和 B 子数组在内存中彼此相邻。我们可以为为每个块调用 co-rank 函数来获得其 A 和 B 子数组的起始和结束位置。
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice info" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="92 59.5 300 300">
  <path d="M292 303.25V272c0-3.516-2.734-6.25-6.25-6.25H267v-100c0-3.516-2.734-6.25-6.25-6.25h-62.5c-3.516 0-6.25 2.734-6.25 6.25V197c0 3.516 2.734 6.25 6.25 6.25H217v62.5h-18.75c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h87.5c3.516 0 6.25-2.734 6.25-6.25Zm-25-175V97c0-3.516-2.734-6.25-6.25-6.25h-37.5c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h37.5c3.516 0 6.25-2.734 6.25-6.25Zm125 81.25c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Info</p><p>回忆一下改进内核内存合并的主要策略有三种:</p>
<ul>
<li>重新组织线程到数据的映射。</li>
<li>重新组织数据本身。</li>
<li>以合并的方式在全局内存和共享内存之间传输数据，并在共享内存中执行不规则访问。</li>
</ul></div>
</p>
<p>下图展示了分段合并内核的块级别设计。A_S 和 B_S 可能无法覆盖块的整个输入子数组，因此在每次迭代期间，块中的所有线程将协作从块的 A 和 B 子数组中加载 x 个元素。这样每个块有足够的输入元素来生成至少 x 个输出数组元素 (<strong>在最坏的情况下，当前输出部分的所有元素可能都来自 A 或 B 的子数组</strong>)。假设每个块负责 y 个输出元素，则需要进行 y/x 次归并。每个块中的线程将在每次迭代中使用 A_S 的一部分和 B_S 的一部分 (深灰色部分)</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB176c6f865fd366df31615935a842b2ab?method=download&amp;shareKey=bc8caa12fb14b0e5829a1fd57c3b8a14" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB176c6f865fd366df31615935a842b2ab?method=download&amp;shareKey=bc8caa12fb14b0e5829a1fd57c3b8a14" alt="Design of a Tiled Merge Kernel">
    </a><figcaption>Design of a Tiled Merge Kernel</figcaption></figure></p>
<p>下面是分段合并内核的实现的第一部分。本质上是线程级基本合并内核的块级版本的代码。每个块的第一个线程负责计算当前块和下一个块的开始输出索引的位置以及他们的 co-rank. 结果被放入共享内存中，以便块中的所有线程都可以看到。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">merge_tiled_kernel</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">tile_size</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="cm">/* Part 1: Identify block-level output &amp; input subarrays */</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// Use extern keywords to determine 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="c1">// the shared memory size at runtime rather than compilation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">int</span> <span class="n">shared_AB</span><span class="p">[];</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span><span class="o">*</span> <span class="n">A_s</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">shared_AB</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// Start index of ShareA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span><span class="o">*</span> <span class="n">B_s</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">shared_AB</span><span class="p">[</span><span class="n">tile_size</span><span class="p">];</span>  <span class="c1">// Start index of ShareB
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">C_curr</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">ceil</span><span class="p">((</span><span class="n">m</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">);</span>  <span class="c1">// Start index of C for this block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">C_next</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">C_curr</span> <span class="o">+</span> <span class="kt">int</span><span class="p">(</span><span class="n">ceil</span><span class="p">((</span><span class="n">m</span> <span class="o">+</span> <span class="n">n</span><span class="p">)</span> <span class="o">/</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">)),</span> <span class="n">m</span> <span class="o">+</span> <span class="n">n</span><span class="p">);</span>  <span class="c1">// End index of C for this block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">A_s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">co_rank</span><span class="p">(</span><span class="n">C_curr</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>  <span class="c1">// Make block level co-rank values visible
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">A_s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">co_rank</span><span class="p">(</span><span class="n">C_next</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>  <span class="c1">// Next threads co-rank values in the block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__synctyhreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">A_curr</span> <span class="o">=</span> <span class="n">A_s</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">A_next</span> <span class="o">=</span> <span class="n">A_s</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">B_curr</span> <span class="o">=</span> <span class="n">C_curr</span> <span class="o">-</span> <span class="n">A_curr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">B_next</span> <span class="o">=</span> <span class="n">C_next</span> <span class="o">-</span> <span class="n">A_next</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>第二部分线程使用它们的 <code>threadIdx.x</code> 的值来确定要加载的元素，因此连续的线程加载连续的元素，内存访问是合并的。每次迭代从 A 和 B 数组中加载当前tile的起始点取决于块的所有线程在之前的迭代中消耗的 A 和 B 元素的总数。下图说明了 while 循环第二次迭代的索引计算。每个块在第一次迭代中消耗的 A 元素部分 为 A 子数组开头的白色小部分 (用竖条标记)。if 语句确保线程只加载 A 子数组剩余部分中的元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/* Part 2: Loading A &amp; B elements into the shared memory */</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">lenC</span> <span class="o">=</span> <span class="n">C_next</span> <span class="o">-</span> <span class="n">C_curr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">lenA</span> <span class="o">=</span> <span class="n">A_next</span> <span class="o">-</span> <span class="n">A_curr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">lenB</span> <span class="o">=</span> <span class="n">B_next</span> <span class="o">-</span> <span class="n">B_curr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">lenC</span> <span class="o">/</span> <span class="n">tile_size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// index of completed merge in 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">C_completed</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">A_completed</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">B_completed</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">while</span> <span class="p">(</span><span class="n">counter</span> <span class="o">&lt;</span> <span class="n">num_iterations</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Each iter threads in a block will generate tile_size C elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="c1">// Loading tile_size A and B elements into shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tile_size</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Coalecsing loading from global memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">lenA</span> <span class="o">-</span> <span class="n">A_completed</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">A_s</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">A_curr</span> <span class="o">+</span> <span class="n">A_completed</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">			<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">lenB</span> <span class="o">-</span> <span class="n">B_completed</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">B_s</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">B_curr</span> <span class="o">+</span> <span class="n">B_completed</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>第三部分则是每个块的线程对共享内存的数组进行归并。在更新索引的部分中最后一次迭代中 A_s 和 B_s 可能没有 tile_size 个元素，调用 co-rank 可能会得到错误结果。但是，由于 while 循环不会进一步迭代，因此不会使用结果，因此不会造成任何影响。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">		<span class="cm">/* Part 3: All threads merge their subarrays in prallel */</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">c_curr</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">tile_size</span> <span class="o">/</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">);</span>  <span class="c1">// Output index in shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="kt">int</span> <span class="n">c_next</span> <span class="o">=</span> <span class="n">c_curr</span> <span class="o">+</span> <span class="p">(</span><span class="n">tile_size</span> <span class="o">/</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="n">c_curr</span> <span class="o">=</span> <span class="p">(</span><span class="n">c_curr</span> <span class="o">&lt;=</span> <span class="n">lenC</span> <span class="o">-</span> <span class="n">C_completed</span><span class="p">)</span> <span class="o">?</span> <span class="nl">c_curr</span> <span class="p">:</span> <span class="n">lenC</span> <span class="o">-</span> <span class="n">C_completed</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="n">c_next</span> <span class="o">=</span> <span class="p">(</span><span class="n">c_next</span> <span class="o">&lt;=</span> <span class="n">lenC</span> <span class="o">-</span> <span class="n">C_completed</span><span class="p">)</span> <span class="o">?</span> <span class="nl">c_next</span> <span class="p">:</span> <span class="n">lenC</span> <span class="o">-</span> <span class="n">C_completed</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="c1">// find co-rank for c_curr and c_next
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="kt">int</span> <span class="n">a_curr</span> <span class="o">=</span> <span class="n">co_rank</span><span class="p">(</span><span class="n">c_curr</span><span class="p">,</span> <span class="n">A_s</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">tile_size</span><span class="p">,</span> <span class="n">lenA</span> <span class="o">-</span> <span class="n">A_completed</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">								<span class="n">B_s</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">tile_size</span><span class="p">,</span> <span class="n">lenB</span> <span class="o">-</span> <span class="n">B_completed</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">b_curr</span> <span class="o">=</span> <span class="n">c_curr</span> <span class="o">-</span> <span class="n">a_curr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">a_next</span> <span class="o">=</span> <span class="n">co_rank</span><span class="p">(</span><span class="n">c_next</span><span class="p">,</span> <span class="n">A_s</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">tile_size</span><span class="p">,</span> <span class="n">lenA</span> <span class="o">-</span> <span class="n">A_completed</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">								<span class="n">B_s</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">tile_size</span><span class="p">,</span> <span class="n">lenB</span> <span class="o">-</span> <span class="n">B_completed</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">b_next</span> <span class="o">=</span> <span class="n">c_next</span> <span class="o">-</span> <span class="n">a_next</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="c1">// merge the subarrays
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">merge_sequential</span><span class="p">(</span><span class="n">A_s</span> <span class="o">+</span> <span class="n">a_curr</span><span class="p">,</span> <span class="n">B_s</span> <span class="o">+</span> <span class="n">b_curr</span><span class="p">,</span> <span class="n">C</span> <span class="o">+</span> <span class="n">C_curr</span> <span class="o">+</span> <span class="n">C_completed</span> <span class="o">+</span> <span class="n">c_curr</span><span class="p">,</span> <span class="n">a_next</span> <span class="o">-</span> <span class="n">a_curr</span><span class="p">,</span> <span class="n">b_next</span> <span class="o">-</span> <span class="n">b_curr</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="c1">// Update completed indices
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">C_completed</span> <span class="o">+=</span> <span class="n">tile_size</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="n">A_completed</span> <span class="o">+=</span> <span class="n">co_rank</span><span class="p">(</span><span class="n">tile_size</span><span class="p">,</span> <span class="n">A_s</span><span class="p">,</span> <span class="n">tile_size</span><span class="p">,</span> <span class="n">B_s</span><span class="p">,</span> <span class="n">tile_size</span><span class="p">);</span>  <span class="c1">// Idx of A_s to generate tile_size Idx of merged A_s and B_s
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">B_completed</span> <span class="o">+=</span> <span class="n">tile_size</span> <span class="o">-</span> <span class="n">A_completed</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="127-a-circular-buffer-merge-kernel">12.7 A Circular Buffer Merge Kernel</h2>
<p>上一节的内核不是那么高效因为下一次迭代 tile 的一部分已经被加载到共享内存中，但是我们每次迭代从全局内存中重新加载整个块，并覆盖掉前一次迭代中的这些元素。下图展示了 merge_circular_buffer_kernel 的主要思想，添加了两个额外的变量 A_S_start 和B_S_start，使得 while 循环的每次迭代动态确定从 A 和 B 的哪个位置开始加载，这样可以利用前一次迭代中剩余的 A_s 和 B_s 元素。修改后每个 for 循环都只加载 A_S_consumed 表示的填充 tile 所需的元素数量。因此，线程在第 i 次 for 循环迭代中加载的A 元素是 <code>A[A_curr+A_S_consumed+i+threadIdx.x]</code>. 取模(%) 操作检查索引值是否大于或等于 tile_size.</p>
<p>!A Circular Buffer Scheme for Managing the Shared Memory Tiles<a href="https://note.youdao.com/yws/api/personal/file/WEB5fab19bd6ec3f60e15cd82672ed06008?method=download&amp;shareKey=8ad20192216748affc4e2f15a1b01b8d" title="A Circular Buffer Scheme for Managing the Shared Memory Tiles"></a></p>
<h2 id="128-thread-coarsening-for-merge">12.8 Thread Coarsening for Merge</h2>
<p>多个线程并行执行归并的代价是每个线程必须执行自己的二进制搜索操作来识别其输出索引的 co-rank. 本章中介绍的所有内核都已经应用了线程粗化，因为它们都是为每个线程处理多个元素而设计的。在完全未粗化的内核中，每个线程将负责单个输出元素。</p>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 11 Prefix sum (scan)-An Introduction to Work Efficiency in Parallel Algorithms</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch11/</link>
      <pubDate>Wed, 11 Sep 2024 23:03:03 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch11/</guid>
      <description>Personal notebook 11 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="11-prefix-sum-scan-an-introduction-to-work-efficiency-in-parallel-algorithms">11 Prefix sum (scan)-An Introduction to Work Efficiency in Parallel Algorithms</h1>
<p>一般来说，如果计算本质上可以被描述为数学递归，即序列中的每一项都是根据前一项定义的，那么它可能被并行化为并行扫描 (<em>parallel scan</em>) 运算。</p>
<h2 id="111-background">11.1 Background</h2>
<p>包含扫描 (<em>inclusive scan</em>) 操作接收一个二元可交换运算符 $\oplus$ 和一个包含 n 个元素的输入数组  $[x_0,x_1,\ldots,x_{n-1}]$，输出数组 $[x_0,(x_0\oplus x_1),\ldots,(x_0\oplus x_1\oplus\ldots\oplus x_{n-1})]$ . 包含扫描的名称体现在输出数组每个位置的结果都有对应输入元素参与。考虑包含扫描的一种直观方式是，接收一组所需香肠的长度的订单，并一次性得出所有所有订单对应的切割点。
排除扫描操作类似于包含扫描操作，只是输出数组的排列略有不同: $[i,x_0,(x_0\oplus x_1),\ldots,(x_0\oplus x_1\oplus\ldots\oplus x_{n-2})]$ . 每个输出元素的计算都与相应输入元素无关。
用包含扫描函数计算排除扫描的结果时，只需将所有元素向右移动，并为第 0 个元素填充恒等值。反之，只需要将所有元素向左移动，并用排除扫描结果的最后一个元素 $\oplus$ 最后一个输入元素来填充最后一个元素。</p>
<h2 id="112-parallel-scan-with-the-kogge-stone-algorithm">11.2 Parallel Scan with the Kogge-Stone Algorithm</h2>
<p>计算位置 i 的输出元素 需要进行 i 次加法运算，因此除非找到一种方法来共享不同输出元素的归约树的部分和，否则这种方法计算复杂度为 $O(N^2)$.
Kogge-Stone 算法最初是为了设计快速加法器电路而发明的，如下图所示，它是一种就地扫描算法，它对最初包含输入元素的数组 XY 进行操作。经过 k 次迭代后，<code>XY[i]</code> 将包含在该位置及之前的最多 <code>2^k</code> 个输入元素的和。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBabaa1819ea6455c00659abbdd350e12e?method=download&amp;shareKey=804e364d2be685b4e8a02798a814eb2a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBabaa1819ea6455c00659abbdd350e12e?method=download&amp;shareKey=804e364d2be685b4e8a02798a814eb2a" alt="A Parallel Inclusive Scan Algorithm Based on Kogge-Stone Adder Design">
    </a><figcaption>A Parallel Inclusive Scan Algorithm Based on Kogge-Stone Adder Design</figcaption></figure></p>
<p>对应的内核函数如下，假设输入最初位于全局内存数组 X 中。让每个线程计算其全局数据索引，即其负责计算输出数组的位置。每个个活动线程首先将其位置的部分和存储到一个临时变量中(在寄存器中)。当步幅值大于 threadIdx.x 时，意味着线程分配的 XY 位置已经累加了所有所需的输入值，退出活动状态。需要额外的 <code>temp</code> 和 <code>__syncthreads()</code> 因为更新中存在读后写数据依赖竞争关系。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define SECTION_SIZE 32
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">Kogge_Stone_Scan_Kernel</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">XY</span><span class="p">[</span><span class="n">SECTION_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="cm">/* Exclusive kernel initilization
</span></span></span><span class="line"><span class="cl"><span class="cm">	if (i &lt; N &amp;&amp; threadIdx.x != 0) {
</span></span></span><span class="line"><span class="cl"><span class="cm">		XY[threadIdx.x] = X[i];
</span></span></span><span class="line"><span class="cl"><span class="cm">	} else {
</span></span></span><span class="line"><span class="cl"><span class="cm">		XY[threadIdx.x] = 0.0f;
</span></span></span><span class="line"><span class="cl"><span class="cm">	}
</span></span></span><span class="line"><span class="cl"><span class="cm">	*/</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">stride</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">		<span class="kt">float</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">temp</span> <span class="o">=</span> <span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// write-after-read dependence
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Only N - stride threads are active
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Kogge-Stone 算法重用了横跨归约树的部分和来降低计算复杂度。在上一章的归约内核中，活动线程在迭代中写入的元素不会在同一迭代中被任何其他活动线程读取，因此不存在读后写竞争条件。如果希望避免在每次迭代中都有 barrier 同步，那么克服竞争条件的另一种方法是为输入和输出使用单独的数组。这种方法需要两个共享内存缓冲区。交替变化不能输入/输出缓冲区的角色，直到迭代完成。这种优化称为双缓冲 (<em>double buffering</em>).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define SECTION_SIZE 32
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">DF_Kogge_Stone_Scan_Kernel</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">XY_in</span><span class="p">[</span><span class="n">SECTION_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">XY_out</span><span class="p">[</span><span class="n">SECTION_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Initialization
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY_in</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY_in</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">bool</span> <span class="n">read_in</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>  <span class="c1">// Alternating ther role of XY_in and XY_out
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">stride</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">read_in</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">XY_out</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY_in</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">XY_in</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">XY_out</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY_in</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">XY_in</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY_out</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">XY_out</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">XY_in</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY_out</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">read_in</span> <span class="o">=</span> <span class="o">!</span><span class="n">read_in</span><span class="p">;</span>  <span class="c1">// 切换数组
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// 将结果写回全局内存
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">read_in</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY_in</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY_out</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="113-speed-and-work-efficiency-consideration">11.3 Speed and Work Efficiency Consideration</h2>
<p>算法的工作效率（<em>work efficiency</em>）是指算法所完成的工作接近于计算所需的最小工作量的程度。在每次迭代中，非活动线程的数量等于步长。因此我们可以计算出工作量为</p>
$$
\sum_{stride}(N-\mathrm{stride}), \text{for strides} 1, 2, 4, \ldots N/2(\mathrm{log}_2N \text{terms}) = N\log_2N - (N-1)
$$<p>因此，Kogge-Stone 算法的计算复杂度为 $O(N\log_2N)$.</p>
<p>使用计算步数 (compute steps) 的概念作为比较扫描算法的近似指标。顺序扫描用 N-1 步来处理 N 个输入元素；若 CUDA 设备有 P 个执行单元，Kogge-Stone 内核执行需要步数为 $O(N\log_2N)/P$. Kogge-Stone 内核相比串行代码所做的额外工作有两个问题。首先，使用硬件执行并行内核的效率要低得多。第二，所有额外的工作消耗额外的能量，不利于移动应用等场景。Kogge-Stone 内核的强大之处在于，当有足够的硬件资源时，它可以达到非常好的执行速度。</p>
<h2 id="114-parallel-scan-with-the-brent-kung-algorithm">11.4 Parallel Scan with the Brent-Kung Algorithm</h2>
<p>对一组值进行求和最快的方法是使用归约树，如果有足够的执行单元，就可以在 $O(N\log_2N)$  时间内计算 N 个值的求和结果。该树还可以生成几个子序列的和，它们可用于计算某些扫描输出值。
下图展示了基于 Brent-Kung 加法器设计的并行包含扫描算法的步骤。图中上半部分，花 4 步计算所有 16 个元素的和。下半部分是使用反向树将部分和分配到可以使用部分和的位置，以计算这些位置的结果。约简树中的求和总是在对一个连续的范围内的输入元素进行。因此，求和累积到 XY 的每个位置的值总是可以表示为输入元素的一个 xi…xj 的范围，其中 xi 是开始位置， xj 是结束位置 (包括)。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBda8f1aa90d4fcd75e9b2a976aec9f4c3?method=download&amp;shareKey=3887f9dfcd0ffaacbd1a9ce6a0554d38" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBda8f1aa90d4fcd75e9b2a976aec9f4c3?method=download&amp;shareKey=3887f9dfcd0ffaacbd1a9ce6a0554d38" alt="A Parallel Inclusive Scan Algorithm Based on the Brent–Kung Adder Design">
    </a><figcaption>A Parallel Inclusive Scan Algorithm Based on the Brent–Kung Adder Design</figcaption></figure></p>
<p>下图展示了反向树中每个位置 (列) 的状态，包括已经累积到该位置的值以及在反向树的每级 (行) 上需要的额外输入元素值 (浅灰色表示 2，深灰色表示 1，黑色表示 0).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBff0a3e454e8d276a662e5c76b155939b?method=download&amp;shareKey=70741f95168876e0d97cae8bd475d933" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBff0a3e454e8d276a662e5c76b155939b?method=download&amp;shareKey=70741f95168876e0d97cae8bd475d933" alt="Progression of Values in XY After Each Level of Additions in the Reverse Tree.">
    </a><figcaption>Progression of Values in XY After Each Level of Additions in the Reverse Tree.</figcaption></figure></p>
<p>上半部分归约树的内核代码如下，和第十章不同的是</p>
<ol>
<li>我们把求和结果写到最大索引的位置。</li>
<li>我们将线程索引组织成 $2^n-1$ 的形式 (n 为树的高度)。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">stride</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">((</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+=</span> <span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这种归约方式的一个缺点是存在控制发散问题。因此需要将线程的连续部分映射到索引为 $k*2^n-1$ 形式的 XY 位置。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Mapping a continous section of threads to the XY positions
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&lt;=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">stride</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// index of the left child
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">if</span> <span class="p">(</span><span class="n">index</span> <span class="o">&lt;</span> <span class="n">SECTION_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+=</span> <span class="n">XY</span><span class="p">[</span><span class="n">index</span> <span class="o">-</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>反向树的实现要复杂一些。步长从 <code>SECTION_SIZE/4</code> 减小到 1. 在每次迭代中，我们需要将 XY 元素索引值从步长减去 1 后的两倍的位置向右推到距离其一个步长的位置。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Reverse tree stride value decreases from SECTION_SIZE / 4 to 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">SECTION_SIZE</span> <span class="o">/</span> <span class="mi">4</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">stride</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>  <span class="c1">// index of the left child
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">if</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="n">SECTION_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="n">stride</span><span class="p">]</span> <span class="o">+=</span> <span class="n">XY</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以看到 Brent-Kung 算法无论在归约阶段还是分发阶段，都不需要超过 <code>SECTION_SIZE/2</code> 的线程。并行扫描中的运算总数，包括归约树 (N-1 次) 和反向树 ( $N-1-log_2N$ 次) 阶段，总共 $2N-2-log_2N$ 次。当输入长度变大时，Brent-Kung 算法执行的操作数量永远不会超过顺序算法执行的操作数量的 2 倍。</p>
<p>Brent-Kung 算法的活动线程的数量通过归约树比 Kogge-Stone 算法下降得快得多。然而，一些非活动线程可能仍然会消耗 CUDA 设备中的执行资源，因为它们通过 SIMD 绑定到其他活动线程。这使得在 CUDA 设备上前者工作效率上的优势不那么明显。在有充足执行资源的情况下，由于需要额外的步骤来执行反向树阶段，Brent-Kung 的时间是 Kogge-Stone 的两倍。</p>
<h2 id="115-coarsening-for-even-more-work-efficiency">11.5 Coarsening for Even More Work Efficiency</h2>
<p>如下图所示，粗化扫描分为三个阶段。在第一阶段，我们让每个线程对其相邻的子线程执行串行扫描。需要注意如果每个线程通过访问全局内存的输入直接执行扫描，则它们的访问不会合并。所以我们以合并的方式在共享内存和全局内存之间传输数据，并在共享内存中执行不是那么好的内存访问模式。在第二阶段，每个块中的所有线程协作并对由每个部分的最后一个元素组成的逻辑数组执行扫描操作。在第三阶段，每个线程将其前一个部分的最后一个元素的新值与自身部分除最后一个的所有元素相加。对应的内核代码如下。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB169e018f54e6c49c493de068d8a5f3f6?method=download&amp;shareKey=a54b5ab82631a6c403d2709f702f48c0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB169e018f54e6c49c493de068d8a5f3f6?method=download&amp;shareKey=a54b5ab82631a6c403d2709f702f48c0" alt="A Three-phase Parallel Scan for Higher Work Efficiency">
    </a><figcaption>A Three-phase Parallel Scan for Higher Work Efficiency</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define CORASE_FACTOR 4
</span></span></span><span class="line"><span class="cl"><span class="cp">#define SUBSECTION_SIZE (SECTION_SIZE / CORASE_FACTOR)
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">Corasened_Scan_Kernel</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Partition X into blockDim.x subsections
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">	<span class="c1">// Load X into shared memory in coalesced fashion
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">XY</span><span class="p">[</span><span class="n">SECTION_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">subXY</span><span class="p">[</span><span class="n">SUBSECTION_SIZE</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SECTION_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Part 1: Compute prefix sum of each subsection in sequenial 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SUBSECTION_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">SUBSECTION_SIZE</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">SUBSECTION_SIZE</span> <span class="o">+</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Part 2: Compute prefix sum of the last element of each subsection in parallel
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">lastElemId</span> <span class="o">=</span> <span class="p">(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">CORASE_FACTOR</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="n">subXY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY</span><span class="p">[(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">SUBSECTION_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">temp</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="n">SUBSECTION_SIZE</span><span class="p">;</span> <span class="n">stride</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">temp</span> <span class="o">=</span> <span class="n">subXY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">subXY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">subXY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Part 3: Add the reduction sum of the previous subsection to the current subsection (except the last element)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SUBSECTION_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">SUBSECTION_SIZE</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">subXY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Store back to Y
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SECTION_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">Y</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">XY</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="116-segmented-parallel-scan-for-arbitrary-length-inputs">11.6 Segmented Parallel Scan for Arbitrary-length Inputs</h2>
<p>对于长度很大的输入数据，我们首先将其划分为多个部分，以便每个部分都可以放入流多处理器的共享内存中，并由单个块进行处理。如下图所示，第一步在每个块内部先进行扫描，完成后每个扫描块的最后一个输出元素为该扫描块的所有输入元素的和。第二步将每个扫描块的最后一个结果元素收集到一个数组中，并对这些输出元素执行扫描。第三步将第二步扫描输出值与其对应扫描块的值相加。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB22da8ba64a41795b1047cae1caf6f6d3?method=download&amp;shareKey=99c2a7dcbbdeaf1be80feb644be643c0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB22da8ba64a41795b1047cae1caf6f6d3?method=download&amp;shareKey=99c2a7dcbbdeaf1be80feb644be643c0" alt="A Hierarchical Scan for Arbitrary Length Inputs">
    </a><figcaption>A Hierarchical Scan for Arbitrary Length Inputs</figcaption></figure></p>
<p>我们可以用三个内核实现分段扫描。第一个内核与 11.5 节的内核基本相同，第二个内核只是单个线程块的并行扫描内核，第三个内核将 S 数组和 Y 数组作为输入，并将其输出写回 Y.</p>
<h2 id="117-single-pass-scan-for-memory-access-efficiency">11.7 Single-pass Scan for Memory Access Efficiency</h2>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 10 Reduction and Minimizing Divergence</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch10/</link>
      <pubDate>Tue, 10 Sep 2024 21:07:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch10/</guid>
      <description>Personal notebook 10 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="10-reduction-and-minimizing-divergence">10 Reduction and Minimizing Divergence</h1>
<p>归约 (<em>Reduction</em>) 是从输入数组计算出一个数的运算。</p>
<h2 id="101-background">10.1 Background</h2>
<p>归约是从输入数组计算出一个数的运算，通常是通过对数组中的元素进行某种二元运算来实现的。如果二元操作符具有定义良好的恒等值 (例如加法中的 0，乘法中的 1)，则可以为基于该操作符进行运算的一个数组中的值定义归约操作。可以通过顺序遍历数组的每个元素来进行归约。下面伪代码为运算符的一般归约形式，它被定义为接受两个输入并返回一个值的函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">acc</span> <span class="o">=</span> <span class="n">IDENTITY</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">Operator</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="102-reduction-trees">10.2 Reduction Trees</h2>
<p>并行归约的基本思想如下图所示，时间竖直向下增加，水平方向为线程在每个时间点并行执行的活动。并行约简假定输出不随着输入值进行运算的顺序而改变 (即具有交换律)。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3cdc79f24938b60e8ec694cf1efa2314?method=download&amp;shareKey=81f05475b6bb20cc0a5a2f767e1c4d51" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3cdc79f24938b60e8ec694cf1efa2314?method=download&amp;shareKey=81f05475b6bb20cc0a5a2f767e1c4d51" alt="A Parallel Max Reduction Tree">
    </a><figcaption>A Parallel Max Reduction Tree</figcaption></figure></p>
<p>上图中的并行归约模式被称为归约树 (<em>reduction tree</em>)，因为它看起来像一棵叶子是原始输入元素，根是最终结果的树。归约树的边是无实际意义，只是反映了从一个时间步执行的操作到下一个时间步执行的操作的信息流。执行的操作总数是一个几何级数 $\frac{1}{2}N + \frac{1}{2^2}N + \cdots + \frac{1}{N}N = N-1$. 归约树需要 $log_{2}{N}$ 步骤来完成。完成计算所需的资源数量随着时间步的增加而迅速减少，每个时间步的并行度与所需的执行单元数量相同。并行度和资源消耗随着时间步长的剧烈变化让归约树成为一种具有挑战性的并行模式。</p>
<h2 id="103-a-simple-reduction-kernel">10.3 A Simple Reduction Kernel</h2>
<p>从实现一个<strong>在单个线程块内</strong>执行求和归约树的内核开始。其并行执行的情况如下图所示，假设输入数组位于全局内存中，并且在调用内核函数时将其指针作为输入参数传入。每个线程被分配到索引<code>2*threadIdx.x</code> 处，每一步归约的结果也会被写入此处。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1a32016b07d2b257c2c1aa0bdd008b25?method=download&amp;shareKey=618b57308d448ccbd4dcb5e12b391ac0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1a32016b07d2b257c2c1aa0bdd008b25?method=download&amp;shareKey=618b57308d448ccbd4dcb5e12b391ac0" alt="Threads Arrangment of the Input Array in the Simple Kernel">
    </a><figcaption>Threads Arrangment of the Input Array in the Simple Kernel</figcaption></figure></p>
<p>对应的内核代码如下所示，for 循环中的 __syncthreads() 确保任何一个线程开始下一次迭代之前，所有线程都已经完成了上一次迭代的计算。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">SimpleReductionKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// launch single block with  1/2 #elements threads
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">stride</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>  <span class="c1">// Ensure partial sums have been written to the destinition.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="o">*</span><span class="n">output</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="104-minimizing-control-divergence">10.4 Minimizing Control Divergence</h2>
<p>上面代码在每次迭代中对活动和非活动线程的管理导致了控制发散。只有那些线程的 <code>threadIdx.x</code> 为偶数的线程在第二次迭代中执行加法操作。由于控制发散造成的执行资源浪费随着迭代次数的增加而增加，第二次迭代中每个 warp 只有一半的线程执行加法操作，但消耗的计算资源却是相同的。如果输入数组的大小大于32，整个 warp 将在第五次迭代后不再执行加法操作。消耗的执行资源的总数与所有迭代中活动 warp 的总数成正比，计算方式如下。</p>
$$\text{active warps} = (5+\frac{1}{2}+\frac{1}{4}+\cdots+1)*\frac{N}{64}*32$$<p>其中 N/64 代表启动的 warp 总数。每个 warp 在前五次迭代中都处于活动状态，之后每次迭代都只有上次一半的线程在活动状态，直到只剩最后一个。
每次迭代中活动线程计算出的结果个数等于活动线程的总数</p>
$$\text{active threads} = \frac{N}{64}*(32+16+8+4+2+1)+\frac{N}{64}*\frac{1}{2}*1+\frac{N}{64}*\frac{1}{4}*1+\cdots+1$$<p>每个 warp 在前五次迭代中处于活动状态的线程数减半，之后每次迭代中每个处于活动状态的 warp 只有一个线程处于活动状态。这个结果应该非常直观的，因为其正等于完成归约所需的操作总数。
由此我们可以得出当输入大小为 256 时，执行资源利用率为 255/736 = 0.35. 如下图所示，为了减少控制分散应该安排线程和它们计算的位置使得能够随着时间的推移而彼此靠近。也就是说，我们希望步幅随着时间的推移而减少，而不是增加。修改后的内核函数如下，每次迭代中执行加法操作的线程数是相同的，但直到同时进行加法的线程数小于 32 之前，一个 warp 的线程数所走的分支相同。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB284d0ca9febf8b2fdd3644aee218b475?method=download&amp;shareKey=a97e91212ca491a84532186a3398b9a4" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB284d0ca9febf8b2fdd3644aee218b475?method=download&amp;shareKey=a97e91212ca491a84532186a3398b9a4" alt="Arrangement with Less Control Divergence">
    </a><figcaption>Arrangement with Less Control Divergence</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">ConvergentSumReductionKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Decrease stride to reduce control divergence
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="o">*</span><span class="n">output</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这种情况下的进行规约操作消耗的计算资源总数为
</p>
$$(\frac{N}{64}*1 + \frac{N}{64}*\frac{1}{2}*1 + \frac{N}{64}*\frac{1}{4}*1 + \cdots + 1 + 5*1) * 32 $$<p>
5*1 代表最后的五次迭代，只有一个活动的warp，并且它的所有32个线程都消耗执行资源，即使只有一小部分线程是活动状态。执行资源的利用率为 255/384 = 0.66.</p>
<h2 id="105-minimizing-memory-divergence">10.5 Minimizing Memory Divergence</h2>
<p>上面的内核还有内存分散的问题。在每次迭代中，每个线程对全局内存执行 2 次读取和 1 次写入。第一次从自己的位置读取，第二次从离自己 stride 的位置读取，相加后写入到自己的位置。
<a href="/blogs/courselearning/pmpp/pmpp-ch10/#10-3-A-Simple-Reduction-Kernel">10.3</a> 节的内核代码中，第一次迭代每个 warp 中的相邻线程间隔 2 个元素，因此要访问 2 个内存位置，此后每次迭代 stride 都增加，直到第六次迭代时，每个 warp 都只有一个线程处于活动状态，只用访问 1 个位置。因此进行内存访问的总次数为
</p>
$$(5*\frac{N}{64}*2+\frac{N}{64}*1+\frac{N}{64}*\frac{1}{2}*1+\cdots+1)*3$$<p>
<a href="/blogs/courselearning/pmpp/pmpp-ch10/#10-4-Minimizing-Control-Divergence">10.4</a> 节的内核代码中，每个 warp 在任何读或写时只进行一个全局内存请求，直到该 warp 中的所有线程都处于非活动状态。最后五次迭代的线程都位于一个 warp 中，因此进行内存访问的总次数为
</p>
$$((\frac{N}{64}*1+\frac{N}{64}*\frac{1}{2}*1+\frac{N}{64}*\frac{1}{4}*1+\cdots+1)+5)*3$$<p>
对于长度为 2048 的输入，前者和后者全局内存请求的总数分别为 1149 和 204. 后者在使用 DRAM 带宽方面也具有更高的效率。</p>
<h2 id="106-minimizing-global-memory-accesses">10.6 Minimizing Global Memory Accesses</h2>
<p>通过使用共享内存，可以进一步改进 <a href="/blogs/courselearning/pmpp/pmpp-ch10/#10-4-Minimizing-Control-Divergence">10.4</a> 节的内核。在每次迭代中，线程将它们的部分和结果值写入全局内存，这些值在下一次迭代中由相同的线程和其他线程重新读取。如下图所示，通过将部分和结果保存在共享内存中，可以进一步提高执行速度。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB73d407923a5daa0e9e69860be0089c98?method=download&amp;shareKey=b6e4d5ed19de5a3712f1ed1eed127674" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB73d407923a5daa0e9e69860be0089c98?method=download&amp;shareKey=b6e4d5ed19de5a3712f1ed1eed127674" alt="Use Shared Memory to Reduce Accesses from the Global Memory">
    </a><figcaption>Use Shared Memory to Reduce Accesses from the Global Memory</figcaption></figure>
对应的代码如下，每个线程从全局内存加载并 2 个输入元素并将部分和写入共享内存。剩下的所有迭代中的计算都在共享内存中进行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define BLOCK_DIM 512
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">SharedMemoryReductionKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">input_s</span><span class="p">[</span><span class="n">BLOCK_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="n">input_s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>  <span class="c1">// Partial sum of first iteration
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>  <span class="c1">// Ensure all partial sums have been written to shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">input_s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">input_s</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">stride</span><span class="p">];</span>  <span class="c1">// Partial sum of subsequent iterations
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_s</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// Write final sum to output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>全局内存访问的次数减少到初始加载输入数组和最终写入 <code>input[0]</code>，总共只有 (N/32) + 1 个全局内存请求。</p>
<h2 id="107-hierarchical-reduction-for-arbitrary-input-length">10.7 Hierarchical Reduction for Arbitrary Input Length</h2>
<p>由于 <code>__syncthreads()</code> 只对同一块中的线程有效，因此无法在不同块之间同步。下图展示了如何使用分级归约来解决这个问题，其思想是将输入数组划分为多个适合于线程块大小的段。然后，所有块都独立地执行归约树，并使用原子加法操作将它们的结果累积到最终输出。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB857e94f9f4cbf28c1f9a7980fdd83536?method=download&amp;shareKey=29a4f000e0073fa0ae1ee6e4b8e08852" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB857e94f9f4cbf28c1f9a7980fdd83536?method=download&amp;shareKey=29a4f000e0073fa0ae1ee6e4b8e08852" alt="Segmented Multiblock Reduction Using Atomic Operations">
    </a><figcaption>Segmented Multiblock Reduction Using Atomic Operations</figcaption></figure>
对应的内核代码如下。每个线程块处理 <code>2*blockDim.x</code> 个元素。在每个线程块内，我们通过线程所属块的段起始位置加上 <code>threadIdx.x</code> 为每个线程分配其输入元素的位置。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">SegmentedSumReductionKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">input_s</span><span class="p">[</span><span class="n">BLOCK_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">segment</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">;</span>  <span class="c1">// Each block processes 2*blockDim.x elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">segment</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">input_s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>  <span class="c1">// Partial sum of first iteration of each block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>  <span class="c1">// Ensure all partial sums have been written to shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">input_s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">input_s</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="n">stride</span><span class="p">];</span>  <span class="c1">// Partial sum of subsequent iterations
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">output</span><span class="p">,</span> <span class="n">input_s</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>  <span class="c1">// Write final sum to output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="108-thread-coarsening-for-reduced-overhead">10.8 Thread Coarsening for Reduced Overhead</h2>
<p>到目前为止，我们使用过的归约内核都试图通过使用尽可能多的线程来最大化并行性。若线程块大小为 1024 个线程，则需要启动的线程块数量为 N/2048.
下图展示了如何将线程粗化。线程独立地添加它们负责的四个元素，它们不需要同步，直到将所有的四个元素相加之后才能将部分和结果写入共享内存。剩下的步骤与 10.7 节后续相同。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB4358648a96bbd27f045c7beb2f736e73?method=download&amp;shareKey=770c7cecefe849a2acfe2353f538504a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB4358648a96bbd27f045c7beb2f736e73?method=download&amp;shareKey=770c7cecefe849a2acfe2353f538504a" alt="Thread Coarsening in Reduction">
    </a><figcaption>Thread Coarsening in Reduction</figcaption></figure>
对应的内核如下，我们乘以 <code>COARSE_FACTOR</code> 来表示每个线程块的负责的段的大小是原来的 <code>COARSE_FACTOR</code> 倍。部分和累加到局部变量 <code>sum</code> 中，并且因为线程是独立运行的，在循环中不会调用 <code>__syncthreads()</code>.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define COARSE_FACTOR 2
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">CoarsenedSumReductionKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">input_s</span><span class="p">[</span><span class="n">BLOCK_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">segment</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">COARSE_FACTOR</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">segment</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tile</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>  <span class="n">tile</span> <span class="o">&lt;</span> <span class="n">COARSE_FACTOR</span><span class="p">;</span> <span class="n">tile</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Partitial sum is accumulated independently
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">sum</span> <span class="o">+=</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">tile</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">input_s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">input_s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+=</span> <span class="n">input_s</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="n">stride</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">t</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">output</span><span class="p">,</span> <span class="n">input_s</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下图比较了两个原始线程块在没有进行线程粗化下被硬件顺序执行情况，图 A 当第一个线程块完成后，硬件调度第二个线程块，在不同的数据段上执行相同的步骤。图 B 的这个线程块开始需要三个步骤，其中每个线程对它负责的四个元素求和。剩下的三个步骤执行归约树，每个步骤中有一半的线程退出活动状态。相比图 A，图 B 只需要6个步骤 (而不是 8 个)，其中 3 个步骤 (而不是 2 个) 充分利用了硬件。
当我们粗化线程时，并行完成的工作就会减少。因此，增加粗化因子将减少硬件正在利用的数据并行性的数量。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa50e84d0dc2bb278fdf643ea81cfd694?method=download&amp;shareKey=c8899f1ad2ad8059daf269e68a9dac2c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa50e84d0dc2bb278fdf643ea81cfd694?method=download&amp;shareKey=c8899f1ad2ad8059daf269e68a9dac2c" alt="Comparing Parallel Reduction with and without Thread Coarsening">
    </a><figcaption>Comparing Parallel Reduction with and without Thread Coarsening</figcaption></figure></p>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 8 Stencil</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch8/</link>
      <pubDate>Mon, 09 Sep 2024 10:27:42 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch8/</guid>
      <description>Personal notebook 8 of Programming Massively Parallel Processors.</description>
      <content:encoded><![CDATA[<h1 id="8-stencil">8 Stencil</h1>
<p>在流体动力学、热传导、燃烧、天气预报、气候模拟和电磁学等应用领域，模板是求解偏微分方程的数值方法的基础。模板方法的基本思想是，将偏微分方程的求解转化为求解一个局部的线性方程组，然后在该局部进行迭代求解，最后得到全局解。由于求解微分问题时对数值精度的要求，模板处理的数据往往是高精度的浮动数据，对于 tiling 技术来说，这需要消耗更多的片上内存。</p>
<h2 id="backgroud">Backgroud</h2>
<p>用计算机数值计算和求解函数、模型、变量和方程的第一步是将它们转换成离散的表示形式。表示的保真度或这些近似插值技术的函数值的准确性取一方面决于网格点之间的间距:间距越小，近似越准确。离散表示的保真度还取决于所使用数字的精度。本章中将重点关注计算模式，其中模板应用于所有相关的输入网格点以生成所有网格点的输出值，这将被称为模板扫描 (<em>stencil sweep</em>).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB148a72c6a7b7556806a321b46ad917b7?method=download&amp;shareKey=72f1615fc5c62f3e23a68a3a7738feac" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB148a72c6a7b7556806a321b46ad917b7?method=download&amp;shareKey=72f1615fc5c62f3e23a68a3a7738feac" alt="One-dimensional Stencil Example">
    </a><figcaption>One-dimensional Stencil Example</figcaption></figure></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB71159c58a69685bed08948350c22dfcf?method=download&amp;shareKey=20517947036fd220df2744f0d4fbad08" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB71159c58a69685bed08948350c22dfcf?method=download&amp;shareKey=20517947036fd220df2744f0d4fbad08" alt="Two-dimensional &amp; Three-dimensional Stencil Example">
    </a><figcaption>Two-dimensional &amp; Three-dimensional Stencil Example</figcaption></figure></p>
<h2 id="82-parallel-stencil-a-basic-algorithm">8.2 Parallel stencil: A Basic Algorithm</h2>
<p>2D 情况下输出网格的 tiling 如下图所示，其中每个线程块负责一个 <code>4*4</code> 大小的输出 tile. 一个基本的 3D stencil 内核函数如下，其中每个线程块负责计算一个输出 tile 的值，每个线程用于计算一个元素。每个线程执行13次浮点操作 (7 次乘法和 6 次加法)，并加载 7 个输入元素 (每个 4 字节)。因此，这个内核的浮点对计算访存比是 13 / (7*4) = 0.46 OP/B.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb784ac30b171fdcf2a3ec27e6b4351dd?method=download&amp;shareKey=518a1c8267ed726cce9d05ebbe087bce" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb784ac30b171fdcf2a3ec27e6b4351dd?method=download&amp;shareKey=518a1c8267ed726cce9d05ebbe087bce" alt="2D 5-point Stencil Tiling for Output Grid">
    </a><figcaption>2D 5-point Stencil Tiling for Output Grid</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">stencil_kernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">in</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">z</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">out</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">c0</span> <span class="o">*</span> <span class="n">in</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">c1</span> <span class="o">*</span> <span class="n">in</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">c2</span> <span class="o">*</span> <span class="n">in</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">c3</span> <span class="o">*</span> <span class="n">in</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">c4</span> <span class="o">*</span> <span class="n">in</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">c5</span> <span class="o">*</span> <span class="n">in</span><span class="p">[(</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">c6</span> <span class="o">*</span> <span class="n">in</span><span class="p">[(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="83-shared-memory-tiling-for-stencil-sweep">8.3 Shared Memory Tiling for Stencil Sweep</h2>
<p>下图展示了二维五点模板的输入和输出 tile，可以发现五点模板的输入 tile 不包括四个角落的元素。因为每个输出网格点值只使用输入 tile 的 5 个元素，而 <code>3*3</code> 卷积使用 9 个元素。而 3D 情况下七点模板相对于 <code>3*3*3</code> 卷积从将输入网格点加载到共享内存中能获得的收益更低。由于为卷积加载输入 tile 的所有策略都直接应用于模板扫描，下面给出了一个加载到共享内存版本的内核函数，线程块的大小与输入 tile 相同，在计算输出 tile 点值时没有使用部分线程。每个表达式中减去的值1是因为内核假设一个3D七点模板，每边有一个网格点</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB248ae00a16c8ed3da3dc8832ced6ebc0?method=download&amp;shareKey=40a18c41ed91dacafbde0c5beac0aaf6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB248ae00a16c8ed3da3dc8832ced6ebc0?method=download&amp;shareKey=40a18c41ed91dacafbde0c5beac0aaf6" alt="Input and Output Tiles for a 2D 5-point Stencil">
    </a><figcaption>Input and Output Tiles for a 2D 5-point Stencil</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define IN_TILE_DIM 16
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">stencil_shared_mem_tiling_kernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">in</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// upper left corner of input tile
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">z</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">in_s</span><span class="p">[</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">in_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">in</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span><span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> 
</span></span><span class="line"><span class="cl">            <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&gt;=</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span><span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> 
</span></span><span class="line"><span class="cl">            <span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">&gt;=</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// 7 point template
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">c0</span> <span class="o">*</span> <span class="n">in_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">c1</span> <span class="o">*</span> <span class="n">in_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">c2</span> <span class="o">*</span> <span class="n">in_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">c3</span> <span class="o">*</span> <span class="n">in_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c4</span> <span class="o">*</span> <span class="n">in_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">c5</span> <span class="o">*</span> <span class="n">in_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c6</span> <span class="o">*</span> <span class="n">in_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>硬件限制每个块最大为 1024 ，因此 tile 通常比较小。一般 tile 的边长为8，每个块的大小为 512 个线程。相反，卷积通常用于处理二维图像，可以使用更大的 tile 尺寸 (32x32).
第一个缺点是由于 halo cell 的开销，重用率随着 tile 大小的降低而降低。第二个缺点是它对内存合并有不利影响。对于一个 8x8x8 tile，每 warp 的线程将访问全局内存中至少四行 (8<em>8</em>8*4 bytes, 32 threads, 64 bits/DRAM = 4)</p>
<h2 id="84-thread-coarsening">8.4 Thread Coarsening</h2>
<p>下图假设每个输入 tile 由 6x6x6 个网格点组成。为了使输入 tile的内部可见，块的前、左和上面没有画出。假设每个输出 tile 由 4x4x4个网格点组成。分配给处理该 tile 的线程块由与输入 tile 的一个x-y平面 (即 6x6) 相同数量的线程组成。程序一开始，每个块需要将包含计算输出块平面值所需的所有点的三个输入块平面加载到共享内存中。在每次迭代期间，块中的所有线程将处理输出 tile 与迭代值相同的 z 索引对应的 x-y 平面。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBec24cfc7edeb84a1a010b315f5ac49e0?method=download&amp;shareKey=edc686e7586e2c3b7f55e32af7bbd83d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBec24cfc7edeb84a1a010b315f5ac49e0?method=download&amp;shareKey=edc686e7586e2c3b7f55e32af7bbd83d" alt="Mapping of Shared Memory Array after First Iteration">
    </a><figcaption>Mapping of Shared Memory Array after First Iteration</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define OUT_TILE_DIM IN_TILE_DIM - 2
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">stencil_thread_coarsening_kernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">in</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">iStart</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">inPrev_s</span><span class="p">[</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">inNext_s</span><span class="p">[</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">iStart</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">iStart</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">inPrev_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">in</span><span class="p">[(</span><span class="n">iStart</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">iStart</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">iStart</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">in</span><span class="p">[</span><span class="n">iStart</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">OUT_TILE_DIM</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">i</span> <span class="o">+=</span> <span class="n">iStart</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">inNext_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">in</span><span class="p">[(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">			<span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">			<span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">			<span class="n">out</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">c0</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">				<span class="n">c1</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">c2</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">				<span class="n">c3</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c4</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">				<span class="n">c5</span> <span class="o">*</span> <span class="n">inPrev_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c6</span> <span class="o">*</span> <span class="n">inNext_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">inPrev_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">inNext_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>线程粗化内核的优点是，它不要求输入 tile 的所有平面都出现在共享内存中。在任意时刻，只有三层输入 tile 需要在共享内存中。</p>
<h2 id="85-register-tiling">8.5 Register Tiling</h2>
<p>根据计算过程可以发现每个 <code>inPrev_s</code> 和 <code>inNext_s</code> 的元素仅由一个线程在计算具有相同 x-y 索引的输出 tile 网格点时使用。只有 inCurr_s 的元素被多个线程访问，真正需要位于共享内存中。因此我们可以修改内涵函数如下，寄存器变量 <code>inPrev</code> 和 <code>inNext</code> 分别替换共享内存数组 <code>inPrev_s</code> 和 <code>inNext_s</code>. 保留了 <code>inCurr_s</code> 以允许在线程之间共享 x-y 平面相邻网格点值。这样这个内核使用的共享内存量减少到原来的 1/3.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">stencil_register_tiling_coarsening_kernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">in</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">iStart</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">inPrev</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">inCurr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">inNext</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">iStart</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">iStart</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">inPrev</span> <span class="o">=</span> <span class="n">in</span><span class="p">[(</span><span class="n">iStart</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">iStart</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">iStart</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">inCurr</span> <span class="o">=</span> <span class="n">in</span><span class="p">[</span><span class="n">iStart</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">inCurr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">OUT_TILE_DIM</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">i</span> <span class="o">+=</span> <span class="n">iStart</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">inNext</span> <span class="o">=</span> <span class="n">in</span><span class="p">[(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">			<span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">			<span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">			<span class="n">out</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">c0</span> <span class="o">*</span> <span class="n">inCurr</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">				<span class="n">c1</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">c2</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">				<span class="n">c3</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">c4</span> <span class="o">*</span> <span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">				<span class="n">c5</span> <span class="o">*</span> <span class="n">inPrev</span> <span class="o">+</span> <span class="n">c6</span> <span class="o">*</span> <span class="n">inNext</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">	<span class="n">inPrev</span> <span class="o">=</span> <span class="n">inCurr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="n">inCurr</span> <span class="o">=</span> <span class="n">inNext</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="n">inCurr_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">inNext</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>首先，许多对共享内存的读写现在被转移到寄存器中。其次，每个块只消耗三分之一的共享内存。当然，这是以每个线程多使用 3 个寄存器为代价实现的。需要注意<strong>全局内存访问的数量没有改变</strong>。</p>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 9 Parallel Histogram-An Introduction to Atomic Operations and Privatization</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch9/</link>
      <pubDate>Mon, 09 Sep 2024 10:27:42 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch9/</guid>
      <description>Personal notebook 9 of Programming Massively Parallel Processors.</description>
      <content:encoded><![CDATA[<h1 id="9-parallel-histogram-an-introduction-to-atomic-operations-and-privatization">9 Parallel Histogram-An Introduction to Atomic Operations and Privatization</h1>
<p>本章介绍并行直方图计算模式，其中每个输出元素都可以由任何线程更新。因此，当线程更新输出元素时必须注意线程之间的协调，避免任何可能破坏最终结果的干扰。</p>
<h2 id="91-background">9.1 Background</h2>
<p>直方图是数据集中数据值出现的数量计数或百分比的显示。在最常见的直方图形式中，间隔区间沿水平轴绘制，每个间隔中的数据值计数表示为从水平轴上升的矩形或条形的高度。
许多应用领域依赖于直方图来总结数据集进行数据分析。其中一个领域就是计算机视觉。图像子区域直方图的计算过程是计算机视觉中特征 (图像中感兴趣的模式) 提取的重要方法。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf1b2ad27e9184b6ecf628e068c42e7e4?method=download&amp;shareKey=a44ec6cce3fffb07208a7a67ee8005a5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf1b2ad27e9184b6ecf628e068c42e7e4?method=download&amp;shareKey=a44ec6cce3fffb07208a7a67ee8005a5" alt="A Histogram Representation of  “programming massively parallel processors”">
    </a><figcaption>A Histogram Representation of  “programming massively parallel processors”</figcaption></figure></p>
<h2 id="92-atomic-operations-and-a-basic-histogram-kernel">9.2 Atomic Operations and A Basic Histogram Kernel</h2>
<p>如下图所示，并行化直方图计算的最直接的方法是启动数据一样多的线程，让每个线程处理一个元素。每个线程读取其分配的输入元素，并增加对应的隔计数器的值。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBd162f505e52265f5421f0fa883e5d19b?method=download&amp;shareKey=4d8e366384fcc19aebe518ad8fecdc7d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBd162f505e52265f5421f0fa883e5d19b?method=download&amp;shareKey=4d8e366384fcc19aebe518ad8fecdc7d" alt="Basic Parallelization of a Histogram">
    </a><figcaption>Basic Parallelization of a Histogram</figcaption></figure></p>
<p>histo 数组中间隔计数器的增加是对内存位置的更新或 read-modify-write 操作。该操作包括读取内存位置(读)，在原始值上加 1(修改)，并将新值写回内存位置 (写)。在实际过程中会出现读-修改-写竞争条件 (<em>read-modify-write race condition</em>)，在这种情况下，两个或多个同步更新操作的结果会根据所涉及的操作的相对时间而变化。
下图 A 中线程 1 在时间段 1~3 期间完成了其读-修改-写序列的所有三个部分，然后线程 2 在时间段 4 开始，最后结果正确。在图 B 中，两个线程的读-修改-写顺序重叠。线程 1 在时间段 4 时将新值写入 <code>histo[x]</code>。当线程 2 在时间段 3 读取 <code>histo[x]</code>时，它的值仍然是 0，因此最后的写入的值是 1.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfcb1b1249b8c3eaa4b079cc3c6211f60?method=download&amp;shareKey=6c860292730da34f80c4f3020f5e709c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfcb1b1249b8c3eaa4b079cc3c6211f60?method=download&amp;shareKey=6c860292730da34f80c4f3020f5e709c" alt="Race Condition in Updating a histo Array Element">
    </a><figcaption>Race Condition in Updating a histo Array Element</figcaption></figure></p>
<p>原子操作 (<em>atomic operation</em>) 的读、修改和写部分构成一个不可分割的单元，因此称为原子操作。对该位置的其他读-修改-写序列不能与其在时间上有重叠。需要注意<em>原子操作在线程之间不强制任何特定的执行顺序</em>，比如线程 1 可以在线程 2 之前或之后运行。CUDA内核可以通过函数调用对内存位置执行原子加法操作:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">atomicAdd</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">address</span><span class="p">,</span> <span class="kt">int</span> <span class="n">val</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>atomicAdd</code> 是一个内建函数 (intrinsic function)，它被编译成一个硬件原子操作指令。该指令读取全局或共享内存中 <code>address</code> 参数所指向的32位字，将 <code>val</code> 加上旧值中并写入结果回相同地址的内存中。该函数返回地址处的旧值。</p>
<details class="custom-details">
    <summary class="custom-summary">Intrinsic Functions</summary>
    <div>现代处理器通常提供特殊指令，这些指令要么执行关键功能 (如原子操作)，要么大幅提高性能 (如矢量指令)。这些指令通常作为内建函数暴露给程序员，从程序员的角度来看，这些是库函数。然而，它们被编译器以一种特殊的方式处理。每个这样的调用都被翻译成相应的特殊指令。在最终代码中没有函数调用，只有与用户代码一致的特殊指令。</div>
</details><br>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">histo_kernel</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">histo</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">alphabet_position</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="sc">&#39;a&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">alphabet_position</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">alphabet_position</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo</span><span class="p">[</span><span class="n">alphabet_position</span> <span class="o">/</span> <span class="mi">4</span><span class="p">],</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="93-latency-and-throughput-of-atomic-operations">9.3 Latency and Throughput of Atomic Operations</h2>
<p>高内存访问吞吐量的关键是同时进行许多 DRAM 访问。然而，当许多原子操作更新相同的内存位置时，一个后面线程的读-修改-写序列在前一个线程的写操作结束之前不能开始，即如下图所示，同时只能有一个线程在同一内存位置执行原子操作。更新这些间隔的大量争用流量会使得吞吐量降低。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe7de6cb4432d570997a5d51a354269df?method=download&amp;shareKey=244ed14ff414262053d92e3b884321b4" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe7de6cb4432d570997a5d51a354269df?method=download&amp;shareKey=244ed14ff414262053d92e3b884321b4" alt="The Execution of Atomic Operations at the Same Location">
    </a><figcaption>The Execution of Atomic Operations at the Same Location</figcaption></figure></p>
<p>提高原子操作吞吐量的一种方法是减少对竞争严重的位置的访问延迟。现代 GPU 允许在被所有 SM 共享的最后一级缓存中执行原子操作。由于对最后一级缓存的访问时间是几十个周期而不是几百个周期，因此原子操作的吞吐量与早期GPU相比至少提高了一个数量级。</p>
<h2 id="94-privatization">9.4 Privatization</h2>
<p>提高原子操作吞吐量的另一种方法是通过引导流量远离竞争严重的位置。这可以通过一种称为私有化 (<em>privatization</em>) 的技术来实现。其思想是将高度竞争的输出数据结构复制到私有副本中，以便线程的每个子集都可以更新其私有副本。
下图展示了如何将私有化应用于直方图统计。每个线程块由 8 个线程组成，争用只会在同一块中的线程之间以及在最后合并私有副本时发生，而不是更新相同直方图 bin 的所有线程之间发生争用。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb7aa2df247e5dc432476efa8b601df20?method=download&amp;shareKey=82f3b5074429b6bf29ca79a092a0044d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb7aa2df247e5dc432476efa8b601df20?method=download&amp;shareKey=82f3b5074429b6bf29ca79a092a0044d" alt="Reduce Contention of Atomic Operations by Private Copies of Histogram">
    </a><figcaption>Reduce Contention of Atomic Operations by Private Copies of Histogram</figcaption></figure></p>
<p>一个私有化版本的代码如下，为 histo 数组分配足够的设备内存 (<code>gridDim.x*NUM_BINS*4</code> bytes) 来保存直方图的所有私有副本。在执行结束时，每个线程块将把私有副本中的值提交到 块 0 的部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define NUM_BINS 7  </span><span class="c1">// # histo bins 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">histo_private_kernel</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">histo</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">alphabet_position</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="sc">&#39;a&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">alphabet_position</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">alphabet_position</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo</span><span class="p">[</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">+</span> <span class="n">alphabet_position</span> <span class="o">/</span> <span class="mi">4</span><span class="p">],</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">		<span class="c1">//
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span> 
</span></span><span class="line"><span class="cl">			<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">binValue</span> <span class="o">=</span> <span class="n">histo</span><span class="p">[</span><span class="n">blockIdx</span> <span class="o">*</span> <span class="n">NUM_BINS</span> <span class="o">+</span> <span class="n">bin</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo</span><span class="p">[</span><span class="n">bin</span><span class="p">],</span> <span class="n">binValue</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在每个线程块的基础上创建直方图的私有副本的一个好处是线程可以在提交自己的统计结果之前使用 <code>__syncthreads()</code> 来等待彼此。另一个好处是，如果直方图中的 bin 数量足够小，则可以在共享内存中声明直方图的私有副本 (每个线程块一个)。下面代码直方图在共享内存中分配私有副本 <code>histo_s</code> 数组，并由块的线程并行初始化为 0.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">histo_shared_private_kernel</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">histo</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Initializing private bins
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">histo_s</span><span class="p">[</span><span class="n">NUM_BINS</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">histo_s</span><span class="p">[</span><span class="n">bin</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Histogram
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">alphabet_position</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="sc">&#39;a&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">alphabet_position</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">alphabet_position</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo_s</span><span class="p">[</span><span class="n">alphabet_position</span> <span class="o">/</span> <span class="mi">4</span><span class="p">],</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Commit to global memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">unsigned</span> <span class="n">binValue</span> <span class="o">=</span> <span class="n">histo_s</span><span class="p">[</span><span class="n">bin</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">binValue</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo</span><span class="p">[</span><span class="n">bin</span><span class="p">],</span> <span class="n">binValue</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="95-coarsening">9.5 Coarsening</h2>
<p>私有化的开销是需要将私有副本提交到公共副本。每个线程块都会执行一次提交操作，因此，使用的线程块越多，这个开销就越大。如下图所示，我们可以通过减少块的数量来减少私有副本的数量，从而减少提交到公共副本的次数，让每个线程处理多个输入元素。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB685390a635126cba569f8c85254bcfc5?method=download&amp;shareKey=bf894427e4a2c0fdce32f9bf00094c52" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB685390a635126cba569f8c85254bcfc5?method=download&amp;shareKey=bf894427e4a2c0fdce32f9bf00094c52" alt="Contiguous Partition of Input Elements">
    </a><figcaption>Contiguous Partition of Input Elements</figcaption></figure></p>
<p>下面代码是一个连续分区 (<em>contiguous partition</em>) 策略的示例，输入被连续划分成多个段，每个段被分配给一个线程，每个线程从 <code>tid*CFACTOR</code> 迭代到 <code>(tid+1)*CFACTOR</code> 进行所负责部分的统计。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define CFACTOR 3
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">histo_shared_private_contiguous_kernel</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">histo</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Initializing private bins
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">histo_s</span><span class="p">[</span><span class="n">NUM_BINS</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">histo_s</span><span class="p">[</span><span class="n">bin</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Histogram
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">tid</span> <span class="o">*</span> <span class="n">CFACTOR</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">tid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">CFACTOR</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">alphabet_position</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="sc">&#39;a&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">alphabet_position</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">alphabet_position</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo_s</span><span class="p">[</span><span class="n">alphabet_position</span> <span class="o">/</span> <span class="mi">4</span><span class="p">],</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Commit to global memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">unsigned</span> <span class="n">binValue</span> <span class="o">=</span> <span class="n">histo_s</span><span class="p">[</span><span class="n">bin</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">binValue</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo</span><span class="p">[</span><span class="n">bin</span><span class="p">],</span> <span class="n">binValue</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上述在 GPU 上连续分区的思路会导致内存不友好的访问模式，因为 threadIdx 相同的线程访问的不是一块连续的内存区域。因此我们要采用交错分区 (<em>interleaved partition</em>)，如下图所示，即不同线程要处理的分区彼此交错。实际应用中每个线程在每次迭代中应该处理 4 个 char (一个 32 位字)，以充分利用缓存和 SMs 之间的互连带宽。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB166097c6dec3a7d7eed2c82d5706bf55?method=download&amp;shareKey=79fb19c70cf16d76fdc9113eeefd12e8" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB166097c6dec3a7d7eed2c82d5706bf55?method=download&amp;shareKey=79fb19c70cf16d76fdc9113eeefd12e8" alt="Interleaved Partition of Input Elements">
    </a><figcaption>Interleaved Partition of Input Elements</figcaption></figure></p>
<p>下面代码是一个交错分区的示例。在循环的第一次迭代中，每个线程使用其全局线程索引访问数据数组:线程 0 访问元素 0，线程 1 访问元素 1，线程 2 访问元素 2&hellip;所有线程共同处理输入的第一个 <code>blockDim.x*gridDim.x</code> 元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">histo_shared_private_interleaved_kernel</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">histo</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Initializing private bins
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">histo_s</span><span class="p">[</span><span class="n">NUM_BINS</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">histo_s</span><span class="p">[</span><span class="n">bin</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Histogram
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">tid</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">alphabet_position</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="sc">&#39;a&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">alphabet_position</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">alphabet_position</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo_s</span><span class="p">[</span><span class="n">alphabet_position</span> <span class="o">/</span> <span class="mi">4</span><span class="p">],</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Commit to global memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">unsigned</span> <span class="n">binValue</span> <span class="o">=</span> <span class="n">histo_s</span><span class="p">[</span><span class="n">bin</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">binValue</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo</span><span class="p">[</span><span class="n">bin</span><span class="p">],</span> <span class="n">binValue</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="96-aggregation">9.6 Aggregation</h2>
<p>一些数据集在局部区域有大量相同的数据值。如此高度集中的相同值会导致严重的争用，并降低并行直方图计算的吞吐量。一个简单而有效的优化是，如果每个线程正在更新直方图的相同元素，则将连续的更新聚合为单个更新。下面的代码展示了聚合的直方图计算。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">histo_shared_private_interleaved_aggregated_kernel</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">length</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="o">*</span> <span class="n">histo</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Initializing private bins
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">histo_s</span><span class="p">[</span><span class="n">NUM_BINS</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">histo_s</span><span class="p">[</span><span class="n">bin</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Histogram
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">accumulator</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">prevBinIdx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">unsigned</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">tid</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">int</span> <span class="n">alphabet_position</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="sc">&#39;a&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">alphabet_position</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">alphabet_position</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">currBinIdx</span> <span class="o">=</span> <span class="n">alphabet_position</span> <span class="o">/</span> <span class="mi">4</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="k">if</span> <span class="p">(</span><span class="n">currBinIdx</span> <span class="o">!=</span> <span class="n">prevBinIdx</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Update previous statistics
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>				<span class="k">if</span> <span class="p">(</span><span class="n">accumulator</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">					<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo_s</span><span class="p">[</span><span class="n">prevBinIdx</span><span class="p">],</span> <span class="n">accumulator</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">				<span class="p">}</span>
</span></span><span class="line"><span class="cl">				<span class="n">accumulator</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">				<span class="n">prevBinIdx</span> <span class="o">=</span> <span class="n">currBinIdx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>  <span class="c1">// Accumulate statistics
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>				<span class="n">accumulator</span><span class="o">++</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">accumulator</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// Update last bin
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo_s</span><span class="p">[</span><span class="n">prevBinIdx</span><span class="p">],</span> <span class="n">accumulator</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Commit to global memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">bin</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">bin</span> <span class="o">&lt;</span> <span class="n">NUM_BINS</span><span class="p">;</span> <span class="n">bin</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">unsigned</span> <span class="n">binValue</span> <span class="o">=</span> <span class="n">histo_s</span><span class="p">[</span><span class="n">bin</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">binValue</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">histo</span><span class="p">[</span><span class="n">bin</span><span class="p">],</span> <span class="n">binValue</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看出聚合内核需要更多的语句和变量。添加的 if 语句可能会出现控制发散。然而，如果没有争用或存在严重的争用，就很少有控制发散，因为线程要么都在增加累加器值，要么都在连续刷新。</p>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 7 Convolution-An Introduction to Constant Memory and Caching</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch7/</link>
      <pubDate>Fri, 06 Sep 2024 10:50:42 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch7/</guid>
      <description>Personal notebook 7 of Programming Massively Parallel Processors.</description>
      <content:encoded><![CDATA[<h1 id="7-convolution-an-introduction-to-constant-memory-and-caching">7 Convolution-An Introduction to Constant Memory and Caching</h1>
<p>卷积的每个输出数据元素可以相互独立地计算，这是并行计算的理想特性。另一方面，在处理具有边界条件的输出数据元素时，有大量的输入数据共享。这使得卷积可以实现复杂的 tiling 方法和输入数据分段方法。</p>
<h2 id="71-background">7.1 Background</h2>
<p>输入数据向量 $[x_0, x_1, \cdots, x_{n-1}]$ 和包含 2r+1 个元素的 filter 数组 $[f_0, f_1, \cdots, f_{2r}]$， 1D卷积计算公式为
</p>
$$y_i=\sum_{j=-r}^rf_{i+j}\times x_i$$<p>
同样对于 <code>n*n</code> 大小的二维输入，和 <code>r*r</code> 大小的 filter，2D 卷积计算公式为
</p>
$$P_{y,x}=\sum_{j=-r_y}^{r_y}\sum_{k=-r_x}^{r_x}f_{y+j,x+k}\times N_{y,x}$$<h2 id="72-parallel-convolution-a-basic-algorithm">7.2 Parallel Convolution: a Basic Algorithm</h2>
<p>假设二维卷积内核接收五个参数: 输入数组 N 的指针; 滤波器 F 的指针; 输出数组 P 的指针; 方形滤波器的半径 r; 输入输出数组的宽度; 输入和输出数组的高度。如下图所示，一个简单的并行方式是网格中的每个线程计算与自身坐标相同的输出像素。对应的内核函数代码如下，浮点计算与全局内存访问的比仅为 0.25 OP/B (每加载 8 字节执行 2 次运算)</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb705cd006867704636e9e5261467570f?method=download&amp;shareKey=d5710b7dee0a3c91d67011d92b623557" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb705cd006867704636e9e5261467570f?method=download&amp;shareKey=d5710b7dee0a3c91d67011d92b623557" alt="Parallelization and Thread Organization for 2D Convolution">
    </a><figcaption>Parallelization and Thread Organization for 2D Convolution</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">convolution_2D_basic_kernel</span> <span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">N</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">F</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">P</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">									<span class="kt">int</span> <span class="n">r</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">outCol</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">outRow</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">Pvalue</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">fRow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fRow</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="o">*</span><span class="n">r</span><span class="o">+</span><span class="mi">1</span><span class="p">;</span> <span class="n">fRow</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">fCol</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fCol</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">r</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">fCol</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">inRow</span> <span class="o">=</span> <span class="n">outRow</span> <span class="o">-</span> <span class="n">r</span> <span class="o">+</span> <span class="n">fRow</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">inCol</span> <span class="o">=</span> <span class="n">outCol</span> <span class="o">-</span> <span class="n">r</span> <span class="o">+</span> <span class="n">fCol</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="k">if</span> <span class="p">(</span><span class="n">inRow</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">inRow</span> <span class="o">&lt;</span> <span class="n">height</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">				<span class="n">inCol</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">inCol</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">P</span><span class="p">[</span><span class="n">inRow</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">inCol</span><span class="p">]</span> <span class="o">*</span> <span class="n">F</span><span class="p">[</span><span class="n">fRow</span> <span class="o">*</span> <span class="n">r</span> <span class="o">+</span> <span class="n">fCol</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">P</span><span class="p">[</span><span class="n">outRow</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">outCol</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pvalue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="73-constant-memory-and-caching">7.3 Constant Memory and Caching</h2>
<p>可以发现卷积核 F 通常很小，在整个卷积内核的执行过程中不会改变，所有线程都以相同的顺序访问其元素。因此我们可以考虑将其存储在常量内存里，之前说过它和全局内存的区别是线程不能修改常量内存变量的值并且常量内存非常小，目前为 64 KB.
假设已经在主机代码里分配好 F_h 的内存，可以通过 <code>cudaMemcpyToSymbol()</code> 将其从主机内存传输到设备常量内存中。内核函数以全局变量的形式访问常量内存变量。因此，它们的指针不需要作为参数传递给内核函数。</p>
<p><strong>如果主机代码和内核代码位于不同的文件中，内核代码文件必须包含相关的外部声明的头文件，以确保声明对内核可见。</strong></p>
<p>CUDA runtime 知道常量内存变量在内核执行期间不会被修改，因此会让硬件在内核执行期间直接缓存常量内存变量。在不需要支持写的情况下，可以在减小芯片面积和降低功耗的情况下设计用于常量内存变量的专用缓存，被称为常量缓存 (<code>constant caching</code>).</p>
<h2 id="74-tiled-convolution-with-halo-cells">7.4 Tiled Convolution with Halo Cells</h2>
<p>我们定义输出 tile 为每个块处理的输出元素，输入 tile 为计算输出 tile 中元素所需的输入元素的集合。下图给出了一个例子，可以看到输入 tile 大小和输出 tile 大小之间的差异使 tile 卷积核的设计变得复杂。有两种线程组织可以处理这种差异。</p>
<ul>
<li>启动与输入 tile 具有相同维度的线程块。这样因为每个线程只需要加载一个输入元素。但由于输入 tile 比对应的输出 tile 大，在计算输出元素时需要禁用一些线程，降低了资源利用率。</li>
<li>启动与输出 tile 具有相同维度的线程块。这样线程需要迭代以确保加载所有输入 tile 元素。但简化了输出元素的计算。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBda4dfd50e011362c0cc68caaf130a16d?method=download&amp;shareKey=b534279430a6d88b51d9523c3cdf486b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBda4dfd50e011362c0cc68caaf130a16d?method=download&amp;shareKey=b534279430a6d88b51d9523c3cdf486b" alt="Input Tile vs. Output Tile in 2D Convolution">
    </a><figcaption>Input Tile vs. Output Tile in 2D Convolution</figcaption></figure></p>
<p>第一种线程组织方式的内核如下。现在每个块中的线程共同执行 <code>OUT_TILE_DIM^2*(2*FILTER_RADIUS+1)</code> 次浮点运算。分配给输入 tile 元素的每个线程加载一个4字节的输入值。因此每个block加载 <code>IN_TILE_DIM^2*4=(OUT_TILE_DIM+2*FILTER_RADIUS)^2*4</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#define IN_TILE_DIM 32
</span></span></span><span class="line"><span class="cl"><span class="cp">#define FILTER_RADIUS 5
</span></span></span><span class="line"><span class="cl"><span class="cp">#define OUT_TILE_DIM (IN_TILE_DIM - 2*(FILTER_RADIUS))
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="n">__constant__</span> <span class="kt">float</span> <span class="n">F_c</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">convolution_tiled_2D_constant_mem_kernel_1</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// Upper left input tile coord
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Loading input tile
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">N_s</span><span class="p">[</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">height</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">N_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">N_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Calculate output elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">tileCol</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">tileRow</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">height</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">		<span class="n">tileCol</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">tileCol</span> <span class="o">&lt;</span> <span class="n">OUT_TILE_DIM</span> <span class="o">&amp;&amp;</span> <span class="n">tileRow</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">tileRow</span> <span class="o">&lt;</span> <span class="n">OUT_TILE_DIM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="kt">float</span> <span class="n">Pvalue</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">fRow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fRow</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">fRow</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">fCol</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fCol</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">fCol</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">F_c</span><span class="p">[</span><span class="n">fRow</span><span class="p">][</span><span class="n">fCol</span><span class="p">]</span> <span class="o">*</span> <span class="n">N_s</span><span class="p">[</span><span class="n">tileRow</span> <span class="o">+</span> <span class="n">fRow</span><span class="p">][</span><span class="n">tileCol</span> <span class="o">+</span> <span class="n">fCol</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">P</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pvalue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>第二种线程组织方式的内核如下，每个线程现在可能需要加载多个输入 tile 的元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">convolution_tiled_2D_constant_mem_kernel_2</span><span class="p">(</span>  <span class="c1">// OUT_TILE_DIM^2 threads per block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Upper left output tile coord
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Each thread may need to load multiple elements into shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">N_s</span><span class="p">[</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">OUT_TILE_DIM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span><span class="p">;</span> <span class="n">j</span> <span class="o">+=</span> <span class="n">OUT_TILE_DIM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">in_col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span> <span class="o">+</span> <span class="n">j</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">in_row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span> <span class="o">+</span> <span class="n">i</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">			<span class="k">if</span> <span class="p">(</span><span class="n">in_row</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">in_row</span> <span class="o">&lt;</span> <span class="n">height</span> <span class="o">&amp;&amp;</span> <span class="n">in_col</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">in_col</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">N_s</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="n">in_row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">in_col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">N_s</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Calculate output elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">OUT_TILE_DIM</span> <span class="o">&amp;&amp;</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">&lt;</span> <span class="n">OUT_TILE_DIM</span> <span class="o">&amp;&amp;</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">height</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="kt">float</span> <span class="n">Pvalue</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">fRow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fRow</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">fRow</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">fCol</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fCol</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">fCol</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">F_c</span><span class="p">[</span><span class="n">fRow</span><span class="p">][</span><span class="n">fCol</span><span class="p">]</span> <span class="o">*</span> <span class="n">N_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">fRow</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">fCol</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">P</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pvalue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="75-tiled-convolution-using-caches-for-halo-cells">7.5 Tiled Convolution Using Caches for Halo Cells</h2>
<p>当一个块需要它的 halo cell 时，由于相邻块的访问，它们已经在二级缓存中了。因此，对这些  halo cell 的内存访问可以从 L2 缓存提供，而不会造成额外的 DRAM 流量。我们可以对原来的 N 进行这些 halo cell 的访问，而不是将它们加载到 <code>N_ds</code> 中。代码如下，加载 N_s 变得更简单，因为每个线程可以简单地加载与其分配的输出元素具有相同坐标的输入元素。然而，计算P个元素的循环体变得更加复杂。它需要添加条件来检查 helo cell 和 ghost cell.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">convolution_tiled_cached_2D_shared_mem_kernel</span><span class="p">(</span>  <span class="c1">// OUT_TILE_DIM^2 threads per block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">OUT_TILE_DIM</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// loading input tile
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">N_s</span><span class="p">[</span><span class="n">IN_TILE_DIM</span><span class="p">][</span><span class="n">IN_TILE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">height</span> <span class="o">&amp;&amp;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">N_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">N_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Calculate output elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="n">row</span> <span class="o">&lt;</span> <span class="n">height</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="kt">float</span> <span class="n">Pvalue</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// turning off the threads at the edge of the block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">fRow</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fRow</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">fRow</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">fCol</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fCol</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">fCol</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">fCol</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">                    <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">fCol</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">                    <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">fRow</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">                    <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">fRow</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">&lt;</span> <span class="n">IN_TILE_DIM</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                    <span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">F_c</span><span class="p">[</span><span class="n">fRow</span><span class="p">][</span><span class="n">fCol</span><span class="p">]</span> <span class="o">*</span> <span class="n">N_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">fRow</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">fCol</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="n">fRow</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">                        <span class="n">row</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="n">fRow</span> <span class="o">&lt;</span> <span class="n">height</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">                        <span class="n">col</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="n">fCol</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
</span></span><span class="line"><span class="cl">                        <span class="n">col</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="n">fCol</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                        <span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">F_c</span><span class="p">[</span><span class="n">fRow</span><span class="p">][</span><span class="n">fCol</span><span class="p">]</span> <span class="o">*</span> <span class="n">N</span><span class="p">[(</span><span class="n">row</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="n">fRow</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="p">(</span><span class="n">col</span> <span class="o">-</span> <span class="n">FILTER_RADIUS</span> <span class="o">+</span> <span class="n">fCol</span><span class="p">)];</span>
</span></span><span class="line"><span class="cl">                    <span class="p">}</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">N</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pvalue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>Halo Cell: 实际计算区域周围添加的一圈额外的单元格。本质上是 &ldquo;虚拟&rdquo; 单元格，存在于不直接关注的区域之外。</li>
<li>Ghost Cell: 存储来自相邻 tile 的数据副本，使得 block 在无需直接访问彼此的内存的情况下访问相邻的必要数据。</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 6 Performance Considerations</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch6/</link>
      <pubDate>Thu, 05 Sep 2024 22:22:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch6/</guid>
      <description>Personal notebook 6 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="6-performance-considerations">6 Performance Considerations</h1>
<p>并行程序的执行速度根据程序的资源需求和硬件的资源约束之间的相互制约会有很大的变化。管理并行代码和硬件资源约束之间的交互对于在几乎所有并行编程模型中实现高性能非常重要。</p>
<h2 id="61-memory-coalescing">6.1 Memory Coalescing</h2>
<p>影响 CUDA 内核性能最重要的因素之一是访问全局内存中的数据，有限的带宽可能成为瓶颈。CUDA 设备的全局内存是用 DRAM 实现的。数据存储在DRAM单元中，访问时间通常是纳秒级别，相对于亚纳秒级别的时钟周期来说很慢。现代 DRAM 通过并行化设计来提高数据访问速率，通常称为内存访问吞吐量 (<em>memory access throughput</em>).</p>
<details class="custom-details">
    <summary class="custom-summary">Why Are DRAMs So Slow</summary>
    <div><p>DRAM 通过一个个 CMOS 晶体管 (称为 <code>cell</code>) 来存储 0/1. 当给晶体管最上面的一端 (称作栅极) 加上电压或是取消电压，晶体管两端就可以流过电流。cell 中的小电容是存储信息的关键，小电容可以存储电荷，当电容存有电荷，cell 存储 1；当电容不存电荷，存储 0.
当要读取 cell 的存储值，首先打开晶体），然后根据导通后的电容是否会进行充放电信息获得存储值。如果 cell 存储 1，即电容存有电荷，那么当打开开关时电容就会放电；反之则不会。
一个 cell 只能存储 1 比特信息，为了存储大量信息，需要构建起如图所示的 cell 阵列。可以看到每行 cell 的晶体管的栅极都是连在一起的，即都连在字线 (<em>word line</em>) 上，这意味着给字线施加电压，字线对应的一行cell都会被打开。当一行 cell 被打开，cell 电容就会向位线 (<em>bit line</em>) 充放电，一行中的每个 cell 都与一条位线直接相连，读取位线的电压变化，即可知道 cell 的存储信息。</p>
<ul>
<li>字线：用来控制读取哪一个字，一个字由 4字节组成。之所以叫字线，是因为给这根线通电，一行 cell 都会被打开.多个 cell 组合起来就是多个字，因为这根线可以打开多个字，所以叫字线</li>
<li>位线：在读取信息时，每一根线上的电压波动都代表一位比特信息，所以叫做位线。
cell 的读取依靠小电容充放电，电容充放电导致位线产生电压波动，通过读取位线电压波动即可获取信息。小电容充放电所产生的电压波动是很微弱的，充放电所造成的电压波动的时间也是很短的，因此很难直接读取充放电信息，为此 cell 阵列的读取使用到了 sense amplifier，即读出放大器。读出放大器可以捕捉到微弱的电压波动，并根据电压波动的情况在本地还原出 cell 的电容电压，而且放大器内还有锁存器，可以把还原出来的电容电压值保存起来，这样一来 cell 保存的信息就从 cell 电容转移到了放大器本地。
每条位线都要接到一个放大器中。在读取 cell 行前，需要把每根位线都预充电 (precharge) 到电容电压/供电电压最大值的一半。在 DRAM 芯片中，读出放大器把 cell 阵列分成了两半，因为其采用的是差分放大器，需要同时接入两根位线。放大信号波动时需要用一个基准和待测线作比较，接到放大器上的两条位线的其中一条就作为基准。在读出数据之后，根据放大器锁存的值，把各条位线拉到供电电压或接到地，然后 cell 电容就会根据位线电压进行充电或放电，当 cell 电容充放电结束，就可以断开字线，宣告本次 DRAM 读取结束。
简单来说读取一个比特的总体流程是：获得行号，译码行号，开启单元行，放大位线电压波动并暂存数据到放大器，获得列号并根据列号选择一位进行输出，写回数据，关闭字线，重新预充电。而写一个比特的总体流程是：获得行号，译码行号，开启单元行，放大位线电压波动并暂存数据到放大器，获得列号并输入写入数据，根据列号把写入数据送到放大器并改写暂存值，写回数据，关闭字线，重新预充电。
其中花费时间最久的两项是开启单元行和放大电压波动并暂存数据。开启单元行时行地址译码器需要拉高一条字线，然后用这一条字线拉高单元行上所有晶体管的栅极电压，相当于给一个很大的电容充电，非常花费时间。放大器大部分是模拟电路，工作速度不快，因此放大电压波动并暂存数据也很花费时间。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6667e871e68ba1c132af4f6531083e10?method=download&amp;shareKey=3a67fa6fac6a2e83fa7f0e5f0bf2c01c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6667e871e68ba1c132af4f6531083e10?method=download&amp;shareKey=3a67fa6fac6a2e83fa7f0e5f0bf2c01c" alt="DRAM Cell Array">
    </a><figcaption>DRAM Cell Array</figcaption></figure></p>
</div>
</details><br>
<p>由于读取非常耗时，DRAM 每次读取数据都会存储在放大器本地缓存 (<em>row buffer</em> / <em>cache line</em>). 缓存行内的各个字在内存上是相邻的，每当读取 cell 阵列中的一个比特会把其所在缓存行的所有比特都送到输出缓存，这种读取方式叫做突发 (<em>burst</em>). 当 warp 中的所有线程访问连续的全局内存位置时，硬件将所有这些访问合并 (colaesce) 为对连续 DRAM 位置的访问 (即行地址)。
有各种优化策略来实现内存合并。</p>
<ul>
<li>重新排列线程到数据的映射。</li>
<li>重新排列数据本身的布局。</li>
<li><em>corner turning</em>: 以合并的方式在全局内存和共享内存之间传输数据，并在共享内存中执行不利的访问模式。共享内存是用SRAM技术实现的，不需要合并，因此不是连续的地址访问带来的影响不大。
内存合并的主要优点是，能通过将多个内存访问合并为单个访问来减少全局内存流量。</li>
</ul>
<h2 id="62-hiding-memory-latency">6.2 Hiding memory latency</h2>
<p>一个 cell 阵列一次可以提供一个比特，那么 8 个 cell 阵列就可以一次提供 8 个比特，他们共享一组行地址和列地址，被称作一个 <em>bank</em>. 处理器包含一个或多个通道 (<em>channel</em>). 每个通道都是一个带有总线的内存控制器，该总线将一组 DRAM 组连接到处理器。
如下图所示当两个 bank 连接到通道总线时，当第一个 bank 为另一个访问提供服务时，可以在第二个 bank 发起访问。一般来说，如果 cell 阵列访问延迟与数据传输时间之比为 R，则充分利用信道总线的数据传输带宽至少需要 R+1 个 bank 。更多的 bank 减少了针对同一 bank 的多个同时访问的概率，这种现象称为 bank 冲突 (<em>bank conflict</em>). 由于每个 bank 一次只能译码一行字线，因此这些冲突访问的单元阵列访问延迟不能再重叠。拥有更多数量的 bank 会增加这些访问分散到多个 bank 的可能性。第二个原因是每个 cell 阵列的大小限制了每个 bank 可以提供的比特数。因此第四章所说的最大化占用率还有一个额外的好处，那就是确保发出足够的内存访问请求来隐藏 DRAM 访问延迟。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1ca208a23c106f7778f72d2d9a329c34?method=download&amp;shareKey=705ee6d9699bf36549f5740c33688ce0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1ca208a23c106f7778f72d2d9a329c34?method=download&amp;shareKey=705ee6d9699bf36549f5740c33688ce0" alt="Banking Improves the Utilization of Data Transfer Bandwidth of a Channel">
    </a><figcaption>Banking Improves the Utilization of Data Transfer Bandwidth of a Channel</figcaption></figure></p>
<p>分布方案存储如下图所示，通常称为交错数据分布 (<em>interleaved data distribution</em>). 对于一个 4*4 的矩阵，每输出矩阵的每个元素计算将对通道 0 中的两个 bank 以及通道 2 中的两个 bank 进行合并访问。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB0ff74f917a82000ff47b38ea6ca53b82?method=download&amp;shareKey=def9f25f25bee24133ae10ac5eee4696" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB0ff74f917a82000ff47b38ea6ca53b82?method=download&amp;shareKey=def9f25f25bee24133ae10ac5eee4696" alt="An Example of Interleaved Data Distribution">
    </a><figcaption>An Example of Interleaved Data Distribution</figcaption></figure></p>
<h2 id="63-thread-coarsening">6.3 Thread Coarsening</h2>
<p>以最细粒度并行化工作的缺点在于，并行化工作需要付出代价，例如不同线程块对数据的重复加载、冗余工作、同步开销等。如果硬件最由于资源不足而顺序执行，那么这个代价是不必要的。部分序列化工作，减少为并行性付出的代价。因此可以通过为每个线程分配多个最细粒度的工作来解决，通常被称为线程粗化 (<em>thread coarsening</em>).
如下图所示，在之前的 tiled 矩阵乘法里，由于共享内存内容不能跨块共享，每个块必须加载矩阵 M 的 tile 副本。因此可以让块中的每个线程处理两个输出元素。这样，粗化的线程块将加载 M 的 tile 一次，并将它们用于计算为多个输出 tile.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBd0127fed6f7a89006a4338bcd85b6c84?method=download&amp;shareKey=701002f69c07f74fa723fbe036467ff9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBd0127fed6f7a89006a4338bcd85b6c84?method=download&amp;shareKey=701002f69c07f74fa723fbe036467ff9" alt="Thread Coarsening for Tiled Matrix Multiplication">
    </a><figcaption>Thread Coarsening for Tiled Matrix Multiplication</figcaption></figure></p>
<p>下面的代码展示了线程粗化的矩阵乘法内核函数，在 <code>width/TILE_WIDTH</code> 的每次迭代中，一个线程计算原来 <code>COARSE_FACTOR</code> 个线程对应位置的输出。</p>
<p>使用线程粗化时要注意：</p>
<ul>
<li>不要在不必要的时候使用，当并行化的代价可以通过粗化来降低时，粗化是有益的。</li>
<li>不要使用过多的粗化，以免硬件资源得不到充分利用。</li>
<li>避免将资源消耗增加到损害占用的程度。根据内核的不同，线程粗化可能需要每个线程使用更多的寄存器或每个线程块使用更多的共享内存。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">CoarsingMatrixMulKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">M</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Mds</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Nds</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="kt">int</span> <span class="n">by</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="kt">int</span> <span class="n">ty</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Identify the row and column of the P element to work on
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">by</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">colStart</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">*</span> <span class="n">COARSE_FACTOR</span> <span class="o">+</span> <span class="n">tx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Initialize Pvalue for all output elements
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">float</span> <span class="n">Pvalue</span><span class="p">[</span><span class="n">COARSE_FACTOR</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">COARSE_FACTOR</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">Pvalue</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Loop over the M and N tiles required to compute P element
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">ph</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ph</span> <span class="o">&lt;</span> <span class="n">width</span><span class="o">/</span><span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">ph</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="c1">// the COARSE_FACTOR tiles of N needs the same tile of M
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">Mds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">M</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">ph</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">COARSE_FACTOR</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">colStart</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span><span class="p">;</span>  <span class="c1">// Value to be computed in the c th tile
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="c1">// Collaborative loading of N tile into shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>			<span class="n">Nds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">[(</span><span class="n">ph</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">			<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">				<span class="n">Pvalue</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">+=</span> <span class="n">Mds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">Nds</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">			<span class="p">}</span>
</span></span><span class="line"><span class="cl">			<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">COARSE_FACTOR</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">colStart</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">			<span class="n">P</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pvalue</span><span class="p">[</span><span class="n">c</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 4 Compute Architecture and Scheduling</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch4/</link>
      <pubDate>Thu, 05 Sep 2024 09:18:11 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch4/</guid>
      <description>Personal notebook 3 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="compute-architecture-and-scheduling">Compute Architecture and Scheduling</h1>
<p>本章介绍 GPU 计算架构，并说明灵活资源分配、块调度和占用的概念。然后将深入讨论线程调度、延迟容忍、控制发散和同步。</p>
<h2 id="41-architecture-of-a-modern-gpu">4.1 Architecture of a modern GPU</h2>
<p>下图展示了 CUDA GPU 架构，它被组织成一个流式多处理器 (<em>Streaming Multiprocessors, SMs</em>) 数组。每个 SM 都有几个处理单元，称为流处理器或 CUDA core (简称为 <em>core</em>)，如图中 SMs 内部的小块所示，它们共享控制逻辑和内存资源。</p>
<p>SMs 还带有不同的片上存储结构，统称为内存。GPU 还带有千兆字节的片外设备内存，称为全局内存 (<em>global memory</em>).</p>
<blockquote>
<p>虽然旧的GPU使用 DDR DRAM，但从 NVIDIA 的 Pascal 架构开始 GPU 可能使用HBM (High-Bandwidth Memory) 或 HBM2，它们由 DRAM 模块组成，与GPU紧密集成在同一个封装中。</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB4312b496c54bae36f2978ad5ef0fbe56?method=download&amp;shareKey=6caf263b9392411f7d50e7f4d5bcaf80" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB4312b496c54bae36f2978ad5ef0fbe56?method=download&amp;shareKey=6caf263b9392411f7d50e7f4d5bcaf80" alt="Architecture of a CUDA-capable GPU">
    </a><figcaption>Architecture of a CUDA-capable GPU</figcaption></figure></p>
<h2 id="42-block-scheduling">4.2 Block Scheduling</h2>
<p>当调用内核时，CUDA runtime 系统启动执行内核代码的线程网格，<strong>块中的所有线程同时分配给同一个的 SM</strong>. 下图中每个 SM 分配了三个块，但是块需要占用硬件资源来执行，因此同时只能将有限数量的块分配给给定的 SM. 为了确保网格中的所有块都得到执行，runtime 系统维护一个需要执行的块列表，并在先前分配的块完成执行后再将新块分配给 SMs. 以块为基本单元将线程分配给 SMs 保证了<strong>同一块中的线程在同一SM上同时被调度</strong>。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBba45a5209304777991608711b3734d55?method=download&amp;shareKey=59a8744be11db1fad3afad00c6b06363" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBba45a5209304777991608711b3734d55?method=download&amp;shareKey=59a8744be11db1fad3afad00c6b06363" alt="Thread Block Assignment to SMs">
    </a><figcaption>Thread Block Assignment to SMs</figcaption></figure></p>
<h2 id="43-synchronization-and-transparent-scalability">4.3 Synchronization and Transparent Scalability</h2>
<p>CUDA 允许同一块中的线程使用 barrier 同步函数 <code>__syncthreads()</code> 来协调其行动。下图展示了屏障同步的执行情况，箭头表示线程各自执行运行的时间。弯曲线标记了每个线程开始执行 <code> __syncthreads()</code> 的时间。弯曲线右侧的空白区域表示每个线程等待所有线程完成所需的时间。竖线标志着最后一个线程执行 <code> __syncthreads()</code> 的时间，之后所有线程都被允许继续执行 <code> __syncthreads()</code> 之后的代码。</p>
<p>不要在分支语句中使用 <code>__syncthreads()</code></p>
<ul>
<li>放在 if 语句中时，块中的所有线程要么全执行包含 <code>__syncthreads()</code> 的路径，要么都不执行。</li>
<li>if-else 语句中的两个分支都存在，块中的所有线程要么全执行 if 情况下的 <code>__syncthreads()</code> 的路径，要么全执行 else 下的路径。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB973934d16ec550ef1e8998134754ea69?method=download&amp;shareKey=eb626fd61b25664d6884d1c701e58756" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB973934d16ec550ef1e8998134754ea69?method=download&amp;shareKey=eb626fd61b25664d6884d1c701e58756" alt="A Example Execution of Barrier Synchronization">
    </a><figcaption>A Example Execution of Barrier Synchronization</figcaption></figure></p>
<p>系统需要确保所有参与 barrier 同步的线程都能访问足够资源以到达 barrier. 否则，那些到达不了线程可能会导致死锁。因此只有当 runtime 系统确保了块中所有线程有完成执行所需的所有资源时，一个块才能开始执行。
通过禁止不同块中的线程一起执行 barrier 同步，CUDA runtime 系统可以以任何顺序执行块。如下图所示，在只有少量执行资源的系统中，一次执行两个块。反之，可以同时执行多个块。这种在不同硬件上使用不同数量的执行资源执行相同的代码的能力被称为透明可扩展性 (<em>transparent scalability</em>)</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB415e750cfd1c8bd730783cf2aadeafa0?method=download&amp;shareKey=1a0f812fee9a129ac6972abb6a59a12d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB415e750cfd1c8bd730783cf2aadeafa0?method=download&amp;shareKey=1a0f812fee9a129ac6972abb6a59a12d" alt="Transparent Scalability of CUDA Programs">
    </a><figcaption>Transparent Scalability of CUDA Programs</figcaption></figure></p>
<h2 id="44-warps-and-simd-hardware">4.4 Warps and SIMD Hardware</h2>
<p>当一个块被分配给一个 SM 时，它会被进一步划分为 32 个线程为一组的单元，称为 <em>warp</em>. 在 SMs 中，warp 是线程调度的单位。下图展示了一个划分的例子。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbc426e6de3199b6cd4706becd8760ec5?method=download&amp;shareKey=1c78a595dc3474b5fe3314455b89f2cc" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbc426e6de3199b6cd4706becd8760ec5?method=download&amp;shareKey=1c78a595dc3474b5fe3314455b89f2cc" alt="Blocks are Partitioned into Warps for Thread Scheduling">
    </a><figcaption>Blocks are Partitioned into Warps for Thread Scheduling</figcaption></figure></p>
<p>由多维度的线程组成的块，将被投影到线性化的行主布局中来划分。线性布局是以 (z, y, x) 坐标升序的方式排列。下图展示了一个大小为 4*4 块的线性化视图。前 4 个线程的 <code>threadIdx.y</code> 为 0，它们以 <code>threadIdx.x</code> 升序的方式排列。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBd0a03a116716e7f5420af4be591a86ad?method=download&amp;shareKey=1d455651d0780cc68f3bfa1138a4b705" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBd0a03a116716e7f5420af4be591a86ad?method=download&amp;shareKey=1d455651d0780cc68f3bfa1138a4b705" alt="Linear Layout of 2D Threads">
    </a><figcaption>Linear Layout of 2D Threads</figcaption></figure></p>
<p>SM 是单指令多数据 (SIMD) 模型，按顺序执行所有线程，<strong>warp 中的所有线程同时执行一条指令</strong>。下图展示了 SM 中的内核如何被分组为处理块，其中每 8 个内核构成一个处理块 (<em>processing block</em>) 并共享一个指令获取/调度单元。同一 warp 中的线程被分配到相同的处理块，该处理块获取指令并让 warp 中的所有线程对各自负责数据的部分执行该指令。这种设计允许较小比例的硬件专注于控制，而较大比例的硬件专注于提高计算吞吐量。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB9402f58e22b5fbc96784b8fddd078fa6?method=download&amp;shareKey=cad59438c3ce64bf22e7f18cd0d9591c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB9402f58e22b5fbc96784b8fddd078fa6?method=download&amp;shareKey=cad59438c3ce64bf22e7f18cd0d9591c" alt="Processing Blocks Organization">
    </a><figcaption>Processing Blocks Organization</figcaption></figure></p>
<h2 id="45-control-divergence">4.5 Control divergence</h2>
<p>当同一 warp 中的线程执行不同的路径时，这些线程的行为被称作控制发散 (<em>control divergence</em>). 下图展示了一个 warp 在遇到分支语句时的执行方式，即通过两次 pass (执行代码的阶段) 来分别执行 then-path 和 else-path，最终实现所有线程的汇合。</p>
<ul>
<li>Pascal 及之前架构中，warp 需要顺序执行两个 pass，一个 pass 执行完才能开始下一个 pass。
<ul>
<li>Pass 1： 线程 0-23 执行 then-path 的代码 A，线程 24-31 处于 inactive 状态。</li>
<li>Pass 2： 线程 24-31 执行 else-path 的代码 B，线程 0-23 处于 inactive 状态。</li>
<li>Pass 3： 所有线程汇合，执行后续代码 C。</li>
</ul>
</li>
<li>Volta 及之后架构中，warp 可以同时执行两个 pass，不同的线程可以交错执行不同的代码路径。
<ul>
<li>Pass 1： 线程 0-23 开始执行 A 的第一个指令，线程 24-31 开始执行 B 的第一个指令。</li>
<li>Pass 2： 线程 0-23 执行 A 的第二个指令，线程 24-31 执行 B 的第二个指令。</li>
<li>&hellip;</li>
<li>Pass N： 线程 0-23 执行完 A 的所有指令，线程 24-31 执行完 B 的所有指令。</li>
<li>Pass N+1： 所有线程汇合，执行后续代码 C。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB2991b223f66252dc4c44389e5eb3fa54?method=download&amp;shareKey=cb1261dd30f5d7573db9be0049648223" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB2991b223f66252dc4c44389e5eb3fa54?method=download&amp;shareKey=cb1261dd30f5d7573db9be0049648223" alt="Example of a Warp Diverging at an if-else Statement">
    </a><figcaption>Example of a Warp Diverging at an if-else Statement</figcaption></figure></p>
<p>发散也可能出现在其他控制流中。下图展示了 warp 如何执行发散 for 循环。通常来说如果判断条件基于 <code>threadIdx</code> 的值，那么控制语句可能会导致线程发散。由于线程总数需要是线程块大小的倍数，而数据大小可以是任意的，因此具有线程控制发散的控制流程很常见。由以上两个例子可以看出不能假设 warp 中的所有线程都具有相同的执行时间。如果 warp 中的所有线程都必须完成执行的一个阶段，然后才能继续前进，则必须使用 barrier 同步机制 (如 <code>__syncwarp()</code> )来确保正确性。</p>
<p>控制发散对性能的影响随着被处理向量大小的增加而减小。例如对于长度为 100 的向量，4个 warp 中有 1 个将会控制发散 (25%)；对于大小为1000的矢量，32 个 warp 中只有 1 个将会控制发散 (3.125%).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB46f65b52c565fcb503d299083c33932e?method=download&amp;shareKey=56a12c21eb2f91c9ac6b3e6cefc6a6df" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB46f65b52c565fcb503d299083c33932e?method=download&amp;shareKey=56a12c21eb2f91c9ac6b3e6cefc6a6df" alt="Example of a Warp Diverging at a for-loop">
    </a><figcaption>Example of a Warp Diverging at a for-loop</figcaption></figure></p>
<h2 id="46-warp-scheduling-and-latency-tolerance">4.6 Warp scheduling and latency tolerance</h2>
<p>当将线程分配给 SMs 时，分配给 SM 的线程通常比 SM 中 core 的个数还要多，导致每个 SM 只能同时执行分配给它的所有线程的一部分。当要由 warp 执行的指令需要等待先前启动的操作的结果时，不会选择该 warp 执行。而是选择执行另一个不用等待先前指令结果的 warp。这种用其他线程的工作填充某些线程操作延迟时间的机制通常称为延迟容忍 (<em>latency tolerance</em>) 或者延迟隐藏 (<em>latency hiding</em>). 而选择准备执行的 warp 不会在执行时间线中引入任何空闲或浪费的时间的策略被称为零开销线程调度 (<em>zero-overhead thread scheduling</em>). 这种容忍长操作延迟的能力是 GPU 不像 CPU 那样为缓存和分支预测机制分配那么多芯片面积的主要原因，因此可以更专注于浮点数计算和内存读取。</p>
<details class="custom-details">
    <summary class="custom-summary">Threads, Context-switching, and Zero-overhead Scheduling</summary>
    <div>之前介绍过线程由程序的代码、正在执行的代码中的指令、变量的值和数据结构组成。在基于冯·诺伊曼模型的计算机中，程序的代码存储在存储器中。PC (Program Counter) 跟踪正在执行的程序指令的地址。IR (Instruction Register) 保存正在执行的指令。寄存器和内存保存变量和数据结构的值。
现代处理器的设计允许上下文切换 (<em>Context-switching</em>)，多个线程可以通过轮流执行的方式分时复用一个处理器。通过保存和恢复 PC 值以及寄存器和内存的内容，可以暂停线程的执行，并在稍后正确恢复线程的执行。不过保存和恢复寄存器内容可能会增加大量执行时间。
传统的 CPU 从一个线程切换到另一个线程需要将执行状态 (例如被切换线程的寄存器内容) 保存到<font color="red;"><strong>内存</strong></font>中，稍后再从内存中加载，这样会产生空闲周期。GPU SMs 通过在硬件<font color="red;"><strong>寄存器</strong></font>中保存指定 warp 的所有执行状态来实现零开销调度，因此不需要保存和恢复状态。</div>
</details><br>
<h2 id="47-resource-partitioning-and-occupancy">4.7 Resource partitioning and occupancy</h2>
<p>给 SM 分配其所支持的最大 warp 数并不总是可行。分配给 SM 的 warp 数量与其支持的 warp 数量之比称为占用率 (<em>occupancy</em>). 例如，Ampere A100 GPU 每个 SM 最多支持 32 个 block，每个 SM 最多支持 64 个 warp (2048 个线程)，每个 block 最多支持 1024 个线程。意味着块大小可以从 64<del>1024 不等，每个 SM 分别可以有 32</del>2 个块。在这些情况下，分配给SM的线程总数为2048，这使占用率最大化。
SM 中的执行资源包括寄存器、共享内存线程块槽 (每个 SM 最大能被分配的线程块数量) 和线程槽 (每个线程块最大能被分配的线程数量)，这些资源在线程之间动态分配。资源的动态分配可能导致他们之间相互制约，使得资源利用不足。</p>
<ul>
<li>硬件资源支持的影响。当每个块有32个线程时。Ampere A100 GPU 会将 2048 个线程槽分配给 64 个块。然而 Volta SM 只支持 32 个线程块槽，导致占用率只有 50%.</li>
<li>当每个块的最大线程数不能整除块大小时。当块大小为 768，SM 将只能容纳 2 个线程块 (1536个线程)，剩下512个线程槽未使用，占用率为 75%.</li>
<li>寄存器资源限制对占用率的影响。Ampere A100 GPU 允许每个 SM 最多占有 65,536个寄存器。为了达到满占用率每个线程不应该使用超过 32 个寄存器。
这种限制导致资源使用的轻微增加可能导致并行性和性能的显著降低，称为 performance cliff.</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 5 Memory Architecture and Data Locality</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch5/</link>
      <pubDate>Thu, 05 Sep 2024 09:18:11 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch5/</guid>
      <description>Personal notebook 5 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="5-memory-architecture-and-data-locality">5 Memory Architecture and Data Locality</h1>
<p>之前章节所写的 CUDA 内核只能达到底层硬件峰值算里的一小部分。因为全局内存 (通常使用片外 DRAM 实现) 往往具有较长的访问延迟 (数百个时钟周期) 和有限的访问带宽。</p>
<h2 id="51-importance-of--memory-access-efficiency">5.1 Importance of  Memory Access Efficiency</h2>
<p>在之前矩阵乘法的内核函数中，每次迭代里执行一次浮点乘法和一次浮点加法需要访问全局内存两次。因此，从全局内存访问的浮点操作次数 (FLOP) 与字节数 (B) 的比率为 2 FLOP-to-8 B，即 0.25FLOP/B. 计算访存比 (<em>compute to global memory access ratio</em>) 定义为在程序的一个区域内对全局内存访问的单位字节执行的 FLOPS 数。
计算访存比对 CUDA 内核的性能有重大影响。A100 GPU 的全局内存带宽峰值为 1555 GB/s，矩阵乘法内核计算访存比为 0.25 OP/B，因此内核可以执行的单精度 FLOPs 的吞吐量为 389 GFLOPS，仅为 A100 GPU 峰值单精度运算吞吐量 (19,500 GFLOPS) 的 2%. 我们把执行速度受内存带宽限制的程序称为内存瓶颈 (<em>memory bound</em>) 程序。</p>
<details class="custom-details">
    <summary class="custom-summary">Roofline Model</summary>
    <div><p>Rooline 模型用于评估应用程序相在其所运行的硬件的限制上达到的性能。如下图所示，x 轴表示算术或计算强度 (<em>computational intensity</em>)，单位为 FLOP/B. y 轴表示以 GFLOPS 为单位的计算吞吐量。横线表示硬件可以提供的峰值计算吞吐量。
硬件通常关注两个指标:</p>
<ul>
<li>算力 π：也称为计算平台的性能上限，指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 FLOP/s。</li>
<li>带宽 ß：即计算平台的带宽上限，指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是Byte/s。
两个指标相除即可得到计算平台的计算强度上限 I_max = π / ß，它描述的是在这个计算平台上，单位内存交换最多用来进行多少次计算。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6d519969c36c2ceb8f94fda0644c984a?method=download&amp;shareKey=f85b96fc7fa07712421487c9b01f7e1b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6d519969c36c2ceb8f94fda0644c984a?method=download&amp;shareKey=f85b96fc7fa07712421487c9b01f7e1b" alt="Roofline Model">
    </a><figcaption>Roofline Model</figcaption></figure></p>
<p>从图中可以看出算力决定“屋顶”的高度（绿色线段），带宽决定“房檐”的斜率（红色线段）。</p>
<ul>
<li>Memory-Bound: 当模型的计算强度 I 小于硬件的计算强度上限 I_max 时，由于此时模型位于“房檐”区间，因此模型理论性能 P 的大小完全由硬件的带宽上限 ß （房檐的斜率）以及模型自身的计算强度 I 所决定，因此这时候就称模型处于 Memory-Bound 状态。</li>
<li>Compute-Bound: 不管模型的计算强度 I 有多大，它的理论性能 P 最大只能等于硬件的算力 π 。当模型的计算强度 I 大于硬件的计算强度上限 I_max 时，模型在当前硬件处于 Compute-Bound 状态</li>
</ul>
</div>
</details><br>
<p>为了让内核具有更高的性能，需要通过减少内核执行的全局内存访问次数来增加计算访存比。</p>
<h2 id="52-cuda-memory-types">5.2 CUDA memory types</h2>
<p>下图展示了 CUDA 设备的内存。全局内存和常量内存这两种类型的内存都可以被主机写入 (W) 和读取 (R) 。全局内存也可以被设备读写，而常量内存只支持设备对其读取。
另一种类型的内存是本地内存，也可以被读写。<strong>本地内存实际上放在全局内存中</strong>，具有相似的访问延迟，但它不是跨线程共享的。每个线程都有自己的全局内存部分，将其用作自己的私有本地内存，存放私有但不能在寄存器中分配的数据。
寄存器 (<em>register</em>) 和共享内存 (<em>shared memory</em>) 是片上内存。存储在这些类型内存中的变量可以以高度并行的方式以高速访问。其中每个线程只能访问自己的寄存器。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6b7cbeee7a8279269c480cc3dd307c92?method=download&amp;shareKey=c9a05598ffe57e76def11f1ea20593fa" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6b7cbeee7a8279269c480cc3dd307c92?method=download&amp;shareKey=c9a05598ffe57e76def11f1ea20593fa" alt="Overview of CUDA Memory Model">
    </a><figcaption>Overview of CUDA Memory Model</figcaption></figure></p>
<p>与基于冯·诺伊曼模型的计算机类比，CUDA 设备中的全局内存对应于内存框，寄存器对应于寄存器堆。与访问全局内存相比，每次访问寄存器所涉及的指令更少。当算术指令的操作数在寄存器中时，不需要额外的指令使算术逻辑单元(ALU)可以使用该操作数的值。如果操作数值在全局内存中，处理器需要执行内存加载操作让 ALU 能使用操作数。并且从寄存器堆访问所消耗的能量至少比从全局内存访问低一个数量级。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB976785b7c8c819c9e53543306299645d?method=download&amp;shareKey=6af41bb3bef56687bcbe90e6d21f1204" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB976785b7c8c819c9e53543306299645d?method=download&amp;shareKey=6af41bb3bef56687bcbe90e6d21f1204" alt="Memory vs. Registers in a Modern Computer Based on the von Neumann Model">
    </a><figcaption>Memory vs. Registers in a Modern Computer Based on the von Neumann Model</figcaption></figure></p>
<p>下图展示了 CUDA 设备中的共享内存和寄存器。共享内存实际上是一种暂存存储器 (<em>scratchpad memory</em>)，作为片上内存的一部分。当处理器访问存储在共享内存中的数据时，需要执行内存加载操作。CUDA 中共享内存和寄存器之间的一个重要区别是，存储在共享内存中的变量可以被块中的所有线程访问，而寄存器数据是线程私有的。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB9683f5c9b3fdd69aa62d8c8751be45f7?method=download&amp;shareKey=62797256af4f18ce4ebf652d411de315" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB9683f5c9b3fdd69aa62d8c8751be45f7?method=download&amp;shareKey=62797256af4f18ce4ebf652d411de315" alt="Shared Memory vs. Registers in a CUDA Device SM">
    </a><figcaption>Shared Memory vs. Registers in a CUDA Device SM</figcaption></figure></p>
<p>下表给出了将程序变量声明为各种内存类型的 CUDA 语法。</p>
<ul>
<li>所有在内核和设备函数中声明的 automatic scalar variables 都被放入寄存器中。</li>
<li>Automatic array variables 存储在线程的本地内存中。如果所有访问都使用常量索引值，编译器可能决定将将其存储到寄存器中。</li>
<li>块中的所有线程都看到 shared variable 的相同版本。内核执行期间每个块会创建和使用一个私有版本。通常使用共享变量来保存在内核执行阶段经常使用和重用的全局内存数据部分。</li>
<li>Constant variables 通常用于向核函数提供输入。内核函数不能修改常量变量的值。</li>
<li>Global variables 通常用于将信息从一个内核调用传递到另一个内核调用。</li>
</ul>
<table>
  <thead>
      <tr>
          <th>Variable Declaration</th>
          <th>Memory</th>
          <th>Scope</th>
          <th>Lifetime</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Automatic variables other than arrays</td>
          <td>Register</td>
          <td>Thread</td>
          <td>Kernel</td>
      </tr>
      <tr>
          <td>Automatic  array variables</td>
          <td>Local</td>
          <td>Thread</td>
          <td>Kernel</td>
      </tr>
      <tr>
          <td><code>__device__ __shared__ int SharedVar;</code></td>
          <td>Shared</td>
          <td>Block</td>
          <td>Kernel</td>
      </tr>
      <tr>
          <td><code>__device__ int GlobalVar;</code></td>
          <td>Global</td>
          <td>Grid</td>
          <td>Application</td>
      </tr>
      <tr>
          <td><code>__device__ __constant__ int ConstantVar;</code></td>
          <td>Constant</td>
          <td>Grid</td>
          <td>Application</td>
      </tr>
  </tbody>
</table>
<p>在 CUDA 中，指针可以用来指向全局内存中的数据对象，通常有以下两种情况会使用</p>
<ul>
<li>对象由主机函数分配，指向对象的指针由内存分配函数 (如 <code>cudaMalloc</code>) 初始化，作为参数传递给内核函数。</li>
<li>将在全局内存中声明的变量的地址赋给指针变量。</li>
</ul>
<h2 id="53-tiling-for-reduced-memory-traffic">5.3 Tiling for Reduced Memory Traffic</h2>
<p>一种常见的策略是将数据划分为称为 <em>tile</em> 的子集，以便每个 tile 都适合共享内存。能进行划分的一个重要的标准是这些 tile 上的内核计算可以彼此独立地完成。
下图展示了 block(0,0) 的四个线程所完成的计算。这四个线程计算P(0,0), P(0,1), P(1,0) 和 P(1,1). 每个线程在执行过程中访问 M 的 4 个元素和 N 的 4 个元素，可以看出有明显重复的部分。将每个块需要访问的数据先加载到共享内存，这样可以避免每个线程从全局内存里加载重复的数据。全局内存流量的减少与块的维度成正比。每个块大小为 Width*Width 时，全局内存流量将减少为原来的 1/Width.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3ed7adc0960b337ea099f2d34a5474db?method=download&amp;shareKey=18100aa957fb01bf5f29920b908fd02f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3ed7adc0960b337ea099f2d34a5474db?method=download&amp;shareKey=18100aa957fb01bf5f29920b908fd02f" alt="A Small Example of Matrix Multiplication">
    </a><figcaption>A Small Example of Matrix Multiplication</figcaption></figure></p>
<p>按 tile 进行矩阵乘法的基本思想是让线程在各自使用元素来进行内积计算之前，将 M 和 N 元素的子集加载到共享内存中。如下图所示把 M 和 N 分成大小为 <code>2*2</code> 的块。每个线程执行的内积计算现在被划分为几个阶段。在每个阶段，一个块中的所有线程协作将对应的 M 和 N 的 tile 加载到共享内存中。这样每个阶段关注的是输入矩阵元素的一个小子集。这种集中的访问行为称为局部性 (<em>locality</em>).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBae4e4638b9bd98256ba51c1f67530364?method=download&amp;shareKey=d9458ef5fa7b14ad19e1f0e72defa1b7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBae4e4638b9bd98256ba51c1f67530364?method=download&amp;shareKey=d9458ef5fa7b14ad19e1f0e72defa1b7" alt="Tiling M and N to Utilize Shared Memory">
    </a><figcaption>Tiling M and N to Utilize Shared Memory</figcaption></figure></p>
<h2 id="54-a-tiled-matrix-multiplication-kernel">5.4 A Tiled Matrix Multiplication Kernel</h2>
<p>按照上述方法编写的内核函数如下。如下图所示，x 轴方向上坐标为 bx 和 tx 的线程应该负责计算 P 中索引为 <code>bx * tile_width + tx</code> 元素。类似地，y 轴方向上线程要处理的 P 中索引为 <code>by * tile_width + ty</code>. 外循环的每次迭代对应于计算的一个阶段。两次调用 <code>__syncthreads()</code> 的原因不同，第一次被称为写后读 (<em>read-after-write</em>) 依赖关系，因为线程在尝试读取数据之前必须等待其他线程将数据写入正确的位置。第二种被称为读后写 (<em>write-after-read</em>) 依赖，因为线程必须等待所有需要它的线程读取数据，然后才能覆盖它。</p>
<p>写后读依赖是一种真正依赖 (<em>true dependence</em>)，因为读线程确实需要写线程提供的数据，所以它别无选择，只能等待。读后写依赖关系是伪依赖 (<em>false dependence</em>) 关系，因为写线程不需要来自读线程的任何数据。这种依赖性是因为它们访问相同的内存地址，如果它们访问不同的地址，则不存在这种依赖性。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">TilingMatrixMulKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">M</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Mds</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Nds</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  <span class="kt">int</span> <span class="n">by</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="kt">int</span> <span class="n">ty</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Identify the row and column of the P element to work on
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">Row</span> <span class="o">=</span> <span class="n">by</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">Col</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">Pvalue</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// Loop over the M and N tiles required to compute the P elemrnt
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">ph</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ph</span> <span class="o">&lt;</span> <span class="n">width</span><span class="o">/</span><span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">ph</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="c1">// Collaborative loading of M and N tiles into shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">Mds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">M</span><span class="p">[</span><span class="n">Row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">ph</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="n">Nds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">[(</span><span class="n">ph</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span><span class="p">)</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">Col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">Mds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">Nds</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">		<span class="n">P</span><span class="p">[</span><span class="n">Row</span> <span class="o">*</span> <span class="n">width</span> <span class="o">+</span> <span class="n">Col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pvalue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Tiling 技术并不是 GPU 上才能实现。CPU 上的 tiling 依赖缓存来将重用的数据保留在芯片上，而 GPU 上的 tiling 则直接地使用共享内存来存储片上数据。CPU 核心通常只运行一个或两个线程，因此线程可以依赖于缓存来保存最近使用的数据。相反，GPU SM 同时运行多个线程以隐藏延迟，些线程会竞争缓存槽，使得 GPU 缓存不太可靠。</p>
<h2 id="55-boundary-checks">5.5 Boundary Checks</h2>
<p>我们需要扩展 tiling 矩阵乘法内核使其处理任意大小的矩阵。下图展示了 block(0,0) 在 phase 1 的内存访问模式。在不进行边界检查时 thead(0,1) 试图访问 M(0,3) 时实际上获得的是 M(1,0). 同样在 Block(1,1) 在 phase 0 访问时也会出现类似的问题。因此在加载所需的 M 和 N 的 tile 时边界条件为两个索引都小于 Width: <code>Row &lt; Width &amp;&amp; (ph * TILE_WIDT + tx) &lt; Width</code>，否则将 0.0f 存入对应位置。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3510a9da44c7c085a52a178e1a9f1381?method=download&amp;shareKey=ba4ad85a9a2fa48d75e5911e34f0342e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3510a9da44c7c085a52a178e1a9f1381?method=download&amp;shareKey=ba4ad85a9a2fa48d75e5911e34f0342e" alt="Memory Access of Block(0,0) in Phase 1">
    </a><figcaption>Memory Access of Block(0,0) in Phase 1</figcaption></figure></p>
<p>扩展为一般的矩阵乘法内核是很容易的。将 Width 参数替换为三个无符号整数参数: m, k, n; 将用于指代 M 的行数/列数和 P 的行数/列数的 Width 替换为 m/n；将用于指代 M 的列数和 P 的行数的 Width 替换为 k. 修改后代码如下</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB4072b51d6111b15f740c4efb83237a0f?method=download&amp;shareKey=629c82b8ccd78e137a597b5a3e364c4b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB4072b51d6111b15f740c4efb83237a0f?method=download&amp;shareKey=629c82b8ccd78e137a597b5a3e364c4b" alt="Calculation of the Matrix Indexes in Tiled Multiplication">
    </a><figcaption>Calculation of the Matrix Indexes in Tiled Multiplication</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">GEMMKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">M</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">m</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">k</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Mds</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">	<span class="n">__shared__</span> <span class="kt">float</span> <span class="n">Nds</span><span class="p">[</span><span class="n">TILE_WIDTH</span><span class="p">][</span><span class="n">TILE_WIDTH</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">bx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  <span class="kt">int</span> <span class="n">by</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="kt">int</span> <span class="n">ty</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="c1">// Identify the row and column of the P element to work on
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="kt">int</span> <span class="n">Row</span> <span class="o">=</span> <span class="n">by</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">int</span> <span class="n">Col</span> <span class="o">=</span> <span class="n">bx</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">Pvalue</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="c1">// Loop over the M and N tiles required to compute the P element
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">ph</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ph</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="n">TILE_WIDTH</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">ph</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="c1">// Collaborative loading of M and N tiles into shared memory
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="k">if</span> <span class="p">(</span><span class="n">Row</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">ph</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">Mds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">M</span><span class="p">[</span><span class="n">Row</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="n">ph</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">Mds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="p">(</span><span class="n">ph</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span> <span class="o">&lt;</span> <span class="n">k</span> <span class="o">&amp;&amp;</span> <span class="n">Col</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">Nds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">[(</span><span class="n">ph</span> <span class="o">*</span> <span class="n">TILE_WIDTH</span> <span class="o">+</span> <span class="n">ty</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">Col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">Nds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">tx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">TILE_WIDTH</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">			<span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">Mds</span><span class="p">[</span><span class="n">ty</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">Nds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">tx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">		<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		<span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">Row</span> <span class="o">&lt;</span> <span class="n">m</span> <span class="o">&amp;&amp;</span> <span class="n">Col</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="n">P</span><span class="p">[</span><span class="n">Row</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">Col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pvalue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="56-impact-of-memory-usage-on-occupancy">5.6 Impact of Memory Usage on Occupancy</h2>
<p>CUDA 设备提供有限的资源限制了可以同时在给定程序的 SM 中分配的线程数量。上面代码不支持主机代码对共享内存使用情况的任何动态调整，因为共享内存使用的大小是一个常量。
解决的方法是共享内存声明前添加一个 <code>extern</code> 关键字，并在声明中省略数组的大小。当调用内核时，可以根据设备查询结果动态配置每个块要使用的共享内存量，并将其作为第三个执行配置参数提供给内核调用。然后将数组中每个部分的大小作为参数传递给内核函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">size</span> <span class="o">=</span> <span class="p">...;</span>
</span></span><span class="line"><span class="cl"><span class="n">matrixMulKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span><span class="n">dimBlock</span><span class="p">,</span><span class="n">size</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">Md</span><span class="err">，</span><span class="n">Nd</span><span class="err">，</span><span class="n">Pd</span><span class="p">,</span> <span class="n">Width</span><span class="err">，</span><span class="n">size</span><span class="o">/</span><span class="mi">2</span><span class="err">，</span><span class="n">size</span><span class="o">/</span><span class="mi">2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">matrixMulKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">M</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span><span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="n">Mdz_sz</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="n">Nds_sz</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">char</span> <span class="kt">float</span> <span class="n">Mds_Nds</span><span class="p">[];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span><span class="n">Mds</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="p">)</span> <span class="n">Mds_Nds</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span><span class="n">Nds</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span> <span class="n">Mds_Nds</span> <span class="o">+</span> <span class="n">Mds_sz</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 3 Multidimensional Grids and Data</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch3/</link>
      <pubDate>Wed, 04 Sep 2024 21:57:11 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch3/</guid>
      <description>Personal notebook 3 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="3-multidimensional-grids-and-data">3 Multidimensional Grids and Data</h1>
<p>本章将更广泛地介绍线程是如何组织的和如何使用线程和块来处理多维数组。</p>
<h2 id="31-multidimensional-grid-organization">3.1 Multidimensional Grid Organization</h2>
<p>再次强调<strong>网格中的所有线程执行相同的内核函数</strong>，它们依赖于线程索引来区分彼此，并确定各自要处理的数据的部分。这些线程被组织成两级结构: 一个网格由一个或多个块组成，每个块由一个或多个线程组成。调用内核函数时需要指定执行配置参数 <code>gridDim</code> 和 <code>blockDim</code>，<code>gridDim</code> 是一个三维块数组，<code>blockDim</code> 是一个三维线程数组。他们的类型都是 <code>dim3</code>，是包含三个元素 x, y 和 z 的整数向量类型，分别指定了每个维度上的块个数和线程个数。使用少于 3 个维度时可以将未使用的维度大小设置为 1。网格中的所有块都具有相同的维度和大小。<strong>一旦网格启动，网格和块的尺寸将保持不变，直到整个网格完成执行。</strong></p>
<p><strong>当前 CUDA 系统中，每个块的总大小限制为 1024 个线程。只要线程总数不超过 1024，这些线程就可以以任何方式分布在三个维度上。</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">function_name</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridDim</span><span class="p">,</span> <span class="n">blockDim</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>一个例子如下，dimBlock和dimGrid是由程序员定义的主机代码变量。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">dim3</span> <span class="nf">dimGrid</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">dim3</span> <span class="nf">dimBlock</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">vecAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">dimGrid</span><span class="p">,</span> <span class="n">dimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下图展示了 <code>gridDim(2,2,1)</code> 和 <code>blockDim (4,2,2)</code> 情况下线程组织的情况。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBcbb93fc419f4d6121d02e091d5666989?method=download&amp;shareKey=e6560d2a922f6a2c322706cb282ac70f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBcbb93fc419f4d6121d02e091d5666989?method=download&amp;shareKey=e6560d2a922f6a2c322706cb282ac70f" alt="A Multidimensional Example of CUDA Grid Organization">
    </a><figcaption>A Multidimensional Example of CUDA Grid Organization</figcaption></figure></p>
<h2 id="32-mapping-threads-to-multidimensional-data">3.2 Mapping threads to multidimensional data</h2>
<p>选择 1D、2D 或 3D 的线程组织通常基于数据的性质。例如图像是一个二维像素数组。使用由 2D 块组成的 2D 网格可以方便地处理图像中的像素。下图展示了处理大小为 <code>62*76</code> 1F1F 的图片 P 的一种组织方式。假设使用 <code>16*16</code> 大小的块，那么在 y 方向上需要 4 个块，在 x 方向上需要 5 个块。横纵坐标的计算方式为</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">row coordinate = blockIdx.y * blockDim.y + threadIdx.y
</span></span><span class="line"><span class="cl">col coordinate = blockIdx.x * blockDim.x + threadIdx.x
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们将按维度的降序 <code>(z, y, x)</code> 表示多维数据。这种顺序与 <code>gridDim</code> 和 <code> blockDim</code> 维度中数据维度的顺序相反！！！</p>
<p>实际上，由于现代计算机中使用二维存储空间，C 语言中的所有多维数组都是线性化的。虽然可以使用如 <code>Pin_d[j][i]</code> 这样的多维数组语法访问多维数组的元素，但编译器将这些访问转换为指向数组开始元素的基指针，以及从这些多维索引计算出的一维偏移量。
至少有两种方法可以对二维数组进行线性化。将同一行/列的所有元素放置到连续的位置。然后将行/列一个接一个地放入内存空间中。这种排列称为行/列主序布局 (<em>row/column-major layout</em>). <strong>CUDA C 使用行主序布局。</strong></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7aab5f499364badca84a8cf76a1793fb?method=download&amp;shareKey=7df12b5fb5eae00c962e1a3ff98dabec" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7aab5f499364badca84a8cf76a1793fb?method=download&amp;shareKey=7df12b5fb5eae00c962e1a3ff98dabec" alt="Row-major Layout for a 2D C Array">
    </a><figcaption>Row-major Layout for a 2D C Array</figcaption></figure></p>
<p>下面内核代码将每个颜色像素转换为对应的灰度像素。我们计算坐标为 <code>(row, col)</code> 的像素对应的 1D 索引 <code>row * width + col</code>. 这个 1D 索引 <code>grayOffset</code> 就是 <code>Pout</code> 的像素索引，因为输出灰度图像中的每个像素都是 1字节 (unsigned char)。每个彩色像素用三个元素(r, g, b)存储，每个元素为1字节。因此 <code>rgbOffset</code> 给出了 <code>Pin</code> 数组中颜色像素的起始位置。从 <code>Pin</code> 数组的三个连续字节位置读取每个通道对应的值，执行灰度像素值的计算，并使用 <code> grayOffset</code> 将该值写入 <code>Pout</code> 数组。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// we have 3 channels corresponding to RGB
</span></span></span><span class="line"><span class="cl"><span class="c1">// The input image is encoded as unsigned characters [0, 255]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">colorToGreyscaleConversion</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span> <span class="n">Pout</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span> <span class="n">Pin</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">Col</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">Row</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">Col</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="n">Row</span> <span class="o">&lt;</span> <span class="n">height</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// get 1D coordinate for the grayscale image
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">int</span> <span class="n">greyOffset</span> <span class="o">=</span> <span class="n">Row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">Col</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// one can think of the RGB image having
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// CHANNEL times columns than the grayscale image
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">int</span> <span class="n">rgbOffset</span> <span class="o">=</span> <span class="n">greyOffset</span><span class="o">*</span><span class="n">CHANNELS</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="kt">unsigned</span> <span class="kt">char</span> <span class="n">r</span> <span class="o">=</span> <span class="n">Pin</span><span class="p">[</span><span class="n">rgbOffset</span> <span class="o">+</span> <span class="mi">0</span><span class="p">];</span> <span class="c1">// red value for pixel
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">unsigned</span> <span class="kt">char</span> <span class="n">g</span> <span class="o">=</span> <span class="n">Pin</span><span class="p">[</span><span class="n">rgbOffset</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span> <span class="c1">// green value for pixel
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">unsigned</span> <span class="kt">char</span> <span class="n">b</span> <span class="o">=</span> <span class="n">Pin</span><span class="p">[</span><span class="n">rgbOffset</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span> <span class="c1">// blue value for pixel
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      
</span></span><span class="line"><span class="cl">        <span class="c1">// perform the rescaling and store it
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// We multiply by floating point constants
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">Pout</span><span class="p">[</span><span class="n">grayOffset</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.21f</span><span class="o">*</span><span class="n">r</span> <span class="o">+</span> <span class="mf">0.71f</span><span class="o">*</span><span class="n">g</span> <span class="o">+</span> <span class="mf">0.07f</span><span class="o">*</span><span class="n">b</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="33-image-blur-a-more-complex-kernel">3.3 Image blur: a more complex kernel</h2>
<p>图像模糊函数将输出图像像素的值计算为相邻像素 (包括输入图像中像素) 的加权和。简便起见，我们使用相邻像素的平均值来计算结果，对应的代码如下。与 <code>colorToGrayscaleConversion</code> 中使用的策略类似，对每个输出像素使用 1 个线程来计算。<code>col</code>和 <code>row</code> 表示输入像素 patch 的中心像素位置。嵌套的 <code>for</code> 循环遍历 patch 中的所有像素。<code>if</code> 语句的 <code>curRow &lt; 0</code> 和 <code>curCol &lt; 0</code> 条件用于跳过执行超出图像范围的部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">blurKernel</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span><span class="n">in</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span><span class="n">out</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">Col</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">Row</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">Col</span> <span class="o">&lt;</span> <span class="n">width</span> <span class="o">&amp;&amp;</span> <span class="n">Row</span> <span class="o">&lt;</span> <span class="n">height</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">pixVal</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="kt">int</span> <span class="n">pixels</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// Get the average of the surrounding BLUR_SIZE x BLUR_SIZE box
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">blurRow</span> <span class="o">=</span> <span class="o">-</span><span class="n">BLUR_SIZE</span><span class="p">;</span> <span class="n">blurRow</span> <span class="o">&lt;</span> <span class="n">BLUR_SIZE</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">blurRow</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">blurCol</span> <span class="o">=</span> <span class="o">-</span><span class="n">BLUR_SIZE</span><span class="p">;</span> <span class="n">blurCol</span> <span class="o">&lt;</span> <span class="n">BLUR_SIZE</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">blurCol</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="kt">int</span> <span class="n">curRow</span> <span class="o">=</span> <span class="n">Row</span> <span class="o">+</span> <span class="n">blurRow</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="kt">int</span> <span class="n">curCol</span> <span class="o">=</span> <span class="n">Col</span> <span class="o">+</span> <span class="n">blurCol</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">              
</span></span><span class="line"><span class="cl">                <span class="c1">// If the pixel is within the image, add its value to the sum
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="k">if</span><span class="p">(</span><span class="n">curRow</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">curRow</span> <span class="o">&lt;</span> <span class="n">height</span> <span class="o">&amp;&amp;</span> <span class="n">curCol</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">curCol</span> <span class="o">&lt;</span> <span class="n">width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pixVal</span> <span class="o">+=</span> <span class="n">in</span><span class="p">[</span><span class="n">curRow</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">curCol</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">                    <span class="n">pixels</span><span class="o">++</span><span class="p">;</span> <span class="c1">// Keep track of the number of pixels in the avg
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Write our new pixel value out
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">out</span><span class="p">[</span><span class="n">Row</span><span class="o">*</span><span class="n">width</span> <span class="o">+</span> <span class="n">Col</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">char</span><span class="p">)(</span><span class="n">pixVal</span> <span class="o">/</span> <span class="n">pixels</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="34-matrix-multiplication">3.4 Matrix multiplication</h2>
<p>矩阵乘法是 Basic Linear Algebra Subprograms (BLAS) 的重要组成部分。</p>
<ul>
<li>Level 1 形如 $y = \alpha x + y$  的向量运算。</li>
<li>Level 2 形如  $y = \alpha Ax + \beta y$  的矩阵-向量运算。</li>
<li>Level 3 形如  $y = \alpha AB + \beta C$ 的矩阵-矩阵运算。</li>
</ul>
<p>为了用 CUDA 实现矩阵乘法，我们可以采取与 colorToGrayscaleConversion 相同的方法将网格中的线程映射到输出矩阵 P 的元素，即每个线程负责计算 P 中的一个元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Assuming square matrices of size Width x Width
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__global__</span> 
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">MatrixMulKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">M</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Width</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Calculate the row index of the P element and M
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span> <span class="n">Row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Calculate the column index of P and N
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int</span> <span class="n">Col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">((</span><span class="n">Row</span> <span class="o">&gt;=</span> <span class="n">Width</span><span class="p">)</span> <span class="o">||</span> <span class="p">(</span><span class="n">Col</span> <span class="o">&gt;=</span> <span class="n">Width</span><span class="p">))</span> <span class="k">return</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">Pvalue</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// each thread computes one element of the block sub-matrix
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Pvalue</span> <span class="o">+=</span> <span class="n">M</span><span class="p">[</span><span class="n">Row</span><span class="o">*</span><span class="n">Width</span><span class="o">+</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">N</span><span class="p">[</span><span class="n">k</span><span class="o">*</span><span class="n">Width</span><span class="o">+</span><span class="n">Col</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">P</span><span class="p">[</span><span class="n">Row</span><span class="o">*</span><span class="n">Width</span><span class="o">+</span><span class="n">Col</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pvalue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb16353405dede29b1f85c4c05008cda6?method=download&amp;shareKey=6ffa3c49a5c8db350dd70df1f42dd2de" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb16353405dede29b1f85c4c05008cda6?method=download&amp;shareKey=6ffa3c49a5c8db350dd70df1f42dd2de" alt="Matrix Multiplication by Tiling P">
    </a><figcaption>Matrix Multiplication by Tiling P</figcaption></figure></p>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 2 Heterogeneous Data Parallel</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch2/</link>
      <pubDate>Tue, 03 Sep 2024 22:48:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch2/</guid>
      <description>Personal notebook 2 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="2-heterogeneous-data-parallel-computing">2 Heterogeneous Data Parallel Computing</h1>
<p>数据并行 (<em>Data Parallel</em>) 是指在数据集的不同部分上执行的计算工作可以彼此独立地完成，从而可以并行执行的现象。</p>
<h2 id="21-data-parallel">2.1 Data Parallel</h2>
<p>在图像处理中，将彩色像素转换为灰度只需要该像素的数据。模糊图像将每个像素的颜色与附近像素的颜色平均，只需要像素的小邻域的数据。即使是一个看似全局的操作，比如找到图像中所有像素的平均亮度，也可以分解成许多可以独立执行的较小的计算。这种对不同数据块的独立计算是数据并行性的基础。
为了将彩色图像转换为灰度图像，我们通过以下加权和公式计算每个像素的亮度值L. 这些逐像素计算都不依赖于彼此，都可以独立执行。显然，彩色图到灰度图的转换具有大量的数据并行性。
$L=0.21r+0.72g+0.07b$</p>
<details class="custom-details">
    <summary class="custom-summary">Task Parallelism vs. Data Parallelism</summary>
    <div>数据并行并不是并行编程中使用的唯一类型的并行。任务并行 (<em>Task Parallelism</em>) 在并行编程中也得到了广泛的应用。任务并行性通常通过应用程序的任务分解来暴露。例如，一个简单的应用程序可能需要做一个向量加法和一个矩阵-向量乘法。每个都是一个任务。如果两个任务可以独立完成，则存在任务并行性。I/O和数据传输也是常见的任务。</div>
</details><br>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB56d71b169d207ac51adc718f79fb006c?method=download&amp;shareKey=d97c1f60eb44182ac8bb99f8a81035fe" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB56d71b169d207ac51adc718f79fb006c?method=download&amp;shareKey=d97c1f60eb44182ac8bb99f8a81035fe" alt="Data Parallelsim in Image2Grayscale Conversion">
    </a><figcaption>Data Parallelsim in Image2Grayscale Conversion</figcaption></figure></p>
<h2 id="22-cuda-c-program-structure">2.2 CUDA C Program Structure</h2>
<p>CUDA C 用最少的新语法和库函数扩展了流行的 ANSI C 语言。CUDA C 程序的结构反映了计算机中主机 (CPU) 和一个或多个设备 (GPU) 的共存。每个 CUDA C 源文件可以同时包含主机 (<em>host</em>) 代码和设备 (<em>device</em>) 代码。
CUDA程序的执行流程如下图所示。执行从主机代码 (CPU 串行代码) 开始，当调用内核函数 (<em>kernel function</em>) 时，会在设备上启动大量线程<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>来执行内核。由内核调用启动的所有线程统称为网格 (grid)。这些线程是 CUDA 并行执行的主要载体。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBcfe42671ed5897d29371195eb557fa00?method=download&amp;shareKey=2d501a2775f21e11292189f68d89e39a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBcfe42671ed5897d29371195eb557fa00?method=download&amp;shareKey=2d501a2775f21e11292189f68d89e39a" alt="Execution of a CUDA Program">
    </a><figcaption>Execution of a CUDA Program</figcaption></figure></p>
<h2 id="23-a-vector-addition-kernel">2.3 A vector addition kernel</h2>
<p>使用向量加法来展示 CUDA C 程序结构。下面展示了一个简单的传统 C 程序，它由一个主函数和一个向量加法函数组成。</p>
<p>当需要区分主机和设备数据时，我们都会在主机使用的变量名后面加上 “<code>_h</code>”，而在设备使用的变量名后面加上 “<code>_d</code>”.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Compute vector sum h_C = h_A+h_B
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">h_A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">h_B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">h_C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">h_C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">h_A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Memory allocation for h_A, h_B, and h_C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// I/O to read h_A and h_B, N elements each
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// …
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">vecAdd</span><span class="p">(</span><span class="n">h_A</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">h_C</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>并行执行向量加法的一种直接方法是修改 <code>vecAdd</code> 函数并将其计算移到设备上。修改后的结构如下所示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1aadc269025d2f33b2bd42b7838c7cc3?method=download&amp;shareKey=04dc8aeb81948f9dd66f6dccb54e8bd5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1aadc269025d2f33b2bd42b7838c7cc3?method=download&amp;shareKey=04dc8aeb81948f9dd66f6dccb54e8bd5" alt="Structure of the Modified VecAdd">
    </a><figcaption>Structure of the Modified VecAdd</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="c1">// …
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span><span class="n">d_A</span> <span class="o">*</span><span class="n">d_B</span><span class="p">,</span> <span class="o">*</span><span class="n">d_C</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="cm">/*
</span></span></span><span class="line"><span class="cl"><span class="cm">    …
</span></span></span><span class="line"><span class="cl"><span class="cm">    1. // Allocate device memory for A, B, and C
</span></span></span><span class="line"><span class="cl"><span class="cm">       // copy A and B to device memory
</span></span></span><span class="line"><span class="cl"><span class="cm">    2. // Kernel launch code – to have the device
</span></span></span><span class="line"><span class="cl"><span class="cm">       // to perform the actual vector addition
</span></span></span><span class="line"><span class="cl"><span class="cm">    3. // copy C from the device memory
</span></span></span><span class="line"><span class="cl"><span class="cm">       // Free device vectors
</span></span></span><span class="line"><span class="cl"><span class="cm">    */</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="24-device-global-memory-and-data-transfer">2.4 Device Global Memory and Data Transfer</h2>
<p>在当前的CUDA系统中，设备通常是带有自己的 DRAM 的硬件卡，称为 (设备)全局内存 (<em>device global memory</em>). 对于向量加法内核，在调用内核之前，程序员需要在设备全局内存中分配空间，并将数据从主机内存传输到设备全局内存中分配的空间。这对应于 1. 部分。类似地，在设备执行之后，程序员需要将结果数据从设备全局内存传输回主机内存，并释放设备全局内存中不再需要的已分配空间。这对应于 3. 部分。
<code>cudaMalloc</code> 函数可以从主机代码中调用，为对象分配一块设备全局内存。第一个参数是指针变量的地址，该变量将被设置为指向分配的对象。指针变量的地址应强制转换为 <code>void**</code>，这样可以允许 <code>cudaMalloc</code> 函数将分配内存的地址写入所提供的指针变量中，而不考虑其类型<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">cudaError_t</span> <span class="nf">cudaMalloc</span><span class="p">(</span><span class="kt">void</span><span class="o">**</span> <span class="n">devPtr</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">size</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>devPtr</code>：指向指向设备内存的指针的指针。</li>
<li><code>size</code>：要分配的内存大小（以字节为单位）。</li>
</ul>
<hr>
<p>cudaFree 函数通过释放设备内存并将其返回到可用内存池来管理设备内存资源。它只需要 A_d 的值来识别要释放的内存区域，而不需要改变 A_d 指针本身的地址。</p>
<p>在主机代码中对设备全局内存指针进行解引用引用可能导致异常或其他类型的运行错误。</p>
<p>cudaMemcpy 函数是 CUDA 中用于在主机内存和设备内存之间传输数据的核心函数。它允许将数据从主机内存复制到设备内存，或从设备内存复制到主机内存。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">cudaError_t</span> <span class="nf">cudaMemcpy</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span> <span class="n">dst</span><span class="p">,</span> <span class="k">const</span> <span class="kt">void</span><span class="o">*</span> <span class="n">src</span><span class="p">,</span> <span class="n">size_t</span> <span class="n">count</span><span class="p">,</span> <span class="n">cudaMemcpyKind</span> <span class="n">kind</span><span class="p">);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>dst</code>：目标内存地址，可以是主机内存地址或设备内存地址。</li>
<li><code>src</code>： 源内存地址，可以是主机内存地址或设备内存地址。</li>
<li><code>count</code>： 要复制的数据大小（以字节为单位）。</li>
<li><code>kind</code>： 复制方向，可以使用<a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g18fa99055ee694244a270e4d5101e95b">以下枚举值</a>：
<ul>
<li><code>cudaMemcpyHostToDevice</code>：主机内存-&gt;设备内存。</li>
<li><code>cudaMemcpyDeviceToHost</code>：设备内存-&gt;主机内存。</li>
<li><code>cudaMemcpyDeviceToDevice</code>：设备内存-&gt;设备内存。</li>
<li><code>cudaMemcpyHostToHost</code>：主机内存-&gt;主机内存</li>
</ul>
</li>
</ul>
<p>了解完这些后，可以更新代码的框架如下</p>
<details class="custom-details">
    <summary class="custom-summary">Checking and Handling in CUDA</summary>
    <div><p>CUDA API 函数返回一个 <code>cudaError_t</code> 类型的标志，指示当它们处理请求时是否发生错误。
在 CUDA 运行时库的头文件 cuda_runtime.h 中，cudaError_t 被定义为一个 int 类型的别名</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="kt">int</span> <span class="n">cudaError_t</span><span class="p">;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>一个例子如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="o">*</span><span class="n">d_a</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span> <span class="mi">1024</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">printf</span><span class="p">(</span><span class="s">&#34;cudaMalloc failed: %s</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span><span class="n">d_A</span> <span class="o">*</span><span class="n">d_B</span><span class="p">,</span> <span class="o">*</span><span class="n">d_C</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span> <span class="o">%</span><span class="n">d_A</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">h_A</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span> <span class="o">%</span><span class="n">d_B</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_B</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span> <span class="o">%</span><span class="n">d_C</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Kernel invocation code - to be shown later
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_C</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Free device memory for A, B, C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_A</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_B</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_C</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="25-kernel-functions-and-threading">2.5 Kernel functions and threading</h2>
<p>内核函数指所有线程在并行阶段执行的代码，<strong>网格中的所有线程执行相同的内核代码</strong>。。当程序的主机代码调用内核时，CUDA runtime 系统启动一个线程网格，这些线程被组织成一个两级层次结构。每个网格都被组织为线程块 (<em>thread block</em>, 简称为块) 数组。网格的所有块都是相同的大小。在调用内核时，每个线程块中的线程总数由主机代码指定。
同一个内核可以在主机代码的不同部分用不同数量的线程调用。对于给定的线程网格，一个块中的线程数可以在名为 <code>blockDim</code> 的内置变量中获得，它是一个具有三个无符号整数字段 <code>(x, y, z)</code> 的结构体。
下图给出了一个示例，其中每个块由256个线程组成。每个线程都用一个箭头表示，标有线程在块中的索引号的方框。由于数据是一维向量，因此每个线程块被组织为一维线程数组。<code>blockDim.x</code> 的值表示每个块中的线程总数。<code>threadaIdx</code> 变量表示每个线程在块中的坐标。全局索引 i 的计算公式为 <code>i = blockIdx.x * blockDim.x + threadIdx.x</code></p>
<p>许多编程语言都有内置变量。这些变量具有特殊的含义和目的。这些变量的值通常由运行时系统预先初始化，并且在程序中通常是只读的。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB67f8186e554926a97be5d005a8c86056?method=download&amp;shareKey=55b9fec27854c9abf7e06eaff5c5a612" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB67f8186e554926a97be5d005a8c86056?method=download&amp;shareKey=55b9fec27854c9abf7e06eaff5c5a612" alt="Hierarchical Organization in CUDA">
    </a><figcaption>Hierarchical Organization in CUDA</figcaption></figure></p>
<p>向量加法的核函数定义如下。网格中的每个线程对应于原始循环的一次迭代，这被称为循环并行 (<em>loop parallel</em>)，意为原始顺序代码的迭代由线程并行执行。<code>addVecKernel</code> 中有一个 <code>if (i &lt; n)</code> 语句，因为并非所有的向量长度都可以表示为块大小的倍数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__global__</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">vecAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>CUDA C 使用了三个可以在函数声明中使用的限定字。下表展示了这些关键词的意义。</p>
<ul>
<li><code>__host__ </code> 就是在主机上执行的传统 C 函数，只能从另一个主机函数调用。</li>
<li><code>__global__</code> 表示被声明的函数是 CUDA C 内核函数。内核函数在设备上执行，并且可以从主机上调用。</li>
<li><code>__device__</code> 函数在 CUDA 设备上执行，只能从内核函数或其他设备函数调用。</li>
</ul>
<p>可以在函数声明中同时使用 <code>__host__</code>  和 <code>__device__</code>. 编译系统会为同一个函数生成两个版本的目标代码。</p>
<table>
  <thead>
      <tr>
          <th>Qualifier Keyword</th>
          <th>Callable From</th>
          <th>Executed on</th>
          <th>Executed by</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>__host__ </code>(default)</td>
          <td>Host</td>
          <td>Host</td>
          <td>Caller host thread</td>
      </tr>
      <tr>
          <td><code>__global__</code></td>
          <td>Host/Device</td>
          <td>Device</td>
          <td>New grid of device thread</td>
      </tr>
      <tr>
          <td><code>__device__</code></td>
          <td>Device</td>
          <td>Device</td>
          <td>Caller device thread</td>
      </tr>
  </tbody>
</table>
<h2 id="26-calling-kernel-functions">2.6 Calling kernel functions</h2>
<p>实现内核函数之后，剩下的步骤是从主机代码调用该函数来启动网格。当主机代码调用内核时，它通过执行配置参数 (<em>execution configuration parameters</em>) 设置网格和线程块大小配置参数在在传统的C函数参数之前由 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> 之间给出。第一个配置参数给出网格中的块数量。第二个参数指定每个块中的线程数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="nf">vectAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// d_A, d_B, d_C allocations and copies omitted
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Run ceil(n/256) (or by (n + 256 - 1) / 256) blocks of 256 threads each 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">vecAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">ceil</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">256.0</span><span class="p">),</span> <span class="mi">256</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面展示了 <code>vecAdd</code> 函数中的最终主机代码。所有的线程块操作向量的不同部分。它们可以按任意顺序执行。</p>
<blockquote>
<p>实际上，分配设备内存、从主机到设备的输入数据传输、从设备到主机的输出数据传输以及释放设备内存的开销可能会使生成的代码比原始顺序代码慢，这是因为内核完成的计算量相对于处理或传输的数据量来说很小。</p></blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="nf">vecAdd</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="o">*</span><span class="n">d_A</span> <span class="o">*</span><span class="n">d_B</span><span class="p">,</span> <span class="o">*</span><span class="n">d_C</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_A</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">h_A</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_B</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_B</span><span class="p">,</span> <span class="n">h_B</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_C</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">vecAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">ceil</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mf">256.0</span><span class="p">),</span> <span class="mi">256</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_C</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Free device memory for A, B, C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_A</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_B</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_C</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="27-compilation">2.7 Compilation</h2>
<p>NVCC (NVIDIA C Compiler) 处理一个C处理一个CUDA C程序，使用 CUDA 关键字来分离主机代码和设备代码。</p>
<ul>
<li>主机代码是就是普通的ANSI C代码，使用 C/C++ 编译器进行编译，并作为传统的 CPU 进程运行。</li>
<li>设备代码及其相关辅助函数和数据结构的CUDA关键字，由NVCC编译成称为 PTX (Parallel Thread Execution) 文件的虚拟二进制文件, 由 NVCC runtime 组件进一步编译成目标文件，并在支持 cuda 的 GPU 设备上执行。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB0a32f3aa7a8ffb0fbf51b81c298fcc26?method=download&amp;shareKey=9eb69ac9f65a39b57002dcb02da3a39c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB0a32f3aa7a8ffb0fbf51b81c298fcc26?method=download&amp;shareKey=9eb69ac9f65a39b57002dcb02da3a39c" alt="Overview of the Compilation Process of a CUDA C Program">
    </a><figcaption>Overview of the Compilation Process of a CUDA C Program</figcaption></figure></p>
<hr>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>线程由程序的代码、正在执行的代码中的位置以及它的变量和数据结构的值组成。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><code>cudaMalloc</code> 与 C 语言 <code>malloc</code> 函数的格式不同。前者接受两个参数，指针变量其地址作为第一个参数给出。后者只接受一个参数来指定分配对象的大小，返回一个指向分配对象的指针。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>PMPP Learning-Chapter 1 Introduction</title>
      <link>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch1/</link>
      <pubDate>Tue, 03 Sep 2024 21:20:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/pmpp/pmpp-ch1/</guid>
      <description>Personal notebook 1 of Programming Massively Parallel</description>
      <content:encoded><![CDATA[<h1 id="1-introduction">1 Introduction</h1>
<p>基于单个中央处理器 (Central Processor Unit, CPU) 的微处理器外部看起来是按顺序执行指令，例如英特尔和 AMD 的 x86 处理器，随着时钟频率和硬件资源的快速增长，在20世纪80年代和90年代推动了计算机应用程序的性能快速提高和成本降低。可以给桌面应用提供 GFLOPS 级别的浮点运算，给数据中心提供 TFLOPS 级别的浮点运算。然而，由于能源消耗和散热问题，这种趋势从2003年开始放缓。这些问题限制了时钟频率的增加和保持按顺序步骤执行指令的同时在单个 CPU 上每个时钟周期内可以执行的行动。
之后几乎所有的微处理器供应商都转向了在每个芯片上使用多个物理 CPU (称为处理器核心) 来提高处理能力。在这个模型中，传统的CPU可以看作是一个单核CPU。这样就要求必须有多个指令序列并且可以同时在这些处理器核心上执行 (无论是来自相同的应用程序还是来自不同的应用程序)。为了使一个特定的应用程序受益于多个处理器核心，它的工作必须分成多个指令序列，这些指令序列可以同时在这些处理器核心上执行。这种从单个CPU按顺序执行指令到多个内核并行执行多个指令序列的转变造就了并行计算的需求。</p>
<h2 id="11-heterogeneous-parallel-computing">1.1 Heterogeneous parallel computing</h2>
<p>半导体行业确定了设计微处理器的两条主要路线</p>
<ul>
<li><em>Multicore</em> Trajectory: 寻求在转变到多个核时保持顺序程序的执行速度。</li>
<li><em>Many-thread</em> Trajectory: 更多地关注并行应用程序的执行吞吐量。</li>
</ul>
<p>自2003年以来，多线程处理器尤其是 GPU，一直在浮点计算性能上表现优异。多核和多线程之间在峰值性能上的如此大的差距促使许多应用程序开发人员将其软件的计算密集型部分转移到gpu上执行。</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>64-bit double-precision</th>
          <th>32-bit single-precision</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Tesla A100 GPU</td>
          <td>9.7 TFLOPS</td>
          <td>156 TFLOPS</td>
      </tr>
      <tr>
          <td>Intel 24-core Processor</td>
          <td>0.33 TLOPS</td>
          <td>0.66 TLOPS</td>
      </tr>
  </tbody>
</table>
<p>CPU 的设计为面向延迟的 (<em>latency-oriented</em>) 设计。针对顺序代码性能进行了优化。计算单元和操作数传输逻辑的设计是为了最小化计算的有效延迟，代价是增加芯片面积和单位功率的使用。采用复杂的分支预测逻辑和执行控制逻辑来减少条件分支指令的延迟使得每个线程的执行延迟降低。然而，低延迟计算单元、复杂的操作数传递逻辑、大缓存存储器和控制逻辑消耗了芯片面积和功率，否则可以用来提供更多的算术执行单元和内存访问通道。
GPU 的设计是面向吞吐量 (<em>throught-put oriented</em>)的设计。寻求在有限的芯片面积和功耗预算下最大化浮点计算和内存访问吞吐量。许多图形应用程序的速度受到数据从内存系统传输到处理器的速率的限制，必须能够将大量数据加载和存储到 DRAM 中的图形帧缓冲区。
游戏应用程序普遍接受的宽松内存模型(各种系统软件，应用程序和I/O设备期望其内存访问工作的方式)也使 GPU 更容易支持访问内存的大规模并行性。通用处理器必须满足遗留操作系统、应用程序和I/O设备的要求，这些要求对支持并行内存访问提出了更多挑战，从而使提高内存访问的吞吐量 (通常称为内存带宽 <em>memory bandwidth</em>) 变得更加困难。
就功耗和芯片面积而言，减少延迟比增加吞吐量要昂贵得多<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。因此，GPU 的主流解决方案是针对大量线程的执行吞吐量进行优化，<strong>而不是减少单个线程的延迟</strong>。这种设计方法允许分级存储层次和计算具有较长的延迟，从而节省了芯片面积和功耗。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB0619836cbd0c830367d16469ab356a2e?method=download&amp;shareKey=f86f3077eb42bd1e9ca6ed4c31c18a65" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB0619836cbd0c830367d16469ab356a2e?method=download&amp;shareKey=f86f3077eb42bd1e9ca6ed4c31c18a65" alt="CPU and GPU Design Philosophies">
    </a><figcaption>CPU and GPU Design Philosophies</figcaption></figure></p>
<h2 id="12-why-more-speed-or-parallelism">1.2 Why More Speed or Parallelism</h2>
<p>基于人工神经网络的深度学习是通过大幅提高计算吞吐量而实现的新应用。虽然自 20 世纪 70 年代以来，神经网络得到了积极的关注，但由于需要太多的标记数据和太多的计算来训练这些网络，它们在实际应用中一直效果不佳。互联网的兴起提供了大量有标签的图片，而 GPU 的兴起则带来了计算吞吐量的激增。因此，自2012年以来，基于神经网络的应用在计算机视觉和自然语言处理方面得到了快速的采用。这种采用彻底改变了计算机视觉和自然语言处理应用，并引发了自动驾驶汽车和家庭辅助设备的快速发展。</p>
<h2 id="13-speeding-up-real-applications">1.3 Speeding up real applications</h2>
<p>并行计算系统相对于串行计算系统所能实现的加速的一个重要因素是可以并行化的应用程序部分，另一个重要因素是从内存访问数据和向内存写入数据的速度有多快。下图展示了顺序和并行应用程序部分的覆盖率。顺序部分和传统的(单核)CPU覆盖部分相互重叠。以前的GPGPU技术对数据并行部分的覆盖非常有限，因为它仅限于可以表示为绘制像素的计算。障碍是指难以扩展单核cpu以覆盖更多数据并行部分的功率限制。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfc0b86a42c4ed9223a9b6539c92712fc?method=download&amp;shareKey=796ebc8414ada67e650c087e44aa66a9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfc0b86a42c4ed9223a9b6539c92712fc?method=download&amp;shareKey=796ebc8414ada67e650c087e44aa66a9" alt="Coverage of Application Portions">
    </a><figcaption>Coverage of Application Portions</figcaption></figure></p>
<h2 id="14-challenges-in-parallel-programming">1.4 Challenges in parallel programming</h2>
<ol>
<li>设计具有与顺序算法相同的算法(计算)复杂度的并行算法可能具有挑战性。</li>
<li>许多应用程序的执行速度受到内存访问延迟和/或吞吐量的限制。</li>
<li>与顺序程序相比，并行程序的执行速度通常对输入数据特征更为敏感。</li>
<li>有些应用程序可以并行化，而不需要跨不同线程的协作 (<em>embarrassingly parallel</em>)。其他应用程序需要使用同步操作 (<em>synchronization operations</em>) 使得线程能相互协作。</li>
</ol>
<hr>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>例如，可以通过将计算单元的数量翻倍来使吞吐量翻倍，但代价是芯片面积和功耗翻倍。然而，将算术延迟减少一半可能需要电流翻倍，代价是使用的芯片面积增加一倍以上，功耗变为四倍。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (10)-Computational Graph Optimization</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch8/</link>
      <pubDate>Sun, 25 Aug 2024 16:08:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch8/</guid>
      <description>Personal notebook 7.</description>
      <content:encoded><![CDATA[<h1 id="pattern-match-and-rewriting">Pattern Match and Rewriting</h1>
<p>下面代码中 <code>MyModule</code> 包含一个带有两个高级算子 <code>relax.opmultiply</code> 和 <code>relax.op.add</code> 的 relax 函数。我们的目标是找到这两个算子，并将其替换为对 <code>relax.ewise_fma</code> 算子的调用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span> 
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">y</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span> <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="bp">cls</span> <span class="o">=</span> <span class="n">MyModule</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv0</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv0</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv0</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>每个 IRModule 都包含一组函数，函数体由一组称为抽象语法树（AST）的数据结构组成。
{% fold info @Abstract Syntax Tree %}
抽象语法树（Abstract Syntax Tree，AST）是一种广泛用于编程语言处理的树状数据结构。它是一种对源代码语法结构的抽象表示，去掉了编程语言的具体语法细节，但保留了代码的结构和语义信息。
AST 是一棵树状结构，其节点表示源代码中的语法结构。例如，变量声明、操作符、函数调用、控制结构（如条件语句、循环）等。每个节点包含与相应语法结构相关的信息，如操作符的类型、变量的名称、常量的值等。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="mi">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这个代码可以转换为如下形式的 AST：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-scss" data-lang="scss"><span class="line"><span class="cl"><span class="nt">Assignment</span>
</span></span><span class="line"><span class="cl"><span class="err">├──</span> <span class="nt">Identifier</span> <span class="o">(</span><span class="nt">a</span><span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="err">└──</span> <span class="nt">BinaryOperation</span>
</span></span><span class="line"><span class="cl">    <span class="err">├──</span> <span class="nt">Identifier</span> <span class="o">(</span><span class="nt">b</span><span class="o">)</span>
</span></span><span class="line"><span class="cl">    <span class="err">└──</span> <span class="nt">Constant</span> <span class="o">(</span><span class="nt">1</span><span class="o">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>{% endfold %}
每个函数都由一个 <code>relax.expr.Function</code> 节点表示。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">relax_func</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">[</span><span class="s2">&#34;main&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">type</span><span class="p">(</span><span class="n">relax_func</span><span class="p">)</span>  <span class="c1"># &lt;class &#39;tvm.relax.expr.Function&#39;&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该函数包含一系列参数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">relax_func</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>  <span class="c1"># [x, y]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该函数包含一个返回值表达式，和函数中的一组 binding blocks.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">func_body</span> <span class="o">=</span> <span class="n">relax_func</span><span class="o">.</span><span class="n">body</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">func_body</span><span class="p">))</span>  <span class="c1"># &lt;class &#39;tvm.relax.expr.SeqExpr&#39;&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>函数主体 SeqExpr 包含一系列 binding.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">relax_func</span><span class="o">.</span><span class="n">body</span><span class="o">.</span><span class="n">blocks</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">[x: R.Tensor((3, 4), dtype=&#34;float32&#34;)
</span></span></span><span class="line"><span class="cl"><span class="s1">y: R.Tensor((3, 4), dtype=&#34;float32&#34;)
</span></span></span><span class="line"><span class="cl"><span class="s1">with R.dataflow():
</span></span></span><span class="line"><span class="cl"><span class="s1">    lv0: R.Tensor((3, 4), dtype=&#34;float32&#34;) = R.multiply(x, y)
</span></span></span><span class="line"><span class="cl"><span class="s1">    gv0: R.Tensor((3, 4), dtype=&#34;float32&#34;) = R.add(lv0, y)
</span></span></span><span class="line"><span class="cl"><span class="s1">    R.output(gv0)]
</span></span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 DataflowBlock 中,我们可以访问各个 binding ,包括 value 和 var.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dataflow_block</span> <span class="o">=</span> <span class="n">func_body</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">dataflow_block</span><span class="p">))</span>  <span class="c1"># &lt;class &#39;tvm.relax.expr.DataflowBlock&#39;&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">binding</span> <span class="o">=</span> <span class="n">dataflow_block</span><span class="o">.</span><span class="n">bindings</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">binding</span><span class="p">))</span>  <span class="c1"># &lt;class &#39;tvm.relax.expr.VarBinding&#39;&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">binding</span><span class="o">.</span><span class="n">var</span><span class="p">)</span>  <span class="c1"># LHS of binding: lv0</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">binding</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>  <span class="c1"># # LHS of binding: R.multiply(x, y)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/relax_func_data_structure.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/relax_func_data_structure.png" alt="Relax Function Data Structure">
    </a><figcaption>Relax Function Data Structure</figcaption></figure></p>
<p>改写程序可以通过递归遍历 MyModule 的 AST ，并生成转换后的 AST 来实现。但是我们可以使用额外的工具支持来简化流程。下面的代码遵循一种称为 visitor pattern 的设计模式，允许我们访问每个 AST 节点并将它们重写为转换后的版本。主要目的是将形如 <code>a * b + c</code> 的表达式转换为 <code>ewise_fma(a, b, c)</code> 的形式。</p>
<p><code>EwiseFMARewriter</code> 继承自 <code>relax.PyExprMutator</code>，这是 TVM 中的一个基类，用于遍历和修改表达式树中的节点。<code>visit_call_</code> 方法被重载来处理 <code>relax.Call</code> 节点，被重载来处理 <code>relax.Call</code> 节点。</p>
<p>如果当前节点不是加法操作，直接返回该节点，表示对该节点不进行任何修改。如果加法的第一个操作数不是乘法操作，或者第一个操作数的绑定值不是一个 <code>relax.Call</code> 节点，直接返回该加法节点。如果匹配成功，构造一个新的 <code>ewise_fma</code> 操作节点，将乘法的两个操作数和加法的第二个操作数作为参数传入。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@relax.expr_functor.mutator</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">EwiseFMARewriter</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">PyExprMutator</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">visit_call_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">relax</span><span class="o">.</span><span class="n">Call</span><span class="p">):</span>  <span class="c1"># Reloaded</span>
</span></span><span class="line"><span class="cl">        <span class="n">call</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">visit_expr_post_order</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">add_op</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">Op</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;relax.add&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">multiply_op</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">Op</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;relax.multiply&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ewise_fma_op</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">Op</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;relax.ewise_fma&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">call</span><span class="o">.</span><span class="n">op</span> <span class="o">!=</span> <span class="n">add_op</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">call</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookup_binding</span><span class="p">(</span><span class="n">call</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">Call</span><span class="p">)</span> <span class="ow">or</span> <span class="n">value</span><span class="o">.</span><span class="n">op</span> <span class="o">!=</span> <span class="n">multiply_op</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">call</span> 
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="n">fma_call</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Call</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">ewise_fma_op</span><span class="p">,</span> <span class="p">[</span><span class="n">value</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">value</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">call</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">fma_call</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="n">updated_fn</span> <span class="o">=</span> <span class="n">EwiseFMARewriter</span><span class="p">()</span><span class="o">.</span><span class="n">visit_expr</span><span class="p">(</span><span class="n">MyModule</span><span class="p">[</span><span class="s2">&#34;main&#34;</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">updated_fn</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-----------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">y</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">lv0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">gv0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">ewise_fma</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">gv0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>使用 <code>remove_all_unused</code> 来删除代码中没有用到的 DataflowBlocks 和 VarBindings.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">relax</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">remove_all_unused</span><span class="p">(</span><span class="n">updated_fn</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-------------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">y</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">gv0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">ewise_fma</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">gv0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="fuse-linear-and-relu">Fuse Linear and ReLU</h1>
<p>下面在端到端模型上进行计算图的改写。采用的还是之前使用的 FashionMNIST MLP 模型。为了简化过程，直接使用高级运算符构建模型。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pickle</span> <span class="k">as</span> <span class="nn">pkl</span>
</span></span><span class="line"><span class="cl"><span class="n">mlp_params</span> <span class="o">=</span> <span class="n">pkl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s2">&#34;fasionmnist_mlp_params.pkl&#34;</span><span class="p">,</span> <span class="s2">&#34;rb&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">bb</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">BlockBuilder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="s2">&#34;x&#34;</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">w0</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">[</span><span class="s2">&#34;w0&#34;</span><span class="p">],</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b0</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">[</span><span class="s2">&#34;b0&#34;</span><span class="p">],</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w1</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">[</span><span class="s2">&#34;w1&#34;</span><span class="p">],</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b1</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">mlp_params</span><span class="p">[</span><span class="s2">&#34;b1&#34;</span><span class="p">],</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv0</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">w0</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv0</span><span class="p">,</span> <span class="n">b0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">lv1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">lv2</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">w1</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">b1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_output</span><span class="p">(</span><span class="n">lv4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">MLPModel</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">MLPModel</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lv</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">lv2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv5</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">lv4</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv6</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv5</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">lv6</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们的目标是对 matmul 和 add 进行算子融合。具体实现步骤与 FMA 相似：</p>
<ol>
<li>识别 matmul 和 add 算子。</li>
<li>生成另一个调用 matmul 和 add 算子的子函数。</li>
<li>将 matmul 和 add 替换为融合后的子函数。</li>
</ol>
<p>下面代码定义了一个名为 <code>DenseAddFusor</code> 的类，用于在 TVM 的 Relax 框架中将特定的矩阵乘法和加法操作模式融合成一个高效的原语函数。</p>
<ul>
<li><code>transform</code> 方法遍历模块中的每个函数。如果函数已经被标记为 primitive（即已经被融合过），则跳过。对每个函数应用  <code>visit_expr</code> 以进行模式匹配和潜在的融合操作，然后删除未使用的变量，并更新函数。最后，返回更新后的 <code>IRModule</code>。</li>
<li><code>visit_call_</code> 方法用于访问 <code>relax.Call</code> 节点（表示操作符调用）。它首先递归处理子表达式，然后尝试匹配特定模式。<code>match_call</code> 是一个内部函数，用于检查某个节点是否是特定操作符的调用。如果当前节点不是 <code>add</code> 操作，或者 <code>add</code> 操作的第一个参数不是 <code>matmul</code>（矩阵乘法）操作，则直接返回当前节点，不进行修改。如果匹配成功，则提取 <code>matmul</code> 的两个操作数 <code>x</code> 和 <code>w</code> 以及 <code>add</code> 的第二个操作数 <code>b</code>，准备进行融合。</li>
<li>通过 <code>relax.BlockBuilder</code>定义一个名为 <code>fused_dense_addX</code>新的融合函数，其中 <code>X</code> 是一个递增的计数器。该函数接收 <code>x</code>、<code>w</code>、<code>b</code> 作为参数，首先进行矩阵乘法，然后将结果与 <code>b</code> 相加，最终输出结果。</li>
<li>给新生成的融合函数添加一个属性 Primitive，标记为已经融合的原语函数。通过 <code>builder_</code> 更新全局模块，将融合函数添加到模块中 (GlobalVar 用于指代存储在 IRModule 中的全局函数)。返回一个新的 <code>relax.Call</code> 节点，该节点调用生成的融合函数，并传递原始的输入参数 <code>x</code>、<code>w</code>、<code>b</code>。</li>
</ul>
<details class="custom-details">
    <summary class="custom-summary">VisitExpr</summary>
    <div><p>TVM 中的 <code>VisitExpr</code> 流程是一种递归遍历 IR 节点的机制,它是实现各种 IR 转换和优化的基础。具体流程如下:</p>
<ol>
<li>首先创建一个 <code>ExprVisitor</code> 或 <code>ExprMutator</code> 的子类实例,这个子类会实现各种具体的访问逻辑。</li>
<li>调用 <code>visit_expr</code> 方法,传入根 IR 节点。这个方法会触发整个遍历过程的启动。</li>
<li><code>visit_expr</code> 方法会首先调用 <code>visit_expr_post_order</code> 方法,这个方法会以深度优先的方式遍历所有子节点。</li>
<li>对于每个子节点,<code>visit_expr_post_order</code> 会根据节点的具体类型,调用相应的 <code>visit_XXX_</code> 方法。这些 <code>visit_XXX_</code> 方法是由访问器子类实现的,包含了具体的访问逻辑。</li>
<li>在 <code>visit_XXX_</code> 方法中,如果遇到子节点,会递归调用 <code>visit_expr_post_order</code> 方法继续遍历。</li>
<li>当遍历完整个 IR 树后,<code>visit_expr</code> 方法会返回最终的结果,即经过转换和修改的 IR 节点。</li>
</ol></div>
</details><br>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@relax.expr_functor.mutator</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DenseAddFusor</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">PyExprMutator</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">:</span> <span class="n">IRModule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mod_</span> <span class="o">=</span> <span class="n">mod</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># cache pre-defined ops</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">add_op</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">Op</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;relax.add&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dense_op</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">Op</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&#34;relax.matmul&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">IRModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">global_var</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod_</span><span class="o">.</span><span class="n">functions_items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># avoid already fused primitive function</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="s2">&#34;Primitive&#34;</span> <span class="ow">in</span> <span class="n">func</span><span class="o">.</span><span class="n">attrs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="ow">and</span> <span class="n">func</span><span class="o">.</span><span class="n">attrs</span><span class="p">[</span><span class="s2">&#34;primitive&#34;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span>
</span></span><span class="line"><span class="cl">            <span class="n">updated_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">visit_expr</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">updated_fn</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">analysis</span><span class="o">.</span><span class="n">remove_all_unused</span><span class="p">(</span><span class="n">updated_fn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">builder_</span><span class="o">.</span><span class="n">update_func</span><span class="p">(</span><span class="n">global_var</span><span class="p">,</span> <span class="n">updated_fn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">builder_</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">visit_call_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">relax</span><span class="o">.</span><span class="n">Call</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">call</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">visit_expr_post_order</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">match_call</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">Call</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">node</span><span class="o">.</span><span class="n">op</span> <span class="o">==</span> <span class="n">op</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="c1"># pattern match dense =&gt; add</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">match_call</span><span class="p">(</span><span class="n">call</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_op</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">call</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookup_binding</span><span class="p">(</span><span class="n">call</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">call</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">match_call</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_op</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">call</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">w</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">b</span> <span class="o">=</span> <span class="n">call</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="c1"># construct a new fused primitive function</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_x</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="s2">&#34;x&#34;</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">struct_info</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">struct_info</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_w</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="s2">&#34;w&#34;</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">struct_info</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">struct_info</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">param_b</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="s2">&#34;b&#34;</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">struct_info</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">struct_info</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="n">bb</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">BlockBuilder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="n">fn_name</span> <span class="o">=</span> <span class="s2">&#34;fused_dense_add</span><span class="si">%d</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">fn_name</span><span class="p">,</span> <span class="p">[</span><span class="n">param_x</span><span class="p">,</span> <span class="n">param_w</span><span class="p">,</span> <span class="n">param_b</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="n">lv0</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">param_x</span><span class="p">,</span> <span class="n">param_w</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="n">gv0</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_output</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv0</span><span class="p">,</span> <span class="n">param_b</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">gv0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="c1"># add primitive attribute to the fused functions</span>
</span></span><span class="line"><span class="cl">        <span class="n">fused_fn</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">get</span><span class="p">()[</span><span class="n">fn_name</span><span class="p">]</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&#34;Primitive&#34;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">global_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">builder_</span><span class="o">.</span><span class="n">add_func</span><span class="p">(</span><span class="n">fused_fn</span><span class="p">,</span> <span class="n">fn_name</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="c1"># construct call into the fused function</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">relax</span><span class="o">.</span><span class="n">Call</span><span class="p">(</span><span class="n">global_var</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tvm.ir.transform.module_pass</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;DenseAddFuse&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">FuseDenseAddPass</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;The wrapper for the LowerTensorIR pass.&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">transform_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">DenseAddFusor</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">MLPFused</span> <span class="o">=</span> <span class="n">FuseDenseAddPass</span><span class="p">()(</span><span class="n">MLPModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">MLPFused</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>融合后的 MLPFused 对应的 TensorIR 如下</p>
<blockquote>
<p>TVM 框架中使用 module_pass 来管理各种优化操作。这种机制允许将不同的优化操作（如图优化、代码生成、算子融合等）组织成一个流水线（pipeline），按顺序对模块进行处理。将 DenseAddFusor 封装为一个 module_pass，使得它能够轻松集成到 TVM 的 Pass 流水线中，与其他 Pass 一起工作，从而保证优化过程的整体性和一致性。</p></blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fused_dense_add0</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">w</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">b</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">R</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;Primitive&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fused_dense_add1</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">w</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">b</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">R</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;Primitive&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">fused_dense_add0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lv</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">lv2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv6</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">fused_dense_add1</span><span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">lv4</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">lv6</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上面的例子中，我们创建了两个前缀为 fuse_matmul_add 的子函数。 这些子函数包含有融合后算子的计算信息。 这种重写的替代方法是简单地为融合算子创建一个单独的原语算子（如ewise_fma）。 但是，当我们尝试融合更多算子时，可能存在指数级数量的组合。 将融合操作分组在一起的子函数为后续的 pass 保留了原始信息，进而便于分析，无需为每个融合 pattern 引入专用的高级算子。</p>
<h1 id="map-to-tensorir-calls">Map to TensorIR Calls</h1>
<p>为了进一步进行底层优化和代码生成，我们需要将这些高级原语运算转换为相应的 TensorIR 函数。下面代码主要功能是将 Relax 表达式树中的高层次算子（ <code>matmul</code>、<code>add</code>、<code>relu</code>）转换为对应的 TensorIR 表示，从而使得这些算子能够映射到底层的张量操作（tensor operations）。这种转换使得编译器可以生成更接近硬件的高效代码，并为后续的代码优化和生成做好准备。</p>
<ol>
<li>调用 <code>transform</code> 方法会遍历 <code>mod_</code> 中的所有函数:
<ul>
<li>对于每个函数,首先调用 <code>visit_expr</code> 方法,这会触发 <code>VisitExpr</code> 流程</li>
<li><code>visit_expr</code> 方法会调用 <code>visit_expr_post_order</code>方法进行深度优先遍历</li>
<li>在遍历过程中对于每个 <code>relax.Call</code> 节点,会调用 <code>visit_call_</code> 方法</li>
<li><code>visit_call_</code> 方法会检查 <code>op_map</code> 字典,如果当前操作在字典中,则调用对应的转换函数( <code>map_dense</code>, <code>map_add</code>, <code>map_relu</code>)</li>
<li>这些转换函数会使用 <code>bb.call_te</code> 方法,将 Relax IR 操作转换为 TensorIR 操作</li>
</ul>
</li>
<li>在 <code>transform</code> 方法的最后,会调用 <code>builder_.get()</code> 方法,返回转换后的新 IR 模块。</li>
<li>最后 <code>LowerToTensorIRPass</code> 类将 <code>LowerToTensorIR</code> 转换器包装成一个可注册到 TVM 优化 pipeline 的 pass.</li>
</ol>
<p><code>module_pass</code> 的 <code>opt_level</code> 参数决定了优化 pass 在优化 pipeline 中的执行顺序。 TVM 的优化 pipeline 是由多个 <code>module_pass</code> 组成的,每个 <code>module_pass</code> 都有一个 <code>opt_level</code> 属性来指定它的优化级别。</p>
<p>当 TVM 进行优化时,它会按照 <code>opt_level</code> 从低到高的顺序依次应用各个 <code>module_pass</code>. <code>opt_level=0</code> 的 pass 会首先被执行。这些 pass 通常会执行一些基础的、必要的转换,为后续的优化奠定基础。 随后会执行 <code>opt_level=1</code> 的 pass,这些 pass 可能会执行一些更复杂的优化,比如循环优化、内存访问优化等。依此类推,<code>opt_level</code> 越高的 pass 会在优化 pipeline 的后期执行,它们执行的优化通常也越复杂和深入。</p>
<p>通过合理地设置 <code>opt_level</code>,开发者可以控制各个优化 pass 的执行顺序,从而构建出针对性强、性能优秀的优化 pipeline 。这种灵活的优化管理机制是 TVM 的一大特点。</p>
<p>对于 <code>LowerToTensorIRPass</code>,它的 <code>opt_level</code> 被设置为 0, 说明它是一个基础的 pass, 主要用于将高级的 Relax IR 操作转换为底层的 TensorIR 操作。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@relax.expr_functor.mutator</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LowerToTensorIR</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">PyExprMutator</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">:</span> <span class="n">IRModule</span><span class="p">,</span> <span class="n">op_map</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mod_</span> <span class="o">=</span> <span class="n">mod</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">op_map</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">Op</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">op_map</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">visit_call_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">relax</span><span class="o">.</span><span class="n">Call</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">call</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">visit_expr_post_order</span><span class="p">(</span><span class="n">op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">call</span><span class="o">.</span><span class="n">op</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">op_map</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">op_map</span><span class="p">[</span><span class="n">call</span><span class="o">.</span><span class="n">op</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">builder_</span><span class="p">,</span> <span class="n">call</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">call</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">IRModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">global_val</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod_</span><span class="o">.</span><span class="n">functions_items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span>
</span></span><span class="line"><span class="cl">            <span class="n">updated_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">visit_expr</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">builder_</span><span class="o">.</span><span class="n">update_func</span><span class="p">(</span><span class="n">global_val</span><span class="p">,</span> <span class="n">updated_fn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">          
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">builder_</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">map_dense</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">call</span><span class="o">.</span><span class="n">args</span> 
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">call_te</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">matmul</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">map_add</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">call</span><span class="o">.</span><span class="n">args</span>           
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">call_te</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">map_relu</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">call_te</span><span class="p">(</span><span class="n">topi</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">call</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">op_map</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;relax.matmul&#34;</span><span class="p">:</span> <span class="n">map_dense</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;relax.add&#34;</span><span class="p">:</span> <span class="n">map_add</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;relax.nn.relu&#34;</span><span class="p">:</span> <span class="n">map_relu</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tvm.ir.transform.module_pass</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;LowerToTensorIR&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LowerToTensorIRPass</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;The wrapper for the LowerTensorIR pass.&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">transform_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">LowerToTensorIR</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">op_map</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="n">MLPModelTIR</span> <span class="o">=</span> <span class="n">LowerToTensorIRPass</span><span class="p">()(</span><span class="n">MLPFused</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">MLPModelTIR</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>融合后的 TensorIR 如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span><span class="lnt">90
</span><span class="lnt">91
</span><span class="lnt">92
</span><span class="lnt">93
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">lv</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">b</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">T_add</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;T_add&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">v_ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">T_add</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T_add</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">]</span> <span class="o">=</span> <span class="n">lv</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">v_ax1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">add1</span><span class="p">(</span><span class="n">lv</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">b</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">),),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">T_add</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;T_add&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">v_ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">T_add</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T_add</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">]</span> <span class="o">=</span> <span class="n">lv</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">v_ax1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">w</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">T_matmul_NN</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;layout_free_buffers&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;T_matmul_NN&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">],</span> <span class="n">w</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">T_matmul_NN</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T_matmul_NN</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T_matmul_NN</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T_matmul_NN</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">matmul1</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">w</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">T_matmul_NN</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;layout_free_buffers&#34;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;T_matmul_NN&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">],</span> <span class="n">w</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">T_matmul_NN</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T_matmul_NN</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T_matmul_NN</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T_matmul_NN</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">lv2</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv2</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">lv2</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fused_dense_add0</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">w</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">b</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">R</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;Primitive&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">matmul</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="p">(</span><span class="n">lv</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fused_dense_add1</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">w</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">b</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">R</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;Primitive&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">matmul1</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">add1</span><span class="p">,</span> <span class="p">(</span><span class="n">lv</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">fused_dense_add0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lv</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="p">(</span><span class="n">lv2</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv6</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">fused_dense_add1</span><span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">lv4</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">lv6</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在上面的 IRModule 中 <code>fused_matmul_add0</code> 和 <code>fused_matmul_add1</code> 仍然是 relax 函数，它们调用相应的 TensorIR <code>matmul</code> 和 <code>add</code> 函数。 我们可以将它们变成一个单一的 TensorIR 函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">MLPModelFinal</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FuseTIR</span><span class="p">()(</span><span class="n">MLPModelTIR</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">MLPModelFinal</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-----------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fused_dense_add0</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">w</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">b</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">T_add_intermediate</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T_matmul_NN_intermediate</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;T_matmul_NN&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">],</span> <span class="n">w</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;T_add&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">v_ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">T_add_intermediate</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T_add_intermediate</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">v_ax1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fused_dense_add1</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">w</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">b</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">),),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">T_add_intermediate</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T_matmul_NN_intermediate</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;T_matmul_NN&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">],</span> <span class="n">w</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;T_add&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">v_ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">T_add_intermediate</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T_add_intermediate</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T_matmul_NN_intermediate</span><span class="p">[</span><span class="n">v_ax0</span><span class="p">,</span> <span class="n">v_ax1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">v_ax1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">lv2</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv2</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">lv2</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">fused_dense_add0</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lv</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="p">(</span><span class="n">lv2</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">fused_dense_add1</span><span class="p">,</span> <span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">lv4</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">3</span><span class="p">]),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (9)-GPU and Hardware Acceleration, Part 2</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch7/</link>
      <pubDate>Sun, 25 Aug 2024 13:22:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch7/</guid>
      <description>Personal notebook 7.</description>
      <content:encoded><![CDATA[<h1 id="key-elements-of-specialized-code">Key Elements of Specialized Code</h1>
<p>下面用 low-level numpy 写的 python 代码展示了一系列在专用硬件后端可能使用到的操作。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">accel_fill_zero</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">C</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">accel_tmm_add</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">C</span><span class="p">[:]</span> <span class="o">+=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">B</span><span class="o">.</span><span class="n">T</span>         
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">accel_dma_copy</span><span class="p">(</span><span class="n">reg</span><span class="p">,</span> <span class="n">dram</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">reg</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">dram</span><span class="p">[:]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们假设基础的运算单元可以进行 <code>16x16</code>的矩阵乘法 (<code>accel_tmm_add</code>)，接收2个寄存器里的 RHS 输入和表示累加中间结果的 LHS 输入，数据拷贝使用的是专用函数 (<code>accel_dma_copy</code>).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># The basis unit of computation is a 16*16*16 matrix multiplication</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_tmm</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">C</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># a special accumulator memory</span>
</span></span><span class="line"><span class="cl">    <span class="n">C_accumulator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">A_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_reg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">accel_fill_zero</span><span class="p">(</span><span class="n">C_accumulator</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">accel_dma_copy</span><span class="p">(</span><span class="n">A_reg</span><span class="p">[:],</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">16</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span> <span class="n">k</span><span class="o">*</span><span class="mi">16</span> <span class="p">:</span> <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">accel_dma_copy</span><span class="p">(</span><span class="n">B_reg</span><span class="p">[:],</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="mi">16</span> <span class="p">:</span> <span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span> <span class="n">k</span><span class="o">*</span><span class="mi">16</span> <span class="p">:</span> <span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">accel_tmm_add</span><span class="p">(</span><span class="n">C_accumulator</span><span class="p">,</span> <span class="n">A_reg</span><span class="p">,</span> <span class="n">B_reg</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">accel_dma_copy</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">16</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">16</span><span class="p">,</span> <span class="n">j</span><span class="o">*</span><span class="mi">16</span> <span class="p">:</span> <span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">16</span><span class="p">],</span> <span class="n">C_accumulator</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="a-block-with-tensorized-computation">A Block with Tensorized Computation</h1>
<p>专用加速器代码的结构并非以标量计算为单位。迄今为止，我们运行的大多数 TensorIR 代码都包含一个 block，用于计算输出张量中的单个元素。许多专用加速器在张量区域内进行计算。TensorIR中的 block 可以帮助我们将这些相关计算分组。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>   
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MatmulBlockModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">             <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">             <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">j0</span><span class="p">,</span> <span class="n">k0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm-16x16&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi0</span><span class="p">,</span> <span class="n">vj0</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">j0</span><span class="p">,</span> <span class="n">k0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i1</span><span class="p">,</span> <span class="n">j1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">vi1</span><span class="p">,</span> <span class="n">vj1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">k1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">vi1</span><span class="p">,</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">k1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>调用 <code>MatmulBlockModule.show()</code> 后显示的 TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该代码从 <code>A</code> 和 <code>B</code> 的 <code>16x16</code> 区域读取数据，并写入 <code>C</code> 的 <code>16x16</code> 区域。在这种情况下，块的内容包含子区域计算的具体实现的进一步细节。我们称这种区块为<strong>张量区块</strong>，因为它们包含跨越张量子区域的计算。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">j0</span><span class="p">,</span> <span class="n">k0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm-16x16&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi0</span><span class="p">,</span> <span class="n">vj0</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">j0</span><span class="p">,</span> <span class="n">k0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i1</span><span class="p">,</span> <span class="n">j1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">vi1</span><span class="p">,</span> <span class="n">vj1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">k1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">vi1</span><span class="p">,</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">k1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="transforming-loops-around-tensorized-block">Transforming Loops Around Tensorized Block</h1>
<p>我们可以对张量计算块的循环进行变换，这些循环变换可以重新组织计算该块的迭代方式，得到不同的张量程序。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MatmulBlockModule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">block_mm</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;tmm-16x16&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_mm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0_0</span><span class="p">,</span> <span class="n">j0</span><span class="p">,</span> <span class="n">i0_1</span><span class="p">,</span> <span class="n">k0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm-16x16&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">i0_0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">i0_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">vj0</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">j0</span><span class="p">,</span> <span class="n">k0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i1</span><span class="p">,</span> <span class="n">j1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">vi1</span><span class="p">,</span> <span class="n">vj1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">k1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;tmm&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">vi1</span><span class="p">,</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">k1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vj0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj1</span><span class="p">,</span> <span class="n">vk0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk1</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="blockization--creating-tensorized-blocks">Blockization &ndash; Creating Tensorized Blocks</h1>
<p>TensorIR 提供了一种变换原语 <code>blockize</code> 来将循环的子区域组合在一起以形成张量化的计算 block. 例如我们可以将下面2个的 <code>1024x1024</code> 矩阵乘法分解成很多个 <code>16x16</code> 的矩阵乘法。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span> 
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MatmulModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MatmulModule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">j</span><span class="p">,</span> <span class="n">ji</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">k</span><span class="p">,</span> <span class="n">ki</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">ii</span><span class="p">,</span> <span class="n">ji</span><span class="p">,</span> <span class="n">ki</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span><span class="p">,</span> <span class="n">j_0</span><span class="p">,</span> <span class="n">k_0</span><span class="p">,</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">,</span> <span class="n">k_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">i_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">j_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">k_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">k_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<p><code>blockize</code> 是用来将一个或多个块(block)或一个特定循环的子树合并成一个新的块。如果 <code>target</code> 是一个循环的根节点,则会将该循环下的所有块合并成一个新块，如果 <code>target</code> 是一个块的列表,则会将这些块合并成一个新块。然后将新块返回</p>
<p><strong>参数说明</strong> :</p>
<ul>
<li><code>target</code>: 需要被合并的块或循环的根节点。可以是 <code>LoopRV</code> 类型(表示一个循环)或 <code>List[BlockRV]</code> 类型(表示多个块)。</li>
<li><code>preserve_unit_iters</code>: 一个布尔值,表示是否保留块绑定中的单元迭代器。</li>
</ul>
<p><strong>限制条件</strong> :</p>
<ul>
<li><code>blockize</code> 要求给定的循环下只有一个块,且该块的绑定必须能够被该循环的子空间整除。</li>
</ul>
<p>调用 <code>blockize</code> 后的 TensorIR 如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">block_mm</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">blockize</span><span class="p">(</span><span class="n">ii</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span><span class="p">,</span> <span class="n">j_0</span><span class="p">,</span> <span class="n">k_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul_o&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi_o</span><span class="p">,</span> <span class="n">vj_o</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_0</span><span class="p">,</span> <span class="n">j_0</span><span class="p">,</span> <span class="n">k_0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_i_init</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i_init</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">C</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i_init</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">,</span> <span class="n">k_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">,</span> <span class="n">k_1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">C</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="transforming-tensorir-to-introduce-special-memory-scope">Transforming TensorIR to Introduce Special Memory Scope</h1>
<p>正如在 low-level NumPy 代码中提到的，底层 TensorIR 的一个关键要素是加速过程中使用的特殊内存范围。我们可以使用 cache_read 和 write 来创建中间内存阶段。</p>
<p>storage_scope 在这里指的是内存存储范围或存储层次。常见的存储范围包括:</p>
<ul>
<li>global: 表示数据存储在全局内存中。这是最高层次的内存范围。</li>
<li>shared: 表示数据存储在GPU的共享内存中。</li>
<li>local: 表示数据存储在CPU或GPU的寄存器中。这是最底层的内存范围。</li>
</ul>
<p><code>global.A_reg</code> 表示数据将被缓存到一个名为 A_reg 的全局内存缓存中。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/hardware_specialization_abc.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/hardware_specialization_abc.png" alt="Storage Scope">
    </a><figcaption>Storage Scope</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A_reg</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">cache_read</span><span class="p">(</span><span class="n">block_mm</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">storage_scope</span><span class="o">=</span><span class="s2">&#34;global.A_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B_reg</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">cache_read</span><span class="p">(</span><span class="n">block_mm</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">storage_scope</span><span class="o">=</span><span class="s2">&#34;global.B_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">A_reg</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">B_reg</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">write_back_block</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">cache_write</span><span class="p">(</span><span class="n">block_mm</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">storage_scope</span><span class="o">=</span><span class="s2">&#34;global.accumulator&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">reverse_compute_at</span><span class="p">(</span><span class="n">write_back_block</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-----------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">A_global_A_reg</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.A_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">B_global_B_reg</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.B_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">C_global_accumulator</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.accumulator&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span><span class="p">,</span> <span class="n">j_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k_0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;A_global.A_reg&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">k_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_global.B_reg&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">k_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul_o&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi_o</span><span class="p">,</span> <span class="n">vj_o</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_0</span><span class="p">,</span> <span class="n">j_0</span><span class="p">,</span> <span class="n">k_0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">],</span> <span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                        <span class="k">for</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                                <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_i_init</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i_init</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                                <span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i_init</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">,</span> <span class="n">k_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">,</span> <span class="n">k_1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">],</span> <span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">],</span> <span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">])</span>    
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">]</span> <span class="o">+</span> <span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_global.accumulator&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="tensorization">Tensorization</h1>
<p>现在我们已经创建了一组映射到 TensorIR 中相应计算阶段的块。剩下的步骤是映射部分张量块，以使用映射到硬件加速指令的特定实现。这一映射过程称为<strong>张量化</strong>。为了实现张量化，我们首先注册一个 TensorIntrin，其中包含计算和实现的描述。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tmm16_desc</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">offset_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.A_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">offset_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.B_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">offset_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.accumulator&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;root&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vii</span><span class="p">,</span> <span class="n">vjj</span><span class="p">,</span> <span class="n">vkk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vii</span><span class="p">,</span> <span class="n">vjj</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">vii</span><span class="p">,</span> <span class="n">vjj</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vii</span><span class="p">,</span> <span class="n">vkk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vjj</span><span class="p">,</span> <span class="n">vkk</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tmm16_impl</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">offset_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.A_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">offset_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.B_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">offset_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.accumulator&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sa</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="c1">#T.var(&#34;int32&#34;)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sb</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="c1">#T.var(&#34;int32&#34;)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sc</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="c1">#T.var(&#34;int32&#34;)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;root&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">T</span><span class="o">.</span><span class="n">call_extern</span><span class="p">(</span><span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="s2">&#34;tmm16&#34;</span><span class="p">,</span> <span class="n">C</span><span class="o">.</span><span class="n">access_ptr</span><span class="p">(</span><span class="s2">&#34;w&#34;</span><span class="p">),</span> <span class="n">A</span><span class="o">.</span><span class="n">access_ptr</span><span class="p">(</span><span class="s2">&#34;r&#34;</span><span class="p">),</span> <span class="n">B</span><span class="o">.</span><span class="n">access_ptr</span><span class="p">(</span><span class="s2">&#34;r&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                          <span class="n">sa</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">sc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">TensorIntrin</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&#34;tmm16&#34;</span><span class="p">,</span> <span class="n">tmm16_desc</span><span class="p">,</span> <span class="n">tmm16_impl</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>首先我们用 <code>decompose_reduction</code> 将 <code>C_global_accumulator</code> 的初始化和更新部分分开成 <code>T.block(&quot;matmul_init&quot;)</code> 和 <code>T.block(&quot;matmul_o_update&quot;)</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block_mm</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#---------------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">A_global_A_reg</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.A_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">B_global_B_reg</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.B_reg&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">C_global_accumulator</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;global.accumulator&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span><span class="p">,</span> <span class="n">j_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul_o_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi_o</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_0</span><span class="p">,</span> <span class="n">j_0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_i_init</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i_init</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i_init</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i_init</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k_0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;A_global.A_reg&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">k_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_global.B_reg&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">k_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul_o_update&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi_o</span><span class="p">,</span> <span class="n">vj_o</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_0</span><span class="p">,</span> <span class="n">j_0</span><span class="p">,</span> <span class="n">k_0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">],</span> <span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">],</span> <span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span><span class="p">:</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">,</span> <span class="n">k_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_1</span><span class="p">,</span> <span class="n">j_1</span><span class="p">,</span> <span class="n">k_1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">],</span> <span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">],</span> <span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">])</span>    
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">]</span> <span class="o">+</span> <span class="n">A_global_A_reg</span><span class="p">[</span><span class="n">vi_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vi_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B_global_B_reg</span><span class="p">[</span><span class="n">vj_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vj_i</span><span class="p">,</span> <span class="n">vk_o</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">vk_i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_global.accumulator&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">ax1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_global_accumulator</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后我们调用 <code>tensorize</code>，将 <code>block_mm</code>（对应于 <code>matmul_o_update</code> block ）映射到 <code>tmm16_impl</code>. 这里我们使用 <code>T.call_extern</code> 来调用环境中的外部函数。 下游编译步骤可以轻松地将实现映射到实现操作的指令。或者我们可以将 <code>tmm16</code> 映射到实现这种张量化计算的微内核。 以下代码显示了如何通过外部 C++ 代码执行此操作。</p>
<p>具体实现步骤如下:</p>
<ol>
<li>定义 C++ 风格的 <code>tmm16</code> 函数: 这个函数实现了一个 16x16 矩阵乘法的计算逻辑。它接受三个输入张量 <code>aa</code>、<code>bb</code> 和 <code>cc</code>，以及对应的步长 <code>stride_a</code>、<code>stride_b</code> 和 <code>stride_c</code>。函数使用三重循环执行矩阵乘法的计算,将结果累加到 <code>cc</code> 张量中。</li>
<li>使用 TVM 的 <code>clang</code> 模块将 C++ 代码编译为 LLVM IR 代码: 首先创建一个临时目录 <code>temp</code> 用于存储生成的 LLVM IR 文件。然后调用 <code>clang.create_llvm()</code> 函数,传入 C++ 代码字符串 <code>cc_code</code>。<code>create_llvm()</code> 函数会将 C++ 代码编译为 LLVM IR 代码,并保存到 <code>ll_path</code> 指定的文件中。最后返回生成的 LLVM IR 代码。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">tmm_kernel</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">cc_code</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">        extern &#34;C&#34; int tmm16(float *cc, float *aa, float *bb, int stride_a, int stride_b, int stride_c) {
</span></span></span><span class="line"><span class="cl"><span class="s1">            for (int i = 0; i &lt; 16; i++) {
</span></span></span><span class="line"><span class="cl"><span class="s1">                for (int j = 0; i &lt; 16; j++) {
</span></span></span><span class="line"><span class="cl"><span class="s1">                    for (int k = 0; k &lt; 16; k++) {
</span></span></span><span class="line"><span class="cl"><span class="s1">                        cc[i * stride_c + j] += aa[i * stride_a + k] * bb[j * stride_b + k];
</span></span></span><span class="line"><span class="cl"><span class="s1">                    }
</span></span></span><span class="line"><span class="cl"><span class="s1">                }
</span></span></span><span class="line"><span class="cl"><span class="s1">            }
</span></span></span><span class="line"><span class="cl"><span class="s1">            return 0;
</span></span></span><span class="line"><span class="cl"><span class="s1">        }
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">utils</span><span class="p">,</span> <span class="n">clang</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">temp</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">tempdir</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">ll_path</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&#34;temp.ll&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create LLVM ir from c source code</span>
</span></span><span class="line"><span class="cl">    <span class="n">ll_code</span> <span class="o">=</span> <span class="n">clang</span><span class="o">.</span><span class="n">create_llvm</span><span class="p">(</span><span class="n">cc_code</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">ll_path</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">ll_code</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>调用 <code>sch.tensorize(block_mm, &quot;tmm16&quot;)</code>报错，原因未知。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">发生异常: TVMError
</span></span><span class="line"><span class="cl">TVMError: invalid unordered_map&lt;K, T&gt; key
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;C:\Users\17725\Desktop\Machine Learning Compilation\chapter7.py&#34;</span>, line 186, in &lt;module&gt;
</span></span><span class="line"><span class="cl">    sch.tensorize<span class="o">(</span>block_mm, <span class="s2">&#34;tmm16&#34;</span><span class="o">)</span>
</span></span><span class="line"><span class="cl">tvm._ffi.base.TVMError: TVMError: invalid unordered_map&lt;K, T&gt; key
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (8)-GPU and Hardware Acceleration, Part 1</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch6/</link>
      <pubDate>Sat, 24 Aug 2024 09:28:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch6/</guid>
      <description>Personal notebook 6.</description>
      <content:encoded><![CDATA[<h1 id="gpu-architecture">GPU Architecture</h1>
<p>典型的 GPU 包含一系列流多处理器 (Stream Multi-processor, SM)，每个多处理器都有许多内核 (core). GPU 具有高度并行性，可以同时执行多项任务。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/gpu_arch.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/gpu_arch.png" alt="GPU Architecture">
    </a><figcaption>GPU Architecture</figcaption></figure></p>
<p>要对 GPU 进行编程，我们需要创建一组线程块 (thread blocks)，每个 thread 映射到单个核心，而 block 映射到流式多处理器 (SM)。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/gpu_stream_processors.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/gpu_stream_processors.png" alt="GPU Programming">
    </a><figcaption>GPU Programming</figcaption></figure></p>
<p>我们以两个长度为1024的向量加法 <code>C=A+B</code>为例，我们先把外循环 split 成两部分</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>  
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModuleVecAdd</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">             <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">             <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1024</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;S&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MyModuleVecAdd</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_C</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block_C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>得到的 TensorIR 如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span><span class="p">,</span> <span class="n">i_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="n">i_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="build-and-run-the-tensorir-function-on-gpu">Build and Run the TensorIR Function on GPU</h1>
<p>一个CUDA程序的计算被组织成三层次：网格（Grid）、线程块（Block）和线程（Thread）。网格是一个二维的数组，包含多个线程块。每个线程块也是一个二维的数组，包含多个线程。每个线程执行相同的代码，但是在执行时可以使用不同的数据。每个线程由两个索引进行表示 <code>threadIdx.x</code>和 <code>blockIdx.x</code>. 在实际应用中，有多维线程索引，但这里我们为了简化问题，将它们固定为一维表示。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/gpu_thread_blocks.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/gpu_thread_blocks.png" alt="GPU Thread Block">
    </a><figcaption>GPU Thread Block</figcaption></figure></p>
<ul>
<li><code>sch.bind(i0, &quot;blockIdx.x&quot;)</code> 将 <code>i0</code> 循环绑定到 GPU 的 block 索引，以便将计算分发到不同的 GPU block 上。</li>
<li><code>sch.bind(i1, &quot;threadIdx.x&quot;)</code> 将 <code>i1</code> 循环绑定到 GPU 的 thread 索引，以便将计算分发到每个 block 内的不同的 GPU thread 上。</li>
</ul>
<p>可以看到循环变量变成了 <code>T.thead_binding</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="s2">&#34;blockIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="s2">&#34;threadIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#--------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;blockIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;threadIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="n">i_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后我们可以在GPU上构建并测试程序的正确性</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">rt_mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;cuda&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">A_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">A_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">A_np</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">B_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">B_np</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">C_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">rt_mod</span><span class="p">[</span><span class="s2">&#34;main&#34;</span><span class="p">](</span><span class="n">A_nd</span><span class="p">,</span> <span class="n">B_nd</span><span class="p">,</span> <span class="n">C_nd</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">C_nd</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">A_np</span> <span class="o">+</span> <span class="n">B_np</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="window-sum-example">Window Sum Example</h1>
<p>滑动窗口求和可以被视为权重为 <code>[1,1,1]</code>的卷积，对输入进行滑动并将三个相邻值相加。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/window_sum.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/window_sum.png" alt="Window Sum">
    </a><figcaption>Window Sum</figcaption></figure></p>
<p>跟上一节一样我们将循环split后把外循环和内循环分别bind到block和thread上</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span> 
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModuleWindowSum</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1027</span><span class="p">,</span> <span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">             <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1024</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;S&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MyModuleWindowSum</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">nthread</span> <span class="o">=</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl"><span class="n">block_C</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block_C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">nthread</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="s2">&#34;blockIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="s2">&#34;threadIdx.x&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对应的TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1027</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;blockIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;threadIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="n">i_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">:</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="cache-in-shared-memory">Cache in Shared Memory</h2>
<p>我们可以看到在窗口滑动的过程中有一部分数据是重复的。每个 block 包含所有线程都可以在块内访问的共享内存 (shared memory)，为了避免重复从 global memory 加载，我们可以把部分数据缓存到共享内存上</p>
<ol>
<li><code>B[vi] = A[vi] + A[vi + 1] + A[vi + 2]</code> 这一行代码会重复读取 <code>A</code> 缓冲区中的数据。</li>
<li><code>sch.cache_read(block_C, read_buffer_index=0, storage_scope=&quot;shared&quot;)</code> 创建了一个名为 <code>A_shared</code> 的共享内存缓存，用于存储 <code>A</code> 缓冲区中的一部分数据。</li>
</ol>
<ul>
<li><code>block_C</code> 指示缓存与 <code>C</code> block 相关联。</li>
<li><code>read_buffer_index=0</code> 指示缓存 <code>A</code> 缓冲区，因为 <code>A</code> 是 <code>C</code> block 中的第一个读取缓冲区。</li>
<li><code>storage_scope=&quot;shared&quot;</code> 指示缓存使用共享内存。</li>
</ul>
<ol start="3">
<li><code>sch.compute_at(A_shared, i1)</code> 将 <code>A_shared</code> 的计算位置设置为 <code>i1</code> 循环，这意味着 <code>A_shared</code> 将在每个 thread 中被计算。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MyModuleWindowSum</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">nthread</span> <span class="o">=</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl"><span class="n">block_C</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block_C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">nthread</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="s2">&#34;blockIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="s2">&#34;threadIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">A_shared</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">cache_read</span><span class="p">(</span><span class="n">block_C</span><span class="p">,</span> <span class="n">read_buffer_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">storage_scope</span><span class="o">=</span><span class="s2">&#34;shared&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">A_shared</span><span class="p">,</span> <span class="n">i1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>变换后的TensorIR如下，主要进行了</p>
<ol>
<li>
<p><strong>共享内存分配：</strong> 在每个 GPU block 的共享内存中分配了一个大小为 <code>(1027,)</code> 的缓冲区 <code>A_shared</code>。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A_shared</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1027</span><span class="p">,),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;shared&#34;</span><span class="p">)</span>
</span></span></code></pre></div></li>
<li>
<p>添加了一个新的 block <code>A_shared</code>，循环遍历每个 thread并将 <code>A</code> 缓冲区中的数据缓存到 <code>A_shared</code> 中：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;blockIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;threadIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">ax0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">130</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;A_shared&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1027</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">A_shared</span><span class="p">[</span><span class="n">v0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">A_shared</span><span class="p">[</span><span class="n">v0</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v0</span><span class="p">]</span>
</span></span></code></pre></div></li>
<li>
<p>码更新了 <code>C</code> block 中的计算，使其从 <code>A_shared</code> 中读取数据：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="n">i_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A_shared</span><span class="p">[</span><span class="n">vi</span><span class="p">:</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A_shared</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">A_shared</span><span class="p">[</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">A_shared</span><span class="p">[</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
</span></span></code></pre></div></li>
</ol>
<hr>
<p><code>rane(130)</code> 的出现是因为需要将 <code>A</code> 缓冲区中的数据缓存到共享内存 <code>A_shared</code> 中。每个 GPU block 处理的数据范围是 <code>128</code> 个元素，对应于 <code>i1</code> 循环的范围。由于窗口求和操作需要访问 <code>A</code> 缓冲区中当前元素的三个相邻元素，因此每个 thread 需要访问 <code>128 + 2 = 130</code> 个元素。为了确保每个 thread 都能访问到所需的数据，需要将 <code>A</code> 缓冲区中 <code>130</code> 个元素缓存到 <code>A_shared</code> 中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1027</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">A_shared</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1027</span><span class="p">,),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;shared&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;blockIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;threadIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">ax0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">130</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;A_shared&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1027</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">A_shared</span><span class="p">[</span><span class="n">v0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                        <span class="n">A_shared</span><span class="p">[</span><span class="n">v0</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="n">i_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A_shared</span><span class="p">[</span><span class="n">vi</span><span class="p">:</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A_shared</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">A_shared</span><span class="p">[</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">A_shared</span><span class="p">[</span><span class="n">vi</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="get-cuda--source">Get CUDA  Source</h2>
<p>我们可以检查相应的底层代码（CUDA ）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">rt_mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;cuda&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">rt_mod</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_source</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>生成的代码包含两部分：</p>
<ul>
<li>在主机 (CPU) 上的调用 GPU 程序的部分；</li>
<li>相应计算的 CUDA 内核。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cp">#if (((__CUDACC_VER_MAJOR__ == 11) &amp;&amp; (__CUDACC_VER_MINOR__ &gt;= 4)) || \
</span></span></span><span class="line"><span class="cl"><span class="cp">     (__CUDACC_VER_MAJOR__ &gt; 11))
</span></span></span><span class="line"><span class="cl"><span class="cp">#define TVM_ENABLE_L2_PREFETCH 1
</span></span></span><span class="line"><span class="cl"><span class="cp">#else
</span></span></span><span class="line"><span class="cl"><span class="cp">#define TVM_ENABLE_L2_PREFETCH 0
</span></span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#ifdef _WIN32
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">using</span> <span class="n">uint</span> <span class="o">=</span> <span class="kt">unsigned</span> <span class="kt">int</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="n">uchar</span> <span class="o">=</span> <span class="kt">unsigned</span> <span class="kt">char</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="n">ushort</span> <span class="o">=</span> <span class="kt">unsigned</span> <span class="kt">short</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="kt">int64_t</span> <span class="o">=</span> <span class="kt">long</span> <span class="kt">long</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">using</span> <span class="kt">uint64_t</span> <span class="o">=</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#else
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="cp">#define uint unsigned int
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="cp">#define uchar unsigned char
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="cp">#define ushort unsigned short
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="cp">#define int64_t long long
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="cp">#define uint64_t unsigned long long
</span></span></span><span class="line"><span class="cl"><span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span><span class="k">extern</span> <span class="s">&#34;C&#34;</span> <span class="n">__global__</span> <span class="kt">void</span> <span class="n">__launch_bounds__</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> <span class="n">main_kernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">B</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="k">extern</span> <span class="s">&#34;C&#34;</span> <span class="n">__global__</span> <span class="kt">void</span> <span class="n">__launch_bounds__</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> <span class="n">main_kernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">B</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">A_shared</span><span class="p">[</span><span class="mi">130</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">ax0</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ax0</span> <span class="o">&lt;</span> <span class="mi">130</span><span class="p">;</span> <span class="o">++</span><span class="n">ax0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">A_shared</span><span class="p">[</span><span class="n">ax0</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[((((</span><span class="kt">int</span><span class="p">)</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="mi">128</span><span class="p">)</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)];</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">__syncthreads</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="n">B</span><span class="p">[((((</span><span class="kt">int</span><span class="p">)</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="mi">128</span><span class="p">)</span> <span class="o">+</span> <span class="p">((</span><span class="kt">int</span><span class="p">)</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">))]</span> <span class="o">=</span> <span class="p">((</span><span class="n">A_shared</span><span class="p">[((</span><span class="kt">int</span><span class="p">)</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)]</span> <span class="o">+</span> <span class="n">A_shared</span><span class="p">[(((</span><span class="kt">int</span><span class="p">)</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span> <span class="o">+</span> <span class="n">A_shared</span><span class="p">[(((</span><span class="kt">int</span><span class="p">)</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)]);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="matrix-multiplication">Matrix Multiplication</h1>
<p>下面我们对原始的 <code>1024*1024</code>的矩阵乘法进行优化</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModuleMatmul</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">             <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">             <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="local-blocking">Local Blocking</h2>
<p>下面的blocking 函数使用了一种称为 局部阻塞 的优化策略，将矩阵乘法的计算分解成更小的块，并使用共享内存缓存来提高性能。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/gpu_local_blocking.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/gpu_local_blocking.png" alt="Local Blocking">
    </a><figcaption>Local Blocking</figcaption></figure></p>
<ol>
<li>将三个循环 <code>i</code>、<code>j</code> 和 <code>k</code> 分别拆分成多个循环，例如将 <code>i</code> 拆分成 <code>i0</code>、<code>i1</code> 和 <code>i2</code>，分别对应于 block 索引、thread 索引和局部循环索引。</li>
<li><code>k1</code>表示矩阵计算被拆分成多少个小块，<code>k0</code>决定了每个线程需要进行多少次累加操作。调整循环的顺序，以便在每个 thread 中计算 <code>k0</code> 循环的所有迭代，从而利用共享内存缓存。</li>
<li>使用 <code>cache_write</code> 函数创建一个名为 <code>C_local</code> 的共享内存缓存，用于存储 <code>C</code> 矩阵的中间结果。</li>
<li>使用 <code>reverse_compute_at</code> 函数将 <code>C_local</code> 的计算位置设置为 <code>j1</code> 循环，以便在每个 thread 中计算 <code>C_local</code> 的所有迭代，从而利用共享内存缓存。</li>
<li>将 <code>i0</code> 和 <code>j0</code> 绑定到 GPU 的 <code>blockIdx.y</code> 和 <code>blockIdx.x</code> 线程索引，将 <code>i1</code> 和 <code>j1</code> 绑定到 GPU 的 <code>threadIdx.y</code> 和 <code>threadIdx.x</code> 线程索引。</li>
<li>使用 <code>unroll</code> 函数展开 <code>k1</code> 循环，以便在每个 thread 中展开计算，从而提高性能。</li>
<li>使用 <code>decompose_reduction</code> 函数分解 <code>k0</code> 循环，以便在每个 thread 中计算 <code>k0</code> 循环的所有迭代，从而利用共享内存缓存。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">blocking</span><span class="p">(</span><span class="n">sch</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">tile_local_y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">tile_local_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">tile_block_y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">tile_block_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">tile_k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_C</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">C_local</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">cache_write</span><span class="p">(</span><span class="n">block_C</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;local&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block_C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">tile_block_y</span><span class="p">,</span> <span class="n">tile_local_y</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">j0</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">j2</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">j</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">tile_block_x</span><span class="p">,</span> <span class="n">tile_local_x</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">k0</span><span class="p">,</span> <span class="n">k1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">tile_k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">unroll</span><span class="p">(</span><span class="n">k1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="n">j0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">k0</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">j2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">reverse_compute_at</span><span class="p">(</span><span class="n">C_local</span><span class="p">,</span> <span class="n">j1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="s2">&#34;blockIdx.y&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">j0</span><span class="p">,</span> <span class="s2">&#34;blockIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="s2">&#34;threadIdx.y&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">j1</span><span class="p">,</span> <span class="s2">&#34;threadIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block_C</span><span class="p">,</span> <span class="n">k0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">sch</span>  
</span></span></code></pre></td></tr></table>
</div>
</div><p>进行 Local Blocking 后的TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MyModuleMatmul</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">blocking</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#---------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">C_local</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">scope</span><span class="o">=</span><span class="s2">&#34;local&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;blockIdx.y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;blockIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;threadIdx.y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">j_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">thread_binding</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">thread</span><span class="o">=</span><span class="s2">&#34;threadIdx.x&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">for</span> <span class="n">i_2_init</span><span class="p">,</span> <span class="n">j_2_init</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">64</span> <span class="o">+</span> <span class="n">i_1</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">i_2_init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">64</span> <span class="o">+</span> <span class="n">j_1</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">j_2_init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_local</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                                <span class="n">C_local</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="k">for</span> <span class="n">k_0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="k">for</span> <span class="n">k_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">unroll</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                                <span class="k">for</span> <span class="n">i_2</span><span class="p">,</span> <span class="n">j_2</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                                    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_update&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">64</span> <span class="o">+</span> <span class="n">i_1</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">i_2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">64</span> <span class="o">+</span> <span class="n">j_1</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">j_2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">k_0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">k_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C_local</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C_local</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">C_local</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_local</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                        <span class="k">for</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_local&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                                <span class="n">v0</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">64</span> <span class="o">+</span> <span class="n">i_1</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                <span class="n">v1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">64</span> <span class="o">+</span> <span class="n">j_1</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="n">ax1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C_local</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                                <span class="n">C</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C_local</span><span class="p">[</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="shared--memory-blocking">Shared  Memory Blocking</h2>
<p>上面的方法没有考虑相邻 thread 位于同一个 block 中，我们可以将它们需要的数据加载到 shared memory 中。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/gpu_shared_blocking.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/gpu_shared_blocking.png" alt="Shared Memory Blocking">
    </a><figcaption>Shared Memory Blocking</figcaption></figure></p>
<p><code>cache_read_and_coop_fetch</code> 函数负责将 <code>A</code> 和 <code>B</code> 矩阵中的数据加载到共享内存中。首先使用 <code>cache_read</code> 创建一个共享内存缓存，用于存储 <code>A</code> 或 <code>B</code> 矩阵的数据。然后使用 <code>compute_at</code> 将缓存的计算位置设置为 <code>k0</code> 循环，在每个线程中计算缓存的所有迭代。最后使用 <code>split</code> 和 <code>vectorize</code> 函数对 <code>k0</code> 循环进行向量化，提高加载数据的效率。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">cache_read_and_coop_fetch</span><span class="p">(</span><span class="n">sch</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">nthread</span><span class="p">,</span> <span class="n">read_idx</span><span class="p">,</span> <span class="n">read_loc</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">read_cache</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">cache_read</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block</span><span class="p">,</span> <span class="n">read_buffer_index</span><span class="o">=</span><span class="n">read_idx</span><span class="p">,</span> <span class="n">storage_scope</span><span class="o">=</span><span class="s2">&#34;shared&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">read_cache</span><span class="p">,</span> <span class="n">loop</span><span class="o">=</span><span class="n">read_loc</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># vertorized cooperative fetch</span>
</span></span><span class="line"><span class="cl">    <span class="n">inner0</span><span class="p">,</span> <span class="n">inner1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">read_cache</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">    <span class="n">inner</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">inner0</span><span class="p">,</span> <span class="n">inner1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">tx</span><span class="p">,</span> <span class="n">vec</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">inner</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">nthread</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">tx</span><span class="p">,</span> <span class="s2">&#34;threadIdx.x&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其余的操作和 Local Blocking 一致</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">blocking_with_shared</span><span class="p">(</span><span class="n">sch</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">tile_local_y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">tile_local_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">tile_block_y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">tile_block_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                        <span class="n">tile_k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_C</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">C_local</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">cache_write</span><span class="p">(</span><span class="n">block_C</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;local&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block_C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">tile_block_y</span><span class="p">,</span> <span class="n">tile_local_y</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">j0</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">j2</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">j</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">tile_block_x</span><span class="p">,</span> <span class="n">tile_local_x</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">k0</span><span class="p">,</span> <span class="n">k1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">tile_k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="n">j0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">,</span> <span class="n">k0</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">j2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">reverse_compute_at</span><span class="p">(</span><span class="n">C_local</span><span class="p">,</span> <span class="n">j1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">i0</span><span class="p">,</span> <span class="s2">&#34;blockIdx.y&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">j0</span><span class="p">,</span> <span class="s2">&#34;blockIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">tx</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">i1</span><span class="p">,</span> <span class="n">j1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">tx</span><span class="p">,</span> <span class="s2">&#34;threadIdx.x&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">nthread</span> <span class="o">=</span> <span class="n">tile_block_y</span> <span class="o">*</span> <span class="n">tile_block_x</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache_read_and_coop_fetch</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="n">block_C</span><span class="p">,</span> <span class="n">nthread</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">k0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cache_read_and_coop_fetch</span><span class="p">(</span><span class="n">sch</span><span class="p">,</span> <span class="n">block_C</span><span class="p">,</span> <span class="n">nthread</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k0</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block_C</span><span class="p">,</span> <span class="n">k0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">sch</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (6)-Exercise of End to End Model Execution</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch5/</link>
      <pubDate>Tue, 20 Aug 2024 12:45:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch5/</guid>
      <description>Personal notebook 5.</description>
      <content:encoded><![CDATA[<h1 id="model-preparation">Model Preparation</h1>
<p>我们采用Pytorch框架先定一个模型，该模型接受一批图像为输入，然后对它们依次作用卷积层，激活层，池化层和全连接层，得到分类结果。并从训练好的模型里加载权重，输入图像来自FashionMNIST数据集，shape为(1, 28, 28)，我们设置batch size=4.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Load the weight map from file.</span>
</span></span><span class="line"><span class="cl"><span class="n">weight_map</span> <span class="o">=</span> <span class="n">pkl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s2">&#34;fasionmnist_mlp_assignment_params.pkl&#34;</span><span class="p">,</span> <span class="s2">&#34;rb&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;T-shirt/top&#39;</span><span class="p">,</span> <span class="s1">&#39;Trouser&#39;</span><span class="p">,</span> <span class="s1">&#39;Pullover&#39;</span><span class="p">,</span> <span class="s1">&#39;Dress&#39;</span><span class="p">,</span> <span class="s1">&#39;Coat&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">               <span class="s1">&#39;Sandal&#39;</span><span class="p">,</span> <span class="s1">&#39;Shirt&#39;</span><span class="p">,</span> <span class="s1">&#39;Sneaker&#39;</span><span class="p">,</span> <span class="s1">&#39;Bag&#39;</span><span class="p">,</span> <span class="s1">&#39;Ankle boot&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">pytorch_model</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">5408</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="nb">list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="nb">list</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">name_map</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;0.weight&#34;</span><span class="p">:</span> <span class="s2">&#34;conv2d_weight&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;0.bias&#34;</span><span class="p">:</span> <span class="s2">&#34;conv2d_bias&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;4.weight&#34;</span><span class="p">:</span> <span class="s2">&#34;linear0_weight&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;4.bias&#34;</span><span class="p">:</span> <span class="s2">&#34;linear0_bias&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;6.weight&#34;</span><span class="p">:</span> <span class="s2">&#34;linear1_weight&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;6.bias&#34;</span><span class="p">:</span> <span class="s2">&#34;linear1_bias&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">param</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">weight_map</span><span class="p">[</span><span class="n">name_map</span><span class="p">[</span><span class="n">name</span><span class="p">]])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="ingest-model-from-pytorch">Ingest Model From Pytorch</h1>
<p>之前我们都是手写T.prim_func来实现神经网络的每一层，这样很容易出错并且不易于调试。TVM提供了 <code>relax.BlockBuilder</code>类可以从头开始一步步构造端到端模型，其中有一个名为 <code>emit_te</code>的API，它可以将一个张量表达式的算子描述转变成一个对应TensorIR函数的 <code>call_tir</code>操作。</p>
<p>在下面的代码中，为了构建一个执行单个ReLU算子的Relax函数，在 <code>emit_te_example</code>中我们首先定义了一个 <code>BlockBuilder</code>实例 <code>bb</code>。同样定义了一个 <code>128x128</code>大小的张量变量 <code>x</code>，它将作为ReLU操作的输入（同时也是Relax函数的输入）。</p>
<p>在这之后，我们用 <code>with bb.function(name, [*input])</code> API构建一个以 <code>x</code>为输入的Relax函数 <code>main</code>。然后我们构建一个dataflow block。在这个dataflow block里，我们首先用 <code>emit_te</code>生成一个调用ReLU算子的 <code>call_tir</code>。 <code>emit_te</code>会在IRModule中生成一个名字为 <code>relu</code>的TensorIR函数，然后在dataflow block中生成 <code>call_tir(relu, (x,), (128, 128), dtype=&quot;float32&quot;)</code>操作。<code>call_tir</code>之后是函数返回。在这一构造之后，BlockBuilder实例 <code>bb</code>包含构建完的IRModule，可以通过 <code>bb.get()</code>得到。</p>
<p><code>emit_te</code> 的作用是将一个 TVM 张量表达式（TE）函数转换为 Relax 中的调用节点（Call Node）。它允许你在 Relax 中使用 TE 函数来进行计算，并生成相应的 TVM Script 代码。该函数首先将 Relax 表达式的参数转换为 TE 张量。然后，它调用 TE 函数，并将转换后的 TE 张量作为参数传递给它。TE 函数执行计算并返回一个 TE 张量或 TE 张量列表。该函数将返回的 TE 张量转换为 Relax 中的 Call Node. 最后，它使用 <code>self.emit</code> 方法将调用节点添加到 Relax BlockBuilder 中，并返回一个新的 Relax 变量，该变量绑定到 Call Node.</p>
<p><strong>函数参数：</strong></p>
<ul>
<li><code>func</code>: 一个可调用对象，它代表一个 TE 函数，该函数接受 Relax 张量作为参数，并返回一个 TE 张量或 TE 张量列表。</li>
<li><code>*args</code>: <code>func</code>输入的位置参数 (relax Tensor)。</li>
<li><code>**kwargs</code>: <code>func</code>输入的的关键字参数 (relax Tensor)。</li>
<li><code>name_hint</code>: 可选参数，用于指定生成的 PrimFunc 的名称。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">B</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">emit_te_example</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># relax.BlockBuilder can construct e2e models </span>
</span></span><span class="line"><span class="cl">    <span class="c1"># step by step in an IRModule that starts empty.</span>
</span></span><span class="line"><span class="cl">    <span class="n">bb</span> <span class="o">=</span><span class="n">relax</span><span class="o">.</span><span class="n">BlockBuilder</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">    <span class="c1"># relax.DynTensorType is the type assigned to tensors with a known dtype and unknown shape.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="s2">&#34;x&#34;</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]):</span>  <span class="c1"># construct a Relax function main with x as input</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Emit a call node according to the te function</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># which should return a te tensor or a list of te tensors. </span>
</span></span><span class="line"><span class="cl">            <span class="n">lv0</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">relu</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_output</span><span class="p">(</span><span class="n">lv0</span><span class="p">)</span>  <span class="c1"># mark the dataflow output </span>
</span></span><span class="line"><span class="cl">        <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>  <span class="c1"># mark the function output </span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>  <span class="c1"># return the constructed IRModule</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到通过BlockBuilder生成的IRModule包含了ReLU的TensorIR实现和一个含有调用ReLU实现的 <code>call_tir</code>的Relax函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">lv</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="construct-irmodule-equals-to-pytorch">Construct IRModule Equals to Pytorch</h1>
<p>我们可以用 <code>BlockBuilder</code>和 <code>emit_te</code>来创建一个和之前定义的PyTorch模型等价的IRModule。首先我们要实现这些算子的张量表达式运算函数。</p>
<p><strong>在加上bias的时候要和reduction操作分开进行</strong>，即不能在一个te.compute里面进行 <code>te.sum+bias[...]</code>的操作，否则会报错</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">TVMError
</span></span><span class="line"><span class="cl">Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
</span></span><span class="line"><span class="cl">File <span class="s2">&#34;D:\Work\tvm\tvm0.18\tvm\src\te\operation\compute_op.cc&#34;</span>, line <span class="m">566</span>
</span></span><span class="line"><span class="cl">InternalError: Check failed: <span class="o">(</span><span class="nv">0</span> <span class="o">==</span> level_<span class="o">)</span> is false: Reductions are only allowed at the top level of compute. Please create another tensor <span class="k">for</span> further composition.
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">my_conv2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>  <span class="c1"># No padding, stride = 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">CI</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">CO</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">KH</span><span class="p">,</span> <span class="n">KW</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">CI</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;k&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">r</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">KH</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;r&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">KW</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;s&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">OH</span> <span class="o">=</span> <span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="n">KH</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">OW</span> <span class="o">=</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">KW</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">conv2d_te</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">CO</span><span class="p">,</span> <span class="n">OH</span><span class="p">,</span> <span class="n">OW</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                           <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                                    <span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">oh</span> <span class="o">+</span> <span class="n">r</span><span class="p">,</span> <span class="n">ow</span> <span class="o">+</span> <span class="n">s</span><span class="p">]</span> <span class="o">*</span> <span class="n">K</span><span class="p">[</span><span class="n">co</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s</span><span class="p">]),</span> 
</span></span><span class="line"><span class="cl">                           <span class="n">name</span><span class="o">=</span><span class="s2">&#34;conv2d&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">CO</span><span class="p">,</span> <span class="n">OH</span><span class="p">,</span> <span class="n">OW</span><span class="p">),</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                                                    <span class="n">conv2d_te</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">my_relu</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="o">*</span><span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">),</span> <span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">my_maxpool2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;i&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">j</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">S</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;j&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">maxpool2d_te</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">W</span><span class="o">//</span><span class="mi">2</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                              <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                                  <span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="o">*</span><span class="n">S</span><span class="o">+</span><span class="n">i</span><span class="p">,</span> <span class="n">ow</span><span class="o">*</span><span class="n">S</span><span class="o">+</span><span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;maxpool2d&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">maxpool2d_te</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">my_flatten</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">flatten_te</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="o">*</span><span class="n">H</span><span class="o">*</span><span class="n">W</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                            <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">//</span><span class="p">(</span><span class="n">H</span><span class="o">*</span><span class="n">W</span><span class="p">),</span> <span class="n">i</span><span class="o">//</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="o">%</span><span class="p">(</span><span class="n">H</span><span class="p">),</span> <span class="n">i</span><span class="o">%</span><span class="p">(</span><span class="n">W</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">flatten_te</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">my_linear</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">FO</span><span class="p">,</span> <span class="n">FI</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span> 
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>  
</span></span><span class="line"><span class="cl">    <span class="n">fi</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">FI</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;FI&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">linear_te</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">FO</span><span class="p">),</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                                                <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">fi</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">fi</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">fi</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">B</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">FO</span><span class="p">),</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">linear_te</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">linear_te</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span>   
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">my_softmax</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">c</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;c&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_val</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="p">),</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">c</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_te</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">max_val</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>  
</span></span><span class="line"><span class="cl">    <span class="n">sum_exp_te</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="p">),</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_te</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">c</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">softmax_te</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">exp_te</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">/</span> <span class="n">sum_exp_te</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">softmax_te</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后我们就可以利用 <code>BlockBuilder</code>构建IRModule</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_model_via_emit_te</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl">    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>  <span class="c1"># BCHW</span>
</span></span><span class="line"><span class="cl">    <span class="n">bb</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">BlockBuilder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="s2">&#34;x&#34;</span><span class="p">,</span> <span class="n">relax</span><span class="o">.</span><span class="n">TensorStructInfo</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="n">conv2d_weight</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">weight_map</span><span class="p">[</span><span class="s2">&#34;conv2d_weight&#34;</span><span class="p">],</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">conv2d_bias</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">weight_map</span><span class="p">[</span><span class="s2">&#34;conv2d_bias&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">linear0_weight</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">weight_map</span><span class="p">[</span><span class="s2">&#34;linear0_weight&#34;</span><span class="p">],</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">linear0_bias</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">weight_map</span><span class="p">[</span><span class="s2">&#34;linear0_bias&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">linear1_weight</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">weight_map</span><span class="p">[</span><span class="s2">&#34;linear1_weight&#34;</span><span class="p">],</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">linear1_bias</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">const</span><span class="p">(</span><span class="n">weight_map</span><span class="p">[</span><span class="s2">&#34;linear1_bias&#34;</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="c1"># Build the model using BlockBuilder</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv_conv</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_conv2d</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">conv2d_weight</span><span class="p">,</span> <span class="n">conv2d_bias</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv_relu1</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_relu</span><span class="p">,</span> <span class="n">gv_conv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv_pool</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_maxpool2d</span><span class="p">,</span> <span class="n">gv_relu1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv_flatten</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_flatten</span><span class="p">,</span> <span class="n">gv_pool</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv_dense1</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_linear</span><span class="p">,</span> <span class="n">gv_flatten</span><span class="p">,</span> <span class="n">linear0_weight</span><span class="p">,</span> <span class="n">linear0_bias</span><span class="p">)</span>   
</span></span><span class="line"><span class="cl">            <span class="n">gv_relu2</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_relu</span><span class="p">,</span> <span class="n">gv_dense1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv_dense2</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_linear</span><span class="p">,</span> <span class="n">gv_relu2</span><span class="p">,</span> <span class="n">linear1_weight</span><span class="p">,</span> <span class="n">linear1_bias</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv_softmax</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_softmax</span><span class="p">,</span> <span class="n">gv_dense2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_output</span><span class="p">(</span><span class="n">gv_softmax</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>得到的IRModule的TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mod</span> <span class="o">=</span> <span class="n">create_model_via_emit_te</span><span class="p">()</span>   
</span></span><span class="line"><span class="cl"><span class="n">exec</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&#34;llvm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">vm</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span><span class="p">(</span><span class="n">exec</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">script</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><details class="custom-details">
    <summary class="custom-summary">mod.script</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">28</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">28</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">3</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv2d</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">3</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;conv2d&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">,</span> <span class="n">v_k</span><span class="p">,</span> <span class="n">v_r</span><span class="p">,</span> <span class="n">v_s</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSSSRRR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_k</span><span class="p">,</span> <span class="n">v_oh</span> <span class="o">+</span> <span class="n">v_r</span><span class="p">,</span> <span class="n">v_ow</span> <span class="o">+</span> <span class="n">v_s</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">v_co</span><span class="p">,</span> <span class="n">v_k</span><span class="p">,</span> <span class="n">v_r</span><span class="p">,</span> <span class="n">v_s</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">conv2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">conv2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">conv2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_k</span><span class="p">,</span> <span class="n">v_oh</span> <span class="o">+</span> <span class="n">v_r</span><span class="p">,</span> <span class="n">v_ow</span> <span class="o">+</span> <span class="n">v_s</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">v_co</span><span class="p">,</span> <span class="n">v_k</span><span class="p">,</span> <span class="n">v_r</span><span class="p">,</span> <span class="n">v_s</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSSS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">conv2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">],</span> <span class="n">C</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">]</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">]</span> <span class="o">+</span> <span class="n">C</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_flatten</span><span class="p">(</span><span class="n">lv2</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">5408</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">5408</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_n</span><span class="p">,</span> <span class="n">v_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv2</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_i</span> <span class="o">//</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">169</span><span class="p">),</span> <span class="n">v_i</span> <span class="o">%</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">169</span><span class="p">)</span> <span class="o">//</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span> <span class="n">v_i</span> <span class="o">%</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lv2</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_i</span> <span class="o">//</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">169</span><span class="p">),</span> <span class="n">v_i</span> <span class="o">%</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">169</span><span class="p">)</span> <span class="o">//</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span> <span class="n">v_i</span> <span class="o">%</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_linear</span><span class="p">(</span><span class="n">lv3</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">5408</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">5408</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">compute_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">FI</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">5408</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">v_FI</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">FI</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv3</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_FI</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">v_j</span><span class="p">,</span> <span class="n">v_FI</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="n">lv3</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_FI</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">v_j</span><span class="p">,</span> <span class="n">v_FI</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute_1&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">v_j</span><span class="p">],</span> <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_linear1</span><span class="p">(</span><span class="n">lv5</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">compute_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">FI</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">v_FI</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">FI</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv5</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_FI</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">v_j</span><span class="p">,</span> <span class="n">v_FI</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="n">lv5</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_FI</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">v_j</span><span class="p">,</span> <span class="n">v_FI</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute_1&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">v_j</span><span class="p">],</span> <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_maxpool2d</span><span class="p">(</span><span class="n">lv1</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">maxpool2d</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">2</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;maxpool2d&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">,</span> <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSSSRR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">oh</span><span class="p">,</span> <span class="n">ow</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv1</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">v_i</span><span class="p">,</span> <span class="n">v_ow</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">maxpool2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">maxpool2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="o">-</span><span class="mf">340282346638528859811704183484516925440.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">maxpool2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">maxpool2d</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span><span class="p">,</span> <span class="n">v_ow</span><span class="p">],</span> <span class="n">lv1</span><span class="p">[</span><span class="n">v_n</span><span class="p">,</span> <span class="n">v_co</span><span class="p">,</span> <span class="n">v_oh</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">v_i</span><span class="p">,</span> <span class="n">v_ow</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_relu</span><span class="p">(</span><span class="n">lv</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">i3</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">26</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_i2</span><span class="p">,</span> <span class="n">v_i3</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSSS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">i3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_i2</span><span class="p">,</span> <span class="n">v_i3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_i2</span><span class="p">,</span> <span class="n">v_i3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_i2</span><span class="p">,</span> <span class="n">v_i3</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">lv</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_i2</span><span class="p">,</span> <span class="n">v_i3</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_relu1</span><span class="p">(</span><span class="n">lv4</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv4</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">lv4</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_softmax</span><span class="p">(</span><span class="n">lv6</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">compute</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">compute_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),))</span>
</span></span><span class="line"><span class="cl">        <span class="n">compute_2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="n">compute_3</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_c</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv6</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_c</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="o">-</span><span class="mf">340282346638528859811704183484516925440.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">],</span> <span class="n">lv6</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_c</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute_1&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv6</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">],</span> <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute_2</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute_2</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lv6</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">-</span> <span class="n">compute_1</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute_2&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_c</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">compute_2</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_c</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute_3</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">compute_3</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute_3</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_3</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">+</span> <span class="n">compute_2</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_c</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;compute_3&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">compute_2</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">],</span> <span class="n">compute_3</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">compute</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">compute</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_2</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">/</span> <span class="n">compute_3</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_conv2d</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_relu</span><span class="p">,</span> <span class="p">(</span><span class="n">lv</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_maxpool2d</span><span class="p">,</span> <span class="p">(</span><span class="n">lv1</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">13</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_flatten</span><span class="p">,</span> <span class="p">(</span><span class="n">lv2</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5408</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_linear</span><span class="p">,</span> <span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">3</span><span class="p">]),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv5</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_relu1</span><span class="p">,</span> <span class="p">(</span><span class="n">lv4</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv6</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_linear1</span><span class="p">,</span> <span class="p">(</span><span class="n">lv5</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">4</span><span class="p">],</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">5</span><span class="p">]),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv7</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_softmax</span><span class="p">,</span> <span class="p">(</span><span class="n">lv6</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">lv7</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<p>我们可以与Pytorch模型的执行结果进行比较来验证正确性。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">build_mod</span><span class="p">(</span><span class="n">mod</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">exec</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&#34;llvm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">vm</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span><span class="p">(</span><span class="n">exec</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">vm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">check_equivalence</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch_model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">rt_mod</span> <span class="o">=</span> <span class="n">build_mod</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="n">label</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_from_pytorch</span> <span class="o">=</span> <span class="n">torch_model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">output_from_relax</span> <span class="o">=</span> <span class="n">rt_mod</span><span class="p">[</span><span class="s2">&#34;main&#34;</span><span class="p">](</span><span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">tvm</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">output_from_pytorch</span><span class="p">,</span> <span class="n">output_from_relax</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">test_data</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;./data&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()])</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">mod</span> <span class="o">=</span> <span class="n">create_model_via_emit_te</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">torch_model</span> <span class="o">=</span> <span class="n">pytorch_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">check_equivalence</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">torch_model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (5)-Automatic Program Optimization</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch4/</link>
      <pubDate>Mon, 19 Aug 2024 21:23:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch4/</guid>
      <description>Personal notebook 4.</description>
      <content:encoded><![CDATA[<h1 id="transform-a-primitive-tensor-function">Transform a Primitive Tensor Function</h1>
<p>之前已经讲过如何通过 <code>tir.Schedule</code>对T.prim_func进行变换，仍以矩阵乘法为例</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">             <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">             <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span> <span class="c1"># type: ignore</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对其进行 <code>split,</code> <code>reorder</code>和 <code>decompose_reduction</code>变换得到的TensorIR如下。</p>
<p>通过以上变换后，矩阵乘法的执行时间减少是由于：</p>
<ol>
<li><strong>循环拆分 (<code>sch.split</code>)</strong> ：</li>
</ol>
<ul>
<li>将 <code>j</code>循环拆分成了两个循环：<code>j_0</code>和 <code>j_1</code>，其中 <code>j_1</code>的因子为4（内层循环）。</li>
<li>提高数据的局部性，因为较小的数据块会在更短的时间内被频繁访问，从而更好地利用缓存。</li>
</ul>
<ol>
<li><strong>循环重排 (<code>sch.reorder</code>)</strong> ：</li>
</ol>
<ul>
<li>将循环的顺序调整为 <code>i, j_0, k, j_1</code>，意味着外层循环先遍历 <code>i</code>和 <code>j_0</code>，内层循环再遍历 <code>k</code>和 <code>j_1</code>。</li>
<li>优先考虑了数据在寄存器或缓存中的重用，尤其是在内层循环操作期间 <code>A</code>矩阵中的元素。</li>
</ul>
<ol>
<li><strong>分解归约 (<code>sch.decompose_reduction</code>)</strong> ：</li>
</ol>
<ul>
<li>将对 <code>k</code>的归约操作分解为初始化阶段和更新阶段，有助于将计算的两个阶段（即设置初始值和实际归约）分开。</li>
<li>提高并行化的机会，并且允许更好地利用向量化指令或其他硬件优化。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">schedule_mm</span><span class="p">(</span><span class="n">sch</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">,</span> <span class="n">jfactor</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_C</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block_C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">j_0</span><span class="p">,</span> <span class="n">j_1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">j</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">jfactor</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j_0</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">j_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block_C</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">sch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MyModule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">schedule_mm</span><span class="p">(</span><span class="n">sch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-----------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j_1_init</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">j_1_init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">j_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_update&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">j_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以比较变换前后的计算用时</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">a_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">c_mm</span> <span class="o">=</span> <span class="n">a_np</span> <span class="o">@</span> <span class="n">b_np</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">a_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a_np</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">b_np</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">c_nd</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Before transformation</span>
</span></span><span class="line"><span class="cl"><span class="n">lib</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span> <span class="s2">&#34;llvm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">f_timer_before</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">time_evaluator</span><span class="p">(</span><span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Time cost of MyModule: </span><span class="si">%.3f</span><span class="s2"> ms&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">f_timer_before</span><span class="p">(</span><span class="n">a_nd</span><span class="p">,</span> <span class="n">b_nd</span><span class="p">,</span> <span class="n">c_nd</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1">#Time cost of MyModule: 1.365 ms</span>
</span></span><span class="line"><span class="cl"><span class="c1"># After transformation</span>
</span></span><span class="line"><span class="cl"><span class="n">lib</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">f_timer_after</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">time_evaluator</span><span class="p">(</span><span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Time cost of MyModule=&gt;schedule_mm: </span><span class="si">%.3f</span><span class="s2"> ms&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">f_timer_after</span><span class="p">(</span><span class="n">a_nd</span><span class="p">,</span> <span class="n">b_nd</span><span class="p">,</span> <span class="n">c_nd</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Time cost of MyModule=&gt;schedule_mm: 1.041 ms</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="transformation-trace">Transformation Trace</h1>
<p>除了 <code>sch.mod</code>字段，<code>tir.Schedule</code>还提供了一个跟踪字段 <code>sch.trace</code>，用于显示变换IRModule的步骤。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">trace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#-------------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">apply_trace</span><span class="p">(</span><span class="n">sch</span><span class="p">:</span> <span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">b0</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">,</span> <span class="n">func_name</span><span class="o">=</span><span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l3</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">b0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">l4</span><span class="p">,</span> <span class="n">l5</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">preserve_unit_iters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">disable_predication</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l4</span><span class="p">,</span> <span class="n">l3</span><span class="p">,</span> <span class="n">l5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">b6</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">b0</span><span class="p">,</span> <span class="n">loop</span><span class="o">=</span><span class="n">l3</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="stochastic-schedule-transformation">Stochastic Schedule Transformation</h1>
<p>在之前的变换中，我们都是指定这些函数的输入参数。实际情况下，我们需要引入随机性，根据不同变换的输入参数得出的执行时间来选择性能最好的一个。</p>
<p><code>sample_perfect_tile</code>函数可以计算任务中的特定循环采样最优的切分策略。</p>
<p><strong>输入参数：</strong></p>
<ul>
<li><code>loop</code>：要切分的循环。</li>
<li><code>n</code>：要切分成几份。</li>
<li><code>max_innermost_factor</code>：允许在最内层循环中采样的最大切分大小。此参数有助于控制平铺的粒度。</li>
<li><code>decision</code>：一个可选的整数列表，表示预先确定的切分决策。如果提供，函数将使用此决策而不是采样。</li>
</ul>
<p>下面函数 <code>stochastic_schedule_mm</code>和 <code>schedule_mm</code>唯一的区别是指定 <code>j_factors</code>采用的是随机的策略。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">stochastic_schedule_mm</span><span class="p">(</span><span class="n">sch</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_C</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">block_C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">j_factors</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">sample_perfect_tile</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">j</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># tvm.tir.expr.Var</span>
</span></span><span class="line"><span class="cl">    <span class="n">j_0</span><span class="p">,</span> <span class="n">j_1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">j</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="n">j_factors</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j_0</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">j_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block_C</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">sch</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以发现，它是对原来的确定性变换的泛化版本，只是多了两个元素：</p>
<ul>
<li>来自 <code>sample_perfect_tile</code> 的随机变量，以及我们在示例中没有涉及的其他采样操作。</li>
<li>根据随机变量采取行动的 <code>schedule</code>操作。</li>
</ul>
<p><code>j_factors</code> 中的元素不是整数。相它们是<strong>符号变量</strong>，指的是正在采样的随机变量。我们可以将这些变量传递给转换 API，以指定factors. 调用 <code>stochastic_schedule_mm</code>后的trace如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MyModule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">stochastic_schedule_mm</span><span class="p">(</span><span class="n">sch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">trace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#------------------------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">apply_trace</span><span class="p">(</span><span class="n">sch</span><span class="p">:</span> <span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">  <span class="n">b0</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">,</span> <span class="n">func_name</span><span class="o">=</span><span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l3</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">b0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">v4</span><span class="p">,</span> <span class="n">v5</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">sample_perfect_tile</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_innermost_factor</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">decision</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">  <span class="n">l6</span><span class="p">,</span> <span class="n">l7</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">loop</span><span class="o">=</span><span class="n">l2</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="n">v4</span><span class="p">,</span> <span class="n">v5</span><span class="p">],</span> <span class="n">preserve_unit_iters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">disable_predication</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l6</span><span class="p">,</span> <span class="n">l3</span><span class="p">,</span> <span class="n">l7</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">b8</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="n">b0</span><span class="p">,</span> <span class="n">loop</span><span class="o">=</span><span class="n">l3</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="search-over-stochastic-transformations">Search Over Stochastic Transformations</h1>
<p><code>stochastic_schedule_mm</code>实际上会根据每个采样步骤的实际决定，创建一个<strong>程序的搜索空间</strong>。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/auto_prog_optim_transformation_search.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/auto_prog_optim_transformation_search.png" alt="Transformation Search Space">
    </a><figcaption>Transformation Search Space</figcaption></figure></p>
<p>我们需要一种搜索算法能找到性能最好的变换。下面的函数使用最直接的搜索算法&ndash;随机搜索。它尝试重复运行 <code>stochastic_schedule_mm</code>，得到一个转换后的IR module，运行benchmark，然后将性能最好的IR module记录下来。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">random_search</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">IRModule</span><span class="p">,</span> <span class="n">num_trails</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">best_result</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">best_sch</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trails</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">sch</span> <span class="o">=</span> <span class="n">stochastic_schedule_mm</span><span class="p">(</span><span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">mod</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">lib</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">f_timer_after</span> <span class="o">=</span> <span class="n">lib</span><span class="o">.</span><span class="n">time_evaluator</span><span class="p">(</span><span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">result</span> <span class="o">=</span> <span class="n">f_timer_after</span><span class="p">(</span><span class="n">a_nd</span><span class="p">,</span> <span class="n">b_nd</span><span class="p">,</span> <span class="n">c_nd</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;=====Attempt </span><span class="si">%d</span><span class="s2">, time-cost: </span><span class="si">%.3f</span><span class="s2"> ms====&#34;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">result</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">trace</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># book keep the best result so far</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">best_result</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">result</span> <span class="o">&lt;</span> <span class="n">best_result</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_result</span> <span class="o">=</span> <span class="n">result</span>
</span></span><span class="line"><span class="cl">            <span class="n">best_sch</span> <span class="o">=</span> <span class="n">sch</span>  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">best_sch</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>实际情况下会使用更高级的算法。还需要提供额外的工具，例如在远程设备上进行基准测试等。TVM 的 <code>meta_schedule</code> API 提供了这些功能。</p>
<p><code>meta_schedule</code>是一个命名空间，用于支持在可能的变换空间中进行搜索。</p>
<ul>
<li>跨多个进程的并行基准测试。</li>
<li>使用 cost model，避免每次都进行基准测试。</li>
<li>在 trace 上进行进化搜索，而不是每次都随机取样。</li>
</ul>
<p><code>tune_tir</code> API 仍使用随机变换来指定好程序的搜索空间并在搜索空间内找到优化的方案。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">database</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">tune_tir</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">mod</span><span class="o">=</span><span class="n">MyModule</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm --num-cores=1&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_trials_global</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_trials_per_iter</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">space</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">space_generator</span><span class="o">.</span><span class="n">ScheduleFn</span><span class="p">(</span><span class="n">stochastic_schedule_mm</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">work_dir</span><span class="o">=</span><span class="s2">&#34;./tune_tmp&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">task_name</span><span class="o">=</span><span class="s2">&#34;main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sch_tuned</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">tir_integration</span><span class="o">.</span><span class="n">compile_tir</span><span class="p">(</span><span class="n">database</span><span class="p">,</span> <span class="n">MyModule</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm --num-cores=1&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sch_tuned</span><span class="o">.</span><span class="n">trace</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><details class="custom-details">
    <summary class="custom-summary">clang error on Windows</summary>
    <div><p>不知道为何Windows上运行clang会出错</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">LocalRunner: An exception occurred
</span></span><span class="line"><span class="cl">Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;D:\Work\Anaconda\envs\tvm-build\lib\site-packages\tvm-0.18.dev0-py3.9-win-amd64.egg\tvm\exec\popen_worker.py&#34;</span>, line 87, in main
</span></span><span class="line"><span class="cl">    <span class="nv">result</span> <span class="o">=</span> fn<span class="o">(</span>*args, **kwargs<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;D:\Work\Anaconda\envs\tvm-build\lib\site-packages\tvm-0.18.dev0-py3.9-win-amd64.egg\tvm\meta_schedule\runner\local_runner.py&#34;</span>, line 148, in _worker_func
</span></span><span class="line"><span class="cl">    <span class="nv">rt_mod</span> <span class="o">=</span> tvm.runtime.load_module<span class="o">(</span>artifact_path<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;D:\Work\Anaconda\envs\tvm-build\lib\site-packages\tvm-0.18.dev0-py3.9-win-amd64.egg\tvm\runtime\module.py&#34;</span>, line 696, in load_module
</span></span><span class="line"><span class="cl">    _cc.create_shared<span class="o">(</span>path + <span class="s2">&#34;.so&#34;</span>, files<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;D:\Work\Anaconda\envs\tvm-build\lib\site-packages\tvm-0.18.dev0-py3.9-win-amd64.egg\tvm\contrib\cc.py&#34;</span>, line 96, in create_shared
</span></span><span class="line"><span class="cl">    _windows_compile<span class="o">(</span>output, objects, options, cwd, ccache_env<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;D:\Work\Anaconda\envs\tvm-build\lib\site-packages\tvm-0.18.dev0-py3.9-win-amd64.egg\tvm\contrib\cc.py&#34;</span>, line 415, in _windows_compile
</span></span><span class="line"><span class="cl">    raise RuntimeError<span class="o">(</span>msg<span class="o">)</span>
</span></span><span class="line"><span class="cl">RuntimeError: Compilation error:
</span></span><span class="line"><span class="cl">clang -O2 -shared -o C:<span class="se">\U</span>sers<span class="se">\1</span>7725<span class="se">\A</span>ppData<span class="se">\L</span>ocal<span class="se">\T</span>emp<span class="se">\t</span>mp96lbzaxg<span class="se">\t</span>vm_tmp_mod.tar.so C:<span class="se">\U</span>sers<span class="se">\1</span>7725<span class="se">\A</span>ppData<span class="se">\L</span>ocal<span class="se">\T</span>emp<span class="se">\t</span>mp96lbzaxg<span class="se">\t</span>vm_tmp_mod<span class="se">\l</span>ib0.o
</span></span><span class="line"><span class="cl">ld.lld: error: undefined symbol: _fltused
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; referenced by C:<span class="se">\U</span>sers<span class="se">\1</span>7725<span class="se">\A</span>ppData<span class="se">\L</span>ocal<span class="se">\T</span>emp<span class="se">\t</span>mp96lbzaxg<span class="se">\t</span>vm_tmp_mod<span class="se">\l</span>ib0.o
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">clang: error: linker <span class="nb">command</span> failed with <span class="nb">exit</span> code <span class="m">1</span> <span class="o">(</span>use -v to see invocation<span class="o">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (4)-End to End Model Execution</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch3/</link>
      <pubDate>Sun, 18 Aug 2024 13:51:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch3/</guid>
      <description>Personal notebook 3.</description>
      <content:encoded><![CDATA[<h1 id="e2e-model-integration">E2E Model Integration</h1>
<p>我们以下图中的 MLP 网络为例，这是一个两层全连接网络，并且省略了最后的 Softmax 层。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/e2e_fashionmnist_mlp_model.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/e2e_fashionmnist_mlp_model.png" alt="img">
    </a><figcaption>MLP Model</figcaption></figure></p>
<p>利用高级Numpy的实现如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">numpy_mlp</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">lv0</span> <span class="o">=</span> <span class="n">data</span> <span class="o">@</span> <span class="n">w0</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">b0</span>
</span></span><span class="line"><span class="cl">    <span class="n">lv1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">lv0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">lv2</span> <span class="o">=</span> <span class="n">lv1</span> <span class="o">@</span> <span class="n">w1</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">b1</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">lv2</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>为了方便说明底层计算过程，用 Low-level Numpy 进行重写后如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_linear0</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Z</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">784</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_relu0</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_linear1</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Z</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_mlp</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">lv0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">lnumpy_linear0</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">lv0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">lv1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">lnumpy_relu0</span><span class="p">(</span><span class="n">lv0</span><span class="p">,</span> <span class="n">lv1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">lnumpy_linear1</span><span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="constructing-an-e2e-irmodule-in-tvmscript">Constructing an E2E IRModule in TVMScript</h1>
<p>同样可以用 TVMScript 构建这个网络的 IRModule，只不过这次除了要用 Primitive Tensor Function (<code>@T.prim_function</code>) 还要用 Relax Function (<code>@R.function</code>) 来抽象神经网络的计算过程。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModule</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">relu0</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">              <span class="n">Y</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># function attr dict</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;relu0&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">linear0</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">W</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;linear0&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Z&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span>  <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">linear1</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">W</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;linear1&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Z&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">w0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">b0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">w1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">b1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="bp">cls</span> <span class="o">=</span> <span class="n">MyModule</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv0</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">linear0</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">relu0</span><span class="p">,</span> <span class="p">(</span><span class="n">lv0</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">linear1</span><span class="p">,</span> <span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="computational-graph-view">Computational Graph View</h2>
<p>该网络的计算图如下，计算图通常具有以下性质：</p>
<ul>
<li>框的每个输入边对应于操作的输入；</li>
<li>每个出边对应于操作的输出；</li>
<li>可以任意调整操作的顺序，只要保证边的拓扑排序（Topological Order）没有改变。</li>
</ul>
<details class="custom-details">
    <summary class="custom-summary">Topological Order</summary>
    <div><p>拓扑排序是针对有向无环图 (DAG) 的一种排序算法，它将图中的节点排成一个线性序列，满足以下条件：</p>
<ul>
<li>对于图中的任意一条边 (u, v)，节点 u 在排序中都出现在节点 v 之前。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB0867619b455c7fb8b6a7466166b2d607?method=download&amp;shareKey=0d7be82ecb2c6266762a2fb6f9f6ab93" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB0867619b455c7fb8b6a7466166b2d607?method=download&amp;shareKey=0d7be82ecb2c6266762a2fb6f9f6ab93" alt="dag">
    </a><figcaption>Example DAG</figcaption></figure></p>
<p>进行拓扑排序较常用的方法：</p>
<ol>
<li>从 DAG 图中选择一个 没有前驱（即入度为0）的顶点并输出。</li>
<li>从图中删除该顶点和所有以它为起点的有向边。</li>
<li>重复 1 和 2 直到当前的 DAG 图为空或 <strong>当前图中不存在无前驱的顶点为止</strong> 。后一种情况说明有向图中必然存在环。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa8a3d1e503e1e6256d807b7d856c6b6e?method=download&amp;shareKey=947b1232dd9adbb40e20ddbf27de18c6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa8a3d1e503e1e6256d807b7d856c6b6e?method=download&amp;shareKey=947b1232dd9adbb40e20ddbf27de18c6" alt="topo sort">
    </a><figcaption>Topological Sort Algorithm</figcaption></figure></p>
</div>
</details><br>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/e2e_computational_graph_call_tir.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/e2e_computational_graph_call_tir.png" alt="Computational Graph View">
    </a><figcaption>Computational Graph View</figcaption></figure></p>
<h2 id="rcall_tir">R.call_tir</h2>
<p><code>R.call_tir</code> 正如名字一样调用一个 <code>T.prim_func</code> 并返回计算结果。它的行为用Numpy表示如下，先根据 <code>shape</code>和 <code>dtype</code>开辟输出数据的内存空间，然后调用函数，最后返回输出结果。<code>R.call_tir</code>函数的输入是这种形式的原因是 <code>T.prim_func</code>函数的输入需要我们先为输出结果开辟内存，称为 <strong>目标传递 (destination passing)</strong> 。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_call_tir</span><span class="p">(</span><span class="n">prim_func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prim_func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">res</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>为了让程序执行具有计算图的性质，我们采用这种方式进行调用</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_mlp_with_call_tir</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">lv0</span> <span class="o">=</span> <span class="n">lnumpy_call_tir</span><span class="p">(</span><span class="n">lnumpy_linear0</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">lv1</span> <span class="o">=</span> <span class="n">lnumpy_call_tir</span><span class="p">(</span><span class="n">lnumpy_relu0</span><span class="p">,</span> <span class="p">(</span><span class="n">lv0</span><span class="p">,</span> <span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out</span> <span class="o">=</span> <span class="n">lnumpy_call_tir</span><span class="p">(</span><span class="n">lnumpy_linear1</span><span class="p">,</span> <span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="dataflow-block">Dataflow Block</h2>
<p>理想情况下，计算图中的操作应为 side-effect free，即一个函数只从其输入中读取并通过其输出返回结果，不会改变程序的其他部分（例如递增全局计数器）。如果要引入包含 side-effect 的操作，就需要定义多个dataflow block，在他们之外或者之间进行操作。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">         <span class="n">w0</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">         <span class="n">b0</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">         <span class="n">w1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">         <span class="n">b1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">lv0</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">linear0</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">gv0</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">relu0</span><span class="p">,</span> <span class="p">(</span><span class="n">lv0</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">gv1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">alloc_tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>  <span class="c1"># side-effect operation</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">linear1</span><span class="p">,</span> <span class="p">(</span><span class="n">gv0</span><span class="p">,</span> <span class="n">gv1</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="build-and-run-the-model">Build and Run the Model</h1>
<p>该网络对应的TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">linear0</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">W</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">Z</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;linear0&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">],</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Z&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">linear1</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">W</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">Z</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;linear1&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">],</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Z&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">relu0</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">Y</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;relu0&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">w0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">b0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">w1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="n">b1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv0</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">linear0</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">relu0</span><span class="p">,</span> <span class="p">(</span><span class="n">lv0</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">linear1</span><span class="p">,</span> <span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以通过下面方式来构造 virtual machine. <code>relax.build</code>返回一个 <code>tvm.relax.Executable</code>对象，然后就可以在指定的硬件上创建virtual machine 来执行计算图。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">ex</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">vm</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span><span class="p">(</span><span class="n">ex</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">nd_res</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&#34;main&#34;</span><span class="p">](</span><span class="n">data_nd</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">nd_params</span><span class="p">[</span><span class="s2">&#34;w0&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">nd_params</span><span class="p">[</span><span class="s2">&#34;b0&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">nd_params</span><span class="p">[</span><span class="s2">&#34;w1&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">nd_params</span><span class="p">[</span><span class="s2">&#34;b1&#34;</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="integrate-existing-libraries-in-the-environment">Integrate Existing Libraries in the Environment</h1>
<p>除了用 <code>T.prim_func</code>构造RelaxIR，我们也可以从现有的深度学习库的函数来构造。</p>
<p>这是通过 <code>R.call_dps_packed</code>来完成的，它用于调用一个目标传递风格 (Destination-Passing Style) 的打包函数 (Packed Function)，并返回输出结果。</p>
<blockquote>
<p><strong>目标传递风格 (Destination-Passing Style):</strong> 目标传递风格是一种函数调用方式，其中函数的输出参数作为函数参数传递给函数。
<strong>打包函数 (Packed Function):</strong> 打包函数是一种函数，其输入参数和输出参数都被打包成一个结构体。
<strong>纯函数 (Pure Function):</strong> 纯函数是指不产生副作用的函数，即函数的执行结果只依赖于输入参数，并且不会修改任何全局状态。</p></blockquote>
<p><strong>示例：</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="s2">&#34;env.linear&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>函数参数：</strong></p>
<ul>
<li><code>func</code>: 可以是字符串或表达式，表示目标传递风格的函数。如果 <code>func</code> 是字符串，它将被转换为 <code>ExternFunc</code> 对象。</li>
<li><code>args</code>: 表达式，表示输入参数。如果 <code>args</code> 是单个表达式，它将被包装成一个 <code>RxTuple</code> 对象。</li>
<li><code>out_sinfo</code>: 可以是 <code>TensorStructInfo</code> 对象或 <code>TensorStructInfo</code> 对象列表，表示 <code>call_dps_packed</code> 函数输出的结构信息。每个 <code>TensorStructInfo</code> 对象表示一个返回的张量的结构信息。</li>
</ul>
<p><strong>函数返回值：</strong></p>
<ul>
<li><code>ret</code>: <code>Call</code> 对象，表示 <code>call_dps_packed</code> 操作符的调用节点。</li>
</ul>
<h2 id="registering-runtime-function">Registering Runtime Function</h2>
<p>为了能够执行调用外部函数的代码，我们需要注册相应的函数。下面这段代码注册了两个自定义函数，分别用于实现线性层和 ReLU 激活函数。</p>
<ol>
<li><strong><code>@tvm.register_func(&quot;env.linear&quot;, override=True)</code>:</strong>
<ul>
<li>使用 <code>@tvm.register_func</code> 装饰器将 <code>torch_linear</code> 函数注册为名为 <code>&quot;env.linear&quot;</code> 的 TVM 函数。</li>
<li><code>override=True</code> 表示如果已经存在同名函数，则覆盖它。</li>
</ul>
</li>
<li><strong><code>torch_linear(x: tvm.nd.NDArray, w: tvm.nd.NDArray, b: tvm.nd.NDArray, out: tvm.nd.NDArray)</code>:</strong>
<ul>
<li>该函数接受四个参数：
<ul>
<li><code>x</code>: 输入张量。</li>
<li><code>w</code>: 权重张量。</li>
<li><code>b</code>: 偏置张量。</li>
<li><code>out</code>: 输出张量。</li>
</ul>
</li>
<li>函数内部：
<ul>
<li>使用 <code>torch.from_dlpack</code> 将 TVM 的 <code>NDArray</code> 对象转换为 PyTorch 的 <code>Tensor</code> 对象。</li>
<li>使用 PyTorch 的 <code>torch.mm</code> 函数进行矩阵乘法，将 <code>x</code> 和 <code>w</code> 的转置相乘，并将结果写入 <code>out</code>。</li>
<li>使用 PyTorch 的 <code>torch.add</code> 函数将 <code>b</code> 加到 <code>out</code> 上。</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>@tvm.register_func(&quot;env.relu&quot;, override=True)</code>:</strong>
<ul>
<li>使用 <code>@tvm.register_func</code> 装饰器将 <code>lnumpy_relu</code> 函数注册为名为 <code>&quot;env.relu&quot;</code> 的 TVM 函数。</li>
<li><code>override=True</code> 表示如果已经存在同名函数，则覆盖它。</li>
</ul>
</li>
<li><strong><code>lnumpy_relu(x: tvm.nd.NDArray, out: tvm.nd.NDArray)</code>:</strong>
<ul>
<li>该函数接受两个参数：
<ul>
<li><code>x</code>: 输入张量。</li>
<li><code>out</code>: 输出张量。</li>
</ul>
</li>
<li>函数内部：
<ul>
<li>使用 <code>torch.from_dlpack</code> 将 TVM 的 <code>NDArray</code> 对象转换为 PyTorch 的 <code>Tensor</code> 对象。</li>
<li>使用 PyTorch 的 <code>torch.maximum</code> 函数计算 <code>x</code> 和 0 之间的最大值，并将结果写入 <code>out</code>。</li>
</ul>
</li>
</ul>
</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.register_func</span><span class="p">(</span><span class="s2">&#34;env.linear&#34;</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">torch_linear</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">NDArray</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                 <span class="n">w</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">NDArray</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                 <span class="n">b</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">NDArray</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                 <span class="n">out</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">NDArray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x_torch</span><span class="p">,</span> <span class="n">w_torch</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out_torch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">out_torch</span><span class="p">,</span> <span class="n">b_torch</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out_torch</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tvm.register_func</span><span class="p">(</span><span class="s2">&#34;env.relu&#34;</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_relu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">NDArray</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                <span class="n">out</span><span class="p">:</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">NDArray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x_torch</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span> <span class="n">out</span><span class="o">=</span><span class="n">out_torch</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后我们就可以创建IRModule并通过上一节所说方法去 build and run.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModuleWithExternCall</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">w0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">b0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">w1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">b1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># block 0</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv0</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="s2">&#34;env.linear&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="s2">&#34;env.relu&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">lv0</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="s2">&#34;env.linear&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ex</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">MyModuleWithExternCall</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">vm</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span><span class="p">(</span><span class="n">ex</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="mixing-tensorir-code-and-libraries">Mixing TensorIR Code and Libraries</h1>
<p>我们可以混合使用T.prim_func和 注册的 runtime 函数来创建 RelaxIR.  以下代码展示了一个例子，其中 linear0 仍在 TensorIR 中实现，而其他函数则被重定向到库函数中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModuleMixture</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">linear0</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">W</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;linear0&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Z&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span>  <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">w0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">b0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">w1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">             <span class="n">b1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="bp">cls</span> <span class="o">=</span> <span class="n">MyModuleMixture</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv0</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">linear0</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">b0</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="s2">&#34;env.relu&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">lv0</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="s2">&#34;env.linear&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="bind-parameters-to-irmodule">Bind Parameters to IRModule</h1>
<p>之前都是通过显示传递参数给 <code>vm[&quot;main&quot;]</code>函数来调用，我们也可以将参数当作常熟与IRModule进行绑定。</p>
<p><code>metadata[&quot;relax.expr.Constant&quot;]</code>对应的是存储常量的隐式字典（虽然没有显示在脚本中，但仍是 IRModule 的一部分）。构建了转换后的 IRModule，现在只需输入数据就可以调用函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">MyModuleWithParams</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">BindParams</span><span class="p">(</span><span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="n">nd_params</span><span class="p">)(</span><span class="n">MyModuleMixture</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">MyModuleWithParams</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#-------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">linear0</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">W</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">Z</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">],</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">vj</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Z&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">Z</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv0</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">linear0</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">1</span><span class="p">]),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="s2">&#34;env.relu&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">lv0</span><span class="p">,),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_dps_packed</span><span class="p">(</span><span class="s2">&#34;env.linear&#34;</span><span class="p">,</span> <span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&#34;relax.expr.Constant&#34;</span><span class="p">][</span><span class="mi">3</span><span class="p">]),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (3)-Schedule Analysis</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tcm-ch10/</link>
      <pubDate>Sat, 17 Aug 2024 14:56:33 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tcm-ch10/</guid>
      <description>Personal notebook 3.</description>
      <content:encoded><![CDATA[<h1 id="looprv--blockrv-object">LoopRV &amp; BlockRV Object</h1>
<p>Schedule要操作的对象主要就是LoopRV和BlockRV，对应于我们TVMScript中的循环变量和计算块部分。下面代码为在 TVM 中注册 <code>LoopRV</code> 的自定义对象类型的过程，并通过 FFI（Foreign Function Interface）机制将 C++ 中的函数暴露给 Python.</p>
<p><strong>注册过程解析：</strong></p>
<ol>
<li><strong>定义类:</strong> 首先，定义一个名为 <code>LoopRV</code> 的类，它继承自 <code>tvm.Object</code> 类。这个类表示一个与循环相关的随机变量。</li>
<li><strong>使用 <code>@_register_object</code> 装饰器:</strong> <code>LoopRV</code> 类使用 <code>@_register_object(&quot;tir.LoopRV&quot;)</code> 装饰器进行注册。这个装饰器会调用 <code>register_object</code> 函数，将 <code>LoopRV</code> 类注册到 TVM 的对象系统中，并使用类型键 &ldquo;tir.LoopRV&rdquo; 来标识它。</li>
<li><strong>FFI 初始化:</strong> <code>tvm._ffi._init_api(&quot;tir.schedule&quot;, __name__)</code> 这行代码使用 <code>_init_api</code> 函数初始化 FFI，将 C++ 中的 &ldquo;tir.schedule&rdquo; 模块的函数暴露给 Python。</li>
<li><strong><code>_init_api</code> 和 <code>_init_api_prefix</code> 函数:</strong> <code>_init_api</code> 函数用于初始化 FFI，它会调用 <code>_init_api_prefix</code> 函数来处理具体的函数注册过程。</li>
<li><strong>函数注册:</strong> <code>_init_api_prefix</code> 函数会遍历所有 C++ 中的全局函数，找到以 &ldquo;tir.schedule&rdquo; 开头的函数，并将其注册到 Python 中。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@_register_object</span><span class="p">(</span><span class="s2">&#34;tir.LoopRV&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LoopRV</span><span class="p">(</span><span class="n">Object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;A random variable that refers to a loop&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Construct a new LoopRV.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">__init_handle_by_constructor__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">_ffi_api</span><span class="o">.</span><span class="n">LoopRV</span>  <span class="c1"># type: ignore # pylint: disable=no-member</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;FFI APIs for tvm.tir.schedule&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tvm._ffi</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tvm</span><span class="o">.</span><span class="n">_ffi</span><span class="o">.</span><span class="n">_init_api</span><span class="p">(</span><span class="s2">&#34;tir.schedule&#34;</span><span class="p">,</span> <span class="vm">__name__</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
</span></span></code></pre></td></tr></table>
</div>
</div><details class="custom-details">
    <summary class="custom-summary">_register_object</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">register_object</span><span class="p">(</span><span class="n">type_key</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>   
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;internal register function&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&#34;_type_index&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">tindex</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_type_index</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">tidx</span> <span class="o">=</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_uint</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="n">_RUNTIME_ONLY</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">check_call</span><span class="p">(</span><span class="n">_LIB</span><span class="o">.</span><span class="n">TVMObjectTypeKey2Index</span><span class="p">(</span><span class="n">c_str</span><span class="p">(</span><span class="n">object_name</span><span class="p">),</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">byref</span><span class="p">(</span><span class="n">tidx</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># directly skip unknown objects during runtime.</span>
</span></span><span class="line"><span class="cl">                <span class="n">ret</span> <span class="o">=</span> <span class="n">_LIB</span><span class="o">.</span><span class="n">TVMObjectTypeKey2Index</span><span class="p">(</span><span class="n">c_str</span><span class="p">(</span><span class="n">object_name</span><span class="p">),</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">byref</span><span class="p">(</span><span class="n">tidx</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">ret</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="k">return</span> <span class="bp">cls</span>
</span></span><span class="line"><span class="cl">            <span class="n">tindex</span> <span class="o">=</span> <span class="n">tidx</span><span class="o">.</span><span class="n">value</span>
</span></span><span class="line"><span class="cl">        <span class="n">_register_object</span><span class="p">(</span><span class="n">tindex</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">cls</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">type_key</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">register</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">register</span><span class="p">(</span><span class="n">type_key</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>装饰器功能:</strong></p>
<ol>
<li><strong>注册对象类型:</strong> 装饰器 <code>register_object</code> 的主要作用是将一个类注册到 TVM 的对象系统中，以便 TVM 能够识别和使用该类。</li>
<li><strong>类型键:</strong> 装饰器接受一个可选参数 <code>type_key</code>，用于指定该对象的类型键。类型键是一个字符串，用于唯一标识该对象类型。如果 <code>type_key</code> 未指定，则使用类的名称作为类型键。</li>
<li><strong>内部注册函数:</strong> 装饰器内部定义了一个名为 <code>register</code> 的函数，该函数负责实际的注册操作。</li>
<li><strong>注册过程:</strong>
<ul>
<li>获取类型索引: <code>register</code> 函数首先获取该类型的索引，如果该类型已经注册，则直接获取已有的索引；否则，调用 TVM 的 C API 函数 <code>TVMObjectTypeKey2Index</code> 获取新的索引。</li>
<li>注册对象: <code>register</code> 函数使用 <code>_register_object</code> 函数将类型索引和类对象注册到 TVM 的对象系统中。</li>
</ul>
</li>
</ol></div>
</details><br>
<p>BlockRV类的定义同理。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@_register_object</span><span class="p">(</span><span class="s2">&#34;tir.BlockRV&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">BlockRV</span><span class="p">(</span><span class="n">Object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;A random variable that refers to a block&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Construct a new BlockRV.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">__init_handle_by_constructor__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">_ffi_api</span><span class="o">.</span><span class="n">BlockRV</span>  <span class="c1"># type: ignore # pylint: disable=no-member</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="schedule-primitive">Schedule Primitive</h1>
<p><code>Schedule</code>是一组改变了计算的顺序，但保留了计算的语义的变换。它的构造函数需要一个 <code>IRModule</code>实例作为参数。我们以下面的矩阵的 element-wise乘法为例来介绍以下可能的变换。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tvm</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>   
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>   
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Declare some variables for use later</span>
</span></span><span class="line"><span class="cl"><span class="n">n</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&#34;n&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&#34;m&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Declare a matrix element-wise multiply</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">([</span><span class="n">C</span><span class="o">.</span><span class="n">op</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># lower 将计算从定义转换成可以调用的IRModule</span>
</span></span><span class="line"><span class="cl"><span class="n">tvm</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">[</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">],</span> <span class="n">simple_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><details class="custom-details">
    <summary class="custom-summary">tvm.lower</summary>
    <div><p><code>tvm.lower</code> 函数是 TVM 中用于将计算图（Compute Graph）降低（lower）到更低级别的表示形式，例如 Relay IR 或 TensorIR ，该函数会返回一个IRModule.</p>
<p><strong>参数解释:</strong></p>
<ul>
<li>
<p><strong><code>inp</code>:</strong> 输入参数，可以是以下三种类型之一：<code>tvm.te.schedule.Schedule</code> 对象：表示计算图的调度信息。</p>
<p><code>tvm.tir.PrimFunc</code> 对象：表示 TensorIR 的主函数。</p>
<p><code>IRModule</code> 对象：表示一个包含多个函数的模块。</p>
</li>
<li>
<p><strong><code>args</code>:</strong> 可选参数，表示输入张量的列表，仅在 <code>inp</code> 是 <code>tvm.te.schedule.Schedule</code> 对象时使用。</p>
</li>
<li>
<p><strong><code>name</code>:</strong> 可选参数，表示生成的函数的名称，默认为 &ldquo;main&rdquo;。</p>
</li>
<li>
<p><strong><code>binds</code>:</strong> 可选参数，表示一个字典，用于指定输入张量的绑定，仅在 <code>inp</code> 是 <code>tvm.te.schedule.Schedule</code> 对象时使用。</p>
</li>
<li>
<p><strong><code>simple_mode</code>:</strong> 可选参数，表示是否使用简化的模式，默认为 <code>False</code>。</p>
</li>
</ul>
</div>
</details><br>
<p>上述代码生成的TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># from tvm.script import ir as I</span>
</span></span><span class="line"><span class="cl"><span class="c1"># from tvm.script import tir as T</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;from_legacy_te_schedule&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="s2">&#34;stride&#34;</span><span class="p">,</span> <span class="s2">&#34;stride&#34;</span><span class="p">),</span> <span class="n">buffer_type</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">B_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="s2">&#34;stride&#34;</span><span class="p">,</span> <span class="s2">&#34;stride&#34;</span><span class="p">),</span> <span class="n">buffer_type</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">C_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="s2">&#34;stride&#34;</span><span class="p">,</span> <span class="s2">&#34;stride&#34;</span><span class="p">),</span> <span class="n">buffer_type</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">C_2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">C_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="p">,),</span> <span class="n">data</span><span class="o">=</span><span class="n">C_1</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">buffer_type</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">A_2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">A_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="p">,),</span> <span class="n">data</span><span class="o">=</span><span class="n">A_1</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">buffer_type</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">B_2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">B_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="p">,),</span> <span class="n">data</span><span class="o">=</span><span class="n">B_1</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">buffer_type</span><span class="o">=</span><span class="s2">&#34;auto&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">C_2</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">C_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">C_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">A_2</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">A_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">A_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">*</span> <span class="n">B_2</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">B_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">B_1</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="merge">Merge</h2>
<h2 id="fuse">Fuse</h2>
<p><code>fuse</code> 方法用于将一组连续的循环合并成一个循环。合并后的循环将包含所有原始循环的迭代空间。</p>
<p><strong>限制条件:</strong></p>
<ol>
<li>循环不能包含任何注解或线程绑定，例如 <code>@T.pragma</code> 或 <code>@T.thread_binding</code></li>
<li>循环必须是连续的，也就是说，每个循环的父循环必须是前一个循环。</li>
<li>循环的起始值必须为 0</li>
<li>每个循环的域不能依赖于其他要合并的循环。</li>
</ol>
<p><strong>参数:</strong></p>
<ul>
<li><code>loops</code>: 一个循环列表，表示要合并的循环。</li>
<li><code>preserve_unit_iters</code>: 一个布尔值，表示是否保留单位迭代的循环。默认值为 <code>True</code>，表示保留单位迭代的循环。</li>
</ul>
<p><strong>返回值:</strong></p>
<ul>
<li><code>fused_loop</code>: 一个新的循环对象，表示合并后的循环。</li>
</ul>
<p>以 <code>B[i, j]=A[i, j]*2</code>为例，<code>fuse</code> 前对应的TensorIR</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span> <span class="o">=</span> <span class="n">IRModule</span><span class="p">({</span><span class="s2">&#34;main&#34;</span><span class="p">:</span> <span class="n">func</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR Before Fuse--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>fuse</code> 后对应的TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_B</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_B</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Fuse--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_j_fused</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span> <span class="o">*</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i_j_fused</span> <span class="o">%</span> <span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span> <span class="o">//</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">i_j_fused</span> <span class="o">%</span> <span class="n">n</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="split">Split</h2>
<p><code>split</code> 方法将一个循环拆分成多个连续的循环，每个循环的迭代次数由 <code>factors</code> 参数指定。</p>
<p><strong>限制条件:</strong></p>
<ol>
<li>要拆分的循环不能有任何注解 (annotation) 或线程绑定 (thread binding).</li>
<li>要拆分的循环必须从 0 开始迭代。</li>
<li>在 <code>factors</code> 列表中，最多只能有一个元素为 <code>None</code>，表示该元素的迭代次数将自动推断。</li>
</ol>
<p><strong>参数:</strong></p>
<ul>
<li><strong><code>loop</code>:</strong> 要拆分的循环对象。</li>
<li><strong><code>factors</code>:</strong> 一个列表，表示拆分后的每个循环的迭代次数。列表中的元素可以是整数、表达式或 <code>None</code>。如果列表中包含 <code>None</code>，则该元素的迭代次数将自动推断。</li>
<li><strong><code>preserve_unit_iters</code>:</strong> 一个布尔值，表示是否保留单位迭代器。如果设置为 <code>True</code>，则会保留单位迭代器，否则会将单位迭代器合并到其他循环中。</li>
<li><strong><code>disable_predication</code>:</strong> 一个布尔值，表示是否禁用谓词 (predicate). 如果设置为 <code>True</code>，则不会创建谓词来保护循环。</li>
</ul>
<p>以 <code>B[i]=A[i]*2</code>为例，<code>split</code>前对应的TensorIR</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">s</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_schedule</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span> <span class="o">=</span> <span class="n">IRModule</span><span class="p">({</span><span class="s2">&#34;main&#34;</span><span class="p">:</span> <span class="n">func</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR Before Split--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>split</code> 后对应的TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_b</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Split--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span><span class="p">,</span> <span class="n">i_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">((</span><span class="n">m</span> <span class="o">+</span> <span class="mi">31</span><span class="p">)</span> <span class="o">//</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">+</span> <span class="n">i_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">i_0</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">+</span> <span class="n">i_1</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="loop-partition">Loop Partition</h2>
<p><code>loop_partition</code> 方法用于将一个循环分割成多个连续的循环</p>
<p><strong>限制条件:</strong></p>
<ul>
<li>循环不能有注解或线程绑定。</li>
<li><code>factors</code> 列表中最多只能有一个元素为 <code>None</code></li>
<li>不支持循环的值未知的情况。</li>
</ul>
<p><strong>参数:</strong></p>
<ul>
<li><code>loop</code>: 要分割的循环。</li>
<li><code>factors</code>: 分割因子列表。</li>
<li><code>preserve_unit_iters</code>: 是否保留单位迭代的循环，默认值为 <code>True</code>。</li>
</ul>
<p>仍以 <code>B[i, j]=A[i, j]*2</code>为例，<code>loop_partition</code>前对应的TensorIR</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl"><span class="n">n</span> <span class="o">=</span> <span class="mi">128</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span> <span class="o">=</span> <span class="n">IRModule</span><span class="p">({</span><span class="s2">&#34;main&#34;</span><span class="p">:</span> <span class="n">func</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR Before Loop Partition--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们指定 <code>factors=[2,64]</code>，相当于把整个循环在2和64处分成3份，<code>loop_partition</code>后对应的TensorIR如下。在使用 <code>loop_partition</code> 后，会创建多个嵌套的块，例如 <code>root</code>、<code>B_i_common</code> 以及每个分割后的循环对应的块，前两个块中会执行一个空的 <code>T.reads</code> 和 <code>T.writes</code> 操作。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_B</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_B</span><span class="p">)</span>  <span class="c1"># return a list of LoopRV</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">loop_partition</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Loop Partition--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;root&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_i_common&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_i0_partition&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_i0&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">v_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">i0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_i1_partition&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">66</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_i1&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">v_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">66</span><span class="p">),</span> <span class="n">i1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">66</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">66</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_i2_partition&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="k">for</span> <span class="n">i2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">66</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B_i2&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                            <span class="n">v_i</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">((</span><span class="mi">66</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">i2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">66</span><span class="p">:</span><span class="mi">128</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="mi">66</span><span class="p">:</span><span class="mi">128</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                            <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="reorder">Reorder</h2>
<p><code>reorder</code> 方法用于重新排列循环的执行顺序。</p>
<p><strong>限制条件:</strong></p>
<ol>
<li>所有循环必须属于同一个循环链，这意味着它们可以按照祖先-后代关系排序，并且它们之间只有单分支循环（即没有 <code>if</code> 语句）。</li>
<li>外层循环的范围不能依赖于内层循环。</li>
<li>每个循环嵌套下的块绑定必须是仿射的，并且块变量必须都为数据并行或归约。</li>
<li><code>ordered_loops</code> 中不能包含重复的循环。</li>
</ol>
<p><strong>参数:</strong></p>
<ul>
<li><code>ordered_loops</code>: 一个或多个循环列表，表示新的循环执行顺序。</li>
</ul>
<p><code>reorder_block_iter_var</code> 方法的功能与reorder相同，只不过它接收的参数为</p>
<ul>
<li><code>block</code>: 待进行变换的BlockRV对象</li>
<li><code>new_order</code>: 整数列表，代表该block新的迭代顺序</li>
</ul>
<p>前面章节已给出很多例子，这里不再赘述。</p>
<h2 id="parallel">Parallel</h2>
<p><code>parallel</code>方法将一个循环 <code>loopRV</code> 标记为并行执行，即循环的迭代可以同时在多个线程或处理器上执行，从而提高计算效率。</p>
<p><strong>限制条件:</strong></p>
<p>为了确保并行化操作的正确性和有效性，该函数需要满足以下条件：</p>
<ol>
<li>循环所在的块必须具有阶段流水线属性。这意味着该块中的计算可以被分解成多个阶段，每个阶段可以独立执行。</li>
<li>循环下的所有块必须是完整块或归约块，并且具有仿射绑定。</li>
<li>对于循环下的每个块，循环只能包含在数据并行块迭代的绑定中。</li>
</ol>
<p><strong>参数:</strong></p>
<ul>
<li><code>loop</code>: 要并行化的循环。</li>
</ul>
<p>以下面的矩阵的 element-wise乘法为例，<code>parallel</code>前对应的TensorIR为</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span> <span class="o">=</span> <span class="n">IRModule</span><span class="p">({</span><span class="s2">&#34;main&#34;</span><span class="p">:</span> <span class="n">func</span><span class="p">})</span>  
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR Before Parallel--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_C</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对外循环进行parallel，可以看到 <code>T.parallel</code>取代了之前的 <code>T.grid</code>，它会将所有迭代分配到多个线程或处理器上同时执行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_c</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">parallel</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Parallel--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_C</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="vectorize">Vectorize</h2>
<p><code>vectorize</code>方法将一个循环 <code>loop</code> 标记为向量化执行，这意味着循环的迭代可以被分组为向量，然后在单个指令中执行，从而提高计算效率。</p>
<p><strong>限制条件:</strong></p>
<ol>
<li>循环所在的块必须具有阶段流水线属性，即该块中的计算可以被分解成多个阶段，每个阶段可以独立执行。</li>
<li>循环下的所有块必须是完整块或归约块，并且具有仿射绑定。</li>
<li>对于循环下的每个块，循环只能包含在数据并行块迭代的绑定中。</li>
</ol>
<p><strong>参数:</strong></p>
<ul>
<li><code>loop</code>: 要向量化的循环。</li>
</ul>
<p>仍以 <code>B[i, j]=A[i, j]*2</code>为例，<code>loop_partition</code>前对应的TensorIR与 <a href="/blogs/courselearning/tvm/tcm-ch10/#Loop-Partition">Loop Partition</a> 中的相同。</p>
<p>Vectorize 是一种重要的优化技术，它利用现代处理器中的 <strong>SIMD</strong> (Single Instruction, Multiple Data)指令，将多个数据同时进行计算，从而提升计算效率。SIMD 指令使用向量寄存器来存储和操作多个数据。向量寄存器的长度通常是 128 位或 256 位，可以存储多个数据。例如，一个 SIMD 指令可以同时对 4 个浮点数进行加法运算。将循环向量化意味着将循环的迭代分组为向量，然后使用 SIMD 指令对这些向量进行操作。<code>T.vectorized</code> 在 TVM 中用来标记一个循环已经被向量化了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_b</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Vectorize--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">vectorized</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="unroll">Unroll</h2>
<p><code>unroll</code> 函数接收一个 <code>LoopRV</code> (循环表示变量) 作为输入，作用是将一个循环展开。它本质上是将循环体复制多次，并将循环计数器替换为具体的数值。有以下几个优点</p>
<ul>
<li>减少循环控制指令的执行次数，从而提高效率。</li>
<li>将循环体中的数据访问集中在一起，从而提高数据局部性，进而提高缓存命中率。</li>
<li>增加指令级并行性，从而提高程序执行速度。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_b</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">unroll</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Vectorize--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">unroll</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="reverse-compute-at">(Reverse) Compute at</h2>
<p><code>compute_at</code>方法的作用是将一个生产者块（producer block）移动到一个特定循环（loop）的内部，并重新生成由该生产者块引起的循环，以确保生产者块生成的缓冲区区域能够覆盖其消费者块在该循环下所使用的区域。<code>reverse_compute_at</code>则是移动消费者块（consumer block）</p>
<ul>
<li><strong>生产者块（producer block）：</strong> 生成数据（通常是缓冲区）的代码块。</li>
<li><strong>消费者块（consumer block）：</strong> 使用生产者块生成的数据的代码块。</li>
</ul>
<p><strong>限制条件：</strong></p>
<ol>
<li><code>block</code> 和 <code>loop</code> 必须在同一个作用域内。</li>
<li>不能将 <code>block</code>移动到它自身所在的循环的祖先循环中。</li>
<li>作用域块必须具有阶段-流水线属性。</li>
<li>作用域块的子树必须满足紧凑数据流条件，即子树中的所有块必须是完整块或归约块。</li>
<li>块不是作用域块的输出块，即块写入的缓冲区在作用域块下分配。</li>
<li>块的所有消费者都在给定的循环下。</li>
</ol>
<p>我们以 <code>C[i,j]=A[i,j] * 2 + 1</code>为例，<code>compute_at</code>前对应的TesnsorIR如下</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>我们在创建 <code>prim_func</code>时的输入只使用了 <code>A, C</code>，否则B就不会是作为中间变量的 <code>T.alloc_buffer</code>，调用 <code>compute_at</code>会因为违反第五条报错。</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">fuc</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">with_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span> <span class="o">=</span> <span class="n">IRModule</span><span class="p">({</span><span class="s2">&#34;main&#34;</span><span class="p">:</span> <span class="n">func</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR Before Compute_at--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_C</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在调用 <code>compute_at</code>之后块B的计算被移动到块C的循环i之下，相当于调用 <code>reverse_compute_at</code>将块C的计算移动到块B的循环i之下，对应的TesnorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loop</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">compute_at</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">loop</span><span class="p">,</span> <span class="n">preserve_unit_loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39; same way
</span></span></span><span class="line"><span class="cl"><span class="s1">block = sch.get_block(&#34;C&#34;)
</span></span></span><span class="line"><span class="cl"><span class="s1">loop, _ = sch.get_loops(sch.get_block(&#34;B&#34;))
</span></span></span><span class="line"><span class="cl"><span class="s1">sch.reverse_compute_at(block, loop, preserve_unit_loops=False)
</span></span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Compute_at--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_C</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">ax0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">ax0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="reverse-compute-inline">(Reverse) Compute Inline</h2>
<p><code>compute_inline</code> 方法用于将一个块（block）内联到其消费者（consumer）中。简单来说就是将一个块的计算逻辑直接嵌入到使用它结果的块中，从而消除中间块，简化计算流程。<code>reverse_compute_inline</code>则是用于将一个块（block）内联到其生产者（producer）中。</p>
<p><strong>限制条件：</strong></p>
<ol>
<li>要内联的块必须是一个完整的非根块（<code>root</code> 块），并且它必须只产生一个缓冲区。</li>
<li>要内联的块不能是其作用域内的唯一叶节点。</li>
<li>要内联的块的代码体必须是一个缓冲区存储语句，例如 <code>A[i, j, k, ...] = ...</code>。该语句的左侧索引必须是不同的原子变量，并且语句中不能包含其他变量。</li>
</ol>
<p>以<a href="/blogs/courselearning/tvm/tcm-ch10/#Reverse-Compute-at">上一节</a>的 <code>C[i,j]=A[i,j] * 2 + 1</code>为例，对应的TensorIR已给出。在执行Compute_inline之后块B的计算逻辑直接嵌入到块C中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;B&#34;</span><span class="p">)</span> <span class="c1"># same: sch.reverse_compute_inline(sch.get_block(&#34;C&#34;))</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">compute_inline</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Compute_inline--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_C</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="decompose-reduction">Decompose Reduction</h2>
<p><code>decompose_reduction</code> 函数用于将一个归约块（reduction block）分解成两个独立的块初始化块（init block）和更新块（update block）</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><ul>
<li><strong>初始化块（init block）：</strong> 从归约块的初始化语句（init statement）转换而来。</li>
<li><strong>更新块（update block）：</strong> 原始的归约块，但去掉了初始化语句。</li>
</ul></div>

<p><strong>限制条件：</strong></p>
<ol>
<li>要分解的块必须是一个归约块。</li>
<li>指定的循环必须是归约块的祖先循环。</li>
<li>指定的循环不能低于与归约块变量相关的所有循环。</li>
</ol>
<p>以矩阵乘法 <code>C = A @ B</code>为例，<code>decompose_reduction</code>前的TensorIR为</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">l</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="s2">&#34;l&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;l&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR Before Decompose Reduction--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_C</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">v_l</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l_1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_l</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">v_l</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_l</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">v_l</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>调用 <code>decompose_reduction</code> 方法后将块 <code>C</code> 分解成一个初始化块和一个更新块，并将初始化块插入到 <code>i</code> 循环之前，对应的TensorIR如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_c</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block_c</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------TensorIR After Decompose Reduction--------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">var_A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">,</span> <span class="n">var_C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">handle</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="n">m</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">(),</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">A</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_A</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">int32</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_B</span><span class="p">,</span> <span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">C</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">match_buffer</span><span class="p">(</span><span class="n">var_C</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_init</span><span class="p">,</span> <span class="n">j_init</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SS&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i_init</span><span class="p">,</span> <span class="n">j_init</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C_update&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">v_l</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">l_1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_l</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">v_l</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (2)-Tensor Program Abstraction Case</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch2/</link>
      <pubDate>Thu, 15 Aug 2024 23:07:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch2/</guid>
      <description>Personal notebook 2.</description>
      <content:encoded><![CDATA[<h1 id="primitive-tensor-function">Primitive Tensor Function</h1>
<p>机器学习编译的过程可以被看作张量函数之间的变换。一个典型的机器学习模型的执行包含许多步将输入张量之间转化为最终预测的计算步骤，其中的每一步都被称为元张量函数 (Primitive Tensor Function)

<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/primitive_tensor_func.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/primitive_tensor_func.png" alt="Primitive Tensor Function">
    </a><figcaption>Primitive Tensor Function</figcaption></figure></p>
<p>通常来说，一个典型的元张量函数实现的抽象包含了以下成分：存储数据的多维数组，驱动张量计算的循环嵌套以及计算部分本身的语句。下图以<a href="https://darkenstar.github.io/2024/08/15/chapter1/#Vector-Add-Example">上一篇</a>中的向量加法为例子进行了分解。

<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/tensor_func_elements.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/tensor_func_elements.png" alt="Tensor Function Elements">
    </a><figcaption>Tensor Function Elements</figcaption></figure></p>
<p>我们称这类抽象为<strong>张量程序抽象</strong>(Tensor Program Abstraction). 张量程序抽象的一个重要性质是，他们能够被一系列有效的程序变换所改变。例如，我们能够通过一组变换操作（如循环拆分、并行和向量化）将下图左侧的一个初始循环程序变换为右侧的程序。

<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/tensor_func_seq_transform.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/tensor_func_seq_transform.png" alt="Tensor Function Transforms">
    </a><figcaption>Tensor Function Transforms</figcaption></figure></p>
<h1 id="learning-one-tensor-program-abstraction--tensorir">Learning one Tensor Program Abstraction &ndash; TensorIR</h1>
<p>我们对于神经网络的一个基本的 Linear+ReLU 层可以用以下的数学公式表示</p>
<ul>
<li>$Y_{ij} = \sum_k A_{ik} B_{kj}$</li>
<li>$C_{ij} = \mathbb{ReLU}(Y_{ij}) = \mathbb{max}(Y_{ij}, 0)$</li>
</ul>
<p>其Numpy实现如下，下面的代码直接调用了Numpy的高级API，看起来非常简洁。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">dtype</span> <span class="o">=</span> <span class="s2">&#34;float32&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">a_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># a @ b is equivalent to np.matmul(a, b)</span>
</span></span><span class="line"><span class="cl"><span class="n">c_mm_relu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">a_np</span> <span class="o">@</span> <span class="n">b_np</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以将上述程序改写成Low-level Numpy，意味着对于复杂的计算我们使用循环进行表示，并且写出开辟数组空间的过程。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_mm_relu</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">C</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该函数执行以下操作：</p>
<ol>
<li><strong>矩阵乘法：</strong> 将两个矩阵 <code>A</code> 和 <code>B</code> 相乘，并将结果存储在 <code>Y</code> 中。</li>
<li><strong>ReLU 激活：</strong> 将 ReLU 激活函数应用于 <code>Y</code> 的元素，并将结果存储在 <code>C</code> 中。</li>
</ol>
<p>可以用以下代码来检查上述实现的正确性：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">c_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">lnumpy_mm_relu</span><span class="p">(</span><span class="n">a_np</span><span class="p">,</span> <span class="n">b_np</span><span class="p">,</span> <span class="n">c_np</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">c_mm_relu</span><span class="p">,</span> <span class="n">c_np</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>示例 numpy 代码包含了实际过程中实现这些计算时可能会用到的所有元素，用Numpy函数内部工作机制 (Under the Hood) 实现了MM-ReLU。</p>
<ul>
<li>开辟多维数组空间。</li>
<li>循环遍历数组的维度。</li>
<li>计算在循环内执行。</li>
</ul>
<p>我们也可以用上一节的TensorIR来实现，TVMScript 是嵌入在 Python AST 中的领域特定语言的 Dialect, 它本质上是 Python 的一个子集，但添加了一些特定于 TVM 的扩展，例如用于描述计算图的特殊语法和语义。</p>
<blockquote>
<p>Dialect 通常指一种语言的变体或子集，它与原始语言共享大部分语法和语义，但也有一些独特的特征。
抽象语法树 (AST) 是源代码的树状表示形式。它将代码的结构以一种层次化的方式呈现，每个节点代表代码中的一个语法元素，例如变量、运算符、函数调用等。</p></blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">mm_relu</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;mm_relu&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>   
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上述 TensorIR 程序的一个示例实例涵盖了大部分内容，包括</p>
<ul>
<li>参数和中间临时内存中的缓冲区声明。</li>
<li>For 循环迭代。</li>
<li><strong>Block</strong> 和 Block Axis属性。</li>
</ul>
<h1 id="transformation">Transformation</h1>
<p><code>TVM</code> 的 <code>tvm.tir.Schedule</code> 提供了一系列用于调度和优化计算图的变换函数。这些函数允许用户灵活地调整计算顺序、内存访问模式和并行化策略，以提高模型的性能。</p>
<p>我们可以用以下函数获得计算块和其对应的循环</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">block_Y</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">,</span> <span class="n">func_name</span><span class="o">=</span><span class="s2">&#34;mm_relu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_Y</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以使用 <code>split</code>函数将一个循环拆成多个循环，用 <code>reorder</code>函数交换循环的顺序，用 <code>reverse_compute_at</code> 函数移动计算块所在的循环，用 <code>decompose_reduction</code>函数将初始化和归约操作分开。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">j0</span><span class="p">,</span> <span class="n">j1</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">j0</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">j1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_C</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">,</span> <span class="s2">&#34;mm_relu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">reverse_compute_at</span><span class="p">(</span><span class="n">block_C</span><span class="p">,</span> <span class="n">j0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_Y</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;Y&#34;</span><span class="p">,</span> <span class="s2">&#34;mm_relu&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">decompose_reduction</span><span class="p">(</span><span class="n">block_Y</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output</span>
</span></span><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">mm_relu</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># function attr dict</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;mm_relu&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># body</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;)</span>
</span></span><span class="line"><span class="cl">        <span class="n">Y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">alloc_buffer</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j_1_init</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">serial</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y_init&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">j_1_init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">j_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;Y_update&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">j_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">ax0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">serial</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vj</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">j_0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">ax0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">],</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对应的 Low-level Numpy 函数如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">lnumpy_mm_relu_v3</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">C</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j0</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Y_init</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">j</span> <span class="o">=</span> <span class="n">j0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">j1</span> 
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Y_update</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">j1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">j</span> <span class="o">=</span> <span class="n">j0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">j1</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># C</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">j</span> <span class="o">=</span> <span class="n">j0</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">j1</span> 
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="why-do-loop-influence-the-exec-time">Why Do Loop Influence the Exec Time</h1>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/cpu_arch.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/cpu_arch.png" alt="CPU Architecture">
    </a><figcaption>CPU Architecture</figcaption></figure>
CPU 带有多级缓存，需要先将数据提取到缓存中，然后 CPU 才能访问它。而且访问已经在缓存中的数据要快得多。CPU 采用的一种策略是获取彼此更接近的数据。 当我们读取内存中的一个元素时，它会尝试将附近的元素（Cache Line）获取到缓存中，当读取下一个元素时它已经在缓存中。 因此，具有连续内存访问的代码通常比随机访问内存不同部分的代码更快。</p>
<p>
<figure class="post-figure">
    <a href="https://mlc.ai/zh/_images/tensor_func_loop_order.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://mlc.ai/zh/_images/tensor_func_loop_order.png" alt="Loop Order">
    </a><figcaption>Loop Order</figcaption></figure>
<code>j1</code> 这一迭代产生了对 <code>B</code> 元素的连续访问。具体来说，它意味着在 <code>j1=0</code> 和 <code>j1=1</code> 时我们读取的值彼此相邻。这可以让我们拥有更好的缓存访问行为。此外，我们使 C 的计算更接近 Y，从而实现更好的缓存行为。</p>
<h1 id="ways-to-create-and-interact-with-tensorir">Ways to Create and Interact with TensorIR</h1>
<h2 id="create-tensorir-via-tvmscript">Create TensorIR via TVMScript</h2>
<p>创建 TensorIR 函数的第一种方法是直接在 TVMScript 中编写函数，它也是一种在变换过程中检查张量函数的有用方法。我们可以打印出 TVMScript，进行一些手动编辑，然后将其反馈给 MLC 流程以调试和尝试可能的（手动）变换，然后将变换后的程序重新应用到 MLC 流程中。</p>
<h2 id="generate-tensorir-code-using-tensor-expression">Generate TensorIR code using Tensor Expression</h2>
<p>张量表达式 (TE) 是一种特定领域的语言，它通过 API 之类的表达式描述一系列计算。MM-ReLU 可以通过以下程序完成</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="s2">&#34;k&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Y</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;Y&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (1)-Tensor Program Abstraction in Action</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch1/</link>
      <pubDate>Thu, 15 Aug 2024 15:07:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch1/</guid>
      <description>Personal notebook 1.</description>
      <content:encoded><![CDATA[<p>My notebook of MLC: <a href="https://mlc.ai/summer22-zh">https://mlc.ai/summer22-zh</a></p>
<h1 id="constructing-tensor-program-by-tvmscript">Constructing Tensor Program by TVMScript</h1>
<p>在机器学习编译 (Machine Learning Compilation) 中，<strong>Tensor Program</strong> 指的是一种表示机器学习模型计算过程的程序，它以张量 (Tensor) 为基本数据单元，并使用张量操作来描述模型的计算步骤。</p>
<h2 id="vector-add-example">Vector-Add Example</h2>
<p>下面这段代码使用 TVM 的 <code>script</code> 模块定义了一个名为 <code>MyModule</code> 的模块，其中包含一个名为 <code>main</code> 的计算函数。</p>
<p>该函数实现了简单的向量加法 (vector add) 操作, 两个输入向量 <code>A</code> 和 <code>B</code> 相加，并将结果存储到输出向量 <code>C</code> 中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tvm</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm.ir.module</span> <span class="kn">import</span> <span class="n">IRModule</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">tir</span> <span class="k">as</span> <span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">             <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">             <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># extra annotations for the function</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># declare a data parallel iterator on spatial domain</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>1. 模块定义:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyModule</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>@tvm.script.ir_module</code>: 用于将 <code>MyModule</code> 类定义为一个 TVM 的 <code>IRModule</code> 对象。<code>IRModule</code> 是 TVM 中用于表示计算图 (Computation Graph) 的标准数据结构。</li>
<li><code>class MyModule:</code>: 定义一个名为 <code>MyModule</code> 的类，该类将包含计算函数。</li>
</ul>
<p><details class="custom-details">
    <summary class="custom-summary">Decorator</summary>
    <div><p>在 Python 中，装饰器 (Decorator) 是一种特殊的函数，它可以用来修改其他函数的行为，而无需直接修改被装饰的函数代码。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">decorator_function</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 在调用被装饰的函数之前执行的操作</span>
</span></span><span class="line"><span class="cl">        <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 在调用被装饰的函数之后执行的操作</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">result</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">wrapper</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@decorator_function</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">my_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 被装饰的函数</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>decorator_function</code>: 装饰器函数，它接收被装饰的函数作为参数，并返回一个包装函数。</li>
<li><code>wrapper</code>: 包装函数，它在调用被装饰的函数之前和之后执行一些操作。</li>
<li><code>@decorator_function</code>: 装饰器语法，将 <code>decorator_function</code> 应用到 <code>my_function</code> 上。</li>
</ul>
<p><strong>装饰器的工作原理:</strong></p>
<ol>
<li>当 Python 遇到 <code>@decorator_function</code> 语法时，它会将 <code>my_function</code> 作为参数传递给 <code>decorator_function</code>。</li>
<li><code>decorator_function</code> 执行，并返回一个包装函数 <code>wrapper</code>。</li>
<li><code>wrapper</code> 函数将替换 <code>my_function</code> 的原始定义。</li>
<li>当调用 <code>my_function</code> 时，实际上是在调用 <code>wrapper</code> 函数。</li>
</ol>
</div>
</details><br>
<strong>2. 计算函数定义:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">            <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">            <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>@T.prim_func</code>: 这是一个装饰器，用于将 <code>main</code> 函数定义为一个 TVM 的 <code>prim_func</code> 对象。<code>prim_func</code> 是 TVM 中用于表示底层计算函数的标准数据结构。</li>
<li><code>def main(...)</code>: 定义一个名为 <code>main</code> 的函数，该函数接受三个参数：
<ul>
<li><code>A</code>: 一个长度为 128 的 <code>float32</code> 类型 Buffer，表示第一个输入向量。</li>
<li><code>B</code>: 一个长度为 128 的 <code>float32</code> 类型 Buffer，表示第二个输入向量。</li>
<li><code>C</code>: 一个长度为 128 的 <code>float32</code> 类型 Buffer，用于存储计算结果。</li>
</ul>
</li>
</ul>
<p><strong>3. 函数属性:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>T.func_attr({&quot;global_symbol&quot;: &quot;main&quot;, &quot;tir.noalias&quot;: True})</code>： 设置函数的属性。
<ul>
<li><code>global_symbol</code>: 设置函数的全局符号名称为 <code>main</code>。</li>
<li><code>tir.noalias</code>: 设置函数的别名属性为 <code>True</code>，表示函数不会修改输入缓冲区。</li>
</ul>
</li>
</ul>
<p><strong>4. 计算循环:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="n">inrange</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>T.block</code> 将计算图分解成多个独立的计算块，每个块对应一个特定的计算任务，可以包含多个迭代器，这些迭代器共同定义了计算块的计算范围。</p>
<ul>
<li><code>for i in range(128)</code>: 定义一个循环，迭代 128 次，用于处理每个向量元素。</li>
<li><code>with T.block(&quot;C&quot;)</code>: 定义一个名为 <code>C</code> 的计算块，该块包含循环的计算逻辑。</li>
</ul>
<p><strong>5. 迭代器定义:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>vi = T.axis.spatial(128, i)</code>: 定义一个名为 <code>vi</code> 的空间迭代器，它遍历 128 个元素，每个元素的索引由 <code>i</code> 确定。</li>
</ul>
<p>一般来说，空间迭代器的访问顺序对最后结果不产生影响。</p>
<p><strong>6. 计算操作:</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><code>C[vi] = A[vi] + B[vi]</code>： 将 <code>A</code> 和 <code>B</code> 中对应元素相加，并将结果存储到 <code>C</code> 中。</li>
</ul>
<p>我们可以通过 <code>MyModule.show()</code> 来显示构建的IRModule.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># function attr dict</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># body</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">serial</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="build-and-run">Build and Run</h2>
<p>我们可以通过 <code>tvm.build</code>函数将一个IRModule转变成可以运行的函数，通过定义的函数名可以获取想要的函数。然后我们可以定义三个 <code>NDArray</code> 数组来调用函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">rt_mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">rt_mod</span><span class="p">[</span><span class="s2">&#34;main&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> 
</span></span><span class="line"><span class="cl"><span class="n">c</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="n">func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong><code>tvm.build</code> 函数的参数:</strong></p>
<ul>
<li><code>func</code>: 要编译的计算图，可以是 <code>tvm.script.ir_module</code> 对象、<code>tvm.relay.Function</code> 对象或其他支持的计算图类型。</li>
<li><code>target</code>: 目标平台，例如，<code>llvm -mcpu=core-avx2</code>、<code>cuda</code>、<code>opencl</code> 等。</li>
<li><code>name</code>: 编译后的模块名称。</li>
</ul>
<h2 id="transform-the-tensor-program">Transform the Tensor Program</h2>
<p>在 TVM 中，<code>tvm.tir.Schedule</code> 是一个用于对计算图进行手动优化的工具。它允许对计算图中的循环、块和操作进行重排序、融合、并行化等操作，以提高计算效率。</p>
<p>下面这段代码做了以下优化：</p>
<ul>
<li><strong>循环切分:</strong> 将循环 <code>i</code> 切分成三个循环，可以更好地利用内存局部性，例如，将 <code>i_1</code> 和 <code>i_2</code> 的大小设置为 4，可以将数据加载到缓存中，减少内存访问次数。</li>
<li><strong>循环重排序:</strong> 按照 <code>i_0</code>、<code>i_2</code> 和 <code>i_1</code> 这个顺序执行。</li>
<li><strong>并行化:</strong> 将 <code>i_0</code> 并行化，可以利用多核 CPU 或 GPU 的计算能力，提高计算速度</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">MyModule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Get block by its name</span>
</span></span><span class="line"><span class="cl"><span class="n">block_c</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Get loops surronding the block</span>
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">i</span><span class="p">,)</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Tile the loop nesting.</span>
</span></span><span class="line"><span class="cl"><span class="n">i_0</span><span class="p">,</span> <span class="n">i_1</span><span class="p">,</span> <span class="n">i_2</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Reorder the loop.</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">i_0</span><span class="p">,</span> <span class="n">i_2</span><span class="p">,</span> <span class="n">i_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">parallel</span><span class="p">(</span><span class="n">i_0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>优化后的计算图如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># function attr dict</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># body</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i_0</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">parallel</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i_2</span><span class="p">,</span> <span class="n">i_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="n">vi</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">i_0</span> <span class="o">*</span> <span class="mi">16</span> <span class="o">+</span> <span class="n">i_1</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="n">i_2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">vi</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="constructing-tensor-program-by-tensor-expression">Constructing Tensor Program by Tensor Expression</h1>
<p>Tensor Expression 指的是一种用于描述张量计算的数学表达式。</p>
<h2 id="construct-vector-add-by-te">Construct Vector-Add by TE</h2>
<p>我们可以通过以下方式来创建和 <a href="/blogs/courselearning/tvm/tvm-ch1/#Vector-Add-Example">上一节</a> 一样的IRModule.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># namespace for tensor expression utility</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># declare the computation using the expression API</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># create a function with the specified list of arguments. </span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># mark that the function name is main</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_mod_from_te</span> <span class="o">=</span> <span class="n">IRModule</span><span class="p">({</span><span class="s2">&#34;main&#34;</span><span class="p">:</span> <span class="n">func</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ir_mod_from_te</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol>
<li>
<p><strong>定义张量:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>这两行代码定义了两个名为 <code>A</code> 和 <code>B</code> 的张量，它们都是一维张量，大小为 128。<code>te.placeholder</code> 函数用于创建占位符张量，它代表输入数据。</p>
</li>
<li>
<p><strong>定义计算:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="mi">128</span><span class="p">,),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>这行代码定义了一个名为 <code>C</code> 的张量，它表示 <code>A</code> 和 <code>B</code> 的元素相加的结果。<code>te.compute</code> 函数用于定义张量计算，它接受两个参数：</p>
<ul>
<li>第一个参数 <code>shape</code>是张量的形状，这里为 <code>(128,)</code>。</li>
<li>第二个参 <code>fcompute</code>数是一个 lambda 函数，它定义了每个元素的计算方式，这里为 <code>A[i] + B[i]</code>，表示 <code>C</code> 的第 <code>i</code> 个元素等于 <code>A</code> 的第 <code>i</code> 个元素加上 <code>B</code> 的第 <code>i</code> 个元素。</li>
</ul>
</li>
<li>
<p><strong>创建 PrimFunc:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span>
</span></span></code></pre></div><p>这行代码使用 <code>te.create_prim_func</code> 函数创建了一个 PrimFunc 对象，它代表一个 TVM 的基本计算函数。<code>te.create_prim_func</code> 函数接受一个参数，即函数的输入参数列表，这里为 <code>[A, B, C]</code></p>
</li>
<li>
<p><strong>设置函数名称:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>这行代码将函数的名称设置为 <code>main</code>，<code>with_attr</code> 函数用于设置函数的属性。</p>
</li>
<li>
<p><strong>创建 IRModule:</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">ir_mod_from_te</span> <span class="o">=</span> <span class="n">IRModule</span><span class="p">({</span><span class="s2">&#34;main&#34;</span><span class="p">:</span> <span class="n">func</span><span class="p">})</span>
</span></span></code></pre></div><p>这行代码创建了一个 IRModule 对象，它包含了 <code>func</code> 函数，并将该函数存储在 IRModule 的 <code>main</code> 字段中。</p>
</li>
</ol>
<h2 id="transforming-a-matrix-multiplication-program">Transforming a matrix multiplication program</h2>
<p>下面代码展示了两个 $1024 \times 1024$ 矩阵相乘的IRModule创建流程。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">M</span> <span class="o">=</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl"><span class="n">K</span> <span class="o">=</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl"><span class="n">N</span> <span class="o">=</span> <span class="mi">1024</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># The default tensor type in tvm</span>
</span></span><span class="line"><span class="cl"><span class="n">dtype</span> <span class="o">=</span> <span class="s2">&#34;float32&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">target</span> <span class="o">=</span> <span class="s2">&#34;llvm&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Algorithm</span>
</span></span><span class="line"><span class="cl"><span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="s2">&#34;k&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;A&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">B</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">placeholder</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;B&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Default schedule</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">create_prim_func</span><span class="p">([</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">with_attr</span><span class="p">(</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">,</span> <span class="s2">&#34;main&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span> <span class="o">=</span> <span class="n">IRModule</span><span class="p">({</span><span class="s2">&#34;main&#34;</span><span class="p">:</span> <span class="n">func</span><span class="p">})</span>
</span></span><span class="line"><span class="cl"><span class="n">ir_module</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">ir_module</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm&#34;</span><span class="p">)</span>  <span class="c1"># The module for CPU backends.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">dev</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">dev</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">dev</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create evaluation function</span>
</span></span><span class="line"><span class="cl"><span class="n">evaluator</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">time_evaluator</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">entry_name</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Baseline: </span><span class="si">%f</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>time_evaluator</code> 是 IRModule 用于评估计算图执行时间的方法。它可以帮助测量不同硬件平台上不同计算图的性能，并进行优化。</p>
<details class="custom-details">
    <summary class="custom-summary">time evaluator</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">IRModule</span><span class="o">.</span><span class="n">time_evaluator</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_repeat_ms</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">f_type</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>参数解释:</strong></p>
<ul>
<li><code>func</code>: 要评估的计算图函数。</li>
<li><code>args</code>: 计算图函数的输入参数，可以是张量或其他数据结构。</li>
<li><code>number</code>: 每次运行计算图的次数，默认值为 1。</li>
<li><code>repeat</code>: 重复运行计算图的次数，默认值为 1。</li>
<li><code>min_repeat_ms</code>: 最小运行时间，单位为毫秒。如果计算图运行时间小于 <code>min_repeat_ms</code>，则会继续运行直到达到 <code>min_repeat_ms</code>。默认值为 0。</li>
<li><code>f_type</code>: 运行模式，可以是 0（默认值）、1 或 2。
<ul>
<li>0：正常运行模式。</li>
<li>1：仅执行编译，不运行计算图。</li>
<li>2：仅执行运行，不编译计算图。</li>
</ul>
</li>
</ul>
<p><strong><code>func.time_evaluator</code> 的返回值:</strong></p>
<p><code>func.time_evaluator</code> 返回一个函数，该函数可以用来执行评估并返回一个包含性能指标的字典。</p>
<p><strong>性能指标:</strong></p>
<ul>
<li><code>mean</code>: 平均运行时间，单位为毫秒。</li>
<li><code>median</code>: 中位数运行时间，单位为毫秒。</li>
<li><code>min</code>: 最小运行时间，单位为毫秒。</li>
<li><code>max</code>: 最大运行时间，单位为毫秒。</li>
<li><code>std</code>: 标准差，单位为毫秒。</li>
</ul>
</div>
</details><br>
<p>代码的大部分流程相同，我们来看计算部分。</p>
<ol>
<li>
<p><strong>定义约简轴 (Reduce axis):</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="s2">&#34;k&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>这行代码定义了一个名为 <code>k</code> 的约简轴，表示在矩阵乘法操作中进行求和的维度，范围为 <code>(0, K)</code></p>
</li>
<li>
<p><strong>定义输入矩阵 (Placeholders):</strong></p>
<pre tabindex="0"><code>A = te.placeholder((M, K), name=&#34;A&#34;)
B = te.placeholder((K, N), name=&#34;B&#34;)
</code></pre><p>这两行代码定义了两个名为 <code>A</code> 和 <code>B</code> 的输入矩阵，它们分别代表矩阵乘法的两个输入矩阵。<code>A</code> 的形状为 <code>(M, K)</code>，<code>B</code> 的形状为 <code>(K, N)</code></p>
</li>
<li>
<p><strong>定义输出矩阵 (Compute):</strong></p>
<pre tabindex="0"><code>C = te.compute((M, N), lambda m, n: te.sum(A[m, k] * B[k, n], axis=k), name=&#34;C&#34;)
</code></pre><p>这行代码定义了一个名为 <code>C</code> 的输出矩阵，它表示矩阵乘法的结果。<code>C</code> 的形状为 <code>(M, N)</code>，采用 <code>te.sum</code>计算结果。</p>
</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">te.sum</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">expr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>参数解释:</strong></p>
<ul>
<li><code>expr</code>: 要进行求和的表达式，可以是张量、标量或其他表达式。</li>
<li><code>axis</code>: 要进行求和的轴，可以是整数、元组或列表。如果 <code>axis</code> 为 <code>None</code>，则对所有轴进行求和。</li>
<li><code>keepdims</code>: 布尔值，表示是否保留求和后的维度。如果为 <code>True</code>，则保留求和后的维度，并将其大小设置为 1。如果为 <code>False</code>，则删除求和后的维度。</li>
<li><code>where</code>: 布尔值张量，表示要进行求和的元素。如果 <code>where</code> 为 <code>None</code>，则对所有元素进行求和。</li>
</ul></div>
</details><br>
<p>创建的IRModule如下所示。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># function attr dict</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># body</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">i2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们可以将循环拆分成外层循环和内层循环可以提高数据局部性。内层循环访问的数据更接近，可以有效利用缓存。下面代码的 <code>block_size</code> 参数控制了内层循环的大小，选择合适的块大小可以最大程度地利用缓存。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">sch</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">Schedule</span><span class="p">(</span><span class="n">ir_module</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_c</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Get loops surronding the block</span>
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">get_loops</span><span class="p">(</span><span class="n">block_c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">block_size</span> <span class="o">=</span> <span class="mi">32</span>
</span></span><span class="line"><span class="cl"><span class="n">yo</span><span class="p">,</span> <span class="n">yi</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">block_size</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">xo</span><span class="p">,</span> <span class="n">xi</span> <span class="o">=</span> <span class="n">sch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">block_size</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">reorder</span><span class="p">(</span><span class="n">yo</span><span class="p">,</span> <span class="n">xo</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">yi</span><span class="p">,</span> <span class="n">xi</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">func</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">sch</span><span class="o">.</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&#34;llvm&#34;</span><span class="p">)</span>  <span class="c1"># The module for CPU backends.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">c</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span> <span class="n">dev</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">func</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">evaluator</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">time_evaluator</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">entry_name</span><span class="p">,</span> <span class="n">dev</span><span class="p">,</span> <span class="n">number</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;after transformation: </span><span class="si">%f</span><span class="s2">&#34;</span> <span class="o">%</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>创建的IRModule如下所示。实际中我们会测试很多不同 <code>block_size</code>对应的执行时间来选择最合适的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@tvm.script.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">B</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">],</span> <span class="n">C</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="s2">&#34;float32&#34;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># function attr dict</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;global_symbol&#34;</span><span class="p">:</span> <span class="s2">&#34;main&#34;</span><span class="p">,</span> <span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># body</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0_0</span><span class="p">,</span> <span class="n">i1_0</span><span class="p">,</span> <span class="n">i2</span><span class="p">,</span> <span class="n">i0_1</span><span class="p">,</span> <span class="n">i1_1</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;C&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">m</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i0_0</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">+</span> <span class="n">i0_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">n</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">spatial</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i1_0</span> <span class="o">*</span> <span class="mi">32</span> <span class="o">+</span> <span class="n">i1_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">i2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>TVM Learning (11)-Add Model Architeture in MLC LLM</title>
      <link>http://localhost:57770/blogs/courselearning/tvm/tvm-ch9/</link>
      <pubDate>Thu, 08 Aug 2024 12:00:00 +0800</pubDate>
      <guid>http://localhost:57770/blogs/courselearning/tvm/tvm-ch9/</guid>
      <description>Add Model Architeture in MLC LLM</description>
      <content:encoded><![CDATA[<h1 id="irmodule-the-key-concept-in-tvm-unity">IRModule: The key concept in TVM Unity</h1>
<p>IRModule 是张量函数的集合，代表我们需要在模型中执行的计算子集。例如，在 MLC-LLM 中，它可以是一个 Transformer 模块。
机器学习编译框架中的 IRModule 就像深度学习框架中的张量，是一切的基础。在整个编译流程中，模型将以 IRModule 的形式导入，然后以 IRModule 到 IRModule 的方式进行转换和优化，然后我们就可以在任何支持的平台上将 IRModule 转化为可运行的模块。IRModule 可以用 python 方式访问，例如，我们可以用 python AST 的形式显示它，以便检查、调整和调试。unity 的主要设计目标之一是实现单一抽象，将所有主要元素封装在同一模块中。这样，我们就能在此基础上进行有机的增量转换。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3a739d336ccf45c1d34addfa952de165?method=download&amp;shareKey=1150d6da998887fe6b987c0e7bbbc777" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3a739d336ccf45c1d34addfa952de165?method=download&amp;shareKey=1150d6da998887fe6b987c0e7bbbc777" alt="TVM Unity.png">
    </a><figcaption>TVM Unity.png</figcaption></figure></p>
<p>TVMScript 是 IRModule 的 python AST 格式，用于在整套转换过程中检查 IRModules 并与之交互。与 IRModule 的交互都可以使用 TVMScript 在 python 中进行。用户将 TVMScript 解析为 IRModule 内部结构，使用 python API 操作 IRModule，并将 IRModule 打印为 TVMScript 格式。</p>
<h2 id="tvmscript-examples">TVMScript Examples</h2>
<p>用 Pytorch 框架实现矩阵乘法一般调用 <code>torch.matmul</code> 或者使用 <code>@</code> 算子。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> 
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">tensor([[ 2.5387,  2.2756, -2.2032,  2.5928, -3.6539],
</span></span></span><span class="line"><span class="cl"><span class="s1">        [ 2.0151,  0.0628, -0.8041, -1.6947,  0.2884],
</span></span></span><span class="line"><span class="cl"><span class="s1">        [-0.8118, -0.0453,  0.0742, -1.2028,  1.3722]])
</span></span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 Relax 中可以用 IRModule 实现相同的功能。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">ir</span> <span class="k">as</span> <span class="n">I</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">relax</span> <span class="k">as</span> <span class="n">R</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">B</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">lv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">lv</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>通过上述 TVMScript 创建的 IRModule 是一个完全图级别的抽象，只包含一个 R.function (Relax 函数： IRModule 中计算图的表示形式)
上述示例包含 Relax 函数中的两个重要概念：高级 Relax 算子和数据流块。</p>
<ul>
<li>Relax 函数包含高级 Relax 算子 <code>R.matmul</code>，它描述计算图中的节点，不包含其底层实现的信息。一个高级 Relax 算子可以映射到不同的底层实现，TVM Unity 的编译流程会生成性能良好的实现。</li>
<li><code>R.dataflow()</code> 是数据流块的一个重要作用域注解。具体来说，在数据流块内，所有操作都必须是 side-effect free. 而在数据流块之外，操作可能包含副作用。</li>
</ul>
<h2 id="a-more-complex-tvmscript-example-2-layer-mlp">A more complex TVMScript example: 2-layer MLP</h2>
<p>下面我们以一个更复杂的两层 MLP 为例，模型结构如下。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa9d8ef25e3caa5e822e9d8768efbafbe?method=download&amp;shareKey=1142a3440c0b476839088b07a5971539" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa9d8ef25e3caa5e822e9d8768efbafbe?method=download&amp;shareKey=1142a3440c0b476839088b07a5971539" alt="2-layer MLP">
    </a><figcaption>2-layer MLP</figcaption></figure></p>
<p>其对应的 Pytoch 实现如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对应的 IRModule 的 TVMScript 表示如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">inp_0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">weight1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">bias1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">weight2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">bias2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">weight1</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv_1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inp_0</span><span class="p">,</span> <span class="n">lv</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv_1</span><span class="p">,</span> <span class="n">bias1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">lv1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">weight2</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">lv2</span><span class="p">,</span> <span class="n">lv4</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4_1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">bias2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">lv4_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">lv4_1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上述 Relax 函数只包含高级 Relax 算子。在 pytorch 中，<code>torch.nn.Linear</code> 计算  $y = xW^T + b$ 在 relax 中，转置由 permute_dims 实现，其次是 矩阵乘法和加法分别由 <code>R.matmul</code> 和 <code>R.add</code> 实现。</p>
<h1 id="compilation-flow-in-tvm-unity">Compilation Flow in TVM Unity</h1>
<ol>
<li>将模型导入 IRModule. 对于静态模型，我们可以使用 pytorch dynamo 将 pytorch 程序跟踪为 fx 图，然后转换为 IRModule。然而，LLM 通常是动态的，因为序列长度和 kv cache 长度都是可变的。在这种情况下，我们需要直接在 IRModule 中建立模型。第一步可以抽象为 LLM -&gt; IRModule 转换。</li>
<li>优化模型。与传统编译器一样，我们可以在 IRModule 上应用 pass (IRModule 到 IRModule 的变换，改变计算但保留了原始 IRModule 的语义)。在这一步中，我们的目标是加速模型计算。在消费类设备上以适当速度运行 LLM 的大多数关键技术，如量化、算子融合和张量函数调度，都是在这一步实现的。</li>
<li>在设备上部署 IRModule。对于每个 IRM 模块，我们都能将其转化为可运行模块，并在 tvm 运行时支持的任何平台上运行。IRModule 上的每个函数都将成为环境中的本地可运行函数。</li>
</ol>
<p>以下是 2 层 MLP 模型的编译流程</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">relax</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tvm</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm.ir.module</span> <span class="kn">import</span> <span class="n">IRModule</span>
</span></span><span class="line"><span class="cl"><span class="n">mod</span> <span class="o">=</span> <span class="n">MLPModule</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">optimize_and_deploy</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">IRModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># step 2. Optimization</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Use default graph optimization pipeline</span>
</span></span><span class="line"><span class="cl">    <span class="n">mod</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">pipeline</span><span class="o">.</span><span class="n">get_pipeline</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Use default tensor function scheduling</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">Target</span><span class="p">(</span><span class="s2">&#34;cuda&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">mod</span>  <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">tir</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">DefaultGPUSchedule</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="c1"># Step 3. Deploy to GPU</span>
</span></span><span class="line"><span class="cl">    <span class="n">ex</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s2">&#34;cuda&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">vm</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span><span class="p">(</span><span class="n">ex</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># test correctness</span>
</span></span><span class="line"><span class="cl">    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">    <span class="n">input_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">weight1_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">bias1_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">weight2_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">bias2_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">tvm_nd_arrays</span> <span class="o">=</span> <span class="p">[</span><span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np_array</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">tvm</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span> <span class="k">for</span> <span class="n">np_array</span> <span class="ow">in</span> <span class="p">[</span><span class="n">input_np</span><span class="p">,</span> <span class="n">weight1_np</span><span class="p">,</span> <span class="n">bias1_np</span><span class="p">,</span> <span class="n">weight2_np</span><span class="p">,</span> <span class="n">bias2_np</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># call into the runnable function converted from IRModule</span>
</span></span><span class="line"><span class="cl">    <span class="n">nd_res</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&#34;main&#34;</span><span class="p">](</span><span class="o">*</span><span class="n">tvm_nd_arrays</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">numpy_res</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_np</span> <span class="o">@</span> <span class="n">weight1_np</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">bias1_np</span><span class="p">)</span> <span class="o">@</span> <span class="n">weight2_np</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">bias2_np</span>
</span></span><span class="line"><span class="cl">    <span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">numpy_res</span><span class="p">,</span> <span class="n">nd_res</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">optimize_and_deploy</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>  
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="build-irmodule-in-pytorch-style">Build IRModule in Pytorch Style</h1>
<p>构建 IRModule 最直接的方法是手动编写 TVMScript。这种方法适用于小型模型，但 LLM 的 IRModule 非常庞大和复杂，手工编写并不现实。TVM Unity 提供了另一个类 nn.Module，可以像 pytorch 模块一样轻松构建 IRModule.
用 Pytorch 手动编写的一个 Linear 层如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TorchLinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 Relax 中的实现如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm.relax.testing</span> <span class="kn">import</span> <span class="n">nn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RelaxLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">RelaxLinear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">((</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;linear_weight&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;linear_bias&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">relax</span><span class="o">.</span><span class="n">Expr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>与 Pytorch 的结构非常相似，只是前向函数实际上并不执行计算。它使用作为输入传递的占位符跟踪算子的计算图。
<code>nn.emit(relax.op.linear(input, self.weight, self.bias))</code> 表示在构建的 IRModule 中添加高级 linear 算子。
通过堆叠 1 个线性层、1 个 relu 层和 1 个线性层，就可以构建例子中的 MLP.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RelaxMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">RelaxMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">RelaxLinear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">lienar2</span> <span class="o">=</span> <span class="n">RelaxLinear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">relax</span><span class="o">.</span><span class="n">Expr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lienar2</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>直接调用 nn.Module 的前向函数就可以代替原先在 <code>with bb.dataflow():</code> 下的操作，将 <code>nn.Module</code> 构建成 IRModule 的步骤如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">build_relax</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>   
</span></span><span class="line"><span class="cl">    <span class="c1"># relax.BlockBuilder can construct end-to-end models step by step in an IRModule that starts empty</span>
</span></span><span class="line"><span class="cl">    <span class="n">bb</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">BlockBuilder</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># relax nn.Module</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">mod</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># create a function called &#34;main&#34; in the IRModule</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="s2">&#34;main&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># define input placeholder to the relax nn.Module</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Placeholder</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;input&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># build dataflow block</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">bb</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># call forward function </span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># The params of the constructed IRModule</span>
</span></span><span class="line"><span class="cl">            <span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="p">]</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># return value of the dataflow block</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span> <span class="o">=</span> <span class="n">bb</span><span class="o">.</span><span class="n">emit_output</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># return value and params of the Relax function</span>
</span></span><span class="line"><span class="cl">        <span class="n">bb</span><span class="o">.</span><span class="n">emit_func_output</span><span class="p">(</span><span class="n">gv</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">bb</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">build_relax</span><span class="p">(</span><span class="n">RelaxMLP</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_weight</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_bias</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_weight_1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_bias_1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">linear_weight</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lv</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">linear_bias</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">lv2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">linear_weight_1</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv5</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">lv4</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;void&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv6</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv5</span><span class="p">,</span> <span class="n">linear_bias_1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">lv6</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="custom-operator-support">Custom Operator Support</h1>
<p>在某些情况下，我们要表示的模型包含一些自定义运算符，而这些运算符没有被提供的 Relax 运算符覆盖（如 LLaMA 中的 Rotary Embedding），或者我们要进行底层优化以加速单个内核。下面介绍如何在 IRModule 中编写自定义算子。</p>
<h2 id="tensorir-low-level-tensor-function">TensorIR: Low-level tensor function</h2>
<p>TVM Unity 在 IRModule TensorIR 中提供了底层张量函数的表示方法，用户可以在其中定义自定义操作符并执行细粒度调度。
下面对比了一个矩阵乘法生成的 TVMScript TensorIR 代码和 low-level Pytorch 代码。<code>@T.prim_func</code>装饰器表示下面的函数是一个原始的张量函数，包含运算符实现的底层细节。</p>
<details class="custom-details">
    <summary class="custom-summary">Destination Passing</summary>
    <div><code>T.prim_func</code> 采用 destination-passing 约定，即在函数外部明确分配输入和输出空间，并将其作为参数传入。destination-passing 约定可以对内存分配进行精细调度，例如合并两个实时间隔不相交的变量的内存分配，这是在内存有限的设备上运行大型模型的关键。</div>
</details><br>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">tir</span> <span class="k">as</span> <span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">rxplaceholder</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">rxplaceholder_1</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">matmul</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">    <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">rxplaceholder</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">],</span> <span class="n">rxplaceholder_1</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">matmul</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="n">matmul</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">matmul</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">rxplaceholder</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span> <span class="o">*</span> <span class="n">rxplaceholder_1</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">torch_matmul</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">W</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">784</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">Y</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="interaction-between-relax-function-and-tensorir">Interaction between Relax function and TensorIR</h2>
<p>为了支持 <code>T.prim_func</code>（底层部分）和 <code>R.function</code>（高层部分）之间的交互，TVM 引入了 <code>call_tir</code>, Relax 中的一个特殊运算符，用于描述计算图中的节点及其张量函数的实现。
<code>torch_call_tir</code> 是一个参考实现，用来说明 call_tir 的含义。实际上，可以有不同的底层方法来优化执行。例如，我们可能会选择提前分配所有输出内存，然后再运行执行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">torch_call_tir</span><span class="p">(</span><span class="n">prim_func</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">out_sinfo</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">out_sinfo</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">out_sinfo</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prim_func</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">res</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下面是 2 层 MLP 的 IRModule，我们使用 <code>call_tir</code> 和张量原语函数 <code>matmul</code> 来替换 Relax 运算符 <code>R.matmul</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">tir_matmul</span><span class="p">(</span><span class="n">rxplaceholder</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">rxplaceholder_1</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">matmul</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">,</span> <span class="n">v_k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i0</span><span class="p">,</span> <span class="n">i1</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">rxplaceholder</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">],</span> <span class="n">rxplaceholder_1</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">matmul</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">matmul</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">matmul</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span> <span class="o">+</span> <span class="n">rxplaceholder</span><span class="p">[</span><span class="n">v_i0</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span> <span class="o">*</span> <span class="n">rxplaceholder_1</span><span class="p">[</span><span class="n">v_k</span><span class="p">,</span> <span class="n">v_i1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">inp_0</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">weight1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">bias1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">             <span class="n">weight2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">bias2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">weight1</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">tir_matmul</span><span class="p">,</span> <span class="p">[</span><span class="n">inp_0</span><span class="p">,</span> <span class="n">lv</span><span class="p">],</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">bias1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv3</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">lv2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv4</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">weight2</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv5</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">lv3</span><span class="p">,</span> <span class="n">lv4</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv6</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lv5</span><span class="p">,</span> <span class="n">bias2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">lv6</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="implement-custom-tensorir-function">Implement Custom TensorIR Function</h2>
<p><code>nn.Module</code> 不仅支持高级 Relax 运算符，还支持自定义 TensorIR 函数。
要构建 TensorIR 函数并在 Relax 图中调用它，我们需要使用 <code>nn.emit_te(f_te_expr,*args)</code>。</p>
<ul>
<li><code>f_te_expr</code> 是一个返回张量表达式（Tensor Expression，TE）的函数，是描述张量计算的 DSL.</li>
<li><code>args</code> 是 <code>f_te_expr</code> 的参数。</li>
</ul>
<p>创建 TE 表达式的方法如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">out_shape</span><span class="p">,</span> <span class="n">f_compute</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>它描述如下的计算模式
<details class="custom-details">
    <summary class="custom-summary">itertools.product</summary>
    <div><p>在 Python 的 itertools 模块中，<code>product</code> 函数用于生成可迭代对象的笛卡尔积。</p>
<p><code>product</code> 函数接受一个或多个可迭代对象作为参数，并返回一个迭代器，该迭代器生成所有可能的组合，其中每个组合包含来自每个输入可迭代对象的单个元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">itertools</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">letters</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">numbers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">letters</span><span class="p">,</span> <span class="n">numbers</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># output：</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;a&#39;, 1)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;a&#39;, 2)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;a&#39;, 3)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;b&#39;, 1)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;b&#39;, 2)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;b&#39;, 3)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>product</code> 函数还支持重复元素，可以使用 repeat 参数指定每个可迭代对象需要重复的次数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">letters</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">letters</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># output：</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;a&#39;, &#39;a&#39;, &#39;a&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;a&#39;, &#39;a&#39;, &#39;b&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;a&#39;, &#39;b&#39;, &#39;a&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;a&#39;, &#39;b&#39;, &#39;b&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;b&#39;, &#39;a&#39;, &#39;a&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;b&#39;, &#39;a&#39;, &#39;b&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;b&#39;, &#39;b&#39;, &#39;a&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&#39;b&#39;, &#39;b&#39;, &#39;b&#39;)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>product</code> 应用场景</p>
<ul>
<li>组合生成: 生成所有可能的组合，例如密码生成、彩票号码生成等。</li>
<li>多维数组遍历: 遍历多维数组的所有元素。</li>
<li>测试用例生成: 生成测试用例，覆盖所有可能的输入组合。</li>
</ul>
</div>
</details><br></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">indices</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">out_shape</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">out_tensor</span><span class="p">[</span><span class="o">*</span><span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">f_compute</span><span class="p">(</span><span class="o">*</span><span class="n">indices</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>用 <code>emit_te</code> 实现 Linear 层来构建 IRModule 的代码如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">te</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RelaxLinearWithEmitTE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">RelaxLinearWithEmitTE</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">((</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;linear_weight&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_features</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;linear_bias&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">relax</span><span class="o">.</span><span class="n">Expr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">my_linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">out_sinfo</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,]</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;k&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">out_sinfo</span><span class="p">,</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">te</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;matmul&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">te</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">out_sinfo</span><span class="p">,</span> <span class="n">fcompute</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;add_bias&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">emit_te</span><span class="p">(</span><span class="n">my_linear</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">RelaxMLPwithEmitTE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_num</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">       <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">RelaxLinearWithEmitTE</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">       <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">RelaxLinearWithEmitTE</span><span class="p">(</span><span class="n">hidden_num</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">relax</span><span class="o">.</span><span class="n">Expr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">relax</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hidden</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">build_relax</span><span class="p">(</span><span class="n">RelaxMLPwithEmitTE</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#----------------------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="nd">@I.ir_module</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_linear</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_weight</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">784</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_bias</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">matmul</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">v_k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_k</span><span class="p">],</span> <span class="n">linear_weight</span><span class="p">[</span><span class="n">v_j</span><span class="p">,</span> <span class="n">v_k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">matmul</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">matmul</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">matmul</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span> <span class="o">*</span> <span class="n">linear_weight</span><span class="p">[</span><span class="n">v_j</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@T.prim_func</span><span class="p">(</span><span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">my_linear1</span><span class="p">(</span><span class="n">lv1</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_weight</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">128</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_bias</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">matmul</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">Buffer</span><span class="p">((</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)),</span> <span class="s2">&#34;float32&#34;</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">T</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&#34;tir.noalias&#34;</span><span class="p">:</span> <span class="n">T</span><span class="o">.</span><span class="n">bool</span><span class="p">(</span><span class="kc">True</span><span class="p">)})</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># with T.block(&#34;root&#34;):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">T</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">            <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="s2">&#34;matmul&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">,</span> <span class="n">v_k</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">axis</span><span class="o">.</span><span class="n">remap</span><span class="p">(</span><span class="s2">&#34;SSR&#34;</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">reads</span><span class="p">(</span><span class="n">lv1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_k</span><span class="p">],</span> <span class="n">linear_weight</span><span class="p">[</span><span class="n">v_j</span><span class="p">,</span> <span class="n">v_k</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="n">T</span><span class="o">.</span><span class="n">writes</span><span class="p">(</span><span class="n">matmul</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="k">with</span> <span class="n">T</span><span class="o">.</span><span class="n">init</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                    <span class="n">matmul</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">matmul</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_j</span><span class="p">]</span> <span class="o">+</span> <span class="n">lv1</span><span class="p">[</span><span class="n">v_i</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span> <span class="o">*</span> <span class="n">linear_weight</span><span class="p">[</span><span class="n">v_j</span><span class="p">,</span> <span class="n">v_k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@R.function</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_weight</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_bias</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_weight_1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">),</span> <span class="n">linear_bias_1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">cls</span> <span class="o">=</span> <span class="n">Module</span>
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_linear</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">linear_weight</span><span class="p">,</span> <span class="n">linear_bias</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv1</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">lv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">lv2</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">call_tir</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">my_linear1</span><span class="p">,</span> <span class="p">(</span><span class="n">lv1</span><span class="p">,</span> <span class="n">linear_weight_1</span><span class="p">,</span> <span class="n">linear_bias_1</span><span class="p">),</span> <span class="n">out_sinfo</span><span class="o">=</span><span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">gv</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&#34;float32&#34;</span><span class="p">)</span> <span class="o">=</span> <span class="n">lv2</span>
</span></span><span class="line"><span class="cl">            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">gv</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>About Me</title>
      <link>http://localhost:57770/about_me/</link>
      <pubDate>Sat, 29 Jun 2024 00:00:00 +0800</pubDate>
      <guid>http://localhost:57770/about_me/</guid>
      <description>My research interests, education, work experience, achievements and publications.</description>
      <content:encoded><![CDATA[<p>Hello, I am WITHER.</p>
<h2 id="-research-interests">🔬 Research Interests</h2>
<ul>
<li>Training and Inference Acceleration</li>
<li>LLM Reasoning</li>
<li>High Performance Computing</li>
</ul>
<h2 id="-education">🧑‍🎓 Education</h2>
<ul>
<li><strong>2019.09 - 2023.06</strong>: Bachelor of Communication Engineering, China University of Geoscience, Wuhan, China.</li>
<li><strong>2023.09 - Now</strong>: Shanghai Jiao Tong University, Shanghai, China.</li>
</ul>
<h2 id="-work-experience">💻 Work Experience</h2>
<ul>
<li><strong>2024.06 - Present</strong>: Full-time Intern, Shanghai AI Laboratory, Shanghai, China.
<ul>
<li>Research on inference acceleration and graph optimization of Large Language Models.</li>
<li>Research on knowlege injection and reasoning of Large Language Models.</li>
</ul>
</li>
</ul>
<h2 id="-achievements">🎉 Achievements</h2>
<h2 id="-publications">📰 Publications</h2>
<h2 id="-hobbies">🤪 Hobbies</h2>
<ul>
<li>🧙 Animations, Comics and Games</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Comparsion of Parallelsim Metods in ViT</title>
      <link>http://localhost:57770/blogs/comparsion-of-parallelsim-metods-in-vit/</link>
      <pubDate>Mon, 13 Nov 2023 16:05:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/comparsion-of-parallelsim-metods-in-vit/</guid>
      <description>Paper reading of .</description>
      <content:encoded><![CDATA[<h1 id="basic-transformer-block">Basic Transformer Block</h1>
<p>符号含义表示如下</p>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Description</th>
          <th>Symbol</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>a</td>
          <td>注意力头数</td>
          <td>n</td>
          <td>并行度大小</td>
      </tr>
      <tr>
          <td>b</td>
          <td>batchsize</td>
          <td>s</td>
          <td>序列长度</td>
      </tr>
      <tr>
          <td>h</td>
          <td>隐藏层维度</td>
          <td>v</td>
          <td>词汇表大小</td>
      </tr>
      <tr>
          <td>L</td>
          <td>tranformer layer 层数数</td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>基本 transformer block 结构如下，输入是形状为 (b, s, h) 的三维张量，其中 b 为 batchsize. 每个变压器层由一个具有注意头的自注意块组成，随后是一个具有两层的 MLP，第一层将隐藏维度增加到 4h，然第二层将其减少到 h. 每个变压器层的输入和输出具有相同的形状.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6446c9e0a905932db1f9e39fa91c01ba?method=download&amp;shareKey=f26e075bcfc51b8c093388f69d39b40d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6446c9e0a905932db1f9e39fa91c01ba?method=download&amp;shareKey=f26e075bcfc51b8c093388f69d39b40d" alt="Basic Transformer Architecture">
    </a><figcaption>Basic Transformer Architecture</figcaption></figure>

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfbdc229aca70349939d6e3306e78c434?method=download&amp;shareKey=cff3f2903a8e16c5c46d607749a4b3c1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfbdc229aca70349939d6e3306e78c434?method=download&amp;shareKey=cff3f2903a8e16c5c46d607749a4b3c1" alt="Self-attention Block">
    </a><figcaption>Self-attention Block</figcaption></figure></p>
<h2 id="model-parameters">Model Parameters</h2>
<p>QKVO Linear 的权重形状均为 <code>h*h</code>, 偏置形状均为 <code>h*1</code>；MLP 两个 Linear 的权重形分别为 <code>h*4h</code> 和 <code>4h*h</code>，偏置形状分别为 <code>4h*1</code> 和 <code>h*1</code>. 因此每个模型的参数量为 <code>(12hh+13h)L</code>，占用大小还要 <code>x2</code>.</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在传统的 LLM 中最后还需要经过 logits layer，将隐藏层维度 <code>h</code> 转换成词汇表大小 <code>v</code>，参数量还要加上 <code>hv</code>.</p></div>

<h2 id="flops-calculation">FLOPs Calculation</h2>
<p>对于浮点数计算量 (FLOPs)，只考虑占主要部分的通用矩阵乘法 (GEMMs). 对于 Attention 部分，QKV Linear 的计算量为 <code>6bshh</code>，attention matrix (<a href="mailto:Q@K.T">Q@K.T</a>) 的计算量为 <code>2bssh</code>, attention@V 的计算量为 <code>2bssh</code>, O Linear 的计算量为 <code>2bshh</code>. MLP 的两个线性层的每一个计算量都为 <code>8shh</code>. 相加后得到正向传播中总计算量为 <code>(24bshh + 4bssh)L</code> bytes.</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在传统的 LLM 中最后还需要经过 logits layer，将隐藏层维度 <code>h</code> 转换成词汇表大小 <code>v</code>，其计算量为 <code>2bshv</code>.</p></div>

<p>反向传播因为要计算输入和权重的梯度，其计算量为正向传播的两倍，因此整个模型的计算量为 <code>72BLshh(1+s/(6h))</code>.</p>
<h1 id="activation-memory">Activation Memory</h1>
<p>激活的定义为在前向传播中产生并且需要在反向传播中进行梯度计算的张量，即不包括模型参数和优化器状态。并且不考虑相对非常小的激活。例如 LayerNorm 层的输入还需要张量每个通道的均值和方差 (大小均为 bs)，由于 h 大小通常超过 1k，因此只考虑输入张量所占激活的大小 bsh，忽略掉 2bs. 假设数据格式为 fp16/bf16，即每个数据占用 2 bytes 的存储空间，需要特殊处理的是 dropout 层的 mak，每个元素均为 unsigned int，只占用 1 byte.</p>
<p>Attention 部分激活占用如下 (共计 11bsh + 5bssa)</p>
<ul>
<li>QKV Linear: 三个线性层需要的输入相同，占用 2bsh bytes.</li>
<li><a href="mailto:Q@K.T">Q@K.T</a>: 需要存储 Q 和 K，占用 4bsh bytes.</li>
<li>Softmax: 需要存储大小为 2bssa bytes 的输入</li>
<li>Softmax droppot: 需要存储一个大小为 bssa bytes 的 mask.</li>
<li>attention@V: 需要存储 dropout 的输出和 V，分别占用 2bssa 和 2bsh bytes.</li>
<li>O Linear: 需要存储注意力的输出，占用 2bsh bytes.</li>
<li>O dropout 需要存储一个大小为 bsh bytes 的 mask;</li>
</ul>
<p>MLP (共计 18bsh): 第一层和第二层的输入分别占用 2bsh 和 8bsh bytes. GeLU 层需要第二层的输入用于反向传播，占用大小为 8bsh bytes. dropout 需要一个大小为 bsh bytes 的 mask.</p>
<p>LayerNorm (共计 4bsh): 需要存储该层的输入，占用 2bsh bytes. 一共有两个 LayerNorm.</p>
<p>加起来就可以得到每个 transformer block 需要激活大小为 bsh(34+5sa/h) bytes.</p>
<h1 id="tensor-parallelsim">Tensor Parallelsim</h1>
<p><a href="https://darkenstar.github.io/2024/10/02/MegatronLM/#Model-Parallel-Transformers">Megatron 张量并行</a> 的思想是将输入进行连续的两个矩阵乘法的第一个按列切分成 t 份，第二个按行切分成 t 份. 在 Transformer block 中体现为利用多头注意力本身的并行性将 Attention 计算中的 QKV 按列进行切分，O Linear 的权重按行进行切分；MLP 中第一个线性层的权重按列进行切分，第二个权重按行进行切分。</p>
<p>在这种并行方式下，前向传播和反向传播均需要进行 2 次 All-Reduce 通信，由于每次 All-Reduce 通信可以看作 Reduce-Scatter + All-Gather, 因此每次每个设备的通信量为 8αbsh bytes，其中 α=(n-1)/n.</p>
<p>对于激活，2*LayerNorm, QKV Linear 的输入, O dropout mask，MLP 第一层的输入和 MLP dropout 不会被切分，因此每个设备每个 block 要占用的激活为 bsh(10+24/n+5as/(hn))</p>
<p>2D Tensor Parallelsim</p>
<p>2D张量并行将激活第一个矩阵的列切分成 m*n 份，第二个权重 (权重形状为 he) 的行被切分成 m 份，列被切分成 n 份。以下图为例，Rank0-Rank2为通信组 x，Rank0-Rank1为 通信组 y. 第一个矩阵经过一次通信组 y 的 AllGather 后与本设备第二个矩阵进行矩阵乘积，得到的部分和经过一次通信组 x 间的ReduceScatter，计算出正确结果。第一次 AllGather 通信每个设备通信的大小为 bsh(n-1)/(mn). 第二次 ReduceScatter 通信每个设备通信的大小为 bse(m-1)/n.</p>
<h1 id="megatron-sequence-parallelsim">Megatron Sequence Parallelsim</h1>
<p>Megatron 张量并行中 LayerNorm 以及 O Linear 和 MLP 之后的 dropouts 在每个设备中都有一个副本。这些模块不需要大量的计算，但需要占用 10bsh bytes 大小的激活内存。<a href="">Megatron-SP</a> 沿着序列维度划分这些模块来减少激活内存，但需要配合 TP 一起使用，本质上是将 TP 中的 All-Reduce 拆成了在 TP 前进行 All-Gather 和在 TP 后进行 Reduce-Scatter. 但除去第一个 LayerNorm 外的每一个模块的激活都得到了切分。Megatron-SP 这里选择每个设备存储自己的部分并在反向传播中插入一次额外的 All-Gather 通信。因此通信量为 10bsh, 每个设备每个 block 需要占用的激活为 bsh/n*(34+5as/h)</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6800d68e35ee4215289de6aa75f01884?method=download&amp;shareKey=e67ffd54e4d1fe7cf3a10e81108af366" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6800d68e35ee4215289de6aa75f01884?method=download&amp;shareKey=e67ffd54e4d1fe7cf3a10e81108af366" alt="Transformer layer with Megatron-SP">
    </a><figcaption>Transformer layer with Megatron-SP</figcaption></figure></p>
<h1 id="pipeline-parallelsim">Pipeline Parallelsim</h1>
<p>流水线张量并行仅仅将 L 个 Transformer block 平均分到 p 个设备上，并没有划分激活所要占用的内存。在考虑 1F1B 策略下 batchsize 进一步被划分成 p 个 micro batch. 第一个 stage 必须存储 p 个 micro batch 的激活。每个 stage 包含 L/p 层，所以无论流水线并行大小 p 如何，第一个 stage 必须存储 p × L/p = L 层的激活值。在 Megatron-LM 中的 interleaving schedule 需要存储 L(1 + (p−1)/(pm)) 层的激活，其中 m 是 interleaving 的数量。</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在使用 output-tensor-deallocation 优化 (输出传到下一个 stage 后就释放) 的情况下，可以为为每个设备节省 bshr 内存，其中 r 是每个设备正在运行的 micro batch 的数量，在第一个 stage r=p 时达到峰值。</p></div>

<h1 id="deepseed-ulysses-sequence-parallel">Deepseed-Ulysses Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/10/21/Deepseed%20Ulysses/">DS-SP</a> 也是利用多头注意力的并行性，首先将输入按序列维度切分到每个设备上，每个设备占有的输入形状为 b*(s/n)*h. 在计算 Attention 之前对 QKV 进行 All-to-All 通信变成按隐藏层维度切分 ((a 要能整除 n))，通信量为 6αbsh/n bytes. 计算完 score@v 之后再进行一次 All-to-All 通信，通信量为 2αbsh/n bytes，总计通信量为 8αbsh/n bytes. 激活占用上 Attention 中 Softmax 及其 dropout mask 和 attention 没有被切分，激活占用量为 bsh(34/n+5sa/h). 因此，它不适合 GQA 和 MQA 情况, GQA 的并行度被限制在了组数，MQA 则完全没法使用。而且由于张量并行也需要在 a 维度上进行划分，SP-Ulysses 和 TP 是冲突的。</p>
<h1 id="ring-attention-sequence-parallel">Ring-Attention Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/09/26/Ring_Attention/#Putting-it-Together">Ring-SP</a> 实际上为环状的 FlashAttention，将输入沿着序列维度切分到每个设备上，在 Attention 计算过程中每个设备向相邻设备通信 KV 并更新自己的 Softmax 矩阵，通信量为 4bsh bytes. 激活占用和 DS-SP 一样为 bsh(34/n+5sa/h).</p>
<h1 id="unified-sequence-parallel">Unified Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/11/14/USP-A%20Unified%20Sequence%20Parallelism%20Approach%20for%20Long%20Context%20Generative%20AI/#Unified-Ulysses-Ring-Sequence-Parallelism">USP</a> 将 SP 进程组分割成两个正交的进程组：SP-Ring 进程组和 SP-Ulysses 进程组。可以将其视为一个 2D mesh ，每一列上运行 SP-Ring，每一行上运行 SP-Ulysses. 具体方法为 QKV 的切分 和 All-to-All 和 DS-Ulysses 相同，然后采用 Ring-Attention 的方式进行计算。如果遇到使用 casual mask 的情况需要加上 balance load 策略，把序列长度分为 2*(ring_degree) 大小，按照 0-&gt;1-&gt;&hellip;-&gt;(ring_degree-1)-&gt;(ring_degree-1)-&gt;&hellip;-&gt;0 的顺序进行分配。USP 消除了 SP-ulysses的头数限制。并且 USP可以通过调整 SP-Ulysses 进程组数目来更好的适应不同带宽的网络结构，可以让 All-to-All 操作在高带宽中运行，而异步 P2P 通信在低带宽部分运行。</p>
<h1 id="comparsion-of-different-parallelsim-in-training">Comparsion of Different Parallelsim in Training</h1>
<table border="1">
  <tr>
    <th rowspan="2"></th>
    <th colspan="4" style="text-align: center;">Communication (FWD+BWD)</th>
    <th rowspan="2">Split Dim</th>
    <th colspan="3" style="text-align: center;">Memory</th>
  </tr>
  <tr>
    <th>Param</th>
    <th>Cost</th>
    <th>Act</th>
    <th>Cost</th>
    <th>P/G</th>
    <th>OS</th>
    <th>Act</th>
  </tr>
  <tr>
    <td>DS-SP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>8*All2All</td>
    <td>(8/N)O(bsh)</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>Ring-SP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>P2P</td>
    <td>4O(bsh)</td>
    <td>L/L</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>DP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>0</td>
    <td>0</td>
    <td>b/b</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>ZeRO1</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>0</td>
    <td>0</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P/N </td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO1</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO2</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>P+(G/N)</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO3</td>
    <td>2*AllGather + ReduceScatter</td>
    <td>18O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>TP</td>
    <td>0</td>
    <td>0</td>
    <td>4*AllReduce</td>
    <td>8O(bsh)</td>
    <td>a/h</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>αA</td>
  </tr>
  <tr>
    <td>Megatron-SP</td>
    <td>0</td>
    <td>0</td>
    <td>6*AllGather + 4*ReduceScatter</td>
    <td>10O(bsh)</td>
    <td>a/h</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
</table>
<h1 id="analysis">Analysis</h1>
<ol>
<li>All2All 通信使得 DS-SP 的通信开销大于 DP. 使用 Ring-SP 时，尽管异步的 P2P 通信是可以重叠的，理想的性能也是只与 DP 相同。因此只有当批 batchsize 不足以进行切分时才考虑使用 SP.</li>
<li>Megatron-SP 通信量高于 DS-SP 和 Ring-SP. SP-Ring 对于 KV 的通信可以与计算重叠。Megatron-SP 的通信量不会随着并行度的增加而减少，而 DS-SP 可以做到。 DS-SP 和 Ring-SP 具有较低的激活通信成本，但需要同步梯度和参数。不过参数通信量相对于激活通信量较小，可以通过计算进行重叠。GQA/MQA 也可以降低它俩的通信成本，而 Megatron-SP 不受影响。</li>
<li>相同配置下使用 USP+Zero3 来代替 Megatron-SP 并不会增加可训练序列的长度。但与 Megatron-SP 相比，USP 能在通过提高并行度来增加可以训练的序列长度。</li>
<li>Megatron-SP 并行维度受限于注意力头数目。USP 可以通过提高 Ring-SP 的并行度来扩展，以在大规模配置下训练更大模型。</li>
</ol>
<h1 id="sora-inference-modeling-analysis-process">Sora Inference Modeling Analysis Process</h1>
<p>我们需要准备模型的输入：</p>
<ol>
<li>隐空间采样的噪声 z，形状与想生成的视频时常和分辨率相关。生成 1s 的视频为 25.5 frames，经过 VAE Encoder 后输出的通道数为 4，帧数会被压缩到 <code>num_frame*5//17</code>，分辨率的长宽分别被压缩到原来的 1/8. 因此 z 的形状应该为 <code>(B, 4, num_frame*5//17, img_size[0]//8, img_size[1]//8)</code>.</li>
<li>输入的 prompt 会经过 DeepFloyd/t5-v1_1-xxl 编码，该编码器最大的 token 数为 300，编码维度为 4096，文本长度不足时会填充到 300. 因此编码后的 prompt 的形状为 <code>(B, 1, 300, 4096)</code>.</li>
<li>当前去噪的时间步 t，形状为 <code>(B, )</code></li>
<li>生成视频的 fps，形状为 <code>(1, )</code></li>
</ol>
<p>还需要准备相关的模型配置，包括 mesh 形状，sub_mesh 的形状，并行策略以及 stage_ids. 如果需要将模型的 transformer block 切分成多段，则需要配置 sub_mesh 和 stage_ids.</p>
<ul>
<li>mesh_shape: (num_x, num_y)</li>
<li>submesh_shape: <code>[(num_x, num_y, loc_x, loc_y), ]</code></li>
<li>stage_ids: <code>[(submesh0_start, submesh0_end), ]</code></li>
<li>strategy: 并行策略</li>
</ul>
<p>然后初始化模型，Sora 的整体结构如下 我们初始化一个 Pipeline(包含整个流程的函数)，它会有一个或多个 Stage 用于保存模型的不同层，与 stage_ids 中对应。我们将模型分解成 Embedding_blocks(PatchEmbed3D, TimestepEmbedder, SizeEmbedder, Captionembedder, t_block), STDiT3_blocks 和 T2IFinalLayer. 将这个分解函数作为 Pipeline 的 sharding_func.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB9688e46ada2523df1ec522a7649be19a?method=download&amp;shareKey=991eba9aad6eca9f41599d2ad4f75c34" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB9688e46ada2523df1ec522a7649be19a?method=download&amp;shareKey=991eba9aad6eca9f41599d2ad4f75c34" alt="Open-Sora">
    </a><figcaption>Open-Sora</figcaption></figure></p>
<h2 id="init-pipeline">Init Pipeline</h2>
<p>我们需要根据配置以及 PipePatch 并行度和 SP 并行度初始化 Pipeline. 这其中会根据 stage_ids 分配每个 Stage 保存模型的哪些层以及对应的 submesh 大小。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">construct_stages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">submeshes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">],</span> <span class="n">stages_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># construct layers for each stage</span>
</span></span><span class="line"><span class="cl">    <span class="n">first_part</span><span class="p">,</span> <span class="n">module_list</span><span class="p">,</span> <span class="n">last_part</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stages_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">submesh</span> <span class="o">=</span> <span class="n">submeshes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage_id</span> <span class="o">=</span> <span class="n">stages_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get stage layers from user config stage ids in module list</span>
</span></span><span class="line"><span class="cl">        <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">module_list</span><span class="p">[</span><span class="n">stage_id</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">stage_id</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">first_part</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># concat module first part(if exists) bef module list to stage_0</span>
</span></span><span class="line"><span class="cl">            <span class="n">layers</span> <span class="o">=</span> <span class="n">first_part</span> <span class="o">+</span> <span class="n">layers</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">num</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">last_part</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># concat module last part(if exists) aft module list to last stage</span>
</span></span><span class="line"><span class="cl">            <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">last_part</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># deepcopy module for xla device tracing use</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage_module</span> <span class="o">=</span> <span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">Stage</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stage_module</span><span class="p">,</span> <span class="n">submesh</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">modules</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="write-sharding-function">Write Sharding Function</h2>
<p>要根据选择的不同的并行策略对每个 Stage 的模型权重，输入，输出进行切分。这里同样我们单独处理 Embedding_blocks, STDiT3_blocks 和 T2IFinalLayer. 让 stage0 包括对 Embedding_blocks 的处理，stage(N-1) 包括对 T2IFinalLayer 的处理。需要注意的是 DS-ulysses 我们需要对 <a href="mailto:Q@K.T">Q@K.T</a> 的结果 和 S@V 的结果也进行切分 SPMD 才会插入正确的 All2All，因此这部分只能放在网络的 forward 里面进行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_one_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># first 5 modules are embedding layers</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_embedding</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">total_len</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">    <span class="n">shard_sora_final</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_first_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_embedding</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_last_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_len</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># skip norm layer mark sharding</span>
</span></span><span class="line"><span class="cl">    <span class="n">shard_sora_final</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">total_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="construct-pipeline">Construct Pipeline</h2>
<p>然后为了处理多 stage 的情况，我们需要保存每个 stage 的输入和输出的形状。这一步相当于放到 cuda 上重走一遍整个模型的 forward，记录下每一层输入和输出的形状，保存为 json 一遍。实际上对于每个固定生成大小的视频进行一次就行，下次直接读取这个文件。因为现在都采用 <a href="https://facebookresearch.github.io/xformers/components/ops.html">xformers.ops.memory_efficient_attention</a>，需要输入张量在 cuda 上，我们需要手动在模型的 forward 函数中写一个 navie 的 attention 计算流程好让 torch_xla 能对张量进行跟踪。</p>
<h2 id="trace-mhlo-graph">Trace mhlo Graph</h2>
<p>根据上一步得到的每个 Stage 的输入形状，创建输入张量，放入 xla_device 上，执行 forward. 最后导出输出的 mhlo 计算图。这里需要注意第一个 stage 包含多个非连续的模块，因此需要单独处理，最后一个 stage 最后一层的输入与其他 block 不同，因此也要单独处理。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">trace_stage_mhlo_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">check_res</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    trace stage nn modules to mhlo graph
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># (NOTE): construct xla mesh before trace tensors generate,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># i.e., before any xla device call to avoid xla computation client construct</span>
</span></span><span class="line"><span class="cl">    <span class="n">xla_mesh</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">xla_mesh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_construct_stage_xla_mesh</span><span class="p">()</span>  <span class="c1"># create mesh from submesh info</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create xla device trace tensors, move module to xla device</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_trace_tensors</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">y_embedded</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">t_mlp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">t_mlp</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t_mlp</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">mod</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>  <span class="c1"># first load to cpu</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># get pipeline exec mode</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">exec_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">exec_mode</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># load lora cofifg</span>
</span></span><span class="line"><span class="cl">    <span class="n">lora_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">lora_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Enter trace mhlo graph for stage: &#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Trigger shard func to mark sharding the model</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_strategy</span><span class="p">,</span> <span class="n">xla_mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">exec_mode</span> <span class="o">==</span> <span class="n">EXEC_MODE</span><span class="o">.</span><span class="n">INFER</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># set stage name &amp; dump file path</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_set_stage_name_dump_file</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">exec_mode</span><span class="p">,</span> <span class="s2">&#34;fw&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_sampling_steps</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl">        <span class="n">timesteps</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">i</span> <span class="o">/</span> <span class="n">num_sampling_steps</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_timesteps</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_sampling_steps</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># FIXME: 原先是为每个stage单独生成trace_tensor, 现在要把上一个的结果传给下一个 stage</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#for i in range(30):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">pipeline_patches_height_list</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">pipeline_patches_height_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,:]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">xla_mesh</span><span class="p">)</span>  <span class="c1"># outputs is a list</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">,</span> <span class="n">xla_mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">check_res</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># check xla results compared to gpu results</span>
</span></span><span class="line"><span class="cl">            <span class="n">check_result_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># use torch xla _get_xla_tensors_hlo interface</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># to eliminate redundant live tensors as ret values</span>
</span></span><span class="line"><span class="cl">            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;XLA_DUMP_POST_OPTIMIZATIONS&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;true&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch_xla</span><span class="o">.</span><span class="n">_XLAC</span><span class="o">.</span><span class="n">_get_xla_tensors_hlo</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="analyze-mhlo-graph">Analyze mhlo Graph</h2>
<p>接下来我们要遍历上一步得出的 mhlo 图。</p>
<h3 id="opview">OpView</h3>
<p>从根节点的 ir 开始遍历上一步导出的整个计算图。根据传入 ir 的类型定义调用对应的 visit 函数读取其属性进行操作。主要通过 rsqrt 的位置来划分一个 Transformer block 中第几个 dot 和 dot_general 对应的是什么操作。对于 Sora 来说划分情况如下。这里需要注意的是 mhlo 图记录的是拓扑排序的顺序，不是程序顺序执行的顺序，因此第一个 block 会掺杂着 Embedding_blocks 的一些 dot 操作。因此我们从第二个 block 的第一个 rsqrt 位置开始统计。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">collect_rms_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">rms_collector</span> <span class="o">=</span> <span class="n">RMSCollector</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">rms_collector</span><span class="o">.</span><span class="n">visit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span> <span class="o">=</span> <span class="n">rms_collector</span><span class="o">.</span><span class="n">rms_locs</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># construct attention block &amp; ffn block ranges</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># exclude the rsqrt in T2IFinalLayer</span>
</span></span><span class="line"><span class="cl">  <span class="n">att_rm_locs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">att_rm_locs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># a block has 4 rsqrt, start from 2nd block to avoid embedding</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cro_attn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">att_rm_locs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># ORG: range(8, len(att_rm_locs), 4): </span>
</span></span><span class="line"><span class="cl">      <span class="n">start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">4</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">          <span class="n">end</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">end</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ffn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><table>
  <thead>
      <tr>
          <th>module</th>
          <th>operator</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td><code>RMSNorm(x)</code></td>
      </tr>
      <tr>
          <td><strong>Self Attention</strong></td>
          <td><code>dot(x, qkvLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(q)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(q, k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(s, v)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(attn, oLinear.weight)</code></td>
      </tr>
      <tr>
          <td><strong>Cross Attention</strong></td>
          <td><code>dot(x, qLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(y, kvLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(q, k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(s, v)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(attn, oLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(x) </code></td>
      </tr>
      <tr>
          <td><strong>Feed Forward Network</strong></td>
          <td><code>dot(x, upLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(x, downLinear.weight)</code></td>
      </tr>
  </tbody>
</table>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">visit_dot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">dot_lineno</span> <span class="o">=</span> <span class="n">_parse_loc_lineno</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">spt_att_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">cro_att_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cro_attn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">spt_qkv_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">ffn_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># lie in RMS ops closed attention block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">spt_att_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">spt_att_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#import pdb;pdb.set_trace()</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">cro_att_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">cro_att_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cro_att_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># lie ffn block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">spt_qkv_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">spt_qkv_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># pixart pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>                 
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Traversal of one block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_att_dot_cnt</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> \
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">attention_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># reset each block level counters</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_att_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">generic_visit</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>保存好一个 Transformer block 中每个 dot 或 dotgeneral 对应的是什么操作后，我们便可以访问这个 ir. 这里需要注意只要两个相乘的矩阵有一个是二维张量 (比如线性层的权重)，mhlo 都会将另一个 reshape 成二维张量。dot 算子 (<code>jaxlib.mlir.dialects._mhlo_ops_gen.DotOp</code>) 两个操作数都是二维的张量，qkvLinear 对应的是第一个 dot 操作。左操作数的 shape 为 <code>(BST,3C)</code>. 当两个相乘的矩阵都是 3 维及以上张量的时候就会生成 dot_general 该算子的两个相乘的矩阵都会被 reshape 成三维张量。Self-Attention 的第一个 dot_general 左操作数的 shape 为 <code>(BTN_A,S,C)</code>. 这样我们就可以得到 <code>BT=(BST)/S, N_A=(BTN_A)/(BT)</code>. 同样我们可以得到 OLinear, FFN 中 upLinear 和 downLinear 权重的形状. 以及 Cross-Attention 模块的对应信息。由于之前遍历是从第二个 block 开始的，因此总层数要 ＋1. 最后将得到的参数打包成一个字典返回。</p>
<h3 id="communication-view">Communication View</h3>
<p>我们以同样的方式定义各种集合通信算子的 visit 函数用于评估该算子的通信量，遍历到对应的 ir 后调用它。</p>
<p>AllReduce 将所有的数据通过规约操作集成到各个设备中。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6e4d9c026bc0632af5040321998fb3ab?method=download&amp;shareKey=f901430ac6bfa781d0b462f0170981d3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6e4d9c026bc0632af5040321998fb3ab?method=download&amp;shareKey=f901430ac6bfa781d0b462f0170981d3" alt="AllReduce">
    </a><figcaption>AllReduce</figcaption></figure></p>
<p>在 Ring-AllReduce 的 ReduceScatter 步骤中，每个进程发送 M 个元素 N-1 次，总共为 M(N-1). 在 AllGather 步骤中，每个进程发送它计算的块的结果。这是额外的 M 个元素发送了 N-1 次。总的通信量加起来是 2M(N-1).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB69d2b3957cd1863481bff0e785dc9a82?method=download&amp;shareKey=32e60903bafe5dbf240af91c67486e1b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB69d2b3957cd1863481bff0e785dc9a82?method=download&amp;shareKey=32e60903bafe5dbf240af91c67486e1b" alt="Ring-AllReduce">
    </a><figcaption>Ring-AllReduce</figcaption></figure></p>
<p>All-Gather 示意图如下，每个设备开始拥有初始的一部分数据，通信后每个设备都有一份完整的数据。总的通信量为 M(N-1).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe7e6e7a1230ed9ba7ba037556e489d51?method=download&amp;shareKey=5afdf2b669a500a6844aa9e281fe1ac3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe7e6e7a1230ed9ba7ba037556e489d51?method=download&amp;shareKey=5afdf2b669a500a6844aa9e281fe1ac3" alt="AllGather">
    </a><figcaption>AllGather</figcaption></figure></p>
<p>All2All 示意图如下，每个设备把自己的第 i 块数据发送给第 i 个设备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBddc785dcc80dd741fc1f469a85823cd4?method=download&amp;shareKey=085c0a5681116b4e1683d4d6ae5d080f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBddc785dcc80dd741fc1f469a85823cd4?method=download&amp;shareKey=085c0a5681116b4e1683d4d6ae5d080f" alt="All2All">
    </a><figcaption>All2All</figcaption></figure></p>
<p>基于 Bruck 算法的 All2All 流程如下</p>
<ol>
<li>局部循环移位 (Local Shift of Data-Blocks)
每个进程将其本地的数据块重新排列，进行初始的循环移位。对于进程 p 和数据块索引 i: R[i]=S[(p+i)%P]. 其中 S[i] 是进程本地初始的数据，R[i] 是移位后的数据。</li>
<li>全局通信 (Global Communication)
一共进行 log(P) 次通信。
每一步中每个进程将一部分数据发送给相邻的进程，并接收相邻进程发送的数据。若数据块索引 i 用 radix-2 表示的第 k 位为 1，则数据块会被发送到目标进程。
对于进程 p: 发送数据到进程 ((p + 2^k) % P)，接收来自进程 ((p - 2^k) % P) 的数据。
每次发送后，进程将接收到的数据更新到其本地数据中。</li>
<li>局部逆向移位 (Local Inverse Shift of Data-Blocks)
在完成所有全局通信之后，每个进程执行逆向移位，以恢复数据块的正确顺序。对于每个数据块索引 i: R[i]=R[(p−i+P)%P]</li>
</ol>
<p>在进程是 2 次幂的情况下每个设备每次要通信 M*P/2大小的数据，总共为 MPlog(P)/2.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa083a4c6002019e62b23c0b24b59a812?method=download&amp;shareKey=58f1b8055307d53e43ce86b9e1762989" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa083a4c6002019e62b23c0b24b59a812?method=download&amp;shareKey=58f1b8055307d53e43ce86b9e1762989" alt="Example of the Bruck Algorithm with 4 Processes">
    </a><figcaption>Example of the Bruck Algorithm with 4 Processes</figcaption></figure></p>
<h3 id="tflops-view">TFLOPS View</h3>
<p>计算量主要分成两种，element-wise 的操作计算量为元素个数。两个形状分别为 mxk 和 kxn 的矩阵相乘计算量为 2mkn. 被计入 element-wise 操作的算子有 add, subtract, multiply, divide, rsqrt, negate, exponential. 被计入矩阵乘法的算子有 dot, dot_general.</p>
<h2 id="performance-analysis">Performance Analysis</h2>
<p>我们根据提取出的 Transformer block 的信息送入性能分析器进行分析. tx8 的配置如下</p>
<table>
  <thead>
      <tr>
          <th>Parameter</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>TILE_NUM</td>
          <td>16</td>
      </tr>
      <tr>
          <td>SRAM (MB)</td>
          <td>3</td>
      </tr>
      <tr>
          <td>NOC BW (GB/s)</td>
          <td>128</td>
      </tr>
      <tr>
          <td>DRAM BW (GB/s)</td>
          <td>100</td>
      </tr>
      <tr>
          <td>DRAM LATENCY (us)</td>
          <td>0.1</td>
      </tr>
      <tr>
          <td>GEMM (TFLOPS)</td>
          <td>8</td>
      </tr>
      <tr>
          <td>VECTOR (TOPS)</td>
          <td>0.0625</td>
      </tr>
      <tr>
          <td>HOP LATENCY (us)</td>
          <td>0.01</td>
      </tr>
  </tbody>
</table>
<p>根据提取出的信息构建的 STDiT 的 spt_blk, tmp_blk, cross_blk 的参数字典如下.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                  <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_spt&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                  <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_tmp&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span><span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_cro&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;D_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;D_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FD&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FD&#34;</span><span class="p">]}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>根据这些参数再构建每个层的输入输出形状，计算类型和计算量，以 <code>Gate_ResAdd</code> 为例:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">GB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Gate_ResAdd</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">  Construct each op after MHSA on the config file
</span></span></span><span class="line"><span class="cl"><span class="s1">  &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># {name:{type:&#34;&#34;, size:&#34;&#34;, ishape:[], wshape:[]/None, oshape:[]}}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">construct_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">construct_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">GB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;D_O&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_weight_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;D_O&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_output_shape</span> <span class="o">=</span> <span class="n">ResAdd_input_shape</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_compute</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="n">GB</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="o">+</span><span class="s2">&#34;_&#34;</span><span class="o">+</span><span class="s2">&#34;ResAdd&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;ResAdd&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;Vector&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;ishape&#34;</span><span class="p">:</span> <span class="n">ResAdd_input_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;wshape&#34;</span><span class="p">:</span> <span class="n">ResAdd_weight_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;oshape&#34;</span><span class="p">:</span> <span class="n">ResAdd_output_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;compute&#34;</span><span class="p">:</span> <span class="n">ResAdd_compute</span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>就像这样构建整个 Transformer block 的所有操作</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">STDIT2_block</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># {name:{type:&#34;&#34;, size:&#34;&#34;, ishape:[], wshape:[]/None, oshape:[]}}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">construct_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">construct_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_spt&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_tmp&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span><span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_cro&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;D_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;D_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FD&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FD&#34;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cross_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cross_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">mlp_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;mlp&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block</span> <span class="o">=</span> <span class="n">FFN_block</span><span class="p">(</span><span class="n">cross_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">mlp_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;mlp&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">op_list</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">spatial_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="bp">self</span><span class="o">.</span><span class="n">temporal_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="bp">self</span><span class="o">.</span><span class="n">cross_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">op_dict</span> <span class="ow">in</span> <span class="n">op_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">op_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后就可以将构建好的 ops 放入 mapper 进行分析。刚才那些操作会被分成 3 类 <code>vector_mapper</code>, <code>gemm_auto_opt_mapper</code> 和 <code>flashatten_mapper</code>. 我们根据操作的类型送入对应的 mapper 进行分析，具体如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">STDIT2_mapper</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">QKV_fusion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">preset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</span></span><span class="line"><span class="cl">  <span class="n">Layers</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;L&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">ops</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">ops</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;=========================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Spatial Branch Mapping ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  =========================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">TmTn</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_RMSNorm&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_RMSNorm&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_ResAdd&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;==========================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Temporal Branch Mapping ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ==========================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>  <span class="c1"># 切分 30 份也无法满足SRAM要求</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_RMSNorm&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_RMSNorm&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_ResAdd&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;====================================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Cross Branch Mapping 2x per block ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ====================================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#mapping_result[&#39;spatial_RMSNorm&#39;]= vector_mapper(ops[&#39;spatial_RMSNorm&#39;],arch,splits=None,details=details)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_ResAdd&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">  <span class="c1"># HACK: Gate_ResAdd *2 了, cross 无gate 这里无 _2</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;====================================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Feed Forward Network 2x per block ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ====================================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;FFNup&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span><span class="n">fusion_op2</span><span class="o">=</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;SiLU&#39;</span><span class="p">],</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># mapping_result[&#39;FFNgate&#39;] = gemm_auto_opt_mapper(ops[&#39;FFNgate&#39;], arch, TmTn=TmTn, details=details)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># mapping_result[&#39;Hadamard&#39;] = vector_mapper(ops[&#39;Hadamard&#39;], arch, splits=None)</span>
</span></span><span class="line"><span class="cl">  <span class="n">TmTn</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>mapper 会遍历所有可能的切分策略放入 tx8 执行并选择最好的那一个。对于 vector 类型的算子只会沿着 sequence 维度切分；对于 GEMM 算子则会沿着 m, k, n 维度都进行切分；对于 flash-attention 的切分则与原算法相同，外循环遍历 K, V 的每一块，内循环遍历 Q 的每一块。这样就可以得到每个 tx8 上最优的切分方式对应的通信用时，计算用时和利用率。再用之前统计出的每个 die 上通信量除以 die2die 带宽得到通信用时，由此得到总的推理用时。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Friends</title>
      <link>http://localhost:57770/friends/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:57770/friends/</guid>
      <description>&lt;p&gt;&lt;a target=&#34;_blank&#34; href=https://luminolt.cn/ title=luminolt&amp;#39;s&amp;#32;Page class=&#34;friendurl&#34;&gt;
  &lt;div class=&#34;frienddiv&#34;&gt;
    &lt;div class=&#34;frienddivleft&#34;&gt;
      &lt;img class=&#34;myfriend&#34; src=https://luminolt.cn/author/chenghao-chen/avatar_hu15811225952467136947.jpg /&gt;
    &lt;/div&gt;
    &lt;div class=&#34;frienddivright&#34;&gt;
      &lt;div class=&#34;friendname&#34;&gt;luminolt&amp;#39;s Page&lt;/div&gt;
      &lt;div class=&#34;friendinfo&#34;&gt;Cryptography Learner&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/a&gt;
&lt;a target=&#34;_blank&#34; href=https://www.cestlavie.moe/ title=Jonathan523&amp;#39;s&amp;#32;Page class=&#34;friendurl&#34;&gt;
  &lt;div class=&#34;frienddiv&#34;&gt;
    &lt;div class=&#34;frienddivleft&#34;&gt;
      &lt;img class=&#34;myfriend&#34; src=http://localhost:57770/imgs/people/Jonathan523.png /&gt;
    &lt;/div&gt;
    &lt;div class=&#34;frienddivright&#34;&gt;
      &lt;div class=&#34;friendname&#34;&gt;Jonathan523&amp;#39;s Page&lt;/div&gt;
      &lt;div class=&#34;friendinfo&#34;&gt;每一个不曾起舞的日子, 都是对生命的辜负&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/a&gt;&lt;/p&gt;</description>
      <content:encoded><![CDATA[<p><a target="_blank" href=https://luminolt.cn/ title=luminolt&#39;s&#32;Page class="friendurl">
  <div class="frienddiv">
    <div class="frienddivleft">
      <img class="myfriend" src=https://luminolt.cn/author/chenghao-chen/avatar_hu15811225952467136947.jpg />
    </div>
    <div class="frienddivright">
      <div class="friendname">luminolt&#39;s Page</div>
      <div class="friendinfo">Cryptography Learner</div>
    </div>
  </div>
</a>
<a target="_blank" href=https://www.cestlavie.moe/ title=Jonathan523&#39;s&#32;Page class="friendurl">
  <div class="frienddiv">
    <div class="frienddivleft">
      <img class="myfriend" src=http://localhost:57770/imgs/people/Jonathan523.png />
    </div>
    <div class="frienddivright">
      <div class="friendname">Jonathan523&#39;s Page</div>
      <div class="friendinfo">每一个不曾起舞的日子, 都是对生命的辜负</div>
    </div>
  </div>
</a></p>
]]></content:encoded>
    </item>
  </channel>
</rss>
