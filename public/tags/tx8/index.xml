<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tx8 on WITHER</title>
    <link>http://localhost:1313/tags/tx8/</link>
    <description>Recent content in Tx8 on WITHER</description>
    <generator>Hugo -- 0.148.1</generator>
    <language>en</language>
    <copyright>2024-2025 WITHER</copyright>
    <lastBuildDate>Fri, 19 Sep 2025 09:20:48 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/tx8/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TX8 Inference Engine</title>
      <link>http://localhost:1313/blogs/tx8_inferenceengine/</link>
      <pubDate>Thu, 07 Aug 2025 21:47:33 +0800</pubDate>
      <guid>http://localhost:1313/blogs/tx8_inferenceengine/</guid>
      <description>TX8 inference engine description.</description>
      <content:encoded><![CDATA[<h1 id="overview">Overview</h1>
<ol>
<li>vLLM 调用 txda 初始化 context. 程序结束后释放 context.</li>
<li>vLLM 根据 hf_dir 中的配置文件创建模型, 其中 config.json 决定使用 TxNN 中定义的哪个 model class; 也决定了这个 class init 时的参数是什么.</li>
<li>调用 TxNN 的 model class 的 load weight 函数和 forward 函数, 具体实现都依赖于 TxDA.</li>
</ol>
<h1 id="argument">Argument</h1>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">parse_args</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--hf_dir&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Huggingface model path&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--max_tokens&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Max generated tokens&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--tp_size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Model TP size&#39;</span><span class="p">)</span>  <span class="c1"># tensor parallel degree, must Keep consistent with model.json</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;--device&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nb">type</span><span class="o">=</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)],</span>
</span></span><span class="line"><span class="cl">        <span class="n">default</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">help</span><span class="o">=</span><span class="s2">&#34;Tx device idxs; E.g., --device 0,1,2,3&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&#34;--log_file&#34;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&#34;&#34;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&#34;Log file path&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Get case_dir from huggingface model config</span>
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">hf_dir</span><span class="si">}</span><span class="s2">/config.json&#34;</span><span class="p">,</span> <span class="s2">&#34;r&#34;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&#34;utf-8&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">hf_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">args</span><span class="o">.</span><span class="n">case_dir</span> <span class="o">=</span> <span class="n">hf_config</span><span class="p">[</span><span class="s2">&#34;param_dir&#34;</span><span class="p">]</span>  <span class="c1"># kcorebin path, with model_info.json and chip0/, chip1/ ... in the directory</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;Inference info:</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - hf_dir: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">hf_dir</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - case_dir: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">case_dir</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - max_tokens: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">max_tokens</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - tp_size: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">tp_size</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - ENV:TXDA_VISIBLE_DEVICES: &#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;TXDA_VISIBLE_DEVICES&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;Not specified&#39;</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - device: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="se">\n</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="sa">f</span><span class="s2">&#34;| - log_file: </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">log_file</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">args</span>
</span></span></code></pre></div><ul>
<li><code>hf_dir</code>: 指的是模型的 hugging_face 路径。</li>
<li><code>case_dir</code>: 是解析 <code>hf_dir</code> 下的 config.json 文件的字段，指的是后端生成的 kcore 文件夹路径。除此之外还需要在 json 文件中额外配置并行度相关信息。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="nt">&#34;param_dir&#34;</span><span class="p">:</span> <span class="s2">&#34;...&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;tensor_parallel&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;use_tp&#34;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;parallel_size&#34;</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>会根据我们传入的 <code>--device</code> 和 <code>tp_size</code> 参数创建逻辑到实际物理芯片的映射。这里 InternLM3 使用的是 4 卡机器，张量并行度为 2. 计算图中 batchsize=1.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// setDeviceIdx
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;</span> <span class="n">c4_TP2x2</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">}},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;</span> <span class="n">c4_TP2x2_hw</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">}},</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">auto</span> <span class="n">set_TP2_C4_Map</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">user_map</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">hw_map</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                          <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;&amp;</span> <span class="n">user_settings</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it_TP2x2</span> <span class="p">:</span> <span class="n">user_map</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">user_settings</span> <span class="o">==</span> <span class="n">it_TP2x2</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">device_setting_</span> <span class="o">=</span> <span class="n">hw_map</span><span class="p">[</span><span class="n">it_TP2x2</span><span class="p">.</span><span class="n">first</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="p">(</span><span class="n">TxdATPMode</span><span class="o">::</span><span class="n">C4TP2B1</span> <span class="o">==</span> <span class="n">tp_mode_</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_TP2_C4_Map</span><span class="p">(</span><span class="n">c4_TP2x2</span><span class="p">,</span> <span class="n">c4_TP2x2_hw</span><span class="p">,</span> <span class="n">user_settings</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>TxdaContext::initDevice 的主要功能是初始化设备 (TsmDevice)，并将其封装为 TxdaDevice 对象，最终存储到设备组 (device_group_) 中。</p>
<p>TxdaContext::initChipProperty 的主要功能是初始化芯片相关的属性和配置，包括设备组、模型数量、环境变量解析、内存地址初始化以及设备启动参数的设置。</p>
<p>TxdaContext::initModel：负责初始化模型，解析案例目录，设置 TP 大小和 TP 模式，并调用 TxdaModel::init 初始化每个模型 的基本属性，包括案例目录、JSON 配置和编译选项。</p>
<h1 id="txnn">TxNN</h1>
<p>configuration_internlm3.py 中主要负责模型结构参数的初始化 (根 hugging face 中的 config.json 保持一致) 以及张量并行的设置。</p>
<p>input_output_internlm3.py 负责在 python 端为要从后端加载进来的输入和输出注册张量和开辟空间。-</p>
<ul>
<li>InputBuffer 会创建一个 IntermediateKVCache 实例。这是 KV Cache 的实际存储空间，大小根据序列长度、头的数量预先分配好。还会注册各种输入相关的缓冲区
<ul>
<li>input_ids: 当前需要处理的输入 token ID。</li>
<li>start_index, seq_len: 用于管理当前生成序列的位置信息。</li>
<li>k_gather_end_indices: 为每一层配置 对应的 KV cache 的起始索引。</li>
</ul>
</li>
</ul>
<p>OutputBuffer 和 InputBuffer 类似，它也创建了一个 IntermediateKVCache 实例。这个 Cache 用于存放计算完成后更新的 Key 和 Value，这些更新后的值将在下一步生成token时作为输入使用。</p>
<ul>
<li>注册所有KV Cache层: 它立即将所有层的 KV Cache 缓冲区注册到自己的 _buffer 中。</li>
<li>注册输出缓冲区: 预先分配并注册了用于存放最终结果的内存空间：
<ul>
<li>lm_head_reshape_out: 用于存放语言模型最后的输出（通常是 logits），其维度为 (序列长度, 词汇表大小)。</li>
<li>lm_head_argmax_out: 用于存放最终预测出的 token ID（对 logits 取 argmax 后的结果）。</li>
</ul>
</li>
</ul>
<p>里面负责定义模型，在 python 端为数据开辟好内存，GraphBasedInternlm3ForCausalLM 在初始化时，会创建 InputBuffer 和 OutputBuffer 的实例。它为 tensor_parallel 的每一路都创建了缓冲区。</p>
<p>当 forward 方法接收 input_ids. 不在 Python 中执行复杂的数学运算。而是通过 load_input 把后端编译生成的 bin 文件数据填充到 InputBuffer 中.</p>
<p>通过 TxDA context 把加载好的数据放到硬件上然后 launch kernel将 InputBuffer 和 OutputBuffer 的内存地址传递给底层引擎。底层引擎直接从 InputBuffer 指向的内存中读取输入，执行高效的模型计算，然后将结果直接写入 OutputBuffer 指向的内存中。</p>
<h1 id="vllm">vllm</h1>
<p>主要利用的是其推理框架，在 LLMEngine 里添加了自己的 TxNPUExecutor 里面包含自己的 TxNPUWorker，其中包含 TxNPUCacheEngine 和 TxNPUModelRunner.</p>
<p>TxNPUMModelRunner 定义了专门从后端编译出的 chip_out 文件夹中的 param.bin 文件里加载模型参数到 TxNN 的定义模型的 ModelLoader. 然后推理的时候调用的 forward 流程如上。</p>
]]></content:encoded>
    </item>
    <item>
      <title>TX8 Backend</title>
      <link>http://localhost:1313/blogs/tx8_backend/</link>
      <pubDate>Wed, 23 Jul 2025 11:49:02 +0800</pubDate>
      <guid>http://localhost:1313/blogs/tx8_backend/</guid>
      <description>TX8 backend description.</description>
      <content:encoded><![CDATA[<h1 id="tx8-hardware-overview">TX8 Hardware Overview</h1>
<p>TX8 采用的是空间计算型结构 (Special Computing Architecture)，市面上普遍采用的共享内存结构 (Shared Memory Architecture)，它的数据通信交互主要是依赖于 DDR，一个 thread 把 DDR 的数据改变之后，另外一个 thread 再从 DDR 中才能得知到这个数据已经被改变。这么做有一个很明显的缺陷，就是它瓶颈在于内存容量以及访问内存的带宽延迟。空间计算型的结构它是由中间的NOC (Network On Chip) 来构成模块之间的互联。这样很好的避免了这个 DDR 的瓶颈，同时也有了更好的 scale out 能力。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB04f094c9d80d3990221020a51ce93433?method=download&amp;shareKey=f6149480beae12e20d31f124e425ef84" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB04f094c9d80d3990221020a51ce93433?method=download&amp;shareKey=f6149480beae12e20d31f124e425ef84" alt="(a) Shared Memory Architecture (b) Spatial Computing Architecture">
    </a><figcaption>(a) Shared Memory Architecture (b) Spatial Computing Architecture</figcaption></figure></p>
<p>下图为 TX8 两个芯片互连的逻辑结构。每个芯片由 4x4 总计 16 Tile 以 mesh 拓扑结构进行互连。每一个 Tile 是一个计算核心，是一个图灵完备 (Turing Complete) 的系统，既具有调度控制以及计算通信以及存储的能力。片上 NoC 采用的是 stream (一种轻量级 DMA 技术). 片上 DDR 大小为 64GB，芯片之间是通过 high speed IO 进行互连的。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB98d1d8b5916da28e40cf3c77b63dcbe6?method=download&amp;shareKey=1ca36bb999bfe3f524fc2525e1cb0ab7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB98d1d8b5916da28e40cf3c77b63dcbe6?method=download&amp;shareKey=1ca36bb999bfe3f524fc2525e1cb0ab7" alt="Tile">
    </a><figcaption>Tile</figcaption></figure></p>
<p>单芯片与单卡 A100 性能对比如下表所示</p>
<table>
  <thead>
      <tr>
          <th>TX8</th>
          <th>单卡性能</th>
          <th>最大组网性能</th>
          <th>A100</th>
          <th>单卡性能</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>INT8</strong></td>
          <td>256T</td>
          <td>1E</td>
          <td></td>
          <td>624T</td>
      </tr>
      <tr>
          <td><strong>BF16</strong></td>
          <td>128T</td>
          <td>0.5E</td>
          <td></td>
          <td>312T</td>
      </tr>
      <tr>
          <td><strong>TFP32</strong></td>
          <td>128T</td>
          <td>0.5E</td>
          <td></td>
          <td>156T</td>
      </tr>
      <tr>
          <td><strong>FP32</strong></td>
          <td>21T</td>
          <td>40P</td>
          <td></td>
          <td>19.5T</td>
      </tr>
      <tr>
          <td><strong>内存带宽</strong></td>
          <td>200GB/s</td>
          <td>-</td>
          <td><strong>显存带宽</strong></td>
          <td>1935GB/s</td>
      </tr>
      <tr>
          <td><strong>PCIe</strong></td>
          <td>64GB/s</td>
          <td>-</td>
          <td></td>
          <td>64GB/s</td>
      </tr>
      <tr>
          <td><strong>内存容量</strong></td>
          <td>64GB</td>
          <td>128TB</td>
          <td><strong>显存容量</strong></td>
          <td>80GB</td>
      </tr>
      <tr>
          <td><strong>TsingMicro-Link</strong></td>
          <td>1600Gbps</td>
          <td>-</td>
          <td><strong>NV-Link</strong></td>
          <td>600GB/s</td>
      </tr>
  </tbody>
</table>
<h1 id="single-tile">Single Tile</h1>
<p>下图是单 Tile 的硬件结构，实际上每个 Tile 上会有两个 kernel core 和 special core，图中只画了一个。还有个 neural core，主要是负责计算以及数据搬运等等。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8e08cce47b2de4d373c8dd667a988463?method=download&amp;shareKey=ba34989999d6febf20e7abeda09a81b2" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8e08cce47b2de4d373c8dd667a988463?method=download&amp;shareKey=ba34989999d6febf20e7abeda09a81b2" alt="Tile Microarchitecture">
    </a><figcaption>Tile Microarchitecture</figcaption></figure></p>
<ul>
<li>
<p>kernel core 主要用于下发指令。它会从 DDR 中取址，然后送到这个 neural core 的 NCC controller 里面。NCC controller 又会把根据这个指令的类型下发到 CT/NE/LSU. 他们三个是执行不同种类指令的三个小模块，后面会讲到。这三个小模块会从 SPM (Scratched Pad Memory) 上读取数据，然后再计算，或者再存回 SPM上。值得注意的是，LSU 是用来负责这个数据搬运的，所以它可以把这个 SPM 上的数据直接搬到DDR，或者是从 DDR 搬到 SPM 上。CT 和 NE 都是负责计算的模块，其中 scalar unit 位于 NCC controller，是一个负责标量计算的模块。</p>
</li>
<li>
<p>special core 用来和 NOC 进行连接，它可以从 DDR 中读取数据，然后通过配置 DTE 模块和这个远程的 Tile 进行通信。DTE 模块也可以通过 special core 将本 Tile 上的 SPM 与远程 Tile 上的 SPM 进行通信。</p>
</li>
</ul>
<h1 id="cgra-tensor">CGRA Tensor</h1>
<p>CGRA Tensor 模块支持算术运算，逻辑运算，位操作，激活函数，超越函数，规约，池化，数据搬移，格式转换，辅助计算。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9086e7454a92ac6077331ed5c7f4fc56?method=download&amp;shareKey=834faad50224b52bd4c82ccf738f5293" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9086e7454a92ac6077331ed5c7f4fc56?method=download&amp;shareKey=834faad50224b52bd4c82ccf738f5293" alt="CGRA">
    </a><figcaption>CGRA</figcaption></figure></p>
<p>Neural Core Controller 下发指令到 CTRL_UNIT，然后 CTRL_UNIT 下发指令到 RAM_ACC_UNIT. RAM_ACC_UNIT 读入 SPM 的数据，然后送入 Pipe Unit 进行运算之后把结果存回 SPM.</p>
<p>CGRA 指令格式如下。例如 CGRATensor_ArithOp_V_V_abs，指令操作指的是对向量元素求绝对值。</p>
<table>
  <thead>
      <tr>
          <th>指令格式</th>
          <th>CGRATensor_function_format_name.type</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Function</strong></td>
          <td>描述该单元的主要功能，如算数运算、关系运算、逻辑运算等；</td>
      </tr>
      <tr>
          <td><strong>Format</strong></td>
          <td>描述数据的存储方式，如VV、VS、Tensor、VuV 分别表示</br>向量与向量计算、向量与标量计算、Tensor计算、向量与单元向量计算；</td>
      </tr>
      <tr>
          <td><strong>Name</strong></td>
          <td>描述具体的操作，如加、减、乘、除等；</td>
      </tr>
      <tr>
          <td><strong>Type</strong></td>
          <td>表示数据类型，如 bf16/fp32 等；</td>
      </tr>
  </tbody>
</table>
<p>下面具体讲一下在 BN 算子开发中用到的 <code>CGRATensor_ArithOp_V_VuV_mul_loop (bf16 *src, bf16 *dst, bf16 *unit, int rnd, int src_elem_num, int unit_elem_num, int full_src_elem_num, int  full_unit_elem_num)</code>.</p>
<ul>
<li>src/dst/unit 分别表示 也是原数据/存数/单元向量的地址。</li>
<li>src_elem_num 是做一次这个 VuV 中原数据的个数。</li>
<li>unit_elem_num 是做一次这个 VuV 中单元向量数据的个数。</li>
</ul>
<p>在讲 VuV_mul_loop 之前，先来看一下这个 VuV_mul 也就是没有循环的单次版本。分为两次进行，第一次是前四个蓝色的方块与橙色方块相乘，第二次为后四个蓝色方块与橙色方块相乘。VuV_mul_loop 即把这个过程重复很多次，所以要求 <code>full_src_elem_num/full_unit_elem_num == src_elem_num/unit_elem_num</code>，并且<code>unit_elem_num=64</code>.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB367ed5ab2178c80abdeb160cb55b409d?method=download&amp;shareKey=05e534d23de8ab1be6cef33b8c8e0e4e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB367ed5ab2178c80abdeb160cb55b409d?method=download&amp;shareKey=05e534d23de8ab1be6cef33b8c8e0e4e" alt="VuV_mul_loop">
    </a><figcaption>VuV_mul_loop</figcaption></figure></p>
<h1 id="tensor-layout">Tensor Layout</h1>
<p>layout 可以分为以下几种</p>
<ul>
<li>layout_str: 中端使用
<ul>
<li>CNN Op: 1. Feature (NCHW/NHWC) etc. 2. Weight (OIHW/HWOI) etc.</li>
<li>Non-CNN Op: 大模型中常见，Tensor/NTensor，它们的区别是第 0 维是否为 1.</li>
</ul>
</li>
<li>mem_layout: 后端使用，代表了在芯片上的实际排布
<ul>
<li>Tensor/NTensor: 数据的紧密排布</li>
<li>Cx/NCx: 对 Tensor/NTensor 格式化后的结果，方便易硬件读取。</li>
</ul>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>dtype</th>
          <th>channel</th>
          <th>description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>bf16/fp16 <br>/fp32/tf32</td>
          <td>c &lt;= 32</td>
          <td>NHWC, C向4/8/16/32对齐，N 的起始地址向 2048bit 对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 32</td>
          <td>N[CxHW64, HWC0], C0 向 4/8/16/32 对齐，N 的起始地址向2048bit 对齐<br>在一个 batch 内将 tensor 按 C 分成 Cx*64 和 C0两部分</td>
      </tr>
      <tr>
          <td>int8</td>
          <td>c &lt;= 64</td>
          <td>NHWC, C 向 4/8/16/32/64对齐，N的起始地址向2048bit对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 64</td>
          <td>N[CxHW128, HWC0], C0 向 4/8/16/32/64 对齐，N的起始地址向 2048bit 对齐 <br> 在一个 batch 内将 tensor 按 C 分成 Cx*128 和C0 两部分</td>
      </tr>
  </tbody>
</table>
<p>对于 fp16 的 2x1x2x131 的数据，NTensor 格式存储起始地址为 0x0000 按各存储格式排列如下</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB47732820f52aa7a0bfd0d22bb2e61e00?method=download&amp;shareKey=03c9e8f103abf576755a3e33e5d5cbc5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB47732820f52aa7a0bfd0d22bb2e61e00?method=download&amp;shareKey=03c9e8f103abf576755a3e33e5d5cbc5" alt="NTensor Layout">
    </a><figcaption>NTensor Layout</figcaption></figure></p>
<p>NCx: 131 = 64 x 2 + 3, 将 C 分成 2(Cx) 个 64 和 4(C0). batch0 的结束地址是 0x1080 (4224), batch1 起始地址需对齐到 2048bit，即 4224&ndash;&gt;2048*3=6144 (0x1800).</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB53406947a6dafa2bbe98df922f46a954?method=download&amp;shareKey=3e75616e0238e7360bd6930b98941909" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB53406947a6dafa2bbe98df922f46a954?method=download&amp;shareKey=3e75616e0238e7360bd6930b98941909" alt="NCx Layout">
    </a><figcaption>NCx Layout</figcaption></figure></p>
<h1 id="neural-engine">Neural Engine</h1>
<p>Neural engine 类似于 GPU Tensor Core，主要是完成各种矩阵 (op_Gemm) 和卷积 (op_Conv) 类型的高效并行 Tensor 计算。PE Array 它的进行矩阵运算的部分，一次完成 8x16x8 大小的矩阵乘法。然后它的输入有激活 input，还有 psum，还有 weight，也就是权重。</p>
<p>计算之后，还饿可以进行后处理，对这个结果进行 BN/量化/激活等等，然后再到输出，然后我们要用到neural engine 的算子其实并不多，只有 op_Gemm 和 op_Conv.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBf07a7034750a8723bf35f5cb311251e2?method=download&amp;shareKey=d0c242fdf504441bb4284c72b48908c4" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBf07a7034750a8723bf35f5cb311251e2?method=download&amp;shareKey=d0c242fdf504441bb4284c72b48908c4" alt="Neural Engine">
    </a><figcaption>Neural Engine</figcaption></figure></p>
<h1 id="lsu">LSU</h1>
<p>LSU 是负责数据搬运的 DMA 控制器。具体它有三部分:</p>
<ul>
<li>RDMA: Read DDR &ndash;&gt; SPM，对应指令有 op_loadVar，op_loadConst，op_rdmaGather.</li>
<li>WDMA: Write SPM &ndash;&gt; DDR，对应指令有 op_dma_store，op_wdmaScatter.</li>
<li>TDMA: 对所属 Tile SPM 上的数据进行操作，对应指令有 op_reshape，op_gatherScatter.</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd7f94b1349b939c2c20a37abf5e57bbc?method=download&amp;shareKey=fb7dd9facaf7ca29424c72eaa4991000" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd7f94b1349b939c2c20a37abf5e57bbc?method=download&amp;shareKey=fb7dd9facaf7ca29424c72eaa4991000" alt="LSU">
    </a><figcaption>LSU</figcaption></figure></p>
<p>一种经常使用 TDMA 的情况是进行低精度到高精度的转换。以 fp16 -&gt; fp32 为例，首先会调用 op_gatherScatter 指令把紧密排布的低精度数据读进来然后 scatter 到 SPM 上的对应位置以保留空间存储转换后的数据；然后再调用 CGAR convert_fp16_fp32 指令进行精度转换。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB1079912e78f03abc7a30d0db12ffb046?method=download&amp;shareKey=20f9d8abd3fb47b3194540d639a2f9ee" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB1079912e78f03abc7a30d0db12ffb046?method=download&amp;shareKey=20f9d8abd3fb47b3194540d639a2f9ee" alt="fp16 to fp32 Conversion">
    </a><figcaption>fp16 to fp32 Conversion</figcaption></figure></p>
<h1 id="tx8-compiler">TX8 Compiler</h1>
<p>和一般编译器差不多，先获取前端的 Tensorflow/Pytorch 等等生成的 mhlo 计算图，经过中端的处理，然后转到后端。变成后端 IR. 同时又会调用 OPLIB 算子库中的算子来生成 main.c，就是可以直接放在不同平台上运行的主程序。平台可以选择 RISCV 即真实的硬件，或者是 Cmodel 进行模拟。</p>
<p>BEIR 主要是接过中端传进来的 IR，然后进行各类的图优化的 Pass，包括一些算子切分，还有内存调度等等。最终 codegen 这个可编译执行的 main.c 的文件。然后再放在平台上去编译完再运行。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB95d5755abd98d22b6bcdaca440c0e8c4?method=download&amp;shareKey=e72432cabeea4cbab293db667ecb0648" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB95d5755abd98d22b6bcdaca440c0e8c4?method=download&amp;shareKey=e72432cabeea4cbab293db667ecb0648" alt="TX8 Compiler Workflow">
    </a><figcaption>TX8 Compiler Workflow</figcaption></figure></p>
<h1 id="tx8-be">TX8 BE</h1>
<p>后端 IR 使用的是 MLIR，继承 Dialect，定义了许多 Operations, Attributes, Types.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_Dialect <span class="p">:</span> Dialect <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">name =</span> <span class="s">&#34;tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;A low-level dialect for tx8 backend specification&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">useDefaultAttributePrinterParser =</span> <span class="m">1</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="attribute">Attribute</h2>
<p>下面介绍一些常用的 Attribute.</p>
<p><code>parallel_attr</code> 主要是表示 tensor 每个维度上数据并行和张量并行的切分策略。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_ParallelAttr <span class="p">:</span> Tx8be_Attr<span class="p">&lt;</span><span class="s">&#34;Parallel&#34;</span><span class="p">,</span> <span class="s">&#34;parallel_attr&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;Structure of parallel information.&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">parameters =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        <span class="s">&#34;ParallelModeAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>is_dp_inner<span class="p">,</span>    <span class="c">// dp dimension is in the inner, otherwise tp
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_x<span class="p">,</span>    <span class="c">// data parallel dimension at x axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_y<span class="p">,</span>    <span class="c">// data parallel dimension at y axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>dp_dim_z<span class="p">,</span>    <span class="c">// data parallel dimension at z axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_x<span class="p">,</span>    <span class="c">// tensor parallel dimension at x axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_y<span class="p">,</span>    <span class="c">// tensor parallel dimension at y axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;i32&#34;</span> <span class="p">:</span> <span class="err">$</span>tp_dim_z<span class="p">,</span>    <span class="c">// tensor parallel dimension at z axis
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>sharding_is_given<span class="p">,</span>    <span class="c">// true: is given, false: is not
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;::mlir::DenseI32ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>shape_spatial_sharding    <span class="c">// Shape split info
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">assemblyFormat =</span> <span class="s">&#34;`&lt;` struct($params) 1&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>dev_attr</code> 属性包含</p>
<ul>
<li>imm_size，也就是用到的这个辅助空间的大小。</li>
<li>mem_layout 也就是数据的存储数据的排布。</li>
<li>multi_buf_en 指是否使用 double buffer.</li>
<li>out_shape_buf_idx 指的是输出使用第几个缓冲区。</li>
<li>temporal_mem_slice 是单个 Tile 每次处理的数据大小。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_DevAttr <span class="p">:</span> Tx8be_Attr<span class="p">&lt;</span><span class="s">&#34;Dev&#34;</span><span class="p">,</span> <span class="s">&#34;dev_attr&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;Structure of op parameters on device.&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">parameters =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        <span class="s">&#34;uint64_t&#34;</span> <span class="p">:</span> <span class="err">$</span>imm_size<span class="p">,</span>    <span class="c">// Output memory addr offset
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;LayoutModeAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>mem_layout<span class="p">,</span>    <span class="c">// Layout
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;bool&#34;</span> <span class="p">:</span> <span class="err">$</span>multi_buf_en<span class="p">,</span>    <span class="c">// for double buffering
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int32_t&#34;</span> <span class="p">:</span> <span class="err">$</span>multi_buf_num<span class="p">,</span>    <span class="c">// for double buffering
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>out_shape_buf_idx<span class="p">,</span>    <span class="c">// index for dynamic shape buffer on runtime
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>temporal_mem_slice<span class="p">,</span>    <span class="c">// for compute local buffer size
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int32_t&#34;</span> <span class="p">:</span> <span class="err">$</span>source_type<span class="p">,</span>    <span class="c">// Software pipeline stage
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        <span class="s">&#34;int64_t&#34;</span> <span class="p">:</span> <span class="err">$</span>imm_addr<span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="s">&#34;mlir::DenseI64ArrayAttr&#34;</span> <span class="p">:</span> <span class="err">$</span>mem_addr    <span class="c">// use array for multibuffer
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">assemblyFormat =</span> <span class="s">&#34;`&lt;` struct($params) `&gt;`&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>MemScopeMode</code> 用于描述数据存储在哪里。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_MemScopeMode <span class="p">:</span> I32EnumAttr<span class="p">&lt;</span><span class="s">&#34;MemScopeMode&#34;</span><span class="p">,</span> <span class="s">&#34;Specify the memory scope&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="p">[</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;DDR&#34;</span><span class="p">,</span> <span class="m">0</span><span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;SPM&#34;</span><span class="p">,</span> <span class="m">1</span><span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        I32EnumAttrCase<span class="p">&lt;</span><span class="s">&#34;3DDRAM&#34;</span><span class="p">,</span> <span class="m">2</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        let <span class="nl">genSpecializedAttr =</span> <span class="m">0</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">        let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mir::tx8be&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span></code></pre></div><h2 id="types">Types</h2>
<p>定义了很多类型，实际上常用的就是 AnyTensorOrNone.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def AnyTensorOrNone<span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">,</span> NoneType<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Tuple <span class="p">:</span> NestedTupleOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def AnyTensorOrTuple <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyRankedTensor<span class="p">,</span> Tx8be_Tuple<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Pred <span class="p">:</span> TypeAlias<span class="p">&lt;</span>I1<span class="p">,</span> <span class="s">&#34;pred (AKA boolean or 1-bit integer)&#34;</span><span class="p">&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_PredTensor <span class="p">:</span> TensorOf<span class="p">&lt;[</span>Tx8be_Pred<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Token <span class="p">:</span> Type<span class="p">&lt;</span>CPred <span class="s">&#34;{$_self-&gt;isa&lt;TokenType&gt;()}&#34;</span><span class="p">,</span> <span class="s">&#34;token&#34;</span><span class="p">&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_TensorOrTokenOrTuple <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>AnyTensor<span class="p">,</span> Tx8be_Token<span class="p">,</span> Tx8be_Tuple<span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_SInt <span class="p">:</span> SignlessIntOfWidths<span class="p">&lt;[</span><span class="m">4</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="m">16</span><span class="p">,</span> <span class="m">32</span><span class="p">,</span> <span class="m">64</span><span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_UInt <span class="p">:</span> UnsignedIntOfWidths<span class="p">&lt;[</span><span class="m">4</span><span class="p">,</span> <span class="m">8</span><span class="p">,</span> <span class="m">16</span><span class="p">,</span> <span class="m">32</span><span class="p">,</span> <span class="m">64</span><span class="p">]&gt;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">def Tx8be_Int <span class="p">:</span> AnyTypeOf<span class="p">&lt;[</span>Tx8be_SInt<span class="p">,</span> Tx8be_UInt<span class="p">]&gt;</span><span class="err">;</span>
</span></span></code></pre></div><h2 id="operations">Operations</h2>
<p>以开发的 BatchNorm_InferenceOp 为例讲解一下 Tx8be 中关于算子的定义。首先 batchnorm 是将通道维度视作样本，计算其他维度的平均值和方差后进行归一化的操作。</p>
$$
\begin{aligned}
BatchNorm\colon y&=\gamma\:\frac{x-Mean(x)}{\sqrt{Var(x)+\varepsilon}}+\beta\\
Mean(x)&=\frac{1}{N}\sum_{i=1}^{N}x_{i}\\
Var(x)&=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-Mean(x))^{2}\end{aligned}$$<p>中括号内是一些需要继承的 <a href="https://mlir.llvm.org/docs/Interfaces/">Interface</a>. 其允许 attributes, operations 和 types 公开方法调用接口，而不需要调用者知道特定的派生类型。</p>
<p>arguments 指定了算子需要的输入，包括参数以及之前介绍到的一些属性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def Tx8be_BatchNorm_InferenceOp <span class="p">:</span> Tx8be_Op<span class="p">&lt;</span><span class="s">&#34;BatchNorm_Inference&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span>DeclareOpInterfaceMethods<span class="p">&lt;</span>oplibinterface<span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">    DeclareOpInterfaceMethods<span class="p">&lt;</span>ShardingInterface<span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">    DeclareOpInterfaceMethods<span class="p">&lt;</span>ComputeInterface<span class="p">&gt;]</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">summary =</span> <span class="s">&#34;BatchNorm inference&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">        Normalizes the operand <span class="kt">tensor</span> across all dimensions except for the c dimension
</span></span><span class="line"><span class="cl">        and produce a result <span class="kt">tensor</span><span class="p">.</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">arguments =</span> <span class="p">(</span>ins
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>input<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>scale<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>offset<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>mean<span class="p">,</span>
</span></span><span class="line"><span class="cl">        AnyTensor<span class="p">:</span><span class="err">$</span>variance<span class="p">,</span>
</span></span><span class="line"><span class="cl">        DefaultValueOptionalStrAttr<span class="p">&lt;</span>StrAttr<span class="p">,</span> <span class="s">&#34;Unknown&#34;</span><span class="p">&gt;:</span><span class="err">$</span>layout_str<span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="c">// The following are backend parameters
</span></span></span><span class="line"><span class="cl"><span class="c"></span>        OptionalAttr<span class="p">&lt;</span>Tx8be_ParallelAttr<span class="p">&gt;:</span><span class="err">$</span>chip_parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        OptionalAttr<span class="p">&lt;</span>Tx8be_ParallelAttr<span class="p">&gt;:</span><span class="err">$</span>tile_parallel<span class="p">,</span>
</span></span><span class="line"><span class="cl">        OptionalAttr<span class="p">&lt;</span>Tx8be_DevAttr<span class="p">&gt;:</span><span class="err">$</span>dev_info
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">results =</span> <span class="p">(</span>outs AnyTensor<span class="p">:</span><span class="err">$</span>output<span class="p">)</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="interface">Interface</h2>
<p>Interface 定义一些通用的方法或行为，这些方法没有具体实现。要通过继承某个 Interface 来具体实现该接口的方法和行为。tx8中定义了 5 个 Interface: OpLibInterface, ComputeInterface, ShapeInferenceOpInterface, ShardingInterface, StreamConfigInterface.</p>
<p>BatchNorm 算子开发中只用到了前四个，下面依次介绍一下。</p>
<p><code>ShapeInferenceOpInterface</code> 定义了两个方法 <code>inferShapes</code> 和 <code>inferLayout</code>. 继承这个接口的话就需要实现这两种方法。根主要是根据输入来推断输出的形状和布局。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ShapeInferenceOpInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ShapeInferenceOpInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="p">[{</span> <span class="p">}],</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;inferShapes&#34;</span><span class="p">,</span>  <span class="c">// method name
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;DynamicShapeParam&#34;</span> <span class="p">:</span> <span class="err">$</span>shapeParam<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="p">[{</span> <span class="p">}],</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;inferLayout&#34;</span><span class="p">,</span>  <span class="c">// method name
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>由于 batchnorm 不对这两者进行改变，因此输出和输入相同。如果是需要改变的算子比如 transpose 就需要进行改变。</p>
<p><code>input_data &lt;shape=3x4x5x6, layout=NCHW&gt; --&gt; transpose&lt;permutation=(0,2,3,1)&gt; --&gt; output_data&lt;shape=3x5x6x4, layout=NHWC&gt;</code></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// BatchNorm_Interface.cpp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">inferLayout</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">in_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">getInput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">cur_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">in_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">i_layout</span> <span class="o">=</span> <span class="n">in_op</span><span class="o">-&gt;</span><span class="n">getAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">).</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">StringAttr</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getValue</span><span class="p">().</span><span class="n">str</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">StringAttr</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">i_layout</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">in_op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;dev_info&#34;</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">i_dev_layout</span> <span class="o">=</span> <span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">in_op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">cur_op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">cur_op</span><span class="p">,</span> <span class="n">i_dev_layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">inferShapes</span><span class="p">(</span><span class="n">DynamicShapeParam</span> <span class="o">&amp;</span><span class="n">shapeParam</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">getOutput</span><span class="p">().</span><span class="n">setType</span><span class="p">(</span><span class="n">getInput</span><span class="p">().</span><span class="n">getType</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">::</span><span class="n">mlir</span><span class="o">::</span><span class="n">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="shardinginterface">ShardingInterface</h2>
<p><code>tileShardingSplit</code> 和前面的 <code>inferShapes</code> 以及 <code>inferLayout</code> 不一样。后两者是从输入信息推出输出的信息。而 <code>tileShardingSplit</code> 是由输出的的切分的因子来推断出各个输入的切分因子。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8d4fed659394922243574186cf74ef3a?method=download&amp;shareKey=d1461507273efadf9613b1496fd1501c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8d4fed659394922243574186cf74ef3a?method=download&amp;shareKey=d1461507273efadf9613b1496fd1501c" alt="BatchNorm ShardingInterface">
    </a><figcaption>BatchNorm ShardingInterface</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ShardingInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ShardingInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">description =</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="p">}]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">    let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*</span><span class="err">/</span><span class="p">[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="c">// vector for diff operand&#39;s info
</span></span></span><span class="line"><span class="cl"><span class="c"></span>            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::ShardingSplitParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;tileShardingSplit&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;ShardingSplitParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::SliceParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;temporalSliceShape&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;SliceParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">        InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*[{</span> 
</span></span><span class="line"><span class="cl">            <span class="p">}],</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;std::vector&lt;tx8be_mlr::WindowParam&gt;&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;backWindow&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;const WindowParam&#34;</span> <span class="p">:</span> <span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><ul>
<li>Sharding 是空间上的切分，意思是将数据分散到不同的 Tile 上。</li>
<li>Split 是时间上的切分，意思是切分到 Tile 上的将数据按流水线方式轮流进行 load.</li>
</ul>
<p><code>temporalSliceShape</code> 返回的是 sharding + split 后一个 Tile 上单次处理的数据的实际 shape.</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB4495c3579079b37d0c2288cc51408601?method=download&amp;shareKey=0513d93d3b51c4782d56c38acb83d0d5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB4495c3579079b37d0c2288cc51408601?method=download&amp;shareKey=0513d93d3b51c4782d56c38acb83d0d5" alt="BatchNorm Sharding Split">
    </a><figcaption>BatchNorm Sharding Split</figcaption></figure>
根据 batchnorm 算子定义 input 只能在通道维度上 sharding.
split 有两种选择</p>
<ol>
<li>对于 input 和 mean，var，scale，shift 都在 C 维度上做相同的切分。</li>
<li>不再 split mean，var，scale，shift，只对 input 的 NHW 进行 split.</li>
</ol>
<p>这里采用的是后者。由于 mean, variance, scale, shift 都是 1x1x1xC 的张量，因此 split 为 (1, 1, 1, 1). 切分搜索得到的符合要求的 ShardingSplitParam (下图中为 cn3) 会继续向上传递。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBc472f9ed0d922130ec05d93efed54186?method=download&amp;shareKey=8bf08e0631cd4c7880b9756969fd4bef" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBc472f9ed0d922130ec05d93efed54186?method=download&amp;shareKey=8bf08e0631cd4c7880b9756969fd4bef" alt="Sharding Split Search">
    </a><figcaption>Sharding Split Search</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ShardingSplitParam</span><span class="o">&gt;</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">tileShardingSplit</span><span class="p">(</span><span class="n">ShardingSplitParam</span> <span class="o">&amp;</span><span class="n">param</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">getOutput</span><span class="p">().</span><span class="n">getType</span><span class="p">().</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">ASSERT</span><span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">param</span><span class="p">.</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="n">param</span><span class="p">.</span><span class="n">outSplit</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape_size</span> <span class="o">=</span> <span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ShardingSplitParam</span><span class="o">&gt;</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">param</span><span class="p">);</span> <span class="c1">// input
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">shape_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// can only shard in dim C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">[</span><span class="n">shape_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// can only split except dim C
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSplit</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramMean</span><span class="p">;</span> <span class="c1">// scale/shift/mean/variance
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">paramMean</span><span class="p">.</span><span class="n">outSharding</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outSharding</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">paramMean</span><span class="p">.</span><span class="n">outSplit</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">shape_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// shape is 1x1x1xC，split must be (1, 1, 1, 1)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramVar</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramScale</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSplitParam</span> <span class="n">paramShift</span> <span class="o">=</span> <span class="n">paramMean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramScale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramShift</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramMean</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">result</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">paramVar</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="oplibinterface">OpLibInterface</h2>
<p><code>OpLibInterface</code> 有四个方法，</p>
<ul>
<li><code>genOpCode</code>: 生成 main.c 文件的时候所调用的一个接口。</li>
<li><code>getOpClockCycle</code>: 获取 OP 的执行时间。</li>
<li><code>getImmSpSize</code>: 获取 SPM 上临时空间所需要的大小。</li>
<li><code>queryOpAttr</code>: 查询这个 OP 的一些属性。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">def</span> <span class="nl">OpLibInterface</span> <span class="p">:</span> <span class="n">OpInterface</span><span class="o">&lt;</span><span class="s">&#34;OpLibInterface&#34;</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">description</span> <span class="o">=</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">        <span class="n">These</span> <span class="n">are</span> <span class="n">the</span> <span class="n">interfaces</span> <span class="k">for</span> <span class="n">connecting</span> <span class="n">tx8be</span><span class="o">-</span><span class="n">oplib</span>
</span></span><span class="line"><span class="cl">        <span class="n">and</span> <span class="n">codegen</span><span class="p">.</span>
</span></span><span class="line"><span class="cl">    <span class="p">}];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">cppNamespace</span> <span class="o">=</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">let</span> <span class="n">methods</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">generate</span> <span class="n">the</span> <span class="n">code</span> <span class="n">of</span> <span class="n">op</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;std::string&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;genOpCode&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span> <span class="s">&#34;OpCodeParam&#34;</span> <span class="o">:</span> <span class="err">$</span><span class="n">param</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">clock</span> <span class="n">cycle</span> <span class="n">of</span> <span class="n">the</span> <span class="n">op</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;uint64_t&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;getOpClockCycle&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">the</span> <span class="n">immediate</span> <span class="n">SPM</span> <span class="n">buffer</span> <span class="n">size</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;uint32_t&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="s">&#34;getImmSpSize&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">InterfaceMethod</span><span class="o">&lt;</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*desc=*/</span><span class="p">[{</span><span class="n">To</span> <span class="n">get</span> <span class="n">the</span> <span class="n">opAttr</span> <span class="n">info</span><span class="p">.}],</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*retType=*/</span><span class="s">&#34;tx8be_mlr::opAttr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*methodName=*/</span><span class="s">&#34;queryOpAttr&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="cm">/*args=*/</span><span class="p">(</span><span class="n">ins</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="o">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>其中 <code>queryOpAttr</code> 接口只需要在对应的接口里给 OpAttr 里的参数赋值。</p>
<ul>
<li><code>alignMode</code>: 算子的对齐要求，有Cx对齐要求，NCx 对齐要求，或者不在意存储格式的。</li>
<li><code>defaultLayout</code>: 算子默认的排布。</li>
<li><code>needPresetToNPU</code>: OP 是否需要进行预设到和硬件匹配的 layout. 当算子用到的指令是带有 NHWC 的配置时候的需要。</li>
<li><code>memInplace</code>: 输入和输出能否使用同一片内存。</li>
<li><code>needLoad</code>: 算子是否需要 load 操作，比如 mask, embedding 就不需要，会跳过loadvar op 生成。bit0 表示 arg idx0，bit1 表示 arg idx1，一共能表示 64 个输入情况。如果是const输入，loadconst 也会跳过codegen 不生成 code.
<blockquote>
<p>一个op可能有多个 input 都没有 load，shape 更新只用最后一个没有 load 的 operand (为 0 的最高位). 如 embedding 的 shape使用最后一个 operand，第一个是 weight 不用管 gshape. scatter有的有load，有的没有，shape 更新只看没有 load 的那个。</p></blockquote>
</li>
<li><code>needStore</code>:  数据是否需要进行 store 操作，会跳过store op 生成。</li>
<li><code>parallel</code>: 是否允许并行模式。</li>
<li><code>alignCx</code>: 最低维度切分是否到 64/128 (i8).</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">OpAttr</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ALIGN_MODE</span> <span class="n">alignMode</span><span class="p">{</span><span class="n">ALIGN_MODE</span><span class="o">::</span><span class="n">NPU_UNKNOWN</span><span class="p">};</span>  <span class="c1">// 算子的对齐要求，有Cx对齐要求，NCx对齐要求，或者不在意存储格式的
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">defaultLayout</span><span class="p">{</span><span class="s">&#34;Tensor&#34;</span><span class="p">};</span>           <span class="c1">// 算子默认的layout
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">needPresetToNPU</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>                   <span class="c1">// op是否需要进行预设到和硬件匹配的layout. 当算子用到的指令是带有 nhwc 的配置时需要
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="n">ENGINE_TYPE</span> <span class="n">engine</span><span class="p">{</span><span class="n">NPU_ENGINE_CT</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="n">memInplace</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>                        <span class="c1">// op的输入和输出能否使用同一片memory，比如add的out使用in0的
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint64_t</span> <span class="n">needLoad</span><span class="p">{</span><span class="mh">0xFFFFFFFFFFFFFFFF</span><span class="p">};</span>         <span class="c1">// 算子是否需要load操作
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint64_t</span> <span class="n">needStore</span><span class="p">{</span><span class="mh">0xFFFFFFFFFFFFFFFF</span><span class="p">};</span>        <span class="c1">// 数据是否需要进行store操作，会跳过store op生成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">parallel</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>                              <span class="c1">// 一般要使能并行模式，不过有的memory可能有问题，就不使能
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">alignCx</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>                               <span class="c1">// 最低维度切分是否到64/128(i8)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><p>batchnorm 允许输入 in 的 layout 为 Cx/NCx，要在 mlir 层的 <code>queryOpAttr()</code> 里将 alignMode 设置为NPU_ALIGN, 维度为 2/3/4，数据类型为 bf16/fp16/fp32/tf32. 其他输入的格式为 fp32. 输出的维度和类型与 in 保持一致。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">OpAttr</span> <span class="n">tx8be</span><span class="o">::</span><span class="n">BatchNorm_InferenceOp</span><span class="o">::</span><span class="n">queryOpAttr</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">OpAttr</span> <span class="n">attr</span><span class="p">;</span>  <span class="c1">// 创建一个 OpAttr 对象
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">attr</span><span class="p">.</span><span class="n">alignMode</span> <span class="o">=</span> <span class="n">ALIGN_MODE</span><span class="o">::</span><span class="n">NPU_ALIGN</span><span class="p">;</span>  <span class="c1">// 设置对齐模式为 NPU_ALIGN
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">attr</span><span class="p">.</span><span class="n">needPresetToNPU</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>  <span class="c1">// 设置需要预设到 NPU
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 获取 in 的形状，并判断其第一个维度是否为 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">batch</span> <span class="o">=</span> <span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getShape</span><span class="p">()[</span><span class="mi">0</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">attr</span><span class="p">.</span><span class="n">defaultLayout</span> <span class="o">=</span> <span class="n">batch</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">?</span> <span class="s">&#34;Tensor&#34;</span> <span class="o">:</span> <span class="s">&#34;NTensor&#34;</span><span class="p">;</span>  <span class="c1">// 根据 batch 的值设置默认布局
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">attr</span><span class="p">;</span>  
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>如下图所示，后端编译器会调用 genOpCode 生成相对应的 main.c. 然后 host.cpp 再把 main.c 放到不同的平台上面去编译完再去执行。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBdb6a91e8bb45cbce292f6fdf1fafd0f4?method=download&amp;shareKey=91e74089c887f70b2b508d9a31b877fd" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBdb6a91e8bb45cbce292f6fdf1fafd0f4?method=download&amp;shareKey=91e74089c887f70b2b508d9a31b877fd" alt="OpLibInterface">
    </a><figcaption>OpLibInterface</figcaption></figure></p>
<p>main.c 主要做的就是 load &ndash;&gt; compute &ndash;&gt; store 这三步。伪代码如下，由于进行了时间上的 split，需要循环多次才能读取完整的数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">while</span><span class="p">(</span><span class="o">!</span><span class="n">input_done</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// load
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_dma_load</span> <span class="n">Input</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">input_done</span> <span class="o">=</span> <span class="n">Input</span><span class="p">.</span><span class="n">load_finish</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">scale</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">shift</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">mean</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">op_dma_load</span> <span class="n">varience</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// compute
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_batchnorm_inference</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">varience</span><span class="p">,</span> <span class="n">out</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// store
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">op_store_var_ncx</span> <span class="n">out</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>op_batchnorm_inference</code> 的定义如下，其中 imm 是辅助空间，此处申请了 2xsizeof(input) Bytes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">uint64_t</span> <span class="nf">op_batchnorm_inference</span><span class="p">(</span><span class="n">BATCHNORM_INFER_PARAM</span> <span class="o">*</span><span class="n">param</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">TSR</span> <span class="o">*</span><span class="n">in</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">scale</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">shift</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">mean</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">var</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">TSR</span> <span class="o">*</span><span class="n">imm</span><span class="p">,</span> <span class="n">TSR</span> <span class="o">*</span><span class="n">out</span><span class="p">);</span>
</span></span></code></pre></div><p>其中 TSR 是一个自定义的结构体，包括数据格式，地址以及一个 L_shape (load shape). 里面记录了张量完整的大小 shape_whole，以及本 Tile 上每个维度起始下标 shape_start，每个维度加载的大小 shape_slice 和 shape 的维度大小 dim.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">L_SHAPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape_whole</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// the whole shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_start</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// start idx of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_slice</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// length of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">shape_real</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>   <span class="c1">// real length of the shape slice
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">dim</span><span class="p">;</span>                         <span class="c1">// dimension of the shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span> <span class="n">L_SHAPE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">G_SHAPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">spatial_start</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>  <span class="c1">// [start, end]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">spatial_end</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">dynamic_offset</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">shape</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">dim</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int32_t</span> <span class="n">done</span><span class="p">;</span>                         <span class="c1">// done for dma load finish
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int32_t</span> <span class="n">batch_offset</span><span class="p">[</span><span class="n">MAX_SHAPE_DIM</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">G_SHAPE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TSR</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Data_Format</span> <span class="n">format</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint64_t</span> <span class="n">addr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">L_SHAPE</span><span class="o">*</span> <span class="n">shape</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TSR</span><span class="p">;</span>
</span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB863424dc56bf5d86b25f817e06a1c716?method=download&amp;shareKey=de577bac9b2b5b108c4a3e8a275d07ea" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB863424dc56bf5d86b25f817e06a1c716?method=download&amp;shareKey=de577bac9b2b5b108c4a3e8a275d07ea" alt="BatchNorm Design">
    </a><figcaption>BatchNorm Design</figcaption></figure></p>
<p>对于非 fp32 类型数据 (以 fp16 为例) 计算过程与空间分配如下图所示。</p>
<ol>
<li>类型转换成 fp32: gatherScatter.</li>
<li>调用 fp16-&gt;fp32 函数进行转换。</li>
<li>循环计算 x-Mean (因为对 in 的 NHW 维度进行了 split)，结果存入 imm_a.</li>
<li>Varience 自加 epsilon(1e-6).</li>
<li>Varience 进行 rsqrt 操作。</li>
<li>Varience 与 x-Mean 进行循环乘。</li>
<li>循环乘 scale.</li>
<li>循环加 shift.</li>
<li>fp32 转回 f16.</li>
<li>gatherScatter 到 out 处。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB427f45ff2571bf94eea6a0b81f897ba1?method=download&amp;shareKey=57a2c93fd8b252f86c47c8e71e325f2c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB427f45ff2571bf94eea6a0b81f897ba1?method=download&amp;shareKey=57a2c93fd8b252f86c47c8e71e325f2c" alt="Batchnorm Computation Flow">
    </a><figcaption>Batchnorm Computation Flow</figcaption></figure></p>
<p>这里需要注意的是 shift(1, 1, 1, C) 和归一化后的 x(N, H, W, C) 相乘的时候，这时候就用到了之前所说的 VuV_mul 和 VuV_mul_loop 指令。</p>
<p>当 C &lt;= 32 时，一个 batch 内的数据排布如下 (以 (4x112x2x30) x (1x1x1x30) 为例)，此时我们在 batch 维度上循环调用 VuV_mul 指令就可以。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB7e47cbd16c9c6777c91a22a0c2685f91?method=download&amp;shareKey=c305ac14d29df963a8884def061ef96f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB7e47cbd16c9c6777c91a22a0c2685f91?method=download&amp;shareKey=c305ac14d29df963a8884def061ef96f" alt="Channel &lt;= 32">
    </a><figcaption>Channel &lt;= 32</figcaption></figure></p>
<p>当 C &gt; 32 时，需要向 64 对齐，一个 batch 内的数据排布如下 (以 (4x112x2x129) x (1x1x1x129) 为例)，每一个 Cx/C0 对应着一次 VuV_mul. 此时我们在 batch 维度上循环调用 VuV_mul_loop 指令就可以。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBfacfc3b8b6722744fa958775ab8a88f4?method=download&amp;shareKey=968a25dfbb972d2e97c7b09f284733be" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBfacfc3b8b6722744fa958775ab8a88f4?method=download&amp;shareKey=968a25dfbb972d2e97c7b09f284733be" alt="Channel &gt; 32">
    </a><figcaption>Channel &gt; 32</figcaption></figure></p>
<p>下面来说明如何调用指令，首先要明确调用的指令是属于哪一个模块的。例如第四步加 epsilon 我们需要调用 addVs 指令，其属于 CGRA 模块。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">enum</span> <span class="nc">OP_INSTR_TYPE</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_CGRA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_NEUR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_RDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_WDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_TDMA</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_SCALAR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_DTE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">I_CSR</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">OP_INSTR_TYPE</span><span class="p">;</span>
</span></span></code></pre></div><p>每个模块下的指令有自己的参数形式，下面列举一些。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// I_CGRA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">CT_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_CT_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_CT_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">CT_Param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_NEUR
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TsmNeInstr</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_NE_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_NE_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TsmNeInstr</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_(R/W)DMA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">DMA_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_DMA_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_DMA_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">DMA_Param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// I_TDMA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">typedef</span> <span class="k">struct</span> <span class="nc">TD_Param</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">inter_type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_TDMA_GR_Ctl_Regs</span> <span class="n">ctrl</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">Ncc_TDMA_GR_Param_Regs</span> <span class="n">param</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">TD_Param</span><span class="p">;</span>
</span></span></code></pre></div><p>还是以 AddVS 指令为例，流程如下</p>
<ol>
<li>声明模块的指令参数。</li>
<li>声明对应的指令类型指针，AddVS 属于 arith 类型的指令。getTsmOpPointer()-&gt;arith_pointer;`.</li>
<li>根据调用指令传入参数，指令会根据传入参数配置好 ct_param 上寄存器的值。然后再进行 TsmExecute. 最后再把单词指令的执行时间进行累加。</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">CT_Param</span> <span class="n">ct_param</span> <span class="o">=</span> <span class="p">{</span><span class="n">I_CGRA</span><span class="p">,</span> <span class="p">{</span><span class="mi">0</span><span class="p">},</span> <span class="p">{</span><span class="mi">0</span><span class="p">}};</span>  <span class="c1">// step 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">TsmArith</span> <span class="o">*</span><span class="n">arith</span> <span class="o">=</span> <span class="p">(</span><span class="n">TsmArith</span> <span class="o">*</span><span class="p">)</span><span class="n">getTsmOpPointer</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">arith_pointer</span><span class="p">;</span>  <span class="c1">// step 2
</span></span></span><span class="line"><span class="cl"><span class="c1">// variance add epsilon
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">float</span> <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">arith</span><span class="o">-&gt;</span><span class="n">addVS</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ct_param</span><span class="p">,</span>  <span class="c1">// engine params
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">varAddr</span><span class="p">,</span>  <span class="c1">// vector address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="o">*</span><span class="p">(</span><span class="kt">uint32_t</span> <span class="o">*</span><span class="p">)(</span><span class="o">&amp;</span><span class="n">epsilon</span><span class="p">),</span>  <span class="c1">// scalar address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">varAddr</span><span class="p">,</span>  <span class="c1">// result address
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">mid_tensor_info</span><span class="p">.</span><span class="n">total_num</span><span class="p">,</span>  <span class="c1">// vector elements num
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">RND_NEAREST_EVEN</span><span class="p">,</span>  <span class="c1">// round method
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">Fmt_FP32</span><span class="p">);</span>  <span class="c1">// data format
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">cycle_single</span> <span class="o">=</span> <span class="n">TsmExecute</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ct_param</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">cycle_total</span> <span class="o">=</span> <span class="n">ADD_VALID_CYCLE</span><span class="p">(</span><span class="n">cycle_total</span><span class="p">,</span> <span class="n">cycle_single</span><span class="p">);</span>
</span></span></code></pre></div><h2 id="computeinteface">ComputeInteface</h2>
<p><code>ComputeInterface</code> 这个接口主要是每个 OP 通过 onednn 得到 CPU 代码。或者计算比较简单的 OP 如果在 onednn 的接口中没有找到对应的计算，也可以在 compute 接口中手写当前 OP 的 CPU 实现的 C++代码。最终生成结果会用来检验算子正确性。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-mlir" data-lang="mlir"><span class="line"><span class="cl">def ComputeInterface <span class="p">:</span> OpInterface<span class="p">&lt;</span><span class="s">&#34;ComputeInterface&#34;</span><span class="p">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">description =</span> <span class="p">[]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">cppNamespace =</span> <span class="s">&#34;::tx8be_mlir&#34;</span><span class="err">;</span>
</span></span><span class="line"><span class="cl">  let <span class="nl">methods =</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    InterfaceMethod<span class="p">&lt;</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">desc=</span><span class="p">*</span><span class="err">/</span><span class="p">[],</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">retType=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;::mlir::LogicalResult&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">methodName=</span><span class="p">*</span><span class="err">/</span><span class="s">&#34;compute&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="err">/</span><span class="p">*</span><span class="nl">args=</span><span class="p">*</span><span class="err">/</span><span class="p">(</span>ins <span class="s">&#34;ComputeParam&amp;&#34;</span><span class="p">:</span><span class="err">$</span>param<span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">&gt;,</span>
</span></span><span class="line"><span class="cl">  <span class="p">]</span><span class="err">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h1 id="test-case">Test Case</h1>
<p>TestCase 主要作用是写单算子或者多个 (单算子的上下文算子) 的测试，包括固定配置测试和随机配置测试,随机配置时主要对于算子支持的不同 dim, layout, dtype, shape 这四项做随机。流程主要做以下几件事。</p>
<p><strong>init_param</strong></p>
<p>通过数组来配置固定测试 case 或者随机测试范围，然后通过指定或随机的方式生成对应的输入，输出的 shape， dim 信息，除此之外参与随机的一般还包括数据对齐方式随机，数据类型随机，即在算子可支持的范围内产生随机的 FP16/FP32 不同的数据类型来保证测试的充分和全面。</p>
<p>除此之外还会生成 MLIR Module. 这个 module 是原来就给定的，在这里做的事情是首先新建一个空的 func. 然后在这个 func 中构造一个 block，里面去填入需要测试的这些 OP 的结构。</p>
<ul>
<li>Module：一个程序的容器，包含多个函数。</li>
<li>Func：定义一个函数，包含多个 Block.</li>
<li>Block：定义函数的基本执行单元，包含多个 Operation.</li>
<li>Operation：表示具体的计算或操作，是程序中的基本指令。</li>
</ul>
<h2 id="mlir-structure">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBfdb91efc229999a9b488d5131959f4b0?method=download&amp;shareKey=a3935de13b565d4c37e06857c6c43f90" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBfdb91efc229999a9b488d5131959f4b0?method=download&amp;shareKey=a3935de13b565d4c37e06857c6c43f90" alt="MLIR Structure">
    </a><figcaption>MLIR Structure</figcaption></figure></h2>
<p>init_data</p>
<p>这个方法主要用来通过上面 Param 生成的 dim、输入或者输出 shape、数据类型来生成随机的数据，数据范围一定要根据算子情况配置，不然无效数值可能会在结果中出现 Nan. 还要考虑一些算子的特点，保证测试的充分性，例如创建 relu 的数据时，最好正负值都有覆盖。</p>
<hr>
<p>compile</p>
<p>compile 方法有两个功能</p>
<ol>
<li>调用 Computelnterface 生成 onednn 或者手写 CPU 算子实现的结果。</li>
<li>添加一些配置参数，跑出 tx8be mlir codegen 的结果。这其中会经历一些非常复杂的 pass，稍后再介绍。</li>
</ol>
<hr>
<p>saveInfoFile</p>
<p>saveInfoFile 方法主要是把创建出的 Data 数据写成.bin 文件保存。并把创建出的 module 的信息保存在 json 文件。</p>
<h1 id="overview-of-workflow">Overview of Workflow</h1>
<p>后端接收的是 MLIR 的计算图，然后经过编译器后端的处理，然后生成最后的 BE IR，其中中包含了一些 Oplib 的算子。最终这个 BEIR 会调用 OP 的算子，然后去跑在 C model 或者是实际的硬件芯片上面。后端编译器主要负责四个方面
layout 初始化和传递、const 管理、切分策略及其 SPM 分配和 DDR 分配。</p>
<h1 id="layout-initialization-and-pass">Layout Initialization and Pass.</h1>
<p>layout 可以分为以下几种</p>
<ul>
<li>layout_str: 中端使用
<ul>
<li>CNN Op: 1. Feature (NCHW/NHWC) etc. 2. Weight (OIHW/HWOI) etc.</li>
<li>Non-CNN Op: 大模型中常见，Tensor/NTensor，它们的区别是第 0 维是否为 1.</li>
</ul>
</li>
<li>mem_layout: 后端使用，代表了在芯片上的实际排布
<ul>
<li>Tensor/NTensor: 数据的紧密排布</li>
<li>Cx/NCx: 对 Tensor/NTensor 格式化后的结果，方便易硬件读取。</li>
</ul>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th>dtype</th>
          <th>channel</th>
          <th>description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>bf16/fp16 <br>/fp32/tf32</td>
          <td>c &lt;= 32</td>
          <td>NHWC, C向4/8/16/32对齐，N 的起始地址向 2048bit 对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 32</td>
          <td>N[CxHW64, HWC0], C0 向 4/8/16/32 对齐，N 的起始地址向2048bit 对齐<br>在一个 batch 内将 tensor 按 C 分成 Cx*64 和 C0两部分</td>
      </tr>
      <tr>
          <td>int8</td>
          <td>c &lt;= 64</td>
          <td>NHWC, C 向 4/8/16/32/64对齐，N的起始地址向2048bit对齐</td>
      </tr>
      <tr>
          <td></td>
          <td>c &gt; 64</td>
          <td>N[CxHW128, HWC0], C0 向 4/8/16/32/64 对齐，N的起始地址向 2048bit 对齐 <br> 在一个 batch 内将 tensor 按 C 分成 Cx*128 和C0 两部分</td>
      </tr>
  </tbody>
</table>
<h2 id="layoutinitpass">layoutInitPass</h2>
<p>layoutInitPass 用于初始化计算图中 GemmOP 和 ConvOP 的 layout_str，其他的所有算子 layout_str 都设置为 UNKNOWN. 下图中的 <code>GemmOP layout_str = &quot;Tensor-Tensor-Tensor&quot;</code> 分别表示两个输入和输出的数据排布。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB95a6e97d5ae8432b8367189a36987f31?method=download&amp;shareKey=8df50a04c4e2ed29be9e93d3da958e35" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB95a6e97d5ae8432b8367189a36987f31?method=download&amp;shareKey=8df50a04c4e2ed29be9e93d3da958e35" alt="LayoutStr">
    </a><figcaption>LayoutStr</figcaption></figure></p>
<h2 id="layouttransmitpass">layoutTransmitPass</h2>
<p>layoutTransmitPass 会用已知的 GemmOP 和 ConvOP layout 信息进行扩散，得到全图的 layout_str.</p>
<ol>
<li>每个算子初始化为一个节点，有inputNodes容器和outputNodes容器分别存放自己的输入和输出节点。</li>
<li>GemmOp 和 ConvOp 作为起始节点，向前和向后推导 layout (算子的 <code>inferlayout()</code> 接口)，新推出layout 的节点作为下一批起始节点递归推导。</li>
<li>遇到无法推导的节点 (如 Reshape，BroadCast) 则终止推导。将其余无法推导的节点 layout 直接初始化为 Tensor.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9d187e7b17a4b2ee65f01169f8c6a141?method=download&amp;shareKey=c405975eba9c1da95539f89ac8b3de8a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9d187e7b17a4b2ee65f01169f8c6a141?method=download&amp;shareKey=c405975eba9c1da95539f89ac8b3de8a" alt="layoutTransmitPass">
    </a><figcaption>layoutTransmitPass</figcaption></figure></p>
<h2 id="layoutaligntonpupass">layoutAlignToNpuPass</h2>
<p>layoutAlignToNpuPass 用于在数据对齐冲突的地方插入 channelNorm，并将 layout_str 映射到 mem_layout. 在 NPU 上某些算子只支持 <code>COMPACT</code> layout，有些只支持 <code>ALIGN</code> layout，有些则都可以 <code>BOTH</code>.</p>
<ol>
<li>输入默认非对齐排布，从输入出发遍历整图，检查当前算子与其所有 user 之间的对齐要求，若冲突，记录插入点 (算子的对齐要求可以在 <code>OpLibInterface</code> 接口中的 <code>queryOpAttr()</code> 方法中查询到).</li>
<li>根据记录的插入点，再次分析插入点前后的算子对齐要求，以确定channelnorm的方向，插入 channelnorm.</li>
<li>赋值 <code>dev_info</code>，将 <code>layout_str</code> 映射到 <code>mem_layout</code>.</li>
</ol>
<blockquote>
<p>dev_info用来描述数据在设备上的一些属性，有成员：imm_size (辅助空间大小), mem_layout, temporal_mem_slice, imm_addr, mem_addr.</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB41f3c705e0f54e9291a5a2a7916f6045?method=download&amp;shareKey=e9af8cbf6c2e348e4584018bcb4d4782" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB41f3c705e0f54e9291a5a2a7916f6045?method=download&amp;shareKey=e9af8cbf6c2e348e4584018bcb4d4782" alt="layoutAlignToNpuPass">
    </a><figcaption>layoutAlignToNpuPass</figcaption></figure></p>
<p>LayoutAlignOptPass 应用几个 RewritePattern 用于删除冗余的 channelnorm.</p>
<ol>
<li><strong>ConstChannelNormErase</strong>: ConstantOp 维度为 1 并且只有 1 个 user 的时候可以删去并且将 devInfolayout 设置为 Cx.</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">ConstChannelNormErase Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// const can be directly considered to be aligned
</span></span></span><span class="line"><span class="cl"><span class="c1">// constop(dim &lt; 2) -&gt; channelNorm -&gt; constop
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">ConstChannelNormErase</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ConstChannelNormErase</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="mi">1</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">txbe</span><span class="o">::</span><span class="n">ConstantOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// If const has multi user, can not erase
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">hasOneUse</span><span class="p">())</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">user</span> <span class="o">=</span> <span class="o">*</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">shape</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">userVec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">userVec</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">userVec</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">user</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">user</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">channelNormUser</span> <span class="p">:</span> <span class="n">userVec</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">channelNormUser</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">user</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// set align=true
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getLayoutStr</span><span class="p">().</span><span class="n">str</span><span class="p">(),</span> <span class="nb">true</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">user</span><span class="o">-&gt;</span><span class="n">use_empty</span><span class="p">())</span> <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<details class="custom-details">
    <summary class="custom-summary">RedudantChannelnormErase Implementation</summary>
    <div><ol start="2">
<li><strong>RedudantChannelnormErase</strong>: 如果该 channelnormOp 的输入是来自一个 constOp 并且只有一个输出，则检查是否还有其他的 channelnormOp 也使用。如果是，则让它们直接使用该 channelnormOp 的结果，以消除多余的 channelnormOp.</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// A pass to erase redundant channel normalization operations
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">struct</span> <span class="nc">RedundantChannelNormErase</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">RedundantChannelNormErase</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">)</span> <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="mi">1</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span> <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span> <span class="n">op</span><span class="p">,</span> <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Define the input operation and its defining operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// def represents the operation that generates the op input data
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">def</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getInput</span><span class="p">().</span><span class="n">getDefiningOp</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Check if the defining operation is a ConstantOp and has more than one result
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">def</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">def</span><span class="o">-&gt;</span><span class="n">getNumResults</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span> <span class="c1">// Fail if conditions are not met
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Get the size in bits of the input shape
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">size</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getInput</span><span class="p">().</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getSizeInBits</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">Operation</span> <span class="o">*</span><span class="n">sameOp</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">;</span> <span class="c1">// Pointer to a potentially redundant operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Iterate over all users of the defining operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">def</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">user</span> <span class="o">==</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Skip if the user is the current operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span> <span class="c1">// Check if the user is another ChannelNormOp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">sameOp</span> <span class="o">=</span> <span class="n">user</span><span class="p">;</span> <span class="c1">// Store the redundant operation
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">sameOp</span><span class="p">)</span> <span class="k">return</span> <span class="n">mlir</span><span class="o">::</span><span class="n">failure</span><span class="p">();</span> <span class="c1">// Fail if no redundant operation is found
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Replace all uses of the redundant operation with the current operation&#39;s results
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">op</span><span class="o">-&gt;</span><span class="n">replaceAllUsesWith</span><span class="p">(</span><span class="n">sameOp</span><span class="o">-&gt;</span><span class="n">getOpResults</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">use_empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Erase the current operation if it has no more uses
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span> <span class="c1">// Return success if the rewrite is completed
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div></div>
</details><br>
<h1 id="const-management">Const Management</h1>
<p>常量统一使用 <code>ConstContainer</code> 类来进行管理。通过 map 来记录每个常量对应的 ParamInfo. 一个常量可能被分配到多个芯片上，每个芯片上数据可能相同，也可能不同。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ParamInfo</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;*</span> <span class="n">data_ptr</span><span class="p">;</span>  <span class="c1">// const value
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">set</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">chip_id</span><span class="p">;</span>  <span class="c1">// which chips has this const, -1 indicates all chip has the same param.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint32_t</span> <span class="n">label</span><span class="p">;</span>  <span class="c1">// Indicates whether the data is assigned to a certain chip_id. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// class ConstContainer {
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">class</span> <span class="nc">ConstContainer</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">ConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">virtual</span> <span class="o">~</span><span class="n">ConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// some public functions
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="k">private</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">ParamInfo</span><span class="o">&gt;&gt;</span> <span class="n">_data</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="p">,</span> <span class="kt">uint64_t</span><span class="o">&gt;&gt;</span> <span class="n">oidToSize</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="p">,</span> <span class="kt">uint32_t</span><span class="o">&gt;</span> <span class="n">oidToNid</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="moveconstantpass">MoveConstantPass</h2>
<p>MoveConstantPass: 创建图的 <code>ConstContainer</code>，然后应用 <code>ConstantToLoadConst</code> Rewrite Pattern. 转换完成后会调用 <code>updateConstContainer</code> 更新 <code>ConstContainer</code> 各个 const 的 ID. 用一个大小为 <code>4*1024*tile_num</code> (DDR_BANK_SIZE) <code>thresholdSize</code> 将大于这个值的 const 全部放在前面，小的放在后面。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">MoveConstantPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// create constant container
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">createConstContainer</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// get module op
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">ModuleOp</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="n">RewritePatternSet</span> <span class="nf">patterns</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">patterns</span><span class="p">.</span><span class="n">insert</span><span class="o">&lt;</span><span class="n">ConstantToLoadConst</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="n">FrozenRewritePatternSet</span> <span class="n">frozen_patterns</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">FrozenRewritePatternSet</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">patterns</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Set config
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">GreedyRewriteConfig</span> <span class="n">config</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">config</span><span class="p">.</span><span class="n">useTopDownTraversal</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">func</span> <span class="p">:</span> <span class="n">module</span><span class="p">.</span><span class="n">getOps</span><span class="o">&lt;</span><span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span><span class="o">&gt;</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Region</span> <span class="o">&amp;</span><span class="n">body</span> <span class="o">=</span> <span class="n">func</span><span class="p">.</span><span class="n">getBody</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">failed</span><span class="p">(</span><span class="n">applyPatternsAndFoldGreedily</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">frozen_patterns</span><span class="p">,</span> <span class="n">config</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">llvm</span><span class="o">::</span><span class="n">errs</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed when move const in main graph.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">subgraph</span> <span class="p">:</span> <span class="n">module</span><span class="p">.</span><span class="n">getOps</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="o">&gt;</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">Region</span> <span class="o">&amp;</span><span class="n">body</span> <span class="o">=</span> <span class="n">subgraph</span><span class="p">.</span><span class="n">getBody</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">failed</span><span class="p">(</span><span class="n">applyPatternsAndFoldGreedily</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">frozen_patterns</span><span class="p">,</span> <span class="n">config</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="n">llvm</span><span class="o">::</span><span class="n">errs</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Failed when move const in subgraph.</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">signalPassFailure</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">TileInfo</span> <span class="n">tinfo</span> <span class="o">=</span> <span class="n">get_tileinfo</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">updateConstContainer</span><span class="p">(</span><span class="n">tinfo</span><span class="p">.</span><span class="n">tile_num</span><span class="p">);</span>  <span class="c1">// update id by thresholdSize
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">updateLdConstop</span><span class="p">();</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>ConstantToLoadConst</code> 首先通过分析该常量的所有 users，来判断这个常量是否需要 LoadConstOp. 如果需要加载，它会将原始常量的数据注册到一个全局容器中并获得一个 ID，然后创建一个新的 LoadConstOp ，并将此 ID 及其他硬件属性赋予它。接着，它会更新所有使用者，将它们的输入从旧的 ConstantOp 重定向到这个新的 LoadConstOp，最后再删除无用的原始常量。最后再更新所有 const 的 ID.</p>
<details class="custom-details">
    <summary class="custom-summary">ConstantToLoadConst Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ConstantToLoadConst</span> <span class="o">:</span> <span class="k">public</span> <span class="n">mlir</span><span class="o">::</span><span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">ConstantToLoadConst</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">MLIRContext</span> <span class="o">*</span><span class="n">context</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="o">:</span> <span class="n">OpRewritePattern</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="cm">/*benefit=*/</span><span class="p">)</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">mlir</span><span class="o">::</span><span class="n">LogicalResult</span>
</span></span><span class="line"><span class="cl">  <span class="n">matchAndRewrite</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ConstantOp</span> <span class="n">op</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">mlir</span><span class="o">::</span><span class="n">PatternRewriter</span> <span class="o">&amp;</span><span class="n">rewriter</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">uint32_t</span> <span class="n">id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Store constant data to constant container 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Determine if this constant operation needs an explicit load instruction.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">needLoad</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">v</span> <span class="o">=</span> <span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Iterate over all operations that use this output value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user_op</span> <span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Get the argument index of the user op that corresponds to our output value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="kt">int32_t</span> <span class="n">arg_idx</span> <span class="o">=</span> <span class="n">getArgumentIdx</span><span class="p">(</span><span class="n">user_op</span><span class="p">,</span> <span class="n">v</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Assert that the user operation implements our custom OpLibInterface.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">ASSERT</span><span class="p">(</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Get the library attributes for this user operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">opAttr</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">).</span><span class="n">queryOpAttr</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Skip if the user is a TupleOp, which might have special handling.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">TupleOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user_op</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">opAttr</span><span class="p">.</span><span class="n">needLoad</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">arg_idx</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// Check if the &#39;needLoad&#39; attribute
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">needLoad</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">ASSERT</span><span class="p">(</span><span class="n">needLoad</span> <span class="o">==</span> <span class="nb">false</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Set attributes
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Safely iterate over the users. This is important because we are modifying the use-list inside the loop.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">use</span> <span class="p">:</span> <span class="n">llvm</span><span class="o">::</span><span class="n">make_early_inc_range</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">().</span><span class="n">getUses</span><span class="p">()))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">Operation</span> <span class="o">*</span><span class="n">userOp</span> <span class="o">=</span> <span class="n">use</span><span class="p">.</span><span class="n">getOwner</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Create the new, hardware-specific LoadConst operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">txbe</span><span class="o">::</span><span class="n">LoadConstOp</span> <span class="n">newLoadConst</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">            <span class="n">rewriter</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">txbe</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">op</span><span class="p">.</span><span class="n">getOutput</span><span class="p">().</span><span class="n">getType</span><span class="p">(),</span> <span class="n">ValueRange</span><span class="p">{},</span> <span class="n">attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">needLoad</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// this constant does not need an explicit load... 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Get a builder to set attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">OpBuilder</span> <span class="nf">builder</span><span class="p">(</span><span class="n">newLoadConst</span><span class="p">.</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Set a &#39;bypasscodegen&#39; attribute, signaling special handling for this op in later stages.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">newLoadConst</span><span class="p">.</span><span class="n">getOperation</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;bypasscodegen&#34;</span><span class="p">,</span> <span class="n">builder</span><span class="p">.</span><span class="n">getBoolAttr</span><span class="p">(</span><span class="nb">true</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Set the layout string attribute on the new LoadConst op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">newLoadConst</span><span class="o">-&gt;</span><span class="n">setAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">,</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getAttr</span><span class="p">(</span><span class="s">&#34;layout_str&#34;</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// CRITICAL STEP: Rewire the user&#39;s operand to point to the result of the new LoadConst op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">userOp</span><span class="o">-&gt;</span><span class="n">setOperand</span><span class="p">(</span><span class="n">use</span><span class="p">.</span><span class="n">getOperandNumber</span><span class="p">(),</span> <span class="n">newLoadConst</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// After all uses have been replaced, erase the original, now-dead ConstantOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">rewriter</span><span class="p">.</span><span class="n">eraseOp</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nf">success</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<h2 id="constnormpass">constNormPass</h2>
<p>constNormPass: 遍历图中的 LoadConstOp. 它会寻找一个特定的模式：如果一个 LoadConstOp 的唯一 user 是一个 ChannelNormOp，那么会通过 <code>constChannelNormErase</code> 函数进行消除和将对其信息同步到 LoadConstOp. 最后通过 <code>processMultiUse</code> 确保所有加载同一个底层常量数据的 LoadConstOp 实例，都具有完全相同的内存布局。</p>
<details class="custom-details">
    <summary class="custom-summary">ConstNormPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ConstNormPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ModuleOp</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">mainGraphFunc</span> <span class="o">=</span> <span class="n">getMainFuncOp</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span> <span class="o">*&gt;</span> <span class="n">deletedChannelnorm</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Walk the main function to find a specific pattern: LoadConst -&gt; ChannelNorm.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">users</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">            <span class="kt">bool</span> <span class="n">flag</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Check if any user is a ChannelNormOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="n">flag</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1">// If the LoadConst has exactly one user, and that user is a ChannelNormOp,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// mark the ChannelNormOp for deletion.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">(</span><span class="n">flag</span> <span class="o">&amp;&amp;</span> <span class="n">users</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="c1">// The erase logic is commented out, maybe handled by constChannelNormErase or done later.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                    <span class="n">deletedChannelnorm</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">it</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Erase all the marked ChannelNormOps. This is done in a separate loop
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// to avoid iterator invalidation issues.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">deletedChannelnorm</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">op</span><span class="o">-&gt;</span><span class="n">erase</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Set up and run a nested pass pipeline.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">OpPassManager</span> <span class="nf">thisPM</span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">getOpName</span><span class="p">().</span><span class="n">value</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// This pipeline will only apply to LoadConstOp operations inside functions.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">OpPassManager</span> <span class="o">&amp;</span><span class="n">loadConstOpPM</span> <span class="o">=</span> <span class="n">thisPM</span><span class="p">.</span><span class="n">nest</span><span class="o">&lt;</span><span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span><span class="o">&gt;</span><span class="p">().</span><span class="n">nest</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Add the ConstNormDoPass to the pipeline.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">loadConstOpPM</span><span class="p">.</span><span class="n">addPass</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">ConstNormDoPass</span><span class="o">&gt;</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Run the newly constructed pipeline on the module.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">result</span> <span class="o">=</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">runPipeline</span><span class="p">(</span><span class="n">thisPM</span><span class="p">,</span> <span class="n">getOperation</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// After the pipeline, run a final cleanup/consistency check function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">processMultiUse</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// change unpack input0 qweight shape after ConstNormDoPass. (Original comment)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// This logic is likely inside the runOnOperation() method of ConstNormDoPass.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="c1">// Collect all users of this LoadConstOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">std</span><span class="o">::</span><span class="n">unordered_set</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">users</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">          
</span></span><span class="line"><span class="cl">          <span class="c1">// Check if any user is an UnpackOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="kt">bool</span> <span class="n">flag</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">UnpackOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="n">flag</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                  <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// If there is exactly one user, and it&#39;s an UnpackOp...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="n">flag</span> <span class="o">&amp;&amp;</span> <span class="n">users</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">users</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// This check seems to ensure we are modifying the correct operand.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="k">if</span> <span class="p">(</span><span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">it</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// Get the original shape and type.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="p">,</span> <span class="mi">6</span><span class="o">&gt;</span> <span class="n">oShape</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                  <span class="k">auto</span> <span class="n">type</span> <span class="o">=</span> <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">                  <span class="k">auto</span> <span class="n">shape</span> <span class="o">=</span> <span class="n">type</span><span class="p">.</span><span class="n">getShape</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                  <span class="c1">// Apply the shape transformation: e.g., for unpacking packed data.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">oShape</span><span class="p">.</span><span class="n">push_back</span><span class="p">((</span><span class="kt">int32_t</span><span class="p">)</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mi">4</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                  <span class="n">oShape</span><span class="p">.</span><span class="n">push_back</span><span class="p">((</span><span class="kt">int32_t</span><span class="p">)</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">                  <span class="c1">// Create a new tensor type with the new shape.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="k">auto</span> <span class="n">oType</span> <span class="o">=</span> <span class="n">mlir</span><span class="o">::</span><span class="n">RankedTensorType</span><span class="o">::</span><span class="n">get</span><span class="p">(</span><span class="n">oShape</span><span class="p">,</span> <span class="n">type</span><span class="p">.</span><span class="n">getElementType</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// Update the type of the LoadConstOp&#39;s result in-place.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">constOp</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">setType</span><span class="p">(</span><span class="n">oType</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">                  <span class="p">}</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>constChannelNormErase</code> 处理 LoadConstOp -&gt; ChannelNormOp 这种模式。让所有原本使用 ChannelNormOp 计算结果的操作，现在改为直接使用 ChannelNormOp 的输入数据。获取 LoadConstOp 当前的设备信息和 layout，计算出一个新的经过对齐的布局 <code>align_dev_layout</code>，然后用这个新布局去更新 LoadConstOp.</p>
<details class="custom-details">
    <summary class="custom-summary">constChannelNormErase Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// This function erases a ChannelNormOp by bypassing it and updating the source constant&#39;s layout.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">constChannelNormErase</span><span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">ChannelNormOp</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Find the defining operation of the ChannelNorm&#39;s operand, which should be a LoadConstOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">defOp</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast_or_null</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getDefiningOp</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// If the source is not a LoadConstOp, do nothing.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">defOp</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Collect all users of the ChannelNormOp&#39;s result.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">llvm</span><span class="o">::</span><span class="n">SmallVector</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*&gt;</span> <span class="n">userVec</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">userVec</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">userVec</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">begin</span><span class="p">(),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">().</span><span class="n">end</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">userVec</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Replace all uses of the ChannelNormOp&#39;s result with the result of the LoadConstOp..
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">user</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// After bypassing, the layout of the source constant might need to be adjusted
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// to reflect the transformation that the ChannelNormOp was supposed to perform.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// set const layout to cx mode 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">dev_layout</span> <span class="o">=</span> <span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">defOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">auto</span> <span class="n">align_dev_layout</span> <span class="o">=</span> <span class="n">get_aligned_layout</span><span class="p">((</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">dev_layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">defOp</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">defOp</span><span class="p">,</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LayoutMode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">align_dev_layout</span><span class="p">));</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<p><code>processMultiUse</code> 保证所有对同一份常量数据的引用，其 mem_layout 都是完全一致的。流程如下</p>
<ol>
<li><code>processMultiUse</code> 遍历计算图中的所有 LoadConstOp，以 <code>const_map_id</code> 为 key，将所有指向同一个物理常量的 LoadConstOp 实例分组存放在一起。</li>
<li>遍历这个 map，只处理那些包含多个 LoadConstOp 实例的组 (<code>kv.second.size() &gt; 1</code>).</li>
<li>在每个组内，确定一个正确的布局。代码逻辑是以组内的第一个 LoadConstOp 的布局为基准，但如果发现组内有 <code>is_cx_layout</code>，则会采用这个优先的布局作为标准。</li>
<li>一旦确定了标准布局，会再次遍历该组内的所有 LoadConstOp 实例。调用 <code>setDevInfoWithLayout</code> 函数，强制将每一个实例的布局属性修改为刚才确定的那个标准布局。</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">processMultiUse Implementation</summary>
    <div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// This function processes multi-use constants to ensure their layouts are consistent.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="n">ConstNormPass</span><span class="o">::</span><span class="n">processMultiUse</span><span class="p">(</span><span class="n">ModuleOp</span> <span class="n">module</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">mainGraphFunc</span> <span class="o">=</span> <span class="n">getMainFuncOp</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// When a const is used by multiple users, multiple loadconsts will be generated,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// but only one loadconst will have its layout set. The others will be skipped.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// We need to go over them uniformly. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// First, find all previous useless constant ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Group all LoadConstOp instances by their underlying constant data ID (const_map_id).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span> <span class="o">*&gt;&gt;</span> <span class="n">allconst</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mainGraphFunc</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">constOp</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="k">auto</span> <span class="n">cOp</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LoadConstOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">constOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="kt">uint32_t</span> <span class="n">t_map_id</span> <span class="o">=</span> <span class="n">cOp</span><span class="p">.</span><span class="n">getConstMapId</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">      <span class="n">allconst</span><span class="p">[</span><span class="n">t_map_id</span><span class="p">].</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">constOp</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Based on duplication, find if the layout needs to be changed to cx. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// Check if there is also a Cx with the same layout. 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Iterate over each group of LoadConstOps that share the same data.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="o">&amp;</span><span class="nl">kv</span> <span class="p">:</span> <span class="n">allconst</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Process only if there are multiple users.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Assume the layout of the first user is the correct one.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">auto</span> <span class="n">layout</span> <span class="o">=</span> <span class="p">(</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">.</span><span class="n">front</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">      <span class="c1">// This loop is for validation, checking if layouts are inconsistent.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">layout2</span> <span class="o">=</span> <span class="p">(</span><span class="n">LAYOUT_MODE</span><span class="p">)</span><span class="n">getDevInfoLayoutMode</span><span class="p">(</span><span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">is_cx_layout</span><span class="p">(</span><span class="n">layout2</span><span class="p">)</span> <span class="o">!=</span> <span class="n">ALIGN_NOT</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">layout</span> <span class="o">=</span> <span class="n">layout2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="k">break</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// Force all LoadConstOps in this group to have the same, correct layout.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">kv</span><span class="p">.</span><span class="n">second</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">ctx</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getContext</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="n">ASSERT</span><span class="p">(</span><span class="n">op</span><span class="o">-&gt;</span><span class="n">hasAttr</span><span class="p">(</span><span class="s">&#34;dev_info&#34;</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="s">&#34;Must have dev_info!&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="n">setDevInfoWithLayout</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="p">(</span><span class="n">tx8be</span><span class="o">::</span><span class="n">LayoutMode</span><span class="p">)</span><span class="n">layout</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div></div>
</details><br>
<h1 id="sharding-search-and-spm-management">Sharding Search and SPM Management</h1>
<p>第一步是对算子进行 Group 划分，插入 load &amp; store. 对每一个 subGraph 会应用如下的 3 个 Pass:</p>
<ul>
<li><strong>GroupPatternPass</strong>：应用配置好的 group config (opt_group).</li>
<li><strong>GroupOptimizationPass</strong>: 如果没有配置，则会为每个 compute op 创建一个 group.</li>
<li><strong>GroupLdStPass</strong>: 为每个需要的 groupOp 插 入loadOp 和 storeOp，并添加 group_tag.
<ul>
<li>group_tag = 0: 需要 load 或 store，意味着该 group 需要后续的切分搜索。</li>
<li>group_tag = 2: 不需要 load 或 store，意味着该 group 的op 都在 DDR 上操作，无需参与后续的切分搜索。</li>
</ul>
</li>
</ul>
<p>SPM 上一定要能放下切分后的结果。Group 是切分搜索和 SPM 分配的基本的单位。思想就是尽量把连续执行的算子组合在一起，一直在 SPM 上运行而不是存回 DDR 再读入，以此来减少访存时间。GroupOp 在 td 文件中定义所包含的输入如下:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-tablegen" data-lang="tablegen"><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">regions</span> <span class="p">=</span> <span class="p">(</span><span class="nv">region</span> <span class="nv">SizedRegion</span><span class="p">&lt;</span><span class="m">1</span><span class="p">&gt;:</span><span class="nv">$body</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">arguments</span> <span class="p">=</span> <span class="p">(</span><span class="nv">ins</span>
</span></span><span class="line"><span class="cl">    <span class="nv">Variadic</span><span class="p">&lt;</span><span class="nv">AnyTensorOrNone</span><span class="p">&gt;:</span><span class="nv">$operands</span><span class="p">,</span>      <span class="c">// 输入参数为 操作数的数量可变的的张量
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">BoolAttr</span><span class="p">,</span> <span class="s">&#34;false&#34;</span><span class="p">&gt;:</span><span class="nv">$pipeline_parallel</span><span class="p">,</span> <span class="c">// 是否用流水线并行
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">I32Attr</span><span class="p">,</span> <span class="s">&#34;1&#34;</span><span class="p">&gt;:</span><span class="nv">$sp_stage_num</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">Tx8e_RegionAttr</span><span class="p">&gt;:</span><span class="nv">$dev_region</span><span class="p">,</span> <span class="c">// 设备的空间属性
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">UI32Attr</span><span class="p">&gt;:</span><span class="nv">$spm_alloc_size</span><span class="p">,</span>   <span class="c">// group占用的spm大小
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">I32Attr</span><span class="p">&gt;:</span><span class="nv">$group_tag</span><span class="p">,</span>         <span class="c">// 0: 正常切分, 1: split nht, 2: 不切分 (reshape)
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">DenseI32ArrayAttr</span><span class="p">&gt;:</span><span class="nv">$stream_online_check</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">OptionalAttr</span><span class="p">&lt;</span><span class="nv">DenseI32ArrayAttr</span><span class="p">&gt;:</span><span class="nv">$stream_offline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">BoolAttr</span><span class="p">,</span> <span class="s">&#34;true&#34;</span><span class="p">&gt;:</span><span class="nv">$need_barrier</span><span class="p">,</span>   <span class="c">// 是否需要tile同步
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">SI32Attr</span><span class="p">,</span> <span class="s">&#34;-1&#34;</span><span class="p">&gt;:</span><span class="nv">$group_id</span><span class="p">,</span>         <span class="c">// group id序号
</span></span></span><span class="line"><span class="cl"><span class="c"></span>    <span class="nv">DefaultValuedAttr</span><span class="p">&lt;</span><span class="nv">SI32Attr</span><span class="p">,</span> <span class="s">&#34;-1&#34;</span><span class="p">&gt;:</span><span class="nv">$template_id</span>      <span class="c">// 复用其他group的id, 小于0为不复用
</span></span></span><span class="line"><span class="cl"><span class="c"></span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">let</span> <span class="nv">results</span> <span class="p">=</span> <span class="p">(</span><span class="nv">outs</span> <span class="nv">Variadic</span><span class="p">&lt;</span><span class="nv">AnyTensorOrNone</span><span class="p">&gt;:</span><span class="nv">$results</span><span class="p">);</span>
</span></span></code></pre></div><p>还有一些常用到的结构体
<code>SecsInfo</code> 记录了单个 Op在分布式策略搜索过程中的所有状态和信息。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">SecsInfo</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">sharding</span><span class="p">;</span>  <span class="c1">// space 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">split</span><span class="p">;</span>  <span class="c1">// time
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">splitry</span><span class="p">;</span>  <span class="c1">// 当前搜索的 sharding 的 split
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">reduceSplit</span><span class="p">;</span>  <span class="c1">// 针对需要进行规约 (Reduction) 的维度的切分策略。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int32_t</span> <span class="n">reducesplit</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 一个标记位，用于指示reduceSplit是否被使用
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ******************** 以下变量为factorSpace使用部分 ********************
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="n">sfinish</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>  <span class="c1">// 标记 split/reduceSplit 相关的策略是否已确定。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 枚举类型，定义了当前算子所处的切分模式，特别关注需要通信的Reduce维度。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="cm">/* SHARDING_MODE 的可能值解释：
</span></span></span><span class="line"><span class="cl"><span class="cm">   * SHARDING_INIT: 初始状态，尚未确定模式。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 0: 不切分规约 (reduce) 维度。意味着数据在每个设备上是完整的，无需通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 1: 单边切分规约维度。例如，只切分权重，不切分输入，数据在不同tile上需要通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 2: 两边都切分规约维度。
</span></span></span><span class="line"><span class="cl"><span class="cm">   * 3: 对权重(weight)的输出通道(output channel)维度进行切分，但不属于张量并行(TP)，可能需要fn/oc通信。
</span></span></span><span class="line"><span class="cl"><span class="cm">  */</span>
</span></span><span class="line"><span class="cl">  <span class="n">SHARDING_MODE</span> <span class="n">shardingMode</span><span class="p">{</span><span class="n">SHARDING_INIT</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="n">rfinish</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>   <span class="c1">// 标记 reduceSplit 相关的策略是否已完成处理。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">nfirst</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 标记搜索方向。1: search from dim0 -&gt; dim n-1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">finish</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 表示该算子的策略搜索是否已全部完成。整个搜索流程: sharding -&gt; shardingmode -&gt; split -&gt; reduceSplit
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span> <span class="n">sliceShapeMin</span><span class="p">;</span> <span class="c1">// 标记切分后的张量 (slice) 在每个维度上是否已达到某个最小尺寸限制。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ******************** 以下变量为sliceInfo使用部分 ********************
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span> <span class="n">TemporalShape</span><span class="p">;</span>  <span class="c1">// 切分后，临时的张量形状
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">reduce_sharding_space</span><span class="p">;</span>  <span class="c1">// 规约维度切分的搜索空间
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">reduce_sharding</span><span class="p">;</span>  <span class="c1">// // 最终选定的规约维度切分策略
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="n">sharding2_finish</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>  <span class="c1">// 标记第二阶段切分 (可能与规约相关) 是否完成
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span></code></pre></div><h2 id="grouppatternpass">GroupPatternPass</h2>
<p><code>GroupPatternPass</code> 其核心功能是在给定的计算图 (subgraphOp) 中，通过一种高效的模式匹配算法，识别出预定义的、可优化的子图模式 (Operator Patterns)，并将匹配到的算子 (Operations) 进行分组。这种分组通常是图优化 (如算子融合、算子调度) 的第一步。</p>
<p>该 Pass 首先获取配置，决定从哪里加载模式 (一个 map，其键是模式，即一个算子序列 <code>std::vector&lt;TX8BE_OPS&gt;</code>，值是一个整数 <code>int</code>，代表模式优先级，越大优先级越高) 。然后，它调用 <code>aca.insertPatterns</code> 将这些模式&quot;编译&quot;到 Automation 引擎中。接着，调用 <code>aca.search</code> 执行匹配。最后，从 manager 中获取匹配结果 (groups) ，并对这些 groups 进行后续处理，例如创建新的逻辑分组和进行拓扑排序。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"> <span class="kt">void</span> <span class="n">GroupPatternPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">     <span class="n">TFUNC_SCOPE</span><span class="p">(</span><span class="n">DEBUG</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">subgraphOp</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span> <span class="c1">// Get the current operation (e.g., a function) the pass is running on.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> 
</span></span><span class="line"><span class="cl">     <span class="n">PatternManager</span> <span class="n">manager</span><span class="p">;</span> <span class="c1">// A manager to hold graph rewriting information.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">Automation</span> <span class="nf">aca</span><span class="p">(</span><span class="o">&amp;</span><span class="n">manager</span><span class="p">);</span> <span class="c1">// Custom &#39;Automation&#39; class for pattern matching logic.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span> 
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">minfo</span> <span class="o">=</span> <span class="n">getModuleConfig</span><span class="p">(</span><span class="n">getModuleByOp</span><span class="p">(</span><span class="n">getOperation</span><span class="p">()));</span> 
</span></span><span class="line"><span class="cl">     <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">path</span> <span class="o">=</span> <span class="s">&#34;&#34;</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">path</span> <span class="o">!=</span> <span class="s">&#34;&#34;</span> <span class="o">?</span> <span class="n">getPatternsFromFile</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>  <span class="c1">// Load patterns from a file if path is specified.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                           <span class="o">:</span> <span class="p">(</span><span class="n">patternConfigMap</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">GroupPatternMode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">minfo</span><span class="p">.</span><span class="n">opt_group</span><span class="p">)));</span> <span class="c1">// Otherwise, load from a pre-defined map using a config key.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">TLOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;[GroupPatternPass] config id: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">minfo</span><span class="p">.</span><span class="n">opt_group</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">     <span class="n">aca</span><span class="p">.</span><span class="n">insertPatterns</span><span class="p">(</span><span class="n">temp</span><span class="p">);</span> <span class="c1">// Insert the loaded patterns into the Automation engine. This is the starting point for building the matching structure.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">TLOG</span><span class="p">(</span><span class="n">INFO</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;[Automation]: </span><span class="se">\n</span><span class="s">&#34;</span> <span class="o">&lt;&lt;</span> <span class="n">printTree</span><span class="p">(</span><span class="n">aca</span><span class="p">.</span><span class="n">root</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">     <span class="n">aca</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="n">subgraphOp</span><span class="p">);</span> <span class="c1">// Execute the search for all patterns on the given subgraph. (search function code is not provided but its role is clear).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">manager</span><span class="p">.</span><span class="n">applyAll</span><span class="p">();</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">manager</span><span class="p">.</span><span class="n">getGroups</span><span class="p">();</span> <span class="c1">// Retrieve the groups of operations that were matched.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="n">manager</span><span class="p">.</span><span class="n">show</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     <span class="k">auto</span> <span class="n">newGroups</span> <span class="o">=</span> <span class="n">createGroups</span><span class="p">(</span><span class="n">subgraphOp</span><span class="p">,</span> <span class="n">groups</span><span class="p">);</span> <span class="c1">// Create new group structures from the matched results.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">group</span> <span class="p">:</span> <span class="n">newGroups</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">         <span class="n">sortTopologically</span><span class="p">(</span><span class="n">group</span><span class="o">-&gt;</span><span class="n">getBlock</span><span class="p">());</span> <span class="c1">// Topologically sort the operations within each new group to maintain data dependencies.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>     <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="p">}</span>
</span></span></code></pre></div><p><code>insertPatterns</code> 对于每一个模式，它首先调用 processPattern 来处理其中的 OR, WILDCARD 算子。</p>
<ul>
<li>当遇到 OR 时，它会将模式拆分。例如，A B OR C D 这样的模式会被拆解成两个独立的模式 A B 和 C D 进行处理。</li>
<li>当遇到 WILDCARD 时，它会生成多个模式。根据代码 <code>for (int i = 0; i &lt; 5; i++)</code> 和 <code>temp.push_back(*(it - 1))</code>，OP * 可能会被扩展成 OP, OP OP, OP OP OP, OP OP OP OP 等一系列重复模式。</li>
<li>它通过递归调用自身，以处理一个模式中包含多个特殊算子的情况。
最终，它返回一个由多个具体、无特殊算子的模式组成的列表。然后，它将这些扩展后的具体模式逐一传递给 <code>insertPattern</code> 函数，以构建 Trie 树。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">insertPatterns</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">patterns</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;&gt;</span> <span class="n">tempPatterns</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">it</span> <span class="p">:</span> <span class="n">patterns</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate through each pattern from the input map.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">processPattern</span><span class="p">(</span><span class="n">it</span><span class="p">.</span><span class="n">first</span><span class="p">);</span> <span class="c1">// Pre-process the pattern. This can expand one pattern into many.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">p</span> <span class="p">:</span> <span class="n">temp</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// For each of the generated concrete patterns...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">insertPattern</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">it</span><span class="p">.</span><span class="n">second</span><span class="p">);</span> <span class="c1">// ...insert it into the main data structure (the Trie).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>insertPattern</code> 接收一个具体的模式，并将其插入到 Trie 树中。Trie 树是实现高效前缀匹配的关键。从root节点开始 遍历模式中的每个 op. 如果当前节点没有指向op的子节点，就创建一个然后移动到该子节点。当模式遍历完成后，在最终的节点上存储完整模式本身 (<code>node-&gt;pattern</code>) 和它的 ID (<code>node-&gt;output</code>) 。这表明一个有效的模式在此结束。</p>
<details class="custom-details">
    <summary class="custom-summary">insertPattern Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">TrieNode</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">TrieNode</span><span class="p">(</span><span class="n">TX8BE_OPS</span> <span class="n">id</span><span class="p">)</span> <span class="o">:</span> <span class="n">id</span><span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="p">{}</span> <span class="c1">// Constructor to initialize the node with an operation ID.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">TX8BE_OPS</span> <span class="n">id</span><span class="p">;</span> <span class="c1">// The operation (Op) type this node represents. This is the &#39;character&#39; in our sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">output</span><span class="p">;</span> <span class="c1">// Stores the integer IDs of the patterns that end at this node. A non-empty vector indicates a valid pattern match.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// Stores the complete operator sequence for the pattern that ends here.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="p">,</span> <span class="n">NodePtr</span><span class="o">&gt;</span> <span class="n">children</span><span class="p">;</span> <span class="c1">// A map from an operation type to the next node in the trie. `NodePtr` is likely a shared_ptr or unique_ptr to another TrieNode.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">insertPattern</span><span class="p">(</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TX8BE_OPS</span><span class="o">&gt;</span> <span class="n">pattern</span><span class="p">,</span> <span class="kt">int</span> <span class="n">index</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">patterns_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">pattern</span><span class="p">);</span> <span class="c1">// Store the raw pattern vector.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">node</span> <span class="o">=</span> <span class="n">root</span><span class="p">;</span> <span class="c1">// Start from the root of the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">op</span> <span class="p">:</span> <span class="n">pattern</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate through each operation in the pattern sequence.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">op</span><span class="p">)</span> <span class="o">==</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If a path for this operation does not exist...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">TrieNode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// ...create a new node in the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="n">node</span> <span class="o">=</span> <span class="n">node</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">op</span><span class="p">];</span> <span class="c1">// Move to the next node in the Trie.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">node</span><span class="o">-&gt;</span><span class="n">pattern</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// At the end of the pattern, mark this node as a terminal node by storing the full pattern.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">node</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">index</span><span class="p">);</span> <span class="c1">// Store the original pattern index/ID at this terminal node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>searchOp</code> 函数的功能是：给定一个起始 Trie 节点 (parentNode) 和一个MLIR算子 (op)，它会尝试将 op 与parentNode 的子节点进行匹配，并在匹配成功后，递归地对其所有后继算子 (users) 进行 DFS 模式匹配，最终返回这条路径上所能找到的“最佳”匹配模式的末端Trie节点。</p>
<p>这里的“最佳”通常指最长的匹配模式，或者在有多个同样长度的模式时，选择优先级最高的那个 (根据节点中的 <code>output.front()</code>) 大小比较来判断。</p>
<details class="custom-details">
    <summary class="custom-summary">searchOp Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">NodePtr</span> <span class="n">Automation</span><span class="o">::</span><span class="n">searchOp</span><span class="p">(</span><span class="n">NodePtr</span> <span class="n">parentNode</span><span class="p">,</span> <span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">auto</span> <span class="n">opId</span> <span class="o">=</span> <span class="n">getOpId</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Get the enumerated ID (e.g., TX8BE_OPS::CONV) for the current MLIR operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isRealOp</span><span class="p">(</span><span class="n">op</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">opId</span><span class="p">)</span> <span class="o">==</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// If the current op is a &#34;real&#34; operation (not a terminator, etc.) but cannot be found in the children of the parent Trie node, it&#39;s a mismatch.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// This &#39;if&#39; block seems to be an early exit for a specific case, possibly redundant with the final return.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">opId</span><span class="p">)</span> <span class="o">!=</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If a path exists in the Trie for the current operation `opId`. This is a potential match.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// If the current op matches, continue downwards
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">currentNode</span> <span class="o">=</span> <span class="n">parentNode</span><span class="o">-&gt;</span><span class="n">children</span><span class="p">[</span><span class="n">opId</span><span class="p">];</span> <span class="c1">// Move to the matched Trie node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">tempNode</span> <span class="o">=</span> <span class="n">currentNode</span><span class="p">;</span> <span class="c1">// `tempNode` will store the longest match found so far starting from this path.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// --- Query Operation Attributes and Users ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">queryInterface</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e_mlir</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Get a specific interface from the operation for querying attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">needStore</span> <span class="o">=</span> <span class="n">queryInterface</span><span class="p">.</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needStore</span><span class="p">;</span> <span class="c1">// Check an attribute, e.g., if the op&#39;s result needs to be stored.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">llvm</span><span class="o">::</span><span class="n">SmallSet</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="mi">1</span><span class="o">&gt;</span> <span class="n">users</span><span class="p">;</span> <span class="c1">// Find all direct users of the current operation&#39;s result.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="n">users</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">sortedUsers</span> <span class="o">=</span> <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">sortOps</span><span class="p">(</span><span class="n">users</span><span class="p">);</span> <span class="c1">// Sort the users, likely topologically or based on some priority.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// --- Recursively Search Through Users ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">user</span> <span class="p">:</span> <span class="n">sortedUsers</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">isRealOp</span><span class="p">(</span><span class="n">user</span><span class="p">))</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip non-essential ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">            <span class="k">auto</span> <span class="n">interface</span> <span class="o">=</span> <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e_mlir</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="k">auto</span> <span class="n">needLoad</span> <span class="o">=</span> <span class="n">interface</span><span class="p">.</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needLoad</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">needStore</span> <span class="o">&amp;&amp;</span> <span class="n">needLoad</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span> <span class="c1">// Skip paths with certain attribute mismatches (e.g., store-load dependency).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Recursively call searchOp for the user operation, starting from the current Trie node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">auto</span> <span class="n">terminalNode</span> <span class="o">=</span> <span class="n">searchOp</span><span class="p">(</span><span class="n">currentNode</span><span class="p">,</span> <span class="n">user</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1">// --- Update Best Match ---
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">tempNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If both the previous best match (`tempNode`) and the new match (`terminalNode`) are valid patterns...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="c1">// Compare priority, take the one with the highest priority as the current node pattern)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="k">if</span> <span class="p">(</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">tempNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">())</span> <span class="p">{</span> <span class="o">/</span> <span class="p">...</span><span class="n">update</span> <span class="err">`</span><span class="n">tempNode</span><span class="err">`</span> <span class="n">to</span> <span class="n">the</span> <span class="k">new</span> <span class="n">one</span> <span class="k">if</span> <span class="n">it</span> <span class="n">has</span> <span class="n">a</span> <span class="n">higher</span> <span class="n">priority</span> <span class="p">(</span><span class="n">assuming</span> <span class="n">the</span> <span class="kt">int</span> <span class="n">ID</span> <span class="n">represents</span> <span class="n">priority</span><span class="p">).</span>
</span></span><span class="line"><span class="cl">                    <span class="n">tempNode</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">                <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span> <span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// If `tempNode` was not a valid pattern end, but `terminalNode` is, update it.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="n">tempNode</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// TFOOTER(TRACE)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">return</span> <span class="n">tempNode</span><span class="p">;</span> <span class="c1">// Return the node corresponding to the longest/best pattern found from this point.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Indicates parent node cannot match current op, return parent node)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">parentNode</span><span class="p">;</span> <span class="c1">// If no match was found for `opId` in the Trie, return the original `parentNode`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<p><code>search</code>遍历计算子图 (subgraph) 中的每一个算子，并以该算子为起点，尝试进行模式匹配。</p>
<ol>
<li>预处理阶段 (第一个 walk)
在正式开始匹配之前，函数会先遍历一次整个子图，目的是收集和注册一些元数据：</li>
</ol>
<ul>
<li><code>manager_-&gt;opOrder_</code>: 一个 vector 记录图中所有算子的出现顺序。</li>
<li><code>manager_-&gt;opIndexMap_</code>: 为每个算子分配一个唯一的整数索引。
这些信息对于后续的管理和可能的图变换 (如拓扑排序) 非常重要。</li>
</ul>
<ol start="2">
<li>逐点匹配阶段 (第二个 walk)它再次遍历子图中的每一个算子 op 每次都是从 Trie 树的根节点 root 开始 <code>searchOp(root, op)</code> 函数。意味着尝试从零开始匹配所有已知的模式。 searchOp 会返回从 op 开始能找到的最长/最优的匹配模式的末端节点 (terminalNode).</li>
</ol>
<ul>
<li>如果其 output 列表不为空，说明 searchOp 成功地找到了一条完整的匹配路径。函数就会将这个匹配结果记录下来：在 manager 中更新 Pattern 对象，并在本地的 result map 中建立从起始算子 op到模式ID的映射。</li>
<li>反之说明从 op 开始无法匹配任何完整的模式，于是就什么也不做，继续检查下一个算子。</li>
</ul>
<details class="custom-details">
    <summary class="custom-summary">search Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">Automation</span><span class="o">::</span><span class="n">search</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span> <span class="n">subgraph</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// k: the starting operation of a matched pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// v: the type/ID of the matched pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">result</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">initDefsMap</span><span class="p">(</span><span class="n">subgraph</span><span class="p">);</span> <span class="c1">// Initialize manager with definition information from the subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">subgraph</span><span class="o">-&gt;</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// First pass: walk through the subgraph to gather metadata.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">opOrder_</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Record the sequential order of all operations.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">opIndexMap_</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span><span class="o">++</span><span class="p">;</span> <span class="c1">// Assign a unique index to each operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">});</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Second pass: walk through the subgraph again to perform the actual pattern matching.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">subgraph</span><span class="o">-&gt;</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Skip the return operation of the subgraph as it&#39;s not part of a computational pattern.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphReturnOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">WalkResult</span><span class="o">::</span><span class="n">skip</span><span class="p">();</span> <span class="c1">// In newer MLIR, this might be `return;`. Skips processing this op&#39;s children.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">auto</span> <span class="n">pattern</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">Pattern</span><span class="o">&gt;</span><span class="p">(</span><span class="n">op</span><span class="p">);</span> <span class="c1">// Create a Pattern object, representing a potential match starting at `op`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">patterns_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">pattern</span><span class="p">);</span> <span class="c1">// Add this potential pattern to the manager&#39;s list.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">manager_</span><span class="o">-&gt;</span><span class="n">patternMap_</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">pattern</span><span class="p">;</span> <span class="c1">// Map the operation `op` to its corresponding Pattern object.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="c1">// terminalNode 就是最后匹配到的一个Node (terminalNode is the final matched Node)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// This is the main call to the recursive search function, starting from the Trie root for each `op`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">auto</span> <span class="n">terminalNode</span> <span class="o">=</span> <span class="n">searchOp</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">op</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1">// If the Node has an output, it means a match was found. If multiple matches exist, they are replaced based on priority during the search phase
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// The final result is a match for the highest-priority pattern
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Check if the search returned a valid pattern-terminating node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// If a match was found, update the Pattern object with the results from the terminal node.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">pattern</span><span class="o">-&gt;</span><span class="n">setPattern</span><span class="p">(</span><span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">(),</span> <span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">pattern</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Record the result: map the starting operation `op` to the matched pattern&#39;s ID.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">result</span><span class="p">[</span><span class="n">op</span><span class="p">]</span> <span class="o">=</span> <span class="n">terminalNode</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">WalkResult</span><span class="o">::</span><span class="n">advance</span><span class="p">();</span> <span class="c1">// Proceed to the next operation in the walk.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupoptimizationpass">GroupOptimizationPass</h2>
<p>会遍历一个计算 subGraph 中的所有 OP. 对于每一个通过筛选的普通计算操作，会调用 <code>createSingleGroup</code> 函数来为其创建一个专属的 GroupOp.
<code>createSingleGroup</code> 会检查原始 OP 的所有输入。如果输入来自另一个计算操作，那么这个输入就会成为新 GroupOp 的输入。如果输入是 LoadConstOp，则被视为这个分组的内部依赖，而不是外部输入。原始 op 的所有输出会直接成为新 GroupOp 的输出。</p>
<p>新的 GroupOp 拥有上一步定义的输入和输出。原始的操作 op 和它的常量依赖 (dependencies) 被移动到这个新创建的 GroupOp 内部。最后，修改原始操作 OP 的连接关系，使其在分组内部能够正确地接收输入并产生输出。伪代码如下</p>
<pre tabindex="0"><code>for op in subGraph.ops:

  // 检查操作的类型
  if op == (GroupOp || ReturnOp || LoadConstOp || NoneOp):
    continue

  createSingleGroup(op)

------------------------------------
createSingleGroup(op):
  for pre_op in op.inputsOp:
    // 判断前置操作是否为“加载常量”或“空操作”
    if pre_op == (LoadConstOp || NoneOp):
      // 如果是，则将其添加到依赖项 (dependencies) 集合中
      dependencies.add(pre_op)
    else:
      // 如果是其他普通操作，则将其结果添加到新分组的输入 (groupInput) 中
      groupInput.add(pre_op.result)

  for result in op.results:  // 遍历当前操作的所有输出结果
    // 将这些结果添加到新分组的输出 (groupOutput) 中
    groupOutput.add(result)

  // 使用收集好的输入和输出创建一个新的 GroupOp (分组操作) 
  create GroupOp(groupInput, groupOutput)

  // 将依赖项 (如常量) 移动到新分组的末尾 (或内部) 
  move dependencies to group end

  // 将原始操作 op 本身也移动到新分组的末尾 (或内部) 
  move op to group end

  // 修改原始操作 op 的输入和输出，使其在新分组内部正确连接
  change op input and output
</code></pre><p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBca62e625dd418d0b51deb2e46c83f873?method=download&amp;shareKey=3b9dddfeca5108a0665fce242dd1019d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBca62e625dd418d0b51deb2e46c83f873?method=download&amp;shareKey=3b9dddfeca5108a0665fce242dd1019d" alt="GroupOptimizationPass">
    </a><figcaption>GroupOptimizationPass</figcaption></figure></p>
<h2 id="groupldstpass">GroupLdStPass</h2>
<p><code>GroupLdStPass</code> 作用用是处理 GroupOp 的输入和输出，通过显式插入 Load 和 Store 操作，来“固化”和“隔离”GroupOp 的边界。</p>
<p>Load 插入流程</p>
<ol>
<li>识别 Load 需求: 函数遍历 GroupOp 的每一个输入参数v。然后，它查找所有在 GroupOp 外部使用 v 的算子 (userOp) 。通过检查这些userOp的属性 (needLoad) ，它判断哪些 userOp 需要一个显式的 Load 操作来获取 v 的值。</li>
<li>处理特殊布局: 代码中有一段特殊的逻辑 (<code>if(isa&lt;...&gt;)</code>) ，用于处理 Add、Sub 等二元算子。它检查输入的layout 如果存在不匹配的情况 (例如一个NCx布局和一个Tensor布局) ，它可能会强制layout统一，以确保硬件能够正确计算。</li>
<li>插入 LoadVarOp: 在确定了所有需要 Load 的外部用户后，如果这样的用户存在 (<code>usersLoad.size() != 0</code>)，它会在GroupOp的入口处创建一个tx8e::LoadVarOp操作。</li>
<li>重定向数据流: 将所有外部用户对原始输入 v 的连接 (SSA use-def chain) ，全部断开，并重新连接到新创建的LoadVarOp的输出上 (replaceUsesOfWith).</li>
</ol>
<p>Store 插入流程</p>
<ol>
<li>识别存储需求: 函数找到 GroupOp 内部的 return 操作，并遍历它的每一个操作数 (即 GroupOp 的输出值). 通过检查产生这些输出值的内部算子 (pre_op) 的needStore属性，来判断哪些输出需要被显式地Store，以便外部世界能够访问。</li>
<li>插入 StoreVarOp: 如果一个输出值需要被存储，函数会在 GroupOp 的末尾、return 操作之前，创建一个tx8e::StoreVarOp 接收 GroupOp 的内部计算结果。</li>
<li>更新返回结果: StoreVarOp本身也有一个输出。函数会更新 GroupOp 的 return 操作，使其返回 StoreVarOp 的输出，而不是原始的内部计算结果。</li>
</ol>
<details class="custom-details">
    <summary class="custom-summary">GroupLdStPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">GroupLdStPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">subgraph</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span> <span class="n">g_op</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//  For each group&#39;s input, insert a load. If used by multiple ops, multiple loads are inserted
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">v</span> <span class="p">:</span> <span class="n">g_op</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">().</span><span class="n">getArguments</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Iterate over each input argument of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">Operation</span><span class="o">*</span> <span class="n">pre_op</span> <span class="o">=</span> <span class="n">getValidDefiningOp</span><span class="p">(</span><span class="n">v</span><span class="p">);</span> <span class="c1">// Find the operation that produces this input.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">Operation</span><span class="o">*</span><span class="p">,</span> <span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">usersLoad</span><span class="p">;</span> <span class="c1">// A map to store users that need to load this input.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">userOp</span> <span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="n">getUsers</span><span class="p">())</span> <span class="p">{</span> <span class="c1">// Find all users of this input argument.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Check if the user needs a &#39;load&#39; based on its attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="k">if</span> <span class="p">((</span><span class="o">!</span><span class="n">opAttr</span><span class="p">.</span><span class="n">needLoad</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">arg_idx</span><span class="p">)))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// If a load is needed, record the user and its argument index.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="n">usersLoad</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_pair</span><span class="p">(</span><span class="n">userOp</span><span class="p">,</span> <span class="n">arg_idx</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// This block handles complex layout logic for Add/Sub/Mul/Div ops.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// It seems to ensure that if one input to &#39;add&#39; is rank1 tensor, the other is also handled correctly,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// potentially by forcing a specific layout (`LayoutMode::Cx`).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">AddOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">SubOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">DivOp</span><span class="p">,</span> <span class="n">tx8e</span><span class="o">::</span><span class="n">MulOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">userOp</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [复杂布局逻辑]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="p">}</span>
</span></span><span class="line"><span class="cl">          
</span></span><span class="line"><span class="cl">          <span class="k">if</span> <span class="p">(</span><span class="n">usersLoad</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// there are users that require a load operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">NamedAttribute</span><span class="o">&gt;</span> <span class="n">tmp_attrs</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [构建LoadVarOp的属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="c1">// Create the Load operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="k">auto</span> <span class="n">ld</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">LoadVarOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">g_op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">v</span><span class="p">.</span><span class="n">getType</span><span class="p">(),</span> <span class="n">v</span><span class="p">,</span> <span class="n">tmp_attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">              <span class="c1">// ... [设置动态shape属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              
</span></span><span class="line"><span class="cl">              <span class="c1">// For each user that needs the load...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>              <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">userOp</span> <span class="p">:</span> <span class="n">usersLoad</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">// ...replace its use of the original input `v` with the result of the new `Load` operation `ld`.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                  <span class="n">userOp</span><span class="p">.</span><span class="n">first</span><span class="o">-&gt;</span><span class="n">replaceUsesOfWith</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">ld</span><span class="p">.</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">              <span class="p">}</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// For each group&#39;s output, insert a store
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">builder</span><span class="p">.</span><span class="n">setInsertionPointToEnd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">block</span><span class="p">);</span> <span class="c1">// Set the insertion point to the end of the group&#39;s body.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">Operation</span> <span class="o">*</span><span class="n">g_return</span> <span class="o">=</span> <span class="n">g_op</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">().</span><span class="n">getTerminator</span><span class="p">();</span> <span class="c1">// Get the return operation of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">getNumOperands</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// Iterate over each output of the group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">value</span> <span class="o">=</span> <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">getOperand</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">          <span class="k">auto</span> <span class="n">pre_op</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="n">getDefiningOp</span><span class="p">();</span> <span class="c1">// Find the operation inside the group that produces this output.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// Check if this output value needs to be stored for external users.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">OpLibInterface</span><span class="o">&gt;</span><span class="p">(</span><span class="n">pre_op</span><span class="p">)).</span><span class="n">queryOpAttr</span><span class="p">().</span><span class="n">needStore</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">i</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">              <span class="k">continue</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">          <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          <span class="c1">// ... [构建StoreVarOp的属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="c1">// Create the Store operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="k">auto</span> <span class="n">st</span> <span class="o">=</span> <span class="n">builder</span><span class="p">.</span><span class="n">create</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">StoreVarOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">g_op</span><span class="p">.</span><span class="n">getLoc</span><span class="p">(),</span> <span class="n">value</span><span class="p">.</span><span class="n">getType</span><span class="p">(),</span> <span class="n">value</span><span class="p">,</span> <span class="n">st_attrs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">          <span class="c1">// ... [设置动态shape属性]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">          <span class="c1">// Update the group&#39;s return instruction to return the result of the store op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">setOperand</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">st</span><span class="p">.</span><span class="n">getOutput</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">g_return</span><span class="o">-&gt;</span><span class="n">moveBefore</span><span class="p">(</span><span class="n">gBlock</span><span class="p">,</span> <span class="n">block</span><span class="p">.</span><span class="n">end</span><span class="p">());</span> <span class="c1">// Move the return instruction (not standard MLIR, might be custom logic).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">updateIR</span><span class="p">(</span><span class="n">g_op</span><span class="p">);</span> <span class="c1">// Update the IR of the group op.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupmappingpass">GroupMappingPass</h2>
<p><code>GroupMappingPass</code> 作用是将顶层模块 (Module) 中定义的全局维度信息 (x_dim 和 y_dim) 设置到每一个 GroupOp 或 GroupOp 的调用点上。</p>
<details class="custom-details">
    <summary class="custom-summary">GroupMappingPass Implementation</summary>
    <div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// Defines a function to perform a simple mapping of groups.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="kt">void</span> <span class="nf">simpleGroupMapping</span><span class="p">(</span><span class="n">ModuleOp</span> <span class="n">module</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get x and y dimension from the module&#39;s attributes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// These attributes are likely defined globally for the entire compilation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">uint32_t</span> <span class="n">x_dim</span> <span class="o">=</span> <span class="n">module</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">TileDx</span><span class="p">).</span><span class="n">getInt</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="kt">uint32_t</span> <span class="n">y_dim</span> <span class="o">=</span> <span class="n">module</span><span class="o">-&gt;</span><span class="n">getAttrOfType</span><span class="o">&lt;</span><span class="n">IntegerAttr</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tx8e</span><span class="o">::</span><span class="n">ModuleAttr</span><span class="o">::</span><span class="n">TileDy</span><span class="p">).</span><span class="n">getInt</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Create an OpBuilder instance, which is a helper to create/modify MLIR operations.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">OpBuilder</span> <span class="n">builder</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get the &#39;main&#39; function from the module.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">func</span><span class="o">::</span><span class="n">FuncOp</span> <span class="n">main</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">getMainFuncOp</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Get the first block (entry block) of the main function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span><span class="o">&amp;</span> <span class="n">main_block</span> <span class="o">=</span> <span class="n">main</span><span class="p">.</span><span class="n">getBody</span><span class="p">().</span><span class="n">front</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span><span class="o">&amp;</span> <span class="nl">inner</span> <span class="p">:</span> <span class="n">main_block</span><span class="p">.</span><span class="n">getOperations</span><span class="p">())</span> <span class="p">{</span>  <span class="c1">// Iterate over all operations within the main function&#39;s body
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">CallOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// The module&#39;s main function contains CallOps. This implies an indirect call to a subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Find the subgraph definition (&#39;SubraphOp&#39;) using the symbol name from the CallOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span> <span class="n">sg</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">lookupSymbol</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">SubgraphOp</span><span class="o">&gt;</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">          <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">CallOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">).</span><span class="n">getCallee</span><span class="p">());</span>
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">      <span class="c1">// Walk through the operations inside the called subgraph.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// We are looking for the &#39;GroupOp&#39; which is the actual unit of computation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">sg</span><span class="p">.</span><span class="n">walk</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span> <span class="n">gop</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Set a &#39;dev_region&#39; attribute on the located GroupOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">setDevRegionAttr</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">(),</span> <span class="n">gop</span><span class="p">.</span><span class="n">getOperation</span><span class="p">(),</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">});</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">))</span> <span class="p">{</span>  <span class="c1">// The module&#39;s main function directly contains GroupOps.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// Directly set the &#39;dev_region&#39; attribute on the GroupOp found in the main function.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">setDevRegionAttr</span><span class="p">(</span><span class="n">builder</span><span class="p">,</span> <span class="n">module</span><span class="p">.</span><span class="n">getContext</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">                       <span class="n">llvm</span><span class="o">::</span><span class="n">dyn_cast</span><span class="o">&lt;</span><span class="n">tx8e</span><span class="o">::</span><span class="n">GroupOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inner</span><span class="p">).</span><span class="n">getOperation</span><span class="p">(),</span> <span class="n">x_dim</span><span class="p">,</span> <span class="n">y_dim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">GroupMappingPass</span><span class="o">::</span><span class="n">runOnOperation</span><span class="p">()</span> <span class="p">{</span>  <span class="c1">// It will operate on the entire ModuleOp.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">auto</span> <span class="n">module</span> <span class="o">=</span> <span class="n">getOperation</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">  <span class="n">simpleGroupMapping</span><span class="p">(</span><span class="n">module</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div></div>
</details><br>
<h2 id="groupcostpass">GroupCostPass</h2>
<p><code>GroupCostPass</code> 作用是为一个 GroupOp 在所有可能的切分策略中，通过 Cost Model 搜索并应用最优的一个。算法流程如下。</p>
<p>准备阶段 (Preparation):</p>
<ol>
<li>Bailout Condition: <code>if (gop-&gt;hasAttr(&quot;group_tag&quot;) &amp;&amp; ... == 2) return;</code> 如果 GroupOp的 <code>group_tag==2</code>，那么这个 Pass 就无需为它搜索切分策略了，直接返回。</li>
<li>拷贝编译选项: <code>costoption_lg.dynCompile = compileOption_-&gt;dynCompile;</code> 从一个全局的<code>compileOption_</code> 中拷贝了一系列编译参数到局部的 costoption_lg 中. 表明 Pass 的行为可以被外部配置所影响。</li>
<li>创建搜索空间: <code>auto space = std::make_shared&lt;SliceSpace&gt;();</code> 创建了一个名为 space 的对象，这个 SliceSpace 类封装了该 GroupOp 的完整搜索空间。它包含了所有可能的张量切分方式。</li>
<li>模板机制: <code>if (useTemplate) { ... }</code> 检查 <code>compileOption_-&gt;sliceHelpMap</code> 的全局映射。如果之前已经为相似的 GroupOp (由 GroupKey 标识) 计算过最优策略，它就会直接从缓存中读取结果 (sliceHelp) ，从而避免昂贵的重复搜索。如果找到了模板，它会直接应用并提前返回。</li>
</ol>
<p>搜迭代搜索循环 (The Core: Iterative Search Loop)</p>
<ol>
<li><code>while (1)</code> 循环: 这个无限循环是搜索算法的主体。</li>
<li>探索策略: 在循环内部，space对象会生成一个候选的切分策略。这通过 <code>space-&gt;shardingLevel</code> 和<code>space-&gt;factorSpace_</code> 来控制，它们共同定义了当前正在尝试的切分维度和方式。</li>
<li>判断搜索是否完成: <code>if (space-&gt;shardingLevel.isSpaceFinish() &amp;&amp; ...)</code>. 在每次迭代开始时，它会检查是否已经遍历了所有的切分可能性。如果搜索空间已耗尽，循环就会终止。</li>
<li>成本估算: 如果找到一个有效的候选策略，接下来就是估算这个策略的成本。动态构建Pass流水线:</li>
</ol>
<ul>
<li><code>auto pm = std::make_unique&lt;LgPassManager&gt;(...);</code> 添加一系列估算Pass:
<ul>
<li><code>pm-&gt;add_pass(createDataSplitNewPass(space));</code> // 根据策略进行数据切分</li>
<li><code>pm-&gt;add_pass(createTimeStepNewPass(space));</code> // 划分时间步</li>
<li><code>pm-&gt;add_pass(createSPMAllocPass(space));</code>    // 模拟SPM (片上内存) 分配</li>
<li><code>pm-&gt;add_pass(createEstimatePass(space));</code>    // 估算性能/成本</li>
</ul>
</li>
<li>运行估算流程: <code>pm-&gt;run(gop);</code></li>
</ul>
<ol start="5">
<li>比较和选择最优解: 估算完成后，<code>space-&gt;status</code> 会被更新 (SSTATUS_OK 表示估算成功，SSTATUS_SA_MemAlloc 表示内存分配失败) . 如果估算成功，它会获取成本 t，并与已知的 bestCost 进行比较。如果当前策略更优，就更新 bestCost 和 bestStrategy。</li>
</ol>
<p>应用最优策略 (Applying the Best Strategy)</p>
<ol>
<li>应用策略: <code>sliceHelp.strategy = space-&gt;strategy;</code> 和后续的 <code>compileOption_-&gt;IRHelp.ops[gop] = space-&gt;stageOps;</code> 等赋值操作，就是将搜索到的最优策略结果 (包括每个操作的切分方式、循环信息等) 保存到 compileOption_中，供后续的 Pass 使用。</li>
<li>具体计算: <code>gop-&gt;walk(...)</code> 它遍历 GroupOp 内部的操作 (如GemmOp) ，并根据策略 (lSharding, rSharding) 计算出具体的循环边界 (ls, rs) 和分片长度 (pLen) ，这些信息会被存入 gls (<code>GroupLoopSpace</code>) 对象中。</li>
</ol>
<h3 id="datasplitnewpass">DataSplitNewPass</h3>
<p>其中也包括好几个 pass
<code>DS_SpaceInitPass</code> 作用是初始化分布式策略的搜索空间。对 groupOp 中的每一个算子，它会调用 <code>space_-&gt;shardinglevel.init</code> 这个函数会根据算子自身的特性、全局约束 (如 max_sharding) 以及用户配置 (如 opt_search) ，生成该算子所有可能的切分方式。</p>
<p><code>init</code> 函数首先获取了算子的维度 dim 和目标切分路数 maxSharding，然后调用 getShardings 找出一个张量在所有维度上进行整数倍切分、且总切分路数恰好等于 maxSharding 的所有组合来填充 shardings 列表。随后，将这些组合 (并额外加上了不切分的方案) 包装成带有性能评估因子的 ShardingSpace 对象，并存入一个有序集合 <code>std::set&lt;ShardingSpace&gt; spaces</code> 中。ShardingSpace 重载了小于操作符用于对切分策略排序。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ShardingSpace</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">std</span><span class="o">::</span><span class="n">list</span><span class="o">&lt;</span><span class="n">ShardingInfo</span><span class="o">&gt;</span> <span class="n">shardings</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 预估的性能参数，即空间上能用到pow(2,x)个tile
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">uint8_t</span> <span class="n">factor</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 关键点：重载小于操作符，定义排序规则
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">bool</span> <span class="k">operator</span><span class="o">&lt;</span><span class="p">(</span><span class="k">const</span> <span class="n">ShardingSpace</span> <span class="o">&amp;</span><span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 性能高的在前面
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">return</span> <span class="n">factor</span> <span class="o">&gt;</span> <span class="n">other</span><span class="p">.</span><span class="n">factor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="kt">bool</span> <span class="k">operator</span><span class="o">==</span><span class="p">(</span><span class="k">const</span> <span class="n">ShardingSpace</span> <span class="o">&amp;</span><span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">factor</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">factor</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ShardingLevel</span><span class="o">::</span><span class="n">init</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">nFirst</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">opt_search</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// ... 清理和准备工作 ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  
</span></span><span class="line"><span class="cl">  <span class="c1">// 1. 获取算子输出Tensor的维度数量 (Rank) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">int32_t</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">op</span><span class="o">-&gt;</span><span class="n">getResult</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">getType</span><span class="p">().</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">ShapedType</span><span class="o">&gt;</span><span class="p">().</span><span class="n">getRank</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 2. 准备容器
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&gt;</span> <span class="n">shardings</span><span class="p">;</span> <span class="c1">// 用于接收所有合法的sharding方案
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span> <span class="n">tempSharding</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>   <span class="c1">// 一个临时的、大小为dim的向量，用于递归
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 3. 调用核心递归函数，启动搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - curDim=0: 从第0维开始搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - allDim=dim: 总共有dim个维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - curSharded=1: 当前已累乘的切分系数为1 (乘法单位元) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    - maxSharding: 最大切分数目，即为每个 chip 的 tile 数目 (16)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">getShardings</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">tempSharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 4. 手动添加“不切分”的方案
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    递归函数只会寻找乘积等于maxSharding的组合，但[1, 1, ..., 1] (不切分)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">//    是一个非常重要的基础方案，这里手动添加进去。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">shardings</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// ... 后续处理 ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">sharding</span> <span class="p">:</span> <span class="n">shardings</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">ShardingSpace</span> <span class="n">newShardingSpace</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">isValid</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 1. 为每个sharding方案计算性能因子
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">newShardingSpace</span><span class="p">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">getFactor</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">sharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// ... (省略部分逻辑) ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    
</span></span><span class="line"><span class="cl">    <span class="c1">// 2. 将包含factor的ShardingSpace对象插入set中
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">spaces</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">newShardingSpace</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>getShardings</code> 函数采用的是递归算法，目标是找到所有整数向量 <code>s = {s_0, s_1, ..., s_{dim-1}}</code>，使得 <code>s_0 * s_1 * ... * s_{dim-1} == maxSharding</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ShardingLevel</span><span class="o">::</span><span class="n">getShardings</span><span class="p">(</span><span class="kt">int32_t</span> <span class="n">curDim</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">allDim</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">curSharded</span><span class="p">,</span> <span class="kt">int32_t</span> <span class="n">maxSharding</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&gt;&amp;</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;&amp;</span> <span class="n">sharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 1. 递归终止条件 (Base Case) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">if</span> <span class="p">(</span><span class="n">curDim</span> <span class="o">==</span> <span class="n">allDim</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 已经处理完所有维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">curSharded</span> <span class="o">==</span> <span class="n">maxSharding</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// 并且累乘结果正好等于目标
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// // succeeded
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">shardings</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">(</span><span class="n">sharding</span><span class="p">);</span> <span class="c1">// 找到了一个合法解，存入结果列表
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span><span class="p">;</span> <span class="c1">// 回溯
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 2. 递归主体：遍历当前维度的所有可能切分系数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int32_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="n">maxSharding</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 尝试将当前维度(curDim)的切分系数设为 i
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">sharding</span><span class="p">[</span><span class="n">curDim</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 更新已累乘的切分系数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">curSharded</span> <span class="o">*=</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 3. 剪枝优化 (Pruning) ：这是算法效率的关键！
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 如果当前累乘的结果已经超过了目标，那么无论后续维度如何取值，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 最终结果必然大于 maxSharding，所以没有必要继续递归下去了。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="p">(</span><span class="n">curSharded</span> <span class="o">&lt;=</span> <span class="n">maxSharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 如果还有希望，则对下一个维度进行递归搜索
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">getShardings</span><span class="p">(</span><span class="n">curDim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">allDim</span><span class="p">,</span> <span class="n">curSharded</span><span class="p">,</span> <span class="n">maxSharding</span><span class="p">,</span> <span class="n">shardings</span><span class="p">,</span> <span class="n">sharding</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 4. 回溯 (Backtracking) 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 无论上面的递归是否成功，当它返回后，我们需要“撤销”当前的选择，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 以便在 for 循环的下一次迭代中尝试新的值。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">curSharded</span> <span class="o">/=</span> <span class="n">i</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><code>getFactor</code> 遍历每个维度，基于内存对齐等硬件限制，计算出该维度上最大合理的切分数量 maxShardingDim.
将用户提议的切分数量 <code>sharding[i]</code> 与 maxShardingDim 取最小值，得到该维度上的有效切分数量。将所有维度上的有效切分数量相乘，得到总的有效并行度 tileNum. 对 tileNum 取以2为底的对数并向上取整后返回。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">uint8_t</span> <span class="n">ShadingLevel</span><span class="o">::</span><span class="n">getFactor</span><span class="p">(</span><span class="n">mlir</span><span class="o">::</span><span class="n">Operation</span><span class="o">*</span> <span class="n">op</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint32_t</span><span class="o">&gt;</span> <span class="n">sharding</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="kt">int</span> <span class="n">tileNum</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">rank</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// a. 判断是否需要对齐：这里只对最后一个维度特殊处理
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">align</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// b. 获取对齐基数 (alignBase)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    如果需要对齐，则调用 GetAlignBase 获取一个对齐值，否则为1 (相当于不对齐) 。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这个 alignBase 很可能代表硬件一次最优处理的最小数据块大小。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">uint32_t</span> <span class="n">alignBase</span> <span class="o">=</span> <span class="n">align</span> <span class="o">?</span> <span class="n">GetAlignBase</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span> <span class="o">:</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// c. 计算当前维度的最大合理切分路数 (maxShardingDim)
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    一个维度能被切成多少份，不仅取决于它的总大小，还取决于对齐要求。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    例如，一个维度大小为100，但硬件要求必须按16对齐处理，那么最多只能切成 ceil(100/16) = 7 份。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">auto</span> <span class="n">maxShardingDim</span> <span class="o">=</span> <span class="n">CEIL</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alignBase</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// d. 计算“有效”的切分路数并累乘
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这是关键！它在“提议的切分路数(sharding[i])”和“最大合理切分路数(maxShardingDim)”之间取最小值。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这意味着，即使你提议将一个维度切16份，但如果硬件限制最多只能切7份，那也只能算7份的贡献。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">//    这可以防止对一个维度进行“无效的过度切分”。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">tileNum</span> <span class="o">*=</span> <span class="n">MIN</span><span class="p">(</span><span class="n">maxShardingDim</span><span class="p">,</span> <span class="n">sharding</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">ceil</span><span class="p">(</span><span class="n">log2</span><span class="p">(</span><span class="n">tileNum</span><span class="p">)));</span> <span class="c1">// 向上取整
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>一个例子如下</p>
<pre tabindex="0"><code>storeOp outShape[3, 4, 128, 4096]
level0: [1, 1, 1, 16], [1, 1, 2, 8], [1, 1, 4, 4]...   factor=16
level1:[1, 8, 1, 2], [1, 8, 2, 1]....                  factor=8
level2:[1, 16, 1, 1]                                   factor=4 
level3:[16, 1, 1, 1]                                   factor=2
</code></pre><p><code>DS_TileShardingPass</code> 按顺序遍历 groupOp 中的算子，并像一个状态机一样检查和更新各算子的分布式策略状态。其在每次执行时，仅为当前的待定算子，从其预先生成并排好序的候选策略列表中，选出下一个最优的切分方案并进行更新，然后立即终止当次运行。整个图的最终切分方案是通过反复执行此 Pass，将决策从图的入口逐步传播到出口而最终确定的。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBda0206ff3ade37b3cb94b73f3a564489?method=download&amp;shareKey=bd6f962093568ee599a982e7b7b1a300" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBda0206ff3ade37b3cb94b73f3a564489?method=download&amp;shareKey=bd6f962093568ee599a982e7b7b1a300" alt="An Example of Sharding">
    </a><figcaption>An Example of Sharding</figcaption></figure></p>
<p><code>DS_TileSplitPass</code> 首先检查算子是否需要 <code>reduceSplit</code> (例如 GeMM 切分 k 维度). 如果 reduce 维度切分状态为 <code>s.srfinish = true</code> 才会进行后续的 split 方案。</p>
<ol>
<li>从后向前 (或根据 nFirst 标志决定方向) 检查算子的各个维度，找到第一个“还可以再切分”的维度。判断依据是该维度切分后的大小是否已达到系统设定的最小值 (s.sliceShapeMin) .</li>
<li>一旦找到目标维度 updateDim，它会调用一个名为 <code>getNextSplit</code> 的函数。它会根据当前维度的切分值 <code>s.splitTry[updateDim]</code> 计算出下一个可能的切分值。例如，如果当前是 2，getNextSplit 可能会返回 4.</li>
<li>更新与记录：它将这个新的切分值更新到尝试性方案 <code>s.splitTry</code> 中，并记录下这次更新<code>space_-&gt;splitRecord.update(...)</code>.</li>
<li>在对当前算子的循环结束时，它会将探索出的 <code>s.splitTry</code> 赋值给最终方案 <code>s.split</code>.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBab1aedb258273670b60e2b54295f1f6c?method=download&amp;shareKey=afd6b38561485627a11fd118670b8431" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBab1aedb258273670b60e2b54295f1f6c?method=download&amp;shareKey=afd6b38561485627a11fd118670b8431" alt="An Example of Split try of above Sharding">
    </a><figcaption>An Example of Split try of above Sharding</figcaption></figure></p>
<p><code>DS_SlicePropagatePass</code> 后序遍历 (即从 groupOp 的输出到输入) 的方式反向传播切分决策，其逻辑是：对于每一个算子 (消费者)，它会调用该算子实现的 <code>ShardingInterface</code> 接口中的 <code>tileShardingSplit</code> 方法，来精确计算出其上游算子 (生产者) 应该如何切分数据以满足消费者的需求。这如果自动接口推导失败，它会回退去读取算子上预设的 <code>tile_parallel</code> 属性作为人工指令。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBb14ede2d32d997490222f36c1f0acc21?method=download&amp;shareKey=a244cf2ec9b5f31b5f3ef0ff2aa370a0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBb14ede2d32d997490222f36c1f0acc21?method=download&amp;shareKey=a244cf2ec9b5f31b5f3ef0ff2aa370a0" alt="An Example of Propagation">
    </a><figcaption>An Example of Propagation</figcaption></figure></p>
<p><code>DS_UpdateSliceIRPass</code> 核心策略是通过分析图中 reduceOp 来反向推断和划分流水线阶段。通过检查每个 reduceOp 自身的并行复杂度 (例如，tpSplit &gt; 1) 来判断其上游的计算类型，从而为不同的流水线打上诸如 STAGEIC2OC (模型并行规约段) 或 STAGEOC2NH (模式切换段) 的标签。在完成对所有算子的阶段划分后，它会最终计算每个阶段的流水线深度，并整理输出一份包含并行循环类型、算子分组和流水线阶段信息的完整执行。</p>
<ol>
<li>
<p>首先从 reduceOps 栈中取出一个关卡算子。然后，它利用 <code>getNEOPTPSlice</code> 等辅助函数，分析这个算子自身的切分策略，判断它具体采用了哪种张量并行方式。</p>
</li>
<li>
<p>确定连接到当前这个 reduceOp 的上一段流水线是什么类型</p>
</li>
</ol>
<ul>
<li><code>if (tpSplit &gt; 1)</code>: 如果这个关卡算子本身是一个张量并行度大于 1 的算子，代码就判断出：通往这个算子的路径，是一段需要最终进行集合通信 (C) 的路径。因此，它将这段路径的类型标记为 <code>STAGEIC2OC</code>.</li>
<li><code>else if (s.reduceSplit &gt; 1)</code>：如果不是上面那种情况，代码会检查另一种模型并行模式。如果一个算子的规约维度被切分了，同样意味着后续需要一个 AllReduce 集合通信。因此，它把这段路径标记为 <code>STAGEIC2IC</code>.</li>
<li>如果两个条件都不满足，意味着这可能是一个不同并行模式之间的切换，例如从模型并行切换回数据并行，此时会使用默认的 <code>STAGEOC2NH</code> 标记.</li>
</ul>
<ol start="3">
<li>通过 <code>updateLoopStage</code> 函数，将两个 reduceOp 算子之间的所有普通算子，都归类到刚刚在第 2 步中决策出的 lastRuduceLoopStage.</li>
<li>处理完所有的 reduceOp 后遍历所有算子，根据 LoopStageMap_ 中的记录，将算子放入对应的“篮子”里。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBa90c52245265edade98cd9f5b15d59bb?method=download&amp;shareKey=ccf9d74acc3642b1eac881133e98c6b9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBa90c52245265edade98cd9f5b15d59bb?method=download&amp;shareKey=ccf9d74acc3642b1eac881133e98c6b9" alt="DS_UpdateSliceIRPass">
    </a><figcaption>DS_UpdateSliceIRPass</figcaption></figure></p>
<h3 id="ts_swpipelinepass">TS_SwPipelinePass</h3>
<p>TS_SwPipelinePass 核心是调用 getPipeline 函数。其内部通过顺序执行以下三个关键步骤，。</p>
<p><code>getInitPipelineOps</code></p>
<ol>
<li>为每个流水线阶段 (如 STAGENH2OC, STAGEOC2IC等) 创建一个独立的 pipeline 列表。</li>
<li>按 IC -&gt; OC -&gt; NH 顺序来拼接这些列表。在拼接时，它会检查每个阶段的循环次数。如果循环次数大于1：它并不会简单地将操作列表复制多次，而是创建一个特殊的、类型为 PipelineOpsBase 的 <strong>Repeat 节点</strong>。这个节点内部包含需要重复的子流水线 (<code>repeatBase.repeat</code>) 和重复次数 (<code>repeatBase.repeatTimes</code>) . 然后，它将这个Repeat 节点作为一个单一的、原子性的元素，插入到下一个阶段的流水线中。这是一种高效表示嵌套循环的方法。
如果循环次数不为 1：它就直接使用 splice 操作，将当前阶段的算子列表完整地移动并拼接到下一个阶段的尾部。</li>
</ol>
<p>经过层层拼接和嵌套，该函数最终返回一个名为 groupPipeline 的 std::list。这个列表就是一份完整的、线性的逻辑执行剧本，其中所有的嵌套循环都被抽象成了 Repeat 节点。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB58fc5fb9b7b1fb4880eb020bce68afd5?method=download&amp;shareKey=f4ae610fb730689a22fe38a7226ee6e5" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB58fc5fb9b7b1fb4880eb020bce68afd5?method=download&amp;shareKey=f4ae610fb730689a22fe38a7226ee6e5" alt="getInitPipelineOps">
    </a><figcaption>getInitPipelineOps</figcaption></figure></p>
<p><code>pipeline </code></p>
<p>主要工作是处理上一阶段生成的 Repeat 节点，并对流水线的衔接处进行深度优化，以减少气泡 (硬件空闲周期) 。</p>
<ol>
<li>
<p>当它在流水线中遇到一个 Repeat 节点时，它会对该节点内部的子流水线再次调用pipeline函数 (<code>auto inner = pipeline((it).repeat, ...)</code>). 通过这种方式展开任意层级的嵌套循环。</p>
</li>
<li>
<p>在处理循环的边界时，它调用 getRetract 和 doRetract 这对复杂的优化工具。</p>
</li>
</ol>
<ul>
<li><code>getRetract</code>: 在连接两个循环迭代 (或不同的流水线段) 时，通过 canParallel 函数检查后一个迭代的“头部指令”是否可以和前一个迭代的“尾部指令”并行执行，从而计算出最大可以“回缩” (即提前执行) 的指令数量。</li>
<li><code>doRetract</code>: 在 getRetract 探明了可回缩的数量后，doRetract 负责物理地修改流水线。它通过 splice 操作，将后一个迭代头部的指令，合并到前一个迭代尾部的指令列表中，从而填补了潜在的执行空隙。</li>
</ul>
<p><code>getEnginsPipeline</code> 将优化后的操作序列，翻译成具体的、分配到不同硬件引擎的指令。</p>
<ol>
<li>
<p>函数遍历输入的 pipelineOps 列表。列表中的每个元素 opsBase 代表一个流水线周期 (一“帧”) 内需要共同执行的一组MLIR操作。</p>
</li>
<li>
<p>对于每个周期，它创建一个 enginsBase 对象。这个对象是一个结构体，包含了分别对应不同硬件引擎 (如 <code>ld</code> for Load, <code>st</code> for Store, <code>ne</code> for Neural Engine, <code>tdma</code> for DMA) 的成员变量。</p>
</li>
<li>
<p>遍历当前周期的所有 op，通过查询每个 op 的 engine 属性 <code>queryOpAttr().engine</code>，得知这个操作预定由哪个硬件引擎来执行。接着，它将这个 op 的指针存放到 enginsBase 对象中对应的引擎 slot 里。例如，一个 <code>NPU_ENGINE_LOAD</code> 类型的操作会被放入 <code>enginsBase.ld</code> 列表。</p>
</li>
</ol>
<p>函数最终返回一个 <code>std::list&lt;PipelineBase&gt;</code> 描述了在同一个时钟周期内，加载、存储、计算等多个硬件单元应该同时执行**哪些不同的操作。</p>
<h3 id="spmallocpass">SPMAllocPass</h3>
<p>SPMAllocPass 包括三个 pass，下面依次介绍，首先介绍用到的数据结构</p>
<p><code>BufferLabel</code> 作为缓冲区的唯一标识符，将其链接到程序中的特定 <code>mlir::Value</code> ，并注意它是否为 Imm.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @struct BufferLabel
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @brief A unique identifier for a memory buffer.
</span></span></span><span class="line"><span class="cl"><span class="cm"> *
</span></span></span><span class="line"><span class="cl"><span class="cm"> * This struct links a buffer to a specific MLIR Value and tracks whether it&#39;s
</span></span></span><span class="line"><span class="cl"><span class="cm"> * a special &#34;immediate&#34; buffer. It&#39;s used as a key in maps to associate
</span></span></span><span class="line"><span class="cl"><span class="cm"> * MLIR Values with their buffer metadata.
</span></span></span><span class="line"><span class="cl"><span class="cm"> */</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">BufferLabel</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// The MLIR Value that this buffer represents, typically a tensor produced
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// by an operation.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">mlir</span><span class="o">::</span><span class="n">Value</span> <span class="n">v</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// A flag indicating if this buffer holds a special &#34;immediate&#34; value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// Immediate values might be treated differently during allocation (e.g.,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// small constants or internal scratchpads for an op).
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">bool</span> <span class="n">isImm</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @brief Equality operator to compare two labels.
</span></span></span><span class="line"><span class="cl"><span class="cm">     *
</span></span></span><span class="line"><span class="cl"><span class="cm">     * Two labels are considered equal if they refer to the same MLIR Value
</span></span></span><span class="line"><span class="cl"><span class="cm">     * and have the same &#39;isImm&#39; status. This is necessary for using
</span></span></span><span class="line"><span class="cl"><span class="cm">     * BufferLabel as a key in std::map or std::unordered_map.
</span></span></span><span class="line"><span class="cl"><span class="cm">     */</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="k">operator</span><span class="o">==</span><span class="p">(</span><span class="k">const</span> <span class="n">BufferLabel</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">v</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">isImm</span> <span class="o">==</span> <span class="n">other</span><span class="p">.</span><span class="n">isImm</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p><code>ValueBuffer</code> 包含单个缓冲区所需的所有元数据，包括其标识、生存期、大小和最终内存位置。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @struct ValueBuffer
</span></span></span><span class="line"><span class="cl"><span class="cm"> * @brief Represents the metadata for a single memory buffer, including its
</span></span></span><span class="line"><span class="cl"><span class="cm"> * lifetime, size, and allocation information.
</span></span></span><span class="line"><span class="cl"><span class="cm"> */</span>
</span></span><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">ValueBuffer</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// The unique identifier for this buffer.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">BufferLabel</span> <span class="n">label</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Represents the starting point of the buffer&#39;s lifetime (inclusive),
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// measured in pipeline cycles. After memory allocation, this field may be
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// repurposed to store the starting memory address.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">start</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Represents the ending point of the buffer&#39;s lifetime (inclusive),
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// measured in pipeline cycles. After memory allocation, this field may be
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// repurposed to store the ending memory address.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">end</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// The total size of this buffer in bytes, as required by its tensor shape.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">allSize</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Size of an intermediate/temporary buffer that an operator might need
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// internally. This is often allocated contiguously with the main output
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// buffer. For example, the final output address would be &#39;offset + immSize&#39;.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">immSize</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// The final memory offset assigned to this buffer in the scratchpad memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// This value is determined by the final memory allocation pass.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">int64_t</span> <span class="n">offset</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="cm">/**
</span></span></span><span class="line"><span class="cl"><span class="cm">     * @brief Less-than operator, used for sorting ValueBuffer objects.
</span></span></span><span class="line"><span class="cl"><span class="cm">     *
</span></span></span><span class="line"><span class="cl"><span class="cm">     * The active implementation sorts buffers primarily by their lifetime start
</span></span></span><span class="line"><span class="cl"><span class="cm">     * time. This is a common strategy for greedy &#34;first fit&#34; style memory
</span></span></span><span class="line"><span class="cl"><span class="cm">     * allocation algorithms. The commented-out code shows an alternative
</span></span></span><span class="line"><span class="cl"><span class="cm">     * strategy of sorting by buffer size.
</span></span></span><span class="line"><span class="cl"><span class="cm">     */</span>
</span></span><span class="line"><span class="cl">    <span class="kt">bool</span> <span class="k">operator</span><span class="o">&lt;</span><span class="p">(</span><span class="k">const</span> <span class="n">ValueBuffer</span><span class="o">&amp;</span> <span class="n">other</span><span class="p">)</span> <span class="k">const</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// return this-&gt;allSize &lt; other.allSize; // Alternative sorting by size
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">return</span> <span class="k">this</span><span class="o">-&gt;</span><span class="n">start</span> <span class="o">&lt;=</span> <span class="n">other</span><span class="p">.</span><span class="n">start</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></div><p><code>SA_BufferLifePass</code>的核心功能是分析并确定每一个需要存放在 ScratchPad Memory 中的数据块 (Buffer，即mlir::Value对应的张量) 的生命周期。</p>
<ol>
<li>构建“定义-使用”时间表。Pass 的输入是 <code>TS_SwPipelinePass</code> 生成的最终流水线执行序列 pipelineReal. 这个序列的每一项都代表一个流水线周期，以及该周期内各个硬件引擎执行的操作。代码遍历这个流水线序列，逐个周期 (由timeStepNum计数) 地分析。它会构建两个核心的映射表：</li>
</ol>
<ul>
<li><code>opIsTemp</code>: 记录在哪一个时间步 (timeStepNum) ，有哪些值 (mlir::Value) 被定义或产出。例如，ld (加载) 和 ne (计算) 操作的输出都会被记录。</li>
<li><code>consumerOps</code>: 记录在哪一个时间步，有哪些值被作为输入消费掉了。</li>
</ul>
<p>产出：这个步骤完成后，Pass就拥有了一份完整的、按时间步索引的“谁在何时被创建”和“谁在何时被使用”的清单。</p>
<ol start="2">
<li>确定每个Buffer的生命周期。Pass会遍历所有算子和它们的输入 (operands) ，为每一个作为输入的 Value (即inValue) 计算其生命周期。</li>
</ol>
<ul>
<li>确定生命周期终点 (end)：一个 Value 的生命周期，在其被作为输入 (被消费) 时达到一个终点。因此，当代码在时间步 curTs 处理一个消费者算子时，其输入 inValue 的 <code>buf.end</code> 就被设置为 curTs.</li>
<li>确定生命周期起点 (start)：为了找到inValue何时被创建，代码会调用一个 <code>getNearestProducer</code> 的函数。这个函数会拿着当前的消费时间 curTs 和 inValue，去第一步生成的 opIsTemp (定义时间表) 中反向查找，找到离 curTs 最近的、inValue 被定义的那个时间步 <code>buf.start</code>.</li>
</ul>
<p>计算出的 start 和 end，连同 Value 的标识 (BufferLabel) ，被封装在 ValueBuffer 结构体中，并存入一个全局的数据结构 <code>space_-&gt;vBuffer</code> 里。</p>
<ol start="3">
<li>特殊情况处理</li>
</ol>
<ul>
<li><code>In-place</code>: 对于输入和输出复用同一块内存的 in-place 操作，其生命周期计算必须追溯到最初提供这块内存的那个非in-place算子。代码通过 <code>getInplaceIndex</code> 递归地回溯in-place链，以确保生命周期的 start 时间是正确的、最开始的那个定义时间。</li>
<li>中间值 (imm) 与累加值 (Psum): 代码会识别一些特殊的、可能在多个时间步中存在的中间值或累加值 (由getImmSize 或 isPsumValue 识别) . 对于这些值，它们可能会有多个离散的生存区间。Pass 中可能包含一些后处理逻辑，将这些离散的区间合并成一个从“最早的start”到“最晚的end”的连续大区间，以简化后续的内存分配。</li>
</ul>
<p><code>SA_BufferMergePass</code> 的任务就是清理这些冗余或复杂的生命周期记录，具体来说，就是合并那些存在时间上重叠或包含关系的生命周期区间，为后续的内存分配器提供一个最簡洁、无冗余的区间列表。</p>
<p>遍历由上一个 Pass 生成的 space_-&gt;vBuffer 这个map. 其中的每一项，key 是缓冲区的唯一标识 BufferLabel，value是该缓冲区所有生命周期区间的列表 <code>std::vector&lt;ValueBuffer&gt;</code>. 对于每一个value的生命周期列表，它都调用 <code>mergeOverlap</code> 来进行处理。最后，它用函数返回的、经过清理和合并的新的列表，来替换掉 map 中旧的列表。该函数流程如下</p>
<ol>
<li>根据 ValueBuffer 重载的 <code>operator&lt;</code> (即按 start 时间升序) ，将所有生命周期区间进行排序。</li>
<li>遍历已排序的列表，将 start 时间相同的连续区间收集到一个临时的 buf 向量中。遇到一个不同 start 时间的区间时，它会按照结束时间 end 排序之前收集的 buf，然后将处理后的结果 (除了最后一个元素) 重新放回 valueBuf.</li>
<li>合并被完全包含的子区间。它维护着当前最大的生命周期区间 (<code>[usedTSStart, usedTSEnd]</code>). 遍历列表中的每一个区间 <code>*it</code>. 根据 <code>bool isSub = ((*it).start &gt;= usedTSStart) &amp;&amp; ((*it).end &lt;= usedTSEnd);</code> 判断区间是否在时间上被上一个“激活”的区间完全覆盖。</li>
</ol>
<ul>
<li>如果 isSub 为 true，意味着 *it 是一个冗余的子区间。因为只要为那个更大的激活区间分配了内存，这个子区间的内存需求自然也就满足了。因此，代码通过 valueBuf.erase(it); 将这个冗余的子区间直接删除。</li>
<li>如果 isSub 为 false，说明遇到了一个新的、没有被覆盖的生命周期，于是它将成为新的“激活”区间，用于和后续的区间进行比较。</li>
</ul>
<h1 id="compile-option-1-opt_barrier">Compile Option 1: opt_barrier</h1>
<p>由 <code>groupDAGPass</code> 实现。通过 group 间的依赖关系来给 group 定层级，同一层级的 group 只有最后一个 group 需要 barrier.</p>
<ol>
<li>
<p>初始化所有 group 的 need_barrier 属性为 false。</p>
</li>
<li>
<p>从后往前遍历 group，若 group 的结果无 user 或要 return，设置 layerNum 为 0，否则设置为 userOp 的 layerNum + 1. 同时维护两个 vector: firstOpInLayers 和lastOpInLayers 来记录每一层级的第一个 op 和最后一个 op. 遍历结束把 lastOpInLayers中的 group 的 need_barrier 属性设为 true.</p>
</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB8e3541f411fb9039713f3992b450f4b9?method=download&amp;shareKey=4f5dfad0d8b5a4bd8cf49ce94d5ad7c9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB8e3541f411fb9039713f3992b450f4b9?method=download&amp;shareKey=4f5dfad0d8b5a4bd8cf49ce94d5ad7c9" alt="opt_barrier">
    </a><figcaption>opt_barrier</figcaption></figure></p>
<h1 id="compile-option-1-opt_ddr">Compile Option 1: opt_ddr</h1>
<p>由 <code>ddrConstReorderPass</code> 和 <code>ddrVarReorderPass</code> 实现。通过改变 const 和 var 在 ddr 中的排布，使其对齐 DDR_BANK(4096Bytes)，实现加速读取。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd3b8e4e14ab8e2c23f22a625b1d03ddd?method=download&amp;shareKey=948a94db71e0adcddb5f6107ad94a6d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd3b8e4e14ab8e2c23f22a625b1d03ddd?method=download&amp;shareKey=948a94db71e0adcddb5f6107ad94a6d0" alt="opt_ddr">
    </a><figcaption>opt_ddr</figcaption></figure></p>
]]></content:encoded>
    </item>
  </channel>
</rss>
