<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DeepSeek on WITHER</title>
    <link>http://localhost:1313/tags/deepseek/</link>
    <description>Recent content in DeepSeek on WITHER</description>
    <generator>Hugo -- 0.147.7</generator>
    <language>en</language>
    <copyright>2024-2025 WITHER</copyright>
    <lastBuildDate>Sat, 21 Jun 2025 22:00:13 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/deepseek/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DualPipe</title>
      <link>http://localhost:1313/blogs/deepseek/dualpipe/</link>
      <pubDate>Sat, 21 Jun 2025 22:00:13 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepseek/dualpipe/</guid>
      <description>Source code reading of DualPipe</description>
      <content:encoded><![CDATA[<h1 id="preliminary">Preliminary</h1>
<p>本节先回顾流水线并行以及 DeepSeek-V3 中作为 baseline 的 <a href="https://arxiv.org/pdf/1806.03377">PipeDream</a> 论文中的 1F1B 和 <a href="https://openreview.net/pdf?id=tuzTN0eIO5">ZeroBubble</a> 论文中的 ZB1P (ZB-H1 的自动搜索结果).</p>
<h2 id="pipedream-1f1b">PipeDream 1F1B</h2>
<p>1F1B (One-Forward-One-Backward) 的工作流程如图所示，想象一条工厂流水线，用于组装一个复杂的设备。这个设备需要经过多个工位（GPU），每个工位负责一部分装配任务（模型的不同层）。当第一个产品的第一个部件在工位1上加工时，其他所有工位都在闲置等待。当它被传递到工位2时，工位1开始加工第二个产品，但工位3、4…依然在等待。这种在流水线启动和结束阶段产生的设备空闲时间，就是流水线气泡 (Pipeline Bubble). 在大模型训练中，这意味着 GPU 算力被浪费，直接导致训练时间延长和成本增加。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB787301b994e9c90258f8ad84fd1f8b67?method=download&amp;shareKey=db772d656fe8be439988e887fd6910a3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB787301b994e9c90258f8ad84fd1f8b67?method=download&amp;shareKey=db772d656fe8be439988e887fd6910a3" alt="1F1B pipeline Schedule">
    </a><figcaption>1F1B pipeline Schedule</figcaption></figure></p>
<p>后续批次的后向传播永远在前一批次的后向传播全部启动后才开始，为了防止激活占用内存过多，图中 1F1B 的 bs=8，流水线并行过程中最多保存 4 个 batch 的激活，当 batch1 反向传播结束后再进行 batch5 的正向传播。为了减少激活占用，1F1B 中进行反向传播的优先级高于正向传播。</p>
<h2 id="zerobubble-zb1p">ZeroBubble ZB1P</h2>
<p>ZeroBubble 减少气泡的关键是将反向传播中对于权重和输入的梯度计算分开进行。传统上，一个层的反向传播包含两个核心任务:</p>
<ul>
<li>B Pass: 计算关于输入梯度并将其传递给前一层，这是误差反向传播链的一部分。</li>
<li>W Pass: 计算该层自身权重的梯度，用于后续的参数更新。</li>
</ul>
<p>如图所示第 i-1 层的 B Pass 依赖于第 i 层的 B Pass. 但第 i 层的 W Pass，只要在其 B Pass 完成之后，可以被灵活地安排在任何时间点执行。</p>
<h2 id="computation-graph-for-mlp">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBeef090dd44e296a4a4e985200c62e4c7?method=download&amp;shareKey=2d9e68a50dd5b0b3c46f079499ed2bec" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBeef090dd44e296a4a4e985200c62e4c7?method=download&amp;shareKey=2d9e68a50dd5b0b3c46f079499ed2bec" alt="Computation Graph for MLP">
    </a><figcaption>Computation Graph for MLP</figcaption></figure></h2>
<p><strong>Handcrafted Pipeline Schedule</strong></p>
<p>基于这个思想，文中提出了两个手工设计的调度方案作为概念验证:</p>
<ul>
<li>ZB-H1 (Memory Efficient Schedule): 在维持与 1F1B 相似峰值内存消耗的情况下，通过将 W Pass 推迟执行，填充了流水线末尾的 cooldown 气泡，成功将气泡大小减少到 1F1B 的三分之一。</li>
<li>ZB-H2 (Zero Bubble Schedule): 当内存预算更宽松时，在流水线 warm-up 安排更多的 F Pass，并巧妙地重排 W Pass，将整个流水线的执行过程从一个梯形变成了一个平行四边形，从而在理论上完全消除了气泡。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9f702f88f4b48c048d602ddfe7b69ffb?method=download&amp;shareKey=010eea5b8230b6175d7777444e4dcc64" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9f702f88f4b48c048d602ddfe7b69ffb?method=download&amp;shareKey=010eea5b8230b6175d7777444e4dcc64" alt="Handcrafted pipeline schedules ZB-H1 (top)  &amp; ZB-H2 (bottom)">
    </a><figcaption>Handcrafted pipeline schedules ZB-H1 (top)  &amp; ZB-H2 (bottom)</figcaption></figure></p>
<p>文中基于一个标准的 Transformer架构，其中 FFN 的中间层维度是模型隐藏维度 <code>h</code> 的4倍。给出了 F, B, W 各自的计算量和激活占用。其中计算量只统计占据主要部分的矩阵乘法的浮点运算次数。</p>
<ul>
<li><code>b</code>: microbatch size</li>
<li><code>s</code>: sequence length</li>
<li><code>h</code>: hidden dimension size</li>
<li><code>a</code>: number of attention heads</li>
</ul>
<h2 id="transformer-architecture">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd9f810c1f506d383eb59a0c1186e602b?method=download&amp;shareKey=ee9e8a521ed880701b05e9b25b1ae001" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd9f810c1f506d383eb59a0c1186e602b?method=download&amp;shareKey=ee9e8a521ed880701b05e9b25b1ae001" alt="Transformer Architecture">
    </a><figcaption>Transformer Architecture</figcaption></figure></h2>
<p><em>Table1: FLOPs and activations memory required per transformer layer for each pass</em></p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Pass</th>
          <th style="text-align: center">FLOPs</th>
          <th style="text-align: center">Activations Memory Required</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">F</td>
          <td style="text-align: center">$sbh(24h+4s)$</td>
          <td style="text-align: center">0</td>
      </tr>
      <tr>
          <td style="text-align: center">B</td>
          <td style="text-align: center">$sbh(24h+8s)$</td>
          <td style="text-align: center">$sb(34h+5as)$</td>
      </tr>
      <tr>
          <td style="text-align: center">W</td>
          <td style="text-align: center">$sbh(24h)$</td>
          <td style="text-align: center">$32sbh$</td>
      </tr>
  </tbody>
</table>
<hr>
<p>前向传播 $T_F \approx (8bsh^2 + 4bs^2h) + 16bsh^2 = 24bsh^2 + 4bs^2h = sbh(24h + 4s)$. 反向传播关于权重的计算量等于 Linear 层的 GEMM.</p>
<ul>
<li>
<p><strong>Self-Attention</strong>: $6bsh^2 + 2bs^2h + 2bs^2h + 2bsh^2 = 8bsh^2 + 4bs^2h$</p>
<ul>
<li><strong>Q, K, V Projection</strong>：输入 <code>(b, s, h)</code> 通过与权重矩阵 <code>(h, h)</code> 相乘，生成Q, K, V。这涉及到3次矩阵乘法。$\text{FLOPs} \approx 2 \times b \times s \times h \times 3h = 6bsh^2$</li>
<li><strong>Attention Score</strong>:<code>Q</code> <code>(b, a, s, h/a)</code> 与 <code>K^T</code> <code>(b, a, h/a, s)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times a \times s \times (h/a) \times s = 2bshs$.</li>
<li><strong>Score@V</strong>：注意力分数 <code>(b, a, s, s)</code> 与 <code>V</code> <code>(b, a, s, h/a)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times a \times s \times s \times (h/a) = 2bshs$.</li>
<li><strong>O Projecyion</strong>：结果与输出权重矩阵 <code>(h, h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times h \times h = 2bsh^2$.</li>
</ul>
</li>
<li>
<p><strong>FFN FLOPs</strong>: $8bsh^2 + 8bsh^2 = 16bsh^2$</p>
<ul>
<li><strong>Up Projection</strong>：输入 <code>(b, s, h)</code> 与权重矩阵 <code>(h, 4h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times h \times 4h = 8bsh^2$.</li>
<li><strong>Down Projection</strong>：中间结果 <code>(b, s, 4h)</code> 与权重矩阵 <code>(4h, h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times 4h \times h = 8bsh^2$.</li>
</ul>
</li>
</ul>
<hr>
<p>激活占用方面除了 Dropout Mask 是 INT8 类型以外，假设 activations 均以 16-bit float 类型保存。表中的 activation memory 均以字节为单位进行统计。和权重梯度无关的部分只有 dropout 相关的以及 Softmax output.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Category</th>
          <th style="text-align: center">Item</th>
          <th style="text-align: center">Original</th>
          <th style="text-align: center">TP</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><strong>Attention</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$11sbh + 5as^2b$</strong></td>
          <td style="text-align: center"><strong>$3sbh + \frac{8sbh}{t} + \frac{5as^2b}{t}$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">QKV input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">QK^T</td>
          <td style="text-align: center">$4sbh$</td>
          <td style="text-align: center">$\frac{4sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Softmax output</td>
          <td style="text-align: center">$2as^2b$</td>
          <td style="text-align: center">$\frac{2as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout mask</td>
          <td style="text-align: center">$as^2b$</td>
          <td style="text-align: center">$\frac{as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout output</td>
          <td style="text-align: center">$2as^2b$</td>
          <td style="text-align: center">$\frac{2as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">V</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$\frac{2sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear projection input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$\frac{2sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Attention dropout mask</td>
          <td style="text-align: center">$sbh$</td>
          <td style="text-align: center">$sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>MLP</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$19sbh$</strong></td>
          <td style="text-align: center"><strong>$3sbh + \frac{16sbh}{t}$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear1 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">GeLU input</td>
          <td style="text-align: center">$8sbh$</td>
          <td style="text-align: center">$\frac{8sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear2 input</td>
          <td style="text-align: center">$8sbh$</td>
          <td style="text-align: center">$\frac{8sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout mask</td>
          <td style="text-align: center">$sbh$</td>
          <td style="text-align: center">$sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>LayerNorm</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$4sbh$</strong></td>
          <td style="text-align: center"><strong>$4sbh$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">LayerNorm1 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">LayerNorm2 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
  </tbody>
</table>
<hr>
<p>在没有 $T_F = T_B = T_W$ 假设的情况下，ZB-H1 和 ZB-H2 的峰值激活内存和气泡大小如 Table 2 所示。值得注意的是，对于设备 <em>i</em>，其在 ZB-H1 方案下的激活内存为 $(p-i+1)M_B + (i-1)M_W$，在 ZB-H2 方案下的激活内存为 $(2p - 2i + 1)M_B + (2i - 2)M_W$。如 Table 1 所示，<em>W</em> 所需的激活内存小于 <em>B</em> 所需的激活内存。因此，ZB-H1 和 ZB-H2 的峰值激活内存分别为 $pM_B$ 和 $(2p-1)M_B$。</p>
<p><em>Table 2: Comparison between 1F1B and our handcrafted schedules.</em></p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Schedule</th>
          <th style="text-align: center">Bubble size</th>
          <th style="text-align: center">Peak activations memory</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">1F1B</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}+T_{W})$</td>
          <td style="text-align: center">$pM_{B}$</td>
      </tr>
      <tr>
          <td style="text-align: center">ZB-H1</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}-T_{W})$</td>
          <td style="text-align: center">$pM_{B}$</td>
      </tr>
      <tr>
          <td style="text-align: center">ZB-H2</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}-2T_{W})$</td>
          <td style="text-align: center">$(2p-1)M_{B}$</td>
      </tr>
  </tbody>
</table>
<p><strong>Automatic Pipeline Scheduling</strong></p>
<p>手工调度依赖于 F、B、W 的执行时间相等的理想情况。为了应对真实世界中复杂的执行时间和通信延迟，该文开发了一个自动化流水线调度算法。该算法通过一系列启发式策略，在一个给定的内存限制下，自动地为流水线生成一个高效的调度方案。核</p>
<ol>
<li>
<p><strong>Warm-up</strong>：</p>
<ul>
<li>在内存限制的范围内，算法会尽可能多地调度 F pass ，以最小化在第一个 B pass 开始前产生的气泡。</li>
<li>此阶段使用一个超参数来控制是否要调度一个可能会延迟后续B Pass的额外F Pass。</li>
</ul>
</li>
<li>
<p><strong>Steady State</strong>：</p>
<ul>
<li>热身阶段结束后，调度进入一个迭代模式，轮流调度一个F Pass和一个B Pass。</li>
<li>为了填充气泡，算法会伺机插入 W pass. 插入策略是：
<ul>
<li>当出现一个大于 $T_W$ (W Pass 执行时间) 的气泡时，直接插入一个W Pass.</li>
<li>当出现一个小于 $T_W$ 的气泡时，如果这个气泡会导致当前阶段的累计气泡时间成为所有阶段中最长的，那么仍然会插入一个W Pass.</li>
<li>当内存达到上限时，也会插入 W Pass 以回收和释放部分内存。</li>
</ul>
</li>
<li>通常这个启发式策略在稳态阶段会形成一个 1F-1B-1W 的调度模式。</li>
</ul>
</li>
<li>
<p><strong>Global Schedule</strong>：</p>
<ul>
<li>在整个调度过程中，算法始终保证在 F Pass 用完之前，第 i 阶段调度的 F Pass 数量至少比第 i+1 阶段多一个。</li>
<li>当这个数量差超过一时，会使用另一个超参数来决定在不产生额外气泡的前提下，是否要跳过第 i 阶段的一次F Pass调度。</li>
<li>算法会通过 grid search 来寻找这些超参数的最佳组合。</li>
</ul>
</li>
<li>
<p><strong>Final</strong>：当某个阶段的 F Pass 和 B Pass 都执行完毕后，算法会一次性逐个调度完所有剩余的 W Pass.</p>
</li>
</ol>
<hr>
<p><strong>Bypassing Optimizer Synchronization</strong></p>
<p>要实现完美的平行四边形调度，还需要解决优化器同步（Optimizer Synchronization）. 在分布式训练中，通常需要在更新模型参数前，在所有 GPU 间进行一次 All-Reduce，以进行梯度裁剪（Gradient Clipping）或检查数值稳定性 (NaN/INF). 这个同步点会强制所有设备等待，从而破坏平行四边形，重新引入气泡。</p>
<p>论文提出了 Bypassing Optimizer Synchronization，每个 GPU 在执行优化器更新步骤时，不再等待全局同步，而是基于从前一个 GPU 传来的部分 reduce 的信息进行推测性更新。该 micro-batch 完整的全局状态会在下一个迭代的 warp 阶段异步地传回。每个 GPU 在收到最终的全局状态后，会验证自己上一步的更新是否合法。如果发现不一致（例如，全局梯度范数超出了裁剪阈值），它会执行一次原地回滚（In-place Rollback），然后使用正确的全局状态重新执行优化器步骤。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB69834a68b841e1af30873b5a95a2fc90?method=download&amp;shareKey=0544362bccaefd3cad59bb0be406a145" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB69834a68b841e1af30873b5a95a2fc90?method=download&amp;shareKey=0544362bccaefd3cad59bb0be406a145" alt="The Post-validation Strategy to Replace Optimizer Synchronization">
    </a><figcaption>The Post-validation Strategy to Replace Optimizer Synchronization</figcaption></figure></p>
<h1 id="dualpipe">DualPipe</h1>
<p>DualPipe 是一种创新的双向流水线并行算法。它的核心思想是在一组设备上同时处理两个方向的数据流：一个前向流水线和一个反向流水线。使得计算和通信能够更充分地重叠，从而减少流水线气泡（即 GPU 空闲时间）.</p>
<p>与传统的 GPipe（1F1B）只有一个数据流方向不同，DualPipe 将设备对折，形成两条对称的流水线。例如，在一个有 8 个 PP ranks (GPU) 的设置中：</p>
<ul>
<li>前向流水线 (Forward Pipeline): 数据从 rank 0 -&gt; 1 -&gt; 2 -&gt; 3.</li>
<li>反向流水线 (Backward Pipeline): 同时有另一组数据从 rank 7 -&gt; 6 -&gt; 5 -&gt; 4.
Rank 3 和 Rank 4 成为两条流水线的中间节点，它们之间会交换数据。每个设备实际上会处理两个流水线阶段的模型块，一个用于前向流水线，另一个用于反向流水线。</li>
</ul>
<h2 id="initialization">Initialization</h2>
<ul>
<li>modules: 每个 DualPipe 实例接收一个元组，包含两个 nn.Module. <code>modules[0]</code> 用于处理前向-&gt;反向的计算，<code>modules[1]</code> 用于处理反向-&gt;前向的计算。</li>
<li>Rank 角色判断: 代码会根据当前 rank 的 ID 判断其在整个流水线中的位置（是否是第一个、最后一个、是否在后半部分、是否是中间节点）. 这个角色判断对于后续的通信和计算调度至关重要。例如 <code>is_in_second_half</code> 决定了该 rank 的 phase 0 和 phase 1 究竟对应前向流水线还是反向流水线。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DualPipe</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">modules</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 每个 rank 持有两个模型模块</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">process_group</span> <span class="ow">or</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_get_default_group</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算当前 rank 在流水线中的角色</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank_mapping</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">rank</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_last_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 判断 rank 是否在对折后的后半部分</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_in_second_half</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 判断是否为中间的 rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="core-function-step">Core Function: step</h2>
<p><a href="https://github.com/deepseek-ai/DualPipe/blob/main/dualpipe/dualpipe.py#L294">step</a> 方法是 <code>DualPipe</code> 的核心，它协调了所有 micro-batches 的计算和通信。整个过程被划分为 8 个阶段，以实现最大程度的计算-通信重叠。</p>
<p>输入处理: 只有 rank 0 和 rank N-1 会接收外部输入数据 <code>inputs</code> 和 <code>labels</code>. 这些数据被 <code>scatter</code> (<code>dualpipe/utils.py</code>) 切分成 <code>half_num_chunk</code> 个 micro-batch 。Rank 0 的输入用于前向流水线，Rank N-1 的输入用于反向流水线。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 重置状态</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_states</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将输入数据切分成 micro-batch</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">half_num_chunks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">labels</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">half_num_chunks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">input_chunks</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[])</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">([],</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">input_chunks</span> <span class="o">=</span> <span class="p">([],</span> <span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span></code></pre></div><p>接下来是 8 个核心调度阶段的，在此之前会进行一些准备工作：</p>
<ul>
<li>状态重置: <code>_reset_states()</code> 清空上一轮迭代的缓存，如输入/输出块、梯度、损失等。</li>
<li>rank 确定: 计算 <code>num_half_ranks</code>（流水线对折后的一半设备数）和 <code>half_rank</code>（当前秩在对折流水线中的位置. 这些变量将决定每个阶段的循环次数。</li>
<li>数据分发: <code>scatter</code> 函数将输入数据 inputs 和 labels 切分成 half_num_chunks 个 micro-batch 。根据 is_first_rank 或 is_last_rank，将这些 micro-batch 存放到 self.input_chunks 和 self.labels 中。</li>
</ul>
<p>调度示意图如下图所示，红色线分隔了每个步骤</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" alt="DualPipe Schedule">
    </a><figcaption>DualPipe Schedule</figcaption></figure></p>
<p><strong>Step 1: Warm-up Forward nF0</strong></p>
<p>这是一个纯前向计算阶段，用于填满流水线。距离流水线中点越远的 rank（half_rank 越小）执行的预热步骤越多。 <code>_forward_chunk(0)</code> 被调用，在此函数内部:</p>
<ol>
<li><code>_recv_forward(0)</code>: 尝试接收前一个 rank 的数据。对于 rank 0 来说，它直接使用 self.input_chunks 的数据，不接收。</li>
<li><code>_commit_and_wait_comm()</code>: 等待数据接收完成。</li>
<li><code>_forward_compute_chunk(0)</code>: 执行 <code>self.module[0]</code> 的前向计算。</li>
<li><code>_send_forward(0)</code>: 将计算结果异步地发送给下一个 rank.</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">step_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<p><strong>Step 2: Dual Forward nF0F1</strong></p>
<p>两条流水线都开始执行前向计算。两条流水线都开始工作。当前 rank 不仅继续处理 phase 0 的前向计算，也开始处理从另一端（phase 1）传来的数据的前向计算。</p>
<ul>
<li><code>_forward_chunk(0, recv=False, ...)</code> 处理一个 phase 0 的 micro-batch ，但不立即接收下一个，因为前面已经调用了 <code>_recv_forward(0).</code></li>
<li><code>_forward_chunk(1, ...)</code>: 处理一个 phase 1 的 micro-batch 。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 2: nF0F1</span>
</span></span><span class="line"><span class="cl"><span class="n">step_2</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">recv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">send</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">send</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">step_2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_send_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<p><strong>Step 3: 前向-后向-权重混合阶段 (Zero Bubble) nB1W1F1</strong></p>
<p>这是 DualPipe 提高效率的关键。当一条流水线开始进行反向计算时，另一条流水线仍在进行前向计算。</p>
<ul>
<li><code>_backward_chunk(1, enable_zb=True)</code>: 执行反向计算，并启用 Zero Bubble (ZB) 优化。ZB 通过 <code>WeightGradStore</code> 将权重梯度（weight gradients）的计算（通常在反向传播中阻塞）缓存起来，推迟执行，从而让路给其他计算或通信。</li>
<li><code>_weight_chunk()</code>: 执行被推迟的权重梯度计算。</li>
<li><code>_forward_chunk(1)</code>: 同时执行另一个方向的前向计算。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 3: nB1W1F1 (Use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_3</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">recv</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 4: Main Steady State nF0B1F1B0</strong></p>
<p>这是流水线完全填满后的主循环。在一个循环迭代中，一个 rank 会执行两次计算和通信的重叠操作：一次是（前向计算 + 反向计算），另一次也是（前向计算 + 反向计算）. 这里调用 <code>_forward_backward_chunk(0, 1)</code> 和 <code>_forward_backward_chunk(1, 0)</code>. 这个函数尝试将一个方向的前向计算（F）与另一个方向的反向计算（B）打包在一起执行，实现 F&amp;B 重叠。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 4 (Main step): nF0B1F1B0</span>
</span></span><span class="line"><span class="cl"><span class="n">step_4</span> <span class="o">=</span> <span class="n">half_num_chunks</span> <span class="o">-</span> <span class="n">num_ranks</span> <span class="o">+</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># i != 0</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 5 &amp; 6: 后向-后向混合阶段 (Cooldown Backward) nB1F1B0 和 nB1B0</strong></p>
<p>当前向数据流耗尽后，流水线进入收尾阶段。这个阶段主要执行剩余的反向计算。同样，ZB 优化在后半段被启用，以减少气泡。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 5: nB1F1B0</span>
</span></span><span class="line"><span class="cl"><span class="n">step_5</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 6: nB1B0 (The second half of the chunks use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_6</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_6</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">step_6</span> <span class="o">//</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">half_rank</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="n">enable_zb</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">step_6</span> <span class="o">//</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">half_rank</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="n">enable_zb</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 7 &amp; 8: 权重更新收尾阶段 nWB0 和 nW</strong></p>
<ul>
<li>Step 7 将最后的后向计算与权重计算重叠。</li>
<li>Step 8 是纯粹的权重计算阶段，循环调用 _weight_chunk() 直到 WeightGradStore.funcs_queue 队列为空，确保所有梯度都已计算完毕。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 7: nWB0 (Use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_7</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_7</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 8: nW</span>
</span></span><span class="line"><span class="cl"><span class="n">step_8</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="n">WeightGradStore</span><span class="o">.</span><span class="n">funcs_queue</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>
</span></span></code></pre></div><h2 id="computation-communication-overlap">Computation-Communication Overlap</h2>
<p><code>_forward_backward_compute_chunk</code> 函数是实现计算重叠的关键。在理想情况下（如果模型结构支持），它可以将一个 micro-batch 的前向计算和另一个 micro-batch 的反向计算在同一个函数调用中完成。该函数在 step4 使用的<code>_forward_backward_chunk</code> 函数中被调用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_forward_backward_compute_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase0</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">phase1</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">overlapped_forward_backward</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_forward_compute_chunk</span><span class="p">(</span><span class="n">phase0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_compute_chunk</span><span class="p">(</span><span class="n">phase1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># forward &amp; backward</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs0</span><span class="p">,</span> <span class="n">loss0</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module0</span><span class="p">)</span><span class="o">.</span><span class="n">overlapped_forward_backward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">module0</span><span class="p">,</span> <span class="n">inputs0</span><span class="p">,</span> <span class="n">criterion0</span><span class="p">,</span> <span class="n">labels0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">module1</span><span class="p">,</span> <span class="n">loss1</span><span class="p">,</span> <span class="n">outputs1</span><span class="p">,</span> <span class="n">output_grads1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果模型定义了一个 <code>overlapped_forward_backward</code> (@classmethod)，DualPipe 就会调用它。在这个方法里，开发者可以自定义前向和后向计算的交错执行顺序，以达到最佳的重叠效果。DeepSeek-v3 的重叠方法在技术报告里已经讲解。</p>
<h1 id="real-case">Real Case</h1>
<p>通过 <code>examples/example_dualpipe.py </code>中的 main 函数来详细讲解一个完整的 DualPipe 流程。</p>
<ol>
<li>环境初始化和配置</li>
</ol>
<ul>
<li>分布式设置: main 函数首先初始化 PyTorch 的分布式通信组（init_process_group），并为每个进程（rank）分配一个 GPU.</li>
<li>参数配置: 定义了 micro-batch 数量 (num_chunks)、每个 micro-batch 的大小 (micro_batch_size) 等超参数。</li>
<li>P2P通信设置: 在执行 DualPipe 的 step 方法前，必须调用 <code>set_p2p_tensor_shapes</code> 和 <code>set_p2p_tensor_dtype</code> 来告知 DualPipe 在流水线中传递的张量的形状和数据类型。这是因为 DualPipe 需要预先分配内存来接收来自其他 rank 的数据。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">pp_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 判断当前进程的角色</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_first_rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_last_rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 初始化分布式环境</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s2">&#34;env://&#34;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">pp_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">set_default_device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">233</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;CUBLAS_WORKSPACE_CONFIG&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;:4096:8&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 定义流水线参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks</span> <span class="o">=</span> <span class="mi">20</span>
</span></span><span class="line"><span class="cl">    <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">256</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">512</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">pp_size</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">num_chunks</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">hidden_size</span><span class="si">=}</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 设置P2P通信的Tensor形状和类型</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_p2p_tensor_shapes</span><span class="p">([(</span><span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_p2p_tensor_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>模型和参考基准的创建</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 创建一个完整的、未分割的模型</span>
</span></span><span class="line"><span class="cl"><span class="n">full_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pp_size</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建完整的输入数据</span>
</span></span><span class="line"><span class="cl"><span class="n">full_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_chunks</span> <span class="o">*</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">full_l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_chunks</span> <span class="o">*</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 参考步骤：在一个GPU上，用标准的数据并行方式运行完整模型，得到基准结果</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_ref</span><span class="p">,</span> <span class="n">output_ref</span> <span class="o">=</span> <span class="n">ref_step</span><span class="p">(</span><span class="n">full_x</span><span class="p">,</span> <span class="n">full_l</span><span class="p">,</span> <span class="n">full_modules</span><span class="p">,</span> <span class="n">num_chunks</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>创建模型: 代码首先创建了一个完整的 <code>nn.Sequential</code> 模型 (full_modules)，它包含了流水线所有的阶段。</li>
<li>参考步骤 (ref_step): 为了验证 DualPipe 的正确性，<code>ref_step</code> 函数模拟了标准的、非流水线并行的训练过程。它将数据分块，依次通过完整模型计算损失和输出。<code>loss_ref</code> 和 <code>output_ref</code> 将作为后续比较的正确答案。</li>
</ul>
<ol start="3">
<li>DualPipe模型的创建和输入准备</li>
</ol>
<ul>
<li>模型分割: 每个 rank r 会持有两个 PipelineStage: 一个是 <code>full_modules[r]</code>，另一个是 <code>full_modules[pp_size - 1 - r]</code>. 这就是 Dual (双向) 的体现。例如，在一个 4-GPU 的设置中：
<ul>
<li>Rank 0 持有 stage 0 和 stage 3 的模型。</li>
<li>Rank 1 持有 stage 1 和 stage 2 的模型。</li>
<li>Rank 2 持有 stage 2 和 stage 1 的模型。</li>
<li>Rank 3 持有 stage 3 和 stage 0 的模型。</li>
</ul>
</li>
<li>输入数据分割: DualPipe 有两个数据入口点：rank 0 和最后一个 rank.
<ul>
<li>rank 0 接收前半部分的输入 (<code>full_x.chunk(2)[0]</code>) 和 后半部分 的标签 (<code>full_l.chunk(2)[1]</code>).</li>
<li>last rank 接收后半部分的输入 (<code>full_x.chunk(2)[1]</code>) 和 前半部分 的标签 (<code>full_l.chunk(2)[0]</code>).</li>
</ul>
</li>
</ul>
<p>一共有两个数据流: 一个从 rank 0 开始，其对应的标签在最后一个 rank；另一个从最后一个 rank 开始，其对应的标签在 rank 0.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># DualPipe 模型创建</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 每个 rank 获取两个处于对称位置的模型块</span>
</span></span><span class="line"><span class="cl"><span class="n">local_full_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">full_modules</span><span class="p">[</span><span class="n">rank</span><span class="p">],</span> <span class="n">full_modules</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">local_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">),</span> <span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ... 加载权重 ...</span>
</span></span><span class="line"><span class="cl"><span class="n">dualpipe_model</span> <span class="o">=</span> <span class="n">DualPipe</span><span class="p">(</span><span class="n">local_modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># DualPipe输入数据准备</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">full_x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="n">full_l</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">elif</span> <span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">full_x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="n">full_l</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="kc">None</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>执行训练步骤</li>
</ol>
<p>调用 <code>dualpipe_model.step</code>，触发了前面讲解中提到的复杂的8阶段调度流程。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">dualpipe_model</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_chunks</span><span class="o">=</span><span class="n">num_chunks</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">(</span><span class="n">l</span><span class="p">,),</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><ol start="5">
<li>结果验证</li>
</ol>
<p>检查损失</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_ref</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">elif</span> <span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_ref</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span>
</span></span></code></pre></div><p>训练步骤完成后，step 方法会返回计算出的损失。</p>
<ul>
<li>rank0 计算出的 loss 对应的是从 last rank 输入的数据流，等于参考损失的后半部分 (<code>loss_ref.chunk(2)[1]</code>).</li>
<li>同理，last rank 计算出的 loss 对应的是从 rank0 输入的数据流，等于参考损失的前半部分 (<code>loss_ref.chunk(2)[0]</code>).</li>
<li>中间的 ranks 不计算最终损失，返回 None.</li>
</ul>
<p>检查梯度</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">local_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">local_modules</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">p0all</span><span class="p">,</span> <span class="n">p0</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">p1all</span><span class="p">,</span> <span class="n">p1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 手动聚合对称rank的梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">p0</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">p1all</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">p1</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">p0all</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">p_ref</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">local_modules</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(),</span> <span class="n">local_full_modules</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">cal_diff</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">p_ref</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-13</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>由于每个 rank r 持有 r 和 <code>pp_size - 1 - r</code> 两个阶段的模型，如果这两个阶段在逻辑上是同一个权重（例如，在Encoder-Decoder结构中共享权重），那么它们的梯度需要手动聚合。示例通过 <code>dist.all_gather_into_tensor</code> 收集所有 rank 上对称模块的梯度，然后手动将它们相加。最后，将聚合后的梯度与 ref_step 中计算出的参考梯度进行比较，验证反向传播的正确性。</p>
]]></content:encoded>
    </item>
    <item>
      <title>DeepSeek-V3 Technical Report</title>
      <link>http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/</link>
      <pubDate>Fri, 20 Jun 2025 16:37:06 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/</guid>
      <description>Paper Reading of DeepSeek-V3 Technical Report</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>DeepSeek-V3 (671B) 是 MoE 模型，每个 token 会激活 37B 的参数。采用 Multi-head Latent Attention (MLA) 和自创的 DeepSeek MoE 结构，在这两篇文章中已经做过讲解。同时采用了 auxiliary-loss-free 策略来实现专家负载平衡并且使用了 Multi-token Prediction (MTP) 来加速训练。整个预训练数据集一共有 14.8T tokens，通过 Suprvised Fine-Tuning (SFT) 和 强化学习来加强性能。训练时长为 2.788M H800 GPU 小时。</p>
<h1 id="1-introduction">1. Introduction</h1>
<p><strong>模型架构创新:</strong></p>
<ul>
<li>Multi-head Latent Attention (MLA): 加速推理。</li>
<li>DeepSeek MoE: 减少训练开销。</li>
</ul>
<p><strong>增强模型能力的策略:</strong></p>
<ul>
<li>auxiliary-loss-free: 实现负载平衡。</li>
<li>Multi-token Prediction (MTP): 增强模型表现。</li>
</ul>
<p><strong>提高训练效率的方法:</strong></p>
<ul>
<li>FP8 混合精度训练: 加速训练和减少 GPU 内存使用。</li>
<li>DualPipe 流水线并行算法: 减少气泡并且在训练时候通过计算掩盖了大部分通信。</li>
<li>新的节点间 All-to-all 通信算子: 更好地利用 InfiniBand (IB) and NVLink 带宽。</li>
<li>优化了内存后 DeepSeek-V3 训练没有使用 TP.</li>
</ul>
<p><strong>训练过程:</strong></p>
<ul>
<li>pre-training: 在 14.8T tokens 上进行。</li>
<li>stage 1: 扩展最大上下文长度到 32K.</li>
<li>stage 2: 扩展最大上下文长度到 128K.</li>
<li>post-training: 使用 Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) 并且蒸馏了 DeepSeek-R1 系列模型来获得推理能力。</li>
</ul>
<p>DeepSeek-V3 上训练每 1T token 只需要180K H800 GPU小时，即在 2048 个 H800 GPU 的集群上需要 3.7 天。</p>
<h1 id="2-architecture">2. Architecture</h1>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB4af4bca65c0e55a9290c2e4d808cb6b2?method=download&amp;shareKey=d8c9ce5c9b545f9d954d769f4e520fed" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB4af4bca65c0e55a9290c2e4d808cb6b2?method=download&amp;shareKey=d8c9ce5c9b545f9d954d769f4e520fed" alt="Illustration of the Basic Architecture of DeepSeek-V3">
    </a><figcaption>Illustration of the Basic Architecture of DeepSeek-V3</figcaption></figure></p>
<h2 id="21-mla">2.1 MLA</h2>
<p>在相关文章中已经介绍。</p>
<h2 id="22-deepseekmoe-with-auxiliary-loss-free-load-balancing">2.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing</h2>
$$
\begin{align}
\mathbf{h}'_t &= \mathbf{u}_t + \sum_{i=1}^{N_s} \text{FFN}_i^{(s)} (\mathbf{u}_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}_i^{(r)} (\mathbf{u}_t),  \\
g_{i,t} &= \frac{g'_{i,t}}{\sum_{j=1}^{N_r} g'_{j,t}},  \\
g'_{i,t} &= \begin{cases} s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t}|1 \le j \le N_r\}, K_r), \\ 0, & \text{otherwise}, \end{cases} \\
s_{i,t} &= \text{Sigmoid}(\mathbf{u}_t^T \mathbf{e}_i),
\end{align}
$$<p><strong>Basic Architecture of DeepSeekMoE.</strong> 在相关文章中已经介绍，V3 和 V2 不同之处在于使用sigmoid函数来计算亲和分数，并在归一化所有选定的亲和分数之间来产生门控制值。</p>
<p><strong>Auxiliary-Loss-Free Load Balancing.</strong> 为每个专家引入一个偏差项 $b_i$，并将其与相应的亲和力分数$s_{i,t}$ 相加，以确定 top-K 路由</p>
$$
g'_{i,t} = \begin{cases} s_{i,t}, & s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \le j \le N_r\}, K_r), \\ 0, & \text{otherwise}. \end{cases} \tag{16}
$$<p><strong>这个偏置项仅用于路由</strong>，用于和 FFN 输出相乘的门控值还是来自于原先的原先的 $s_{i,t}$. 在每一步结束时，如果其对应的专家过载， DeepSeek-V3 将偏差项减少 $\gamma$ (一个超参数，被称作 bias update speed)，如果其对应的专家负载不足， DeepSeek-V3 将偏差项增加 $\gamma$.</p>
<p>V3 使用 sequence-wise balance loss，类似于 V2 中 Expert-Level Balance Loss。 不同之处在于使用归一化的亲和分数。</p>
$$
\begin{align}
\mathcal{L}_{\text{Bal}} &= \alpha \sum_{i=1}^{N_r} f_i P_i, \\
f_i &= \frac{N_r}{K_r T} \sum_{t=1}^{T} 1 (s_{i,t} \in \text{Topk}(\{s_{j,t}|1 \le j \le N_r\}, K_r)),  \\
s'_{i,t} &= \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}},  \\
P_i &= \frac{1}{T} \sum_{t=1}^{T} s'_{i,t} 
\end{align}
$$<p><strong>Node-Limited Routing.</strong> 对于每个 token 计算每个节点计算亲和度分数前 $\frac {K_r}M$ 的专家求和，选取前 $M$ 个作为路由节点。</p>
<p><strong>No Token-Dropping.</strong> 训练和推理中均不采用。Token Dropping指的是在 MoE 当路由到某个专家的 Token 数量超过了该专家的处理容量时，系统会故意丢跳过那些超出容量的 token，不让它们被这个专家处理。这些 token 通常会通过一个残差连接，将其输入时的状态直接传递到下一层。</p>
<h2 id="23-multi-token-prediction">2.3 Multi-Token Prediction</h2>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBcc8ba55fb1b39401e8287ae38a50d829?method=download&amp;shareKey=3fd271dd7007a3fc5b6b0939e165869f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBcc8ba55fb1b39401e8287ae38a50d829?method=download&amp;shareKey=3fd271dd7007a3fc5b6b0939e165869f" alt="Illustration of Multi-Token Prediction (MTP) implementation">
    </a><figcaption>Illustration of Multi-Token Prediction (MTP) implementation</figcaption></figure></p>
<p>与 Gloeckle 等人使用独立的输出头并行预测 D 个额外 token 不同， DeepSeek-V3 顺序预测额外 token 并在每个预测深度保持完整的因果链。</p>
<p><strong>MTP Modules.</strong> DeepSeek-V3 使用 D 个顺序模块来预测 D 个额外 token。第 k 个 MTP 模块由一个共享嵌入层 Emb(·)、一个共享输出头 OutHead(·)、一个 Transformer 块 TRM_k(·) 和一个投影矩阵 $M_k \in \mathbb{R}^{d \times 2d}$ 组成。对于第 <em>i</em> 个输入 token  $t_i$，在第 <em>k</em> 个预测深度， DeepSeek-V3 首先通过线性映射<strong>结合第 <em>i</em> 个 token 在第 (k−1) 个深度上的表示 $h_i^{k-1} \in \mathbb{R}^d$ 和第 (i+k) 个 token 的嵌入 $\text{Emb}(t_{i+k}) \in \mathbb{R}^d$</strong></p>
$$
h_t^k = M_k[\text{RMSNorm}(h_t^{k-1}); \text{RMSNorm}(\text{Emb}(t_{i+k}))], \tag{21}
$$<p>其中 [·;·] 表示拼接操作。特别地，当 k = 1 时，$h_t^{k-1}$ 指的是由主模型给出的表示。请注意，对于每个 MTP 模块，其嵌入层与主模型共享。合并后的 $h_t^k$ 作为第 k 个深度上 Transformer 块的输入，以在当前深度生成输出表示 $h_t^k$:
</p>
$$
h_{1:T-k}^k = \text{TRM}_k(h_{1:T-k}^k), \tag{22}
$$<p>
其中 T 代表输入序列长度，而 $_{1:T-k}$ 表示切片操作 (包含左右边界)。最后，将 $h_T^k$ 作为输入，共享输出头将计算第 k 个额外预测 token 的概率分布 $p_{t+k+1}^k \in \mathbb{R}^V$，其中 $V$ 是词汇表的大小:
</p>
$$
p_{t+k+1}^k = \text{OutHead}(h_T^k). \tag{23}
$$<p>
输出头 OutHead(·) 将表示线性映射到 logits，随后应用 Softmax 函数来计算第 k 个额外 token 的预测概率。此外，对于每个 MTP 模块，其输出头与主模型共享。DeepSeek-V3 维持预测因果链的原则与 EAGLE (Li et al., 2024b) 的原则相似，但其主要目标是推测解码 (Leviathan et al., 2023; Xia et al., 2023)，DeepSeek-V3 而 利用 MTP 来改进训练。</p>
<p><strong>MTP Training Objective.</strong> 对于每个预测深度，计算一个交叉熵损失 $\mathcal{L}_{\text{MTP}}^k$：</p>
$$
\mathcal{L}_{\text{MTP}}^k = \text{CrossEntropy}(P_{2+k:T+1}^k, t_{2+k:T+1}) = -\frac{1}{T}\sum_{i=2+k}^{T+1} \log p_i^k[t_i], \tag{24}
$$<ul>
<li>$T$: 输入序列长度</li>
<li>$t_i$: 第 i 个位置的真实 (ground-truth) token</li>
<li>$p_i^k[t_i]$: 代表第 $k$ 个 MTP 模块对于第 $i$ 个位置的预测中，赋给真实正确 token $t_i$** 的概率。</li>
</ul>
<p>最后计算所有深度的 MTP 损失的平均值，并乘以一个加权因子 $\lambda$ 来获得总体的 MTP 损失 $\mathcal{L}_{\text{MTP}}$，作为 DeepSeek-V3 的一个额外训练目标：</p>
$$
\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{\text{MTP}}^k. \tag{25}
$$<p><strong>MTP in Inference.</strong> MTP 策略主要旨在提升主模型的性能，因此在推理过程中可以直接丢弃 MTP 模块，主模型可以独立且正常地运作。此外，也可以重新利用这些 MTP 模块进行推测解码 (speculative decoding) ，以进一步改善生成延迟。</p>
<h1 id="3-infrastructure">3. Infrastructure</h1>
<h2 id="31-compute-clusters">3.1 Compute Clusters</h2>
<p>DeepSeek-V3 在 2048 NVIDIA H800 GPU 组成的集群上训练。每个节点有 8 张通过 NVLink 和 NVSwitch 连接的 H800. 节点之间通过 InfiniBand (IB) 连接。</p>
<h2 id="32-training-framework">3.2 Training Framework</h2>
<p>DeepSeek-V3 使用 16-way Pipeline Parallelism (PP), 横跨 8 个节点间的 64-way Expert Parallelism (EP) 以及 ZeRO-1 Data Parallelism (DP). 训练期间不使用 Tensor Parallelism (TP).</p>
<h3 id="321--dualpipe-and-computation-communication-overlap">3.2.1  DualPipe and Computation-Communication Overlap</h3>
<p>DeepSeek-V3 中专家并行导致的跨节点 All-to-all 通信所对应的计算通信比接近 1:1，效率很低。</p>
<p>DualPipe 的核心思想是在一对独立的 forward &amp; backword chunk 内部重叠计算和通信。具体来说，将每个 chunk 分为四个部分: attention, all-to-all dispatch， MLP 和 all-to-all combine. 特别地，对于 backword chunk, attention 和 MLP 都像在 ZeroBubble (Qi et al., 2023b) 中一样，被进一步拆分为两个部分：针对输入的反向传播和针对权重的反向传播。此外，还有一个流水线并行通信组件。如下图所示，对于一对 forward &amp; backword chunk，重排这些组件，并手动调整专用于通信与计算的 GPU SM 的比例。通过这种重叠策略，可以确保 all-to-all 和 PP 通信在执行期间都能够被完全隐藏。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBf348c55ccc87c5e4e388f2df2f18fb76?method=download&amp;shareKey=849010c74a6772b18fff8ca8d5550e8c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBf348c55ccc87c5e4e388f2df2f18fb76?method=download&amp;shareKey=849010c74a6772b18fff8ca8d5550e8c" alt="Overlapping Strategy for a Pair of Individual Forward and Backward Chunks">
    </a><figcaption>Overlapping Strategy for a Pair of Individual Forward and Backward Chunks</figcaption></figure></p>
<p>基于这种高效的重叠策略，完整的 DualPipe 调度方案如下图所示。它采用了一种双向流水线调度，即同时从流水线的两端送入 micro-batches，从而使得一大部分通信可以被完全重叠。这种重叠还确保随着模型规模的进一步扩大，只要保持恒定的计算与通信比率，仍然可以在节点间使用细粒度的专家 (fine-grained experts)，同时实现接近于零的all-to-all通信开销。具体的分析见相关文章。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" alt="DualPipe Schedule">
    </a><figcaption>DualPipe Schedule</figcaption></figure></p>
<h3 id="322-efficient-implementation-of-cross-node-all-to-all-communication">3.2.2 Efficient Implementation of Cross-Node All-to-All Communication</h3>
<p>DeepSeek-V3 定制了高效的跨节点 All-to-all 通信内核，以节省专用于通信的 SM 数量。内核的实现与MoE门控算法和 DeepSeek-V3 集群的网络拓扑共同设计。集群中跨节点 GPU 通过 IB(50 GB/s) 全连接，节点内通信通过 NVLink(160GB/s) 处理。为了有效地利用 IB 和 NVLink 的不同带宽，每个 token 限制最多被 dispatch 到 4 个节点以减少 IB 流量。</p>
<p>经过测试每个 token 在每个节点平均选择 3.2 个专家的同时不会产生额外的 NVLink 通信开销。意味着虽然 DeepSeek-V3 虽然实际上只选择 8 个路由专家，但它可以在保持相同通信成本的情况下最多选择 13 个专家 (4 节点x 3.2 专家/节点). 在这种通信策略下，仅 20 个 SMs 就足以充分利用 IB 和 NVLink 的带宽。详细地说，DeepSeek-V3 采用了 warp specialization 技术，并将 20 个 SMs 划分为 10 个通信通道。在 dispatch 过程中的通信链路为 (1)IB发送，(2) IB-to-NVLink 转发，(3) NVLink 接收由各自的 warp 处理。combine 过程则是相反的通信链路。</p>
<h3 id="323-extremely-memory-saving-with-minimal-overhead">3.2.3 Extremely Memory Saving with Minimal Overhead</h3>
<p>DeepSeek-V3 采取了如下技术来减少训练过程中的内存占用。</p>
<ul>
<li>重计算 RMSNorm 和 MLA 升维投影。</li>
<li>Exponential Moving Average (EMA) 参数被存放在 CPU 中并且异步更新。</li>
<li>MTP 的 Embedding 和输出头在 PP rank 相同的设备上是共享的。</li>
</ul>
<h2 id="33-fp8-training">3.3 FP8 Training</h2>
<p>低精度计算在累加过程中容易出现的问题有:</p>
<ol>
<li>溢出 (Overflow): 当许多数字相加时，它们的和很容易会超出 FP8 格式所能表示的最大值。</li>
<li>精度损失 (Precision Loss/Underflow): 在累加过程中，如果一个很大的中间和与一个很小的乘积相加，这个很小的乘积可能会因为精度限制而被吞掉，直接变成零，对最终结果毫无贡献。</li>
</ol>
<p>DeepSeek-V3 引入了一种细粒度的量化策略: $1\times N_c$ 元素的 tile 分组或 $N_cN_c\times N_c$ 元素的 block 分组。并且在其设计的高精度累加过程过程中，相关的反量化开销在很大程度上得到了缓解。此外，为了进一步减少 MoE 训练中的内存和通信开销，DeepSeek-V3 用 FP8 格式缓存和 dispatch 激活，以 BF16 格式存储低精度优化器状态。相较于 BF16 baseline, FP8 训练的相对误差低于 0.25%.</p>
<h3 id="331-mixed-precision-framework">3.3.1 Mixed Precision Framework</h3>
<p>如图中所示 Fprop(forward pass), Dgrad(activation backward pass) 以及 Wgrad(weight backward pass) GEMM 操作的输入是 FP8 格式，输出为 BF16 或者 FP32 格式。以 FP8 格式进行 Wgrad 允许激活也以 FP8 格式进行存储，减少了内存占用。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBa301c0983af1cc29da1165ca160c0d3e?method=download&amp;shareKey=92f1e462ac7dc2e7fd40cb3dff511153" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBa301c0983af1cc29da1165ca160c0d3e?method=download&amp;shareKey=92f1e462ac7dc2e7fd40cb3dff511153" alt="The Overall Mixed Precision Framework with FP8 Data Format">
    </a><figcaption>The Overall Mixed Precision Framework with FP8 Data Format</figcaption></figure></p>
<p>一些低开销的算子可以使用更高精度并且对训练开销的影响可以忽略不计。DeepSeek-V3 对这些模块使用原格式进行运算：Embedding，输出头，MoE 门控，归一化操作以及 Attention 操作。同时为了数值稳定性，以更高精度存储 master weights(FP32), weight gradients(FP32) &amp; optimizer states(BF16). 这些高精度部分带来的内存开销可以被 DP 减轻。</p>
<h3 id="332-improved-precision-from-quantization-and-multiplication">3.3.2 Improved Precision from Quantization and Multiplication</h3>
<p>DeepSeek-V3 使用了如下技术来提高低精度训练的准确性:</p>
<blockquote>
<p>As a standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes lowprecision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy.</p></blockquote>
<p><strong>Fine-Grained Quantization.</strong> 如下图 所示 DeepSeek-V3 采取更细粒度的方式对输入进行缩放到 FP8 的表示范围: (1) 对于激活以 1x128 tile 进行分组和缩放 (每个 token 的 128 通道为一组); (2) 对于权重以 128x128 进行分组和缩放 (每 128 个输入和输出通道为一组). 虽然原生的 FP8 GEMM 不支持对 reduction 维度进行按组缩放，但可以和下面介绍的 FP32 累加策略进行配合使用。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9db831a1951c8d26e446ee1588f8f55b?method=download&amp;shareKey=fab1571fc5238ec904095a783f855ef3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9db831a1951c8d26e446ee1588f8f55b?method=download&amp;shareKey=fab1571fc5238ec904095a783f855ef3" alt="Fine-grained Quantization">
    </a><figcaption>Fine-grained Quantization</figcaption></figure></p>
<p><strong>Increasing Accumulation Precision.</strong>  NVIDIA H800 GPU 上的 FP8 GEMM 累加精度被限制在 14 bits (远低于 FP32). 为在低精度计算中确保最终的数值精度，DeepSeek-V3 采用了一种结合 Tensor Cores 与 CUDA Cores 的混合计算流程。首先利用 Tensor Cores 的高吞吐量特性来执行 MMA (Matrix Multiply-Accumulate) 运算，中间结果在硬件原生的有限位宽累加器中进行阶段性累加。当累加操作进行 $N_c$ 次后，所产生的部分和将被立即复制到 CUDA Cores 上的 FP32 寄存器中，并与各自对应的细粒度量化缩放因子相乘，从而在执行全精度 FP32 最终累加的同时，高效地完成了反量化操作。这样能将反量化开销无缝融入到高精度累加步骤中，从而以最小的性能代价保证了最终结果的精确性。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB123812d18b874758b234db511dc40e22?method=download&amp;shareKey=982d72c6d03c9bf72d5461d643ad4c65" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB123812d18b874758b234db511dc40e22?method=download&amp;shareKey=982d72c6d03c9bf72d5461d643ad4c65" alt="Increasing Accumulation Precision">
    </a><figcaption>Increasing Accumulation Precision</figcaption></figure></p>
<p>在 H800 架构上，典型的情况是两个 WGMMA 同时存在，当一个 warp group 执行 promotion 到 CUDA Core 操作时，另一个 warp group 能够执行 MMA 操作。实验中取 $N_c=128$，对应于 4 个 WGMMA.</p>
<p><strong>Mantissa over Exponents.</strong> 对所有高精度的张量使用 4EM3 格式。</p>
<details class="custom-details">
    <summary class="custom-summary">How to Compute Float Point Value</summary>
    <div><p>一个常规浮点数 (即非零、非无穷大等特殊值) 的计算公式为：</p>
$$
\text{Value} = (-1)^S \times (1.M)_{\text{binary}} \times 2^{(E_{\text{decimal}} - \text{Bias})}
$$<p>要理解这个公式， DeepSeek-V3 需要拆解里面的三个关键部分：符号、尾数和指数。</p>
<hr>
<ol>
<li>
<p>确定符号 (Sign)</p>
<ul>
<li><code>S = 0</code>，则数值为正，$(-1)^0 = 1$</li>
<li><code>S = 1</code>，则数值为负，$(-1)^1 = -1$</li>
</ul>
</li>
<li>
<p>计算尾数的值 (Mantissa)</p>
</li>
</ol>
<p>不能直接使用 M 的二进制值。在常规浮点数中，标准规定尾数部分永远以 <code>1.</code> 开头。这样，这个 <code>1</code> 就不需要实际存储，从而可以节省一个比特位来提高精度。因此，尾数的实际值是 <code>(1.M)</code> 的二进制形式。</p>
<p>假设尾数位是 $m_1 m_2 m_3 \dots$，其代表的小数值为</p>
$$
m_1 \times 2^{-1} + m_2 \times 2^{-2} + m_3 \times 2^{-3} + \dots
$$<p>.</p>
<p>最终尾数项的值为 $1 + (m_1 \times 2^{-1} + m_2 \times 2^{-2} + m_3 \times 2^{-3} + \dots)$.</p>
<ol start="3">
<li>计算指数的值 (Exponent)</li>
</ol>
<p>指数部分也不能直接使用。为了能表示正、负指数，引入了偏置值 (Bias). 首先从 E 的二进制值计算出其十进制值 $E_{\text{decimal}}$ 后减去 Bias($2^{k-1} - 1$). 其中 k 是指数位的比特数。</p>
<ul>
<li>对于 <strong>E4M3</strong> (k=4)，Bias = $2^{(4-1)} - 1 = 2^3 - 1 = 7$.</li>
<li>对于 <strong>E5M2</strong> (k=5)，Bias = $2^{(5-1)} - 1 = 2^4 - 1 = 15$.</li>
</ul>
<ol start="4">
<li>特殊值说明
当指数 E 全为 0或全为 1 时，代表的是一些特殊值，计算规则也不同:</li>
</ol>
<ul>
<li>E 全为 0:
<ul>
<li>如果 M 也全为 0，代表零 (Zero).</li>
<li>如果 M 不为 0，代表<strong>非规格化数 (Subnormal Numbers)</strong>，计算公式变为 $(-1)^S \times (0.M) \times 2^{(1 - \text{Bias})}$，此时没有隐含的 1.</li>
</ul>
</li>
<li>E 全为 1:
<ul>
<li>如果 M 全为0，代表<strong>无穷大 (Infinity)</strong>。</li>
<li>如果 M 不为0，代表<strong>NaN(Not a Number)</strong>.</li>
</ul>
</li>
</ul>
</div>
</details><br>
<p><strong>Online Quantization.</strong> 采用 online 方式计算每个 1x128 激活 tile 和 128x128 权重 block 的最大绝对值。</p>
<h3 id="333-low-precision-storage-and-communication">3.3.3 Low-Precision Storage and Communication</h3>
<p><strong>Low-Precision Optimizer States.</strong> 用 BF16 格式存储 AdamW 优化器的一阶和二阶动量。优化器存储的 master weights 和 betch 的累加梯度仍以 FP32 格式存储。</p>
<p><strong>Low-Precision Activation.</strong> 大部分激活以 FP8 格式存储，但以下这些是例外。</p>
<ul>
<li><strong>Inputs of the Linear after the attention operator.</strong> 这些激活会在反向传播过程中作为 attention 的输入，对精度比较敏感，因此采用 E5M6 格式存储。量化过程的缩放因子被限制为 2 的整数次幂。</li>
<li><strong>Inputs of the SwiGLU operator in MoE.</strong> 以 FP8 格式存储 SwiGLU 的输入然后再反向传播中重计算。</li>
</ul>
<p><strong>Low-Precision Communication.</strong> 在 dispatch 之前对  MoE up-projections 的输入进行 FP8 量化。专家接收到 FP8 数据后，可以直接进行兼容的 FP8 前向传播。量化过程的缩放因子被限制为 2 的整数次幂。在反向传播进入 MoE down-projections 之前同样使用该策略。前向传播和反向传播 combine 后的结果以 FP16 格式存储。</p>
<h2 id="34-inference-and-deployment">3.4 Inference and Deployment</h2>
<p>为了同时保证 Service-Level Objective (SLO) 和高吞吐量, <em>prefilling</em> 和 <em>decoding</em> 阶段采用了不同的部署策略。</p>
<p>prefilling 阶段的部署单元为 4 个节点 (32 GPUs). 并行策略如下</p>
<ul>
<li>attention part: 采用带有 Sequence Parallel (SP) 的 4-way Tensor Parallel (TP4)，并且和 8-way Data Parallelism (DP8) 一起使用。</li>
<li>MoE part: 采用 32-way Expert Parallelism (EP32), shallow layer 不使用 TP.</li>
</ul>
<p>其他部署细节:</p>
<ul>
<li><em>redundant experts</em>: 部署 32 高负载的专家 (每十分钟统计一次进行调整) 副本。每个 GPU 除了有自己的 8 个专家之外还有 1 个高负载专家。</li>
<li>同时处理两个计算量差不多的 micro-batches，来掩盖 All-to-all 和 TP 的通信。即将一个 micro-batch 的 attention+MoE 和另一个 batch 的 dispatch+combine 重叠。</li>
<li><em>dynamic redundancy</em>: 每个 GPU 上放置 16 个专家，但每次只有 9 个被激活。</li>
</ul>
<p>decoding 阶段的部署单元为 40 个节点 (320 GPUs). 并行策略如下</p>
<ul>
<li>attention part: 采用带有 SP 的 TP4，并且和 DP80 一起使用。</li>
<li>MoE part: 采用 EP320. 256 GPU 被用来放置路由专家，64 GPU 被用来放置共享专家和冗余专家。</li>
</ul>
<p>All-to-all 通过 IB 进行点对点直接传输。同时利用 IBGDA 技术让网卡直接读写 GPU 内存。系统会根据流量统计周期性地判断哪些常规路由专家是当前最热门的，然后动态地让那 64 个GPU去扮演这些热门专家的副本。因为每个 GPU 只被放置一个专家，所以当需要更改冗余策略时系统只需要改变路由逻辑，不需要在物理上移动或重新加载模型权重。</p>
<p>在 decoding 过程中 attention 会耗费更多时间。因此将一个 micro-batch 的 attention 和另一个的 dispatch+MoE+combine 重叠。decoding 阶段每个 GPU 只需要加载一个专家的参数，因此可以分配更多的 SM 给 attention 部分来加速其计算。</p>
<h2 id="35-suggestions-on-hardware-design">3.5 Suggestions on Hardware Design</h2>
<p>基于 All-to-all 实现和 FP8 训练框架，DeepSeek-V3 对 AI 硬件厂商提出了一些建议。</p>
<h3 id="351-communication-hardware">3.5.1 Communication Hardware</h3>
<p>当前通信算子的实现依赖于 SM，DeepSeek-V3 使用了 20 个 H800 SMs (一共 132 个) 用于通信，但使用 SM 进行通信会导致 tensor core 利用率很低。</p>
<p>当前 SM 主要在 All-to-all 通信中执行以下任务:</p>
<ul>
<li>IB 和 NVLink 域之间的数据转发，将目的地为同一节点内多个不同 GPU 的流量，首先汇聚到单个代理GPU上。</li>
<li>在 RDMA 缓冲区 (已注册的 GPU 内存区域) 与模型的输入/输出缓冲区之间进行数据搬运。</li>
<li>为 All-to-all 通信的 combine 阶段执行 reduce 操作。</li>
<li>在需要跨越 IB 和 NVLink 网络域、向多个不同专家进行分块数据传输<span class="sidenote-number"><small class="sidenote">在一个 GPU上 的tokens，其中一些可能要去当前节点内的专家 (通过NVLink)，另一些则要去其他节点上的专家 (通过IB). 在发送之前，GPU必须在自己的内存里进行一次数据重排，把所有目的地是专家 A 的 tokens 打包成一个连续的内存块，所有去专家 B 的 tokens 打包成另一个内存块。</small></span>
的过程中，管理细粒度的内存布局。</li>
</ul>
<h3 id="352-compute-hardware">3.5.2 Compute Hardware</h3>
<p><strong>Higher FP8 GEMM Accumulation Precision in Tensor Cores.</strong> 在目前 NVIDIA Hopper 架构的 Tensor Core 实现中，FP8 GEMM 的累积精度有限。在根据最大指数右移对齐 32 个尾数乘积后，Tensor Core 只使用每个尾数乘积的最高 14 位进行加法，并截断超过此范围的位。将加法结果累加到寄存器中也采用 14 位精度。</p>
<p><strong>Support for Tile- and Block-Wise Quantization.</strong> 目前的 GPU 只支持逐张量量化，缺乏对细粒度量化的原生支持，比如 DeepSeek 的 tile 量化和 block 量化。在当前的实现中，当累加 $N_c$ 次时，部分结果将从 Tensor Core 复制到 CUDA Core，乘以缩放因子，并累加到 CUDA Core 上的FP32 寄存器。尽管与精确的 FP32 累加策略相结合，反量化开销显着减轻，但 Tensor Core 和 CUDA Core 之间频繁的数据移动仍然限制了计算效率。</p>
<p><strong>Support for Online Quantization.</strong> 当前情况下需要从 HBM 中读取 128 个 BF16 激活值 (之前计算的输出) 进行量化，然后将量化后的 FP8 值写回 HBM，然后再次读取以进行 MMA.</p>
<p><strong>Support for Transposed GEMM Operations.</strong> 在当前工作流程中，前向传播的激活被量化为 1x128 FP8 tile 并存储。在反向传播中，矩阵需要被读出、反量化、转置、重新量化为 128x1 tile，并存储在 HBM 中。</p>
<p>DeepSeek-V3 的预训练阶段围绕着高质量的数据构建、精心设计的超参数、长上下文扩展以及全面的性能评测展开。</p>
<h1 id="4-pretraining">4. Pretraining</h1>
<h2 id="41-data-construction">4.1 Data Construction</h2>
<ul>
<li><strong>训练语料</strong>:
<ul>
<li>模型在一个包含 <strong>14.8T</strong> 高质量、多样化 token 的语料库上进行预训练。</li>
<li>与 DeepSeek-V2 相比，新语料提升了数学和编程相关样本的比例，并扩展了除中英文之外的多语言覆盖范围。</li>
<li>数据处理流程经过优化，旨在最小化冗余，同时保持语料的多样性。</li>
</ul>
</li>
<li><strong>FIM 策略</strong>:
<ul>
<li>模型训练中采用了 FIM (Fill-in-Middle) 策略，该策略被证明在不损害常规“下一词预测”能力的同时，赋予了模型根据上下文准确预测中间文本的能力。</li>
<li>FIM 策略在文档层面以 10% 的应用率实施，并采用 Prefix-Suffix-Middle (PSM) 框架构建数据格式。</li>
</ul>
</li>
<li><strong>分词器</strong>:
<ul>
<li>分词器采用 Byte-level BPE，词汇表大小扩展至 <strong>128K</strong>.</li>
<li>为了优化多语言压缩效率，对预分词器和训练数据进行了修改。</li>
<li>为了解决因合并标点和换行符可能导致的 token边界偏差，训练中会随机拆分一部分这类组合 token.</li>
</ul>
</li>
</ul>
<h2 id="42-hyper-parameters">4.2 Hyper-Parameters</h2>
<ul>
<li><strong>模型结构超参数</strong>:
<ul>
<li>总共有 61 层 Transformer，隐藏层维度为 7168.</li>
<li>MLA 注意力头数 $n_h$ 为128，每个头的维度为 128. KV 压缩维度 $d_c$ 为 512，Query 压缩维度 $d_c^{'}$ 为1536.</li>
<li>除了前三层，其余所有 FFN 都被替换为 MoE 层。</li>
<li>每个 MoE 层包含 1个共享专家 和 256个路由专家。每个 token 会激活其中的 8个 路由专。</li>
<li>采用 MTP 策略，预测深度为 1，即除了下一个词，还会额外再预测一个词。</li>
<li>最终模型总参数量为 671B，每个 token 的激活参数量为 37B.</li>
</ul>
</li>
<li><strong>训练超参数</strong>:
<ul>
<li>优化器采用 AdamW，其中 $\beta_{1}=0.9, \beta_{2}=0.95$，权重衰减为 0.1.</li>
<li>预训练阶段的最大序列长度为 4K.</li>
<li>学习率调度：先在 2K 步内线性增长至 $2.2\times10^{-4}$，保持该速率直到消耗10T token，然后在 4.3T token 内余弦衰减至 $2.2\times10^{-5}$，最后在 500B token 的训练中进一步调整。</li>
<li>采用了批次大小调度策略，从 3072 逐步增加到15360.</li>
<li>路由机制被限制为每个 token 最多发送到 4 个节点，以平衡负载。</li>
<li>负载均衡策略主要采用 auxiliary-loss-free，偏置更新速率 $\gamma$ 在前 14.3 Token 时为 0.001，后 500B token 时为 0.0.</li>
<li>对于序列级平衡损失 $\alpha=0.00001$，以防止单一样本内的极端不平衡。</li>
<li>MTP loss 权重 $\lambda$ 对于前 10T token 为 0.3，对于后 4.8T token 为 0.1.</li>
</ul>
</li>
</ul>
<h2 id="43-long-context-extension">4.3 Long Context Extension</h2>
<ul>
<li><strong>扩展方法</strong>: 采用与 DeepSeek-V2 类似的方法，在预训练后应用 <strong>YaRN</strong> 技术进行上下文扩展。</li>
<li><strong>扩展阶段</strong>: 分为两个阶段，分别将上下文窗口从 4K 扩展到 32K，再进一步扩展到 128K.</li>
<li><strong>效果验证</strong>: 通过大海捞针 (Needle In A Haystack) 测试表明，模型在高达 128K 的完整上下文长度内均表现出色且稳定。</li>
</ul>
<h2 id="44-evaluations">4.4 Evaluations</h2>
<ul>
<li><strong>评测范围</strong>: 主要在中英文基准测试以及一个多语言基准上进行评测，与当前最先进的开源基础模型进行比较，如 DeepSeek-V2-Base, Qwen2.5 72B Base, 和 LLaMA-3.1 405B Base.</li>
<li><strong>评测结果</strong>:
<ul>
<li>DeepSeek-V3-Base 全面超越了 DeepSeek-V2-Base 和 Qwen2.5 72B Base，并在绝大多数基准上超过了 LLaMA-3.1 405B Base，成为当前最强的开源模型。</li>
<li>与拥有 11 倍激活参数量的 LLaMA-3.1 405B Base 相比，DeepSeek-V3-Base 在多语言、代码和数学基准上表现出好得多的性能。</li>
<li>在英语和中文语言基准上，DeepSeek-V3-Base 也展现出有竞争力或更好的性能。</li>
</ul>
</li>
<li><strong>训练效率</strong>: 得益于高效的架构和工程优化，DeepSeek-V3 的训练效率极高。每训练 1T token 仅需 180K H800 GPU 小时，远比训练 72B 或 405B 的密集模型便宜。</li>
</ul>
<h2 id="45-discussion">4.5 Discussion</h2>
<p>本节通过一系列消融实验，深入探讨了模型采用的两个关键新策略的有效性，并对负载均衡的不同实现方式进行了对比分析。</p>
<h3 id="451-ablation-studies-for-multi-token-prediction">4.5.1 Ablation Studies for Multi-Token Prediction</h3>
<ul>
<li><strong>实验设置</strong>:
<ul>
<li>在两个不同规模 (一个15.7B，一个228.7B) baseline MoE模型上进行验证。</li>
<li>对比模型在 baseline 模型的基础上增加了一个预测深度为 1 的MTP模块，其他设置 (如训练数据、架构) 保持不变。</li>
<li>为了保证公平比较，在推理阶段会丢弃MTP模块，因此对比模型的推理成本完全相同。</li>
</ul>
</li>
<li><strong>实验结论</strong>:
<ul>
<li>实验结果 (Table 4) 表明，MTP策略在绝大多数评测基准上都能稳定地提升模型性能。</li>
<li>例如，在大型模型上，HumanEval (代码生成) 和 GSM8K (数学推理) 等任务的性能得到了显著提升。</li>
</ul>
</li>
</ul>
<h3 id="452-ablation-studies-for-the-auxiliary-loss-free-balancing-strategy">4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy</h3>
<ul>
<li><strong>实验设置</strong>:
<ul>
<li>同样在两个不同规模 (一个小型15.7B，一个大型228.7B) baseline MoE 模型上进行验证。</li>
<li>baseline 模型完全依赖传统的辅助损失函数来促进专家负载均衡。</li>
<li>对比模型则移除了所有辅助损失，并引入了 Auxiliary-Loss-Free 的均衡策略，其他设置保持一致。</li>
</ul>
</li>
<li><strong>实验结论</strong>:
<ul>
<li>实验结果 (Table 5) 显示，Auxiliary-Loss-Free 策略在绝大多数评测基准上都取得了比纯辅助损失方法更好的模型性能。</li>
<li>在代码和数学等任务上，性能提升尤为明显。</li>
</ul>
</li>
</ul>
<h3 id="453-batch-wise-load-balance-vs-sequence-wise-load-balance">4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance</h3>
<ul>
<li><strong>核心区别</strong>: Auxiliary-Loss-Free 策略是在整个训练批次 (batch-wise) 上实现均衡，而传统的辅助损失则是在每个序列 (sequence-wise) 内部强制实现均衡。</li>
<li><strong>理论优势</strong>: 批次级的均衡约束更为灵活，它不强制每个序列内部的专家使用频率都一样，从而允许专家更好地专精于特定领域 (如代码、数学等).</li>
<li><strong>实验验证</strong>:
<ul>
<li>通过分析模型在不同领域数据上的专家负载，观测到 Auxiliary-Loss-Free 模型展现出了更强的专家特化模式。</li>
<li>进一步的实验表明，只要能实现相似水平的批次级负载均衡，无论是使用 Auxiliary-Loss-Free 方法还是新设计的批次级 Auxiliary-Loss-Free 方法，都能达到相似的优异模型性能，且均优于序列级辅助损失方法。</li>
</ul>
</li>
<li><strong>潜在挑战与解决方案</strong>:
<ul>
<li>批次级均衡可能面临两个挑战：单个序列或小批次内的负载不均，以及推理时因领域切换导致的负载不均。</li>
<li>第一个挑战通过使用大规模的专家并行和数据并行 (确保了每个微批次的规模足够大) 得以自然解决。</li>
<li>第二个挑战则通过在推理部署中采用冗余专家策略来克服。</li>
</ul>
</li>
</ul>
<h1 id="5-post-training">5. Post-Training</h1>
<p>后训练阶段旨在将预训练好的基础模型与人类偏好对齐，并进一步解锁其潜力。该阶段主要包括监督微调 (SFT) 和强化学习 (RL)，并涉及从 DeepSeek-R1 系列模型中蒸馏推理能力。</p>
<h2 id="51-supervised-fine-tuning-sft">5.1 Supervised Fine-Tuning, SFT</h2>
<ul>
<li><strong>数据集构建</strong>: SFT 数据集包含 150 万个实例，涵盖多个领域。
<ul>
<li><strong>推理数据</strong>: 对于数学、代码、逻辑等推理任务，利用内部的 DeepSeek-R1 模型生成数据。虽然 R1 生成的数据准确性高，但存在过度思考、格式不佳和长度过长等问题。为了平衡准确性与简洁性，SFT 训练中会混合使用原始应答和经过精心设计的系统提示词引导下的 R1 应答。</li>
<li><strong>非推理数据</strong>: 对于创意写作、角色扮演等任务，使用 DeepSeek-V2.5 生成应答，并由人类标注员进行验证。</li>
</ul>
</li>
<li><strong>SFT 设置</strong>:
<ul>
<li>模型在 SFT 数据集上微调了 2 个 epoch.</li>
<li>学习率采用余弦衰减策略，从 $5\times10^{-6}$ 降至 $1\times10^{-6}$.</li>
<li>训练序列由多个样本打包而成，但采用样本掩码策略确保样本间相互隔离。</li>
</ul>
</li>
</ul>
<h2 id="52-reinforcement-learning">5.2 Reinforcement Learning</h2>
<h3 id="521-reward-model">5.2.1 Reward Model</h3>
<p>RL 过程采用了 Rule-Based 模型和 Model-Based 的奖励模型。</p>
<ul>
<li><strong>Rule-Based RM</strong>: 用于有明确验证规则的问题，如数学题的确定性答案或代码题的单元测试结果。这种方式可靠性高，不易被模型钻空子。</li>
<li><strong>Model-Based RM</strong>: 用于答案更开放、没有确定性对错的问题。该奖励模型由 DeepSeek-V3 的 SFT 版本训练而来，并通过包含思维链的偏好数据进行训练，以降低 reward hacking 的风险。</li>
</ul>
<h3 id="522-grpo">5.2.2 GRPO</h3>
<ul>
<li>采用了 <strong>GRPO (Group Relative Policy Optimization)</strong> 算法进行强化学习。</li>
<li>GRPO 的一个特点是它不需要一个与策略模型同等大小的 critic 模型，而是从一组采样输出的分数中估计 baseline.</li>
<li>RL 过程融合了来自编码、数学、写作、角色扮演等不同领域的提示词，这不仅使模型与人类偏好更对齐，也提升了在 SFT 数据有限场景下的基准测试性能。</li>
</ul>
<h2 id="53-evaluations">5.3 Evaluations</h2>
<ul>
<li><strong>评测设置</strong>:
<ul>
<li>除了基础模型评测用的基准外，进一步在 IFEval, GPQA, LongBench v2, SWE-Bench Verified, Aider, Codeforces, AIME 2024 等更具挑战性的基准上进行评估。</li>
<li>对比的 baseline 模型包括其他强大的开源和闭源模型，如 Qwen2.5-72B-Inst, LLaMA-3.1-405B-Inst, Claude-3.5-Sonnet, 和 GPT-4o-0513。</li>
</ul>
</li>
<li><strong>Standard Evaluation</strong>:
<ul>
<li>评测结果 (Table 6) 显示，DeepSeek-V3 是表现最好的开源聊天模型。</li>
<li>在知识基准 (MMLU, MMLU-Pro, GPQA-Diamond) 上，其性能与顶级的闭源模型相当或相近。</li>
<li>在长上下文理解基准 (DROP, LongBench v2, FRAMES) 上，表现出顶级水平，例如在 DROP 上取得了 91.6 的 F1 分数，超越了所有其他模型。</li>
<li>在代码和数学基准上表现卓越，尤其是在 AIME, MATH-500 等高难度数学竞赛基准上，绝对得分领先第二名约 10%，优势巨大。</li>
<li>在中文基准上，如 C-SimpleQA，其表现也超越了包括 Qwen2.5 在内的其他模型。</li>
</ul>
</li>
<li><strong>Open-Ended Evaluation</strong>:
<ul>
<li>在 Arena-Hard 基准测试中，DeepSeek-V3 取得了超过 85% 的胜率，与顶级的 Claude-3.5-Sonnet-1022 表现持平，成为首个在该基准上突破 85% 的开源模型。</li>
<li>在 AlpacaEval 2.0 上，其表现同样出色，超越了所有对比的开源和闭源模型。</li>
</ul>
</li>
<li><strong>作为奖励模型的能力</strong>:
<ul>
<li>在 RewardBench 基准上评测其作为奖励模型的判断能力，结果显示 DeepSeek-V3 与最新版本的 GPT-4o 和 Claude-3.5-Sonnet 表现相当。</li>
</ul>
</li>
</ul>
<h2 id="54-discussion">5.4 Discussion</h2>
<ul>
<li><strong>从 DeepSeek-R1 蒸馏知识</strong>:
<ul>
<li>消融实验 (Table 9) 证明，从长思维链 (long-CoT) 模型 DeepSeek-R1 中蒸馏知识的策略非常有效，显著提升了模型在 LiveCodeBench 和 MATH-500 上的性能。</li>
<li>实验也揭示了一个权衡：蒸馏带来了性能提升，但也显著增加了回应的平均长度。因此，在 DeepSeek-V3 的开发中对蒸馏设置进行了仔细选择以求平衡。</li>
</ul>
</li>
<li><strong>Self-Rewarding</strong>:
<ul>
<li>在缺乏明确验证规则的通用场景中，模型开发采用了 constitutional AI 的方法，即<strong>使用 DeepSeek-V3 自身的投票评估结果作为反馈源</strong>来进行优化。</li>
<li>这种自奖励的范式产生了显著的对齐效果，并被认为是实现LLM自我改进的重要方向。</li>
</ul>
</li>
<li><strong>MTP 评测</strong>:
<ul>
<li>模型采用的 MTP 技术可以预测第 2 个token.</li>
<li>评测显示，这个额外预测的 token 的接受率在 85%-90%之间。</li>
<li>结合 speculative decoding 框架，这个高接受率使得模型的解码速度 (TPS) 提升了1.8倍.</li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>DeepSeekMoE</title>
      <link>http://localhost:1313/blogs/deepseek/deepseekmoe/</link>
      <pubDate>Thu, 19 Jun 2025 17:04:18 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepseek/deepseekmoe/</guid>
      <description>Paper Reading of DeepSeekMoE</description>
      <content:encoded><![CDATA[<h1 id="preliminary-mixture-of-experts-for-transformers">Preliminary: Mixture-of-Experts for Transformers</h1>
<p>一个标准的 Transformer backbone LLM 由堆叠层标准 Transformer 块构成，每个块可以表示如下:</p>
$$
\mathbf{h}_t^l = \sum_{i=1}^{N} \left( g_{i,t} \text{FFN}_i(\mathbf{u}_t^l) \right) + \mathbf{u}_t^l \tag{3}
$$$$
g_{i,t} = \begin{cases} s_{i,t} & \text{if } s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \le j \le N\}, K) \\ 0 & \text{otherwise} \end{cases} \tag{4}
$$$$
s_{i,t} = \text{Softmax}_i(\mathbf{u}_t^T \mathbf{e}_i^l) \tag{5}
$$<ul>
<li>$N$: 专家总数</li>
<li>$\text{FFN}_i(\cdot)$: 第 $i$ 个专家的 FFN.</li>
<li>$g_{i,t}$: 第 $i$ 个专家的门控值。</li>
<li>$s_{i,t}$: token 对专家的亲和度。</li>
<li>$\text{Topk}(\cdot, K)$: 在为第 $t$ 个 token 和所有 $N$ 个专家计算出的亲和度分数中，包含 $K$ 个最高分数的集合，</li>
<li>$\mathbf{e}_i^l$: 第 $l$ 层中第 $i$ 个专家的中心。</li>
</ul>
<p>注意到 $g_{i,t}$ 是稀疏的，说明在 $N$ 个门控值中只有 $K$ 个非零。这种稀疏性确保了 MoE 层内的计算效率，即每个 token 只会被分配给 $K$ 个专家并由它们计算。此外，在上述公式中，为了简洁起见，我们省略了层归一化操作。</p>
<h1 id="deepseekmoe-architecture">DeepSeekMoE Architecture</h1>
<p>
<figure class="post-figure">
    <a href="https://arxiv.org/html/2401.06066v1/x2.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://arxiv.org/html/2401.06066v1/x2.png" alt="Illustration of DeepSeek MoE">
    </a><figcaption>Illustration of DeepSeek MoE</figcaption></figure></p>
<h2 id="fine-grained-expert-segmentation">Fine-Grained Expert Segmentation</h2>
<p>在上图 (a) 的情况下将每个专家 FFN 的中间隐藏层维度缩小到原先的 1/m，专家数增加 m 倍。这样可以在参数量和计算量保持不变的情况下使得每个 token 被路由到更多的专家。通过细粒度的专家划分，MoE 层的输出可以表示为</p>
$$
\begin{aligned}
\mathbf{h}_t^l &= \sum_{i=1}^{mN} \left( g_{i,t} \text{FFN}_i(\mathbf{u}_t^l) \right) + \mathbf{u}_t^l \quad&(6)\\
g_{i,t} &= \begin{cases} s_{i,t} & \text{if } s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \le j \le mN\}, mK) \\ 0 & \text{otherwise} \end{cases} \quad&(7)\\
s_{i,t} &= \text{Softmax}_i(\mathbf{u}_t^T \mathbf{e}_i^l) \quad&(8)
\end{aligned}
$$<ul>
<li>$mN$: 细粒度专家的总数。</li>
<li>$mK$: 非零门控值的数量也将增加到。</li>
</ul>
<p>专家参数总数等于 $N$ 乘以标准 FFN 中的参数数量。从组合可能性的角度来看，细粒度专家分割策略显著增强了激活专家的组合灵活性。</p>
<h2 id="shared-expert-isolation">Shared Expert Isolation</h2>
<p>如上图 (c) 所示 将 $K_s$ 个专家隔离出来作为共享专家。无论路由模块如何，每个 token 都会被确定性地分配给这些共享专家。为了保持计算量不变，其他路由专家中激活的专家数量将减少 $K_s$.</p>
$$
\begin{aligned}
\mathbf{h}_t^l &= \sum_{i=1}^{K_S} \text{FFN}_i(\mathbf{u}_t^l) + \sum_{i=K_S+1}^{mN} \left( g_{i,t} \text{FFN}_i(\mathbf{u}_t^l) \right) + \mathbf{u}_t^l \quad&(9)\\
g_{i,t} &= \begin{cases} s_{i,t} & \text{if } s_{i,t} \in \text{Topk}(\{s_{j,t} | K_S+1 \le j \le mN\}, mK - K_S) \\ 0 & \text{otherwise} \end{cases} \quad&(10)\\
s_{i,t} &= \text{Softmax}_i(\mathbf{u}_t^T \mathbf{e}_i^l) \quad&(11)
\end{aligned}
$$<p>于是在 DeepSeekMoE 中，共享专家的数量为 $K_S$，路由专家的总数为 $mN - K_S$，非零门控值的数量是 $mK - K_S$</p>
<h2 id="load-balance-consideration">Load Balance Consideration</h2>
<p>自动学习的路由策略可能会遇到负载不平衡的问题</p>
<ol>
<li>存在路由崩溃的风险，即模型总是只选择少数几个专家，导致其他专家无法得到充分训练。</li>
<li>如果专家分布在多个设备上，负载不平衡会加剧计算瓶颈。</li>
</ol>
<hr>
<p><em>Expert-Level Balance Loss.</em>
</p>
$$ 
\begin{aligned}
\mathcal{L}_{\text{ExpBal}} &= \alpha_1 \sum_{i=1}^{N'} f_i P_i \quad&(12)\\
f_i &= \frac{N'}{K'T} \sum_{t=1}^{T} \mathbf{1}(\text{Token } t \text{ selects Expert } i) \quad&(13)\\
P_i &= \frac{1}{T} \sum_{t=1}^{T} s_{i,t} \quad&(14)
\end{aligned}
$$<p>$\mathcal{L}_{\text{ExpBal}}$ 的目的是<strong>促进专家之间的负载均衡</strong>，避免出现某些专家过载 (被选中太多次) 而其他专家闲置 (很少被选中) 的情况。</p>
<ul>
<li>$N'$: 表示可路由的专家总数，即 $mN - K_S$。</li>
<li>$K'$: 表示每个 token 选择的路由专家数量，即 $mK - K_S$。</li>
<li>$T$: 表示总的 token 数量。</li>
</ul>
<p>该损失函数的解释如下</p>
<ul>
<li>$f_i$ (Expert Load/Utilization):
<ul>
<li>公式 (13) 计算的是专家 $i$ 在一个批次/序列中被选中的频率。</li>
<li>$\mathbf{1}(\text{Token } t \text{ selects Expert } i)$ 是一个指示函数，如果 token $t$ 选中了专家 $i$，则为 1，否则为 0。</li>
<li>$\frac{1}{T} \sum_{t=1}^{T} \mathbf{1}(\text{Token } t \text{ selects Expert } i)$ 得到了专家 $i$ 在 $T$ 个 token 中被选中的平均次数 (频率) 。</li>
<li>前面的 $\frac{N'}{K'}$ 是一个归一化因子。当所有专家被均匀选中时，每个专家被选择的平均次数为$TK'/N'$，此时 $f_i$ 的期望值为 1. 如果专家 $i$ 被选中次数多于平均，则 $f_i > 1$；反之 $f_i < 1$.</li>
</ul>
</li>
</ul>
<p>$f_i$ 可以理解为<strong>专家 $i$ 的归一化负载或利用率</strong>。</p>
<ul>
<li>$P_i$ (Expert Routing Probability):
<ul>
<li>公式 (14) 计算的是专家 $i$ 在所有 token 中平均的门控亲和度分数。</li>
<li>$s_{i,t}$ 是 token $t$ 对专家 $i$ 的原始亲和度分数 (Softmax 之前的输出) 。</li>
</ul>
</li>
</ul>
<p>$P_i$ 可以理解为<strong>专家 $i$ 被门控网络选择的平均倾向性</strong>。</p>
<ul>
<li>$\mathcal{L}_{\text{ExpBal}} = \alpha_1 \sum_{i=1}^{N'} f_i P_i$:
<ul>
<li>这个损失项是 $f_i$ 和 $P_i$ 乘积的和。它的目标是<strong>最小化这个和</strong>。</li>
<li>如果某个专家 $i$ 的 $f_i$ (负载高) 和 $P_i$ (被倾向性选择的概率高) 都很大，那么 $f_i P_i$ 就会很大，导致损失增大。</li>
<li>通过最小化这个损失，模型会被<strong>激励将 token 分配给那些负载较低或被选择倾向性较低的专家</strong>。这有助于分散负载，使得所有专家都能得到训练和利用，从而提高模型的整体效率和性能。</li>
<li>$\alpha_1$ 是一个超参数，用于控制这个平衡损失在总损失中的权重。</li>
</ul>
</li>
</ul>
<hr>
<p><em>Device-Level Balance Loss.</em>
</p>
$$
\begin{aligned}
\mathcal{L}_{\text{DevBal}} &= \alpha_2 \sum_{i=1}^{D} \hat{f}_i \hat{P}_i \quad&(15)\\
\hat{f}_i &= \frac{1}{|\mathcal{E}_i|} \sum_{j \in \mathcal{E}_i} f_j \quad&(16)\\
\hat{P}_i &= \sum_{j \in \mathcal{E}_i} P_j \quad&(17)
\end{aligned}
$$<p>$\mathcal{L}_{\text{DevBal}}$ 的目的是<strong>促进专家在不同计算设备之间的负载均衡</strong>。在分布式训练中，通常会将专家分散到不同的设备上，如果某些设备上的专家过于繁忙，而另一些设备上的专家闲置，就会导致计算瓶颈和效率低下。</p>
<ul>
<li>$D$: 表示计算设备的数量。</li>
<li>$\mathcal{E}_i$: 表示分配给第 $i$ 个设备的所有专家的集合。$|\mathcal{E}_i|$ 是这个集合中专家的数量。</li>
</ul>
<p>该损失的解释如下</p>
<ul>
<li>
<p>$\hat{f}_i$ (Device-level Expert Load): 公式 (16) 计算的是第 $i$ 个设备上所有专家的平均负载 ($f_j$). 代表了<strong>设备 $i$ 的总体计算负载</strong>。</p>
</li>
<li>
<p>$\hat{P}_i$ (Device-level Routing Probability): 公式 (17) 计算的是第 $i$ 个设备上所有专家的平均路由倾向性 ($P_j$) 的总和。代表了<strong>设备 $i$ 的专家集合被门控网络选择的总体倾向性</strong>。</p>
</li>
<li>
<p>$\mathcal{L}_{\text{DevBal}} = \alpha_2 \sum_{i=1}^{D} \hat{f}_i \hat{P}_i$:</p>
<ul>
<li>这个损失项是 $\hat{f}_i$ 和 $\hat{P}_i$ 乘积的和，目标也是<strong>最小化这个和</strong>。</li>
<li>如果某个设备 $i$ 的 $\hat{f}_i$ (负载高) 和 $\hat{P}_i$ (被选择倾向性高) 都很大，那么 $\hat{f}_i \hat{P}_i$ 就会很大，导致损失增大。</li>
<li>通过最小化这个损失，模型会被<strong>激励将 token 路由到那些整体负载较低的设备上的专家</strong>。这确保了分布式训练或推理时，所有设备都能得到更均匀的利用，避免了单个设备成为瓶颈。</li>
<li>$\alpha_2$ 是一个超参数，用于控制这个损失在总损失中的权重。</li>
</ul>
</li>
</ul>
<hr>
<p>超参数 $\alpha_1$ 和 $\alpha_2$ 的设置策略:</p>
<ul>
<li>**小的 $\alpha_1$ (专家级平衡因子) : 用于“减轻路由崩溃的风险”。路由崩溃指的是少数专家被过度使用，导致它们“饱和”而无法有效学习，同时其他专家则完全未被使用。较小的 $\alpha_1$ 意味着我们允许一定程度的专家专业化，但仍会进行微调以避免极端的不平衡。</li>
<li>**大的 $\alpha_2$ (设备级平衡因子) : 用于“促进跨设备的平衡计算”。这意味着我们更强烈地要求模型将计算负载均匀地分散到所有可用的计算设备上，以最大限度地提高并行效率。设备级的负载均衡对于分布式系统而言至关重要。</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>DeepSeekMLA</title>
      <link>http://localhost:1313/blogs/deepseek/deepseekmla/</link>
      <pubDate>Thu, 19 Jun 2025 11:24:45 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepseek/deepseekmla/</guid>
      <description>Principle of DeepSeekV3 MLA</description>
      <content:encoded><![CDATA[<h1 id="preliminary-what-is-rope">Preliminary: What is RoPE</h1>
<h2 id="introduction">Introduction</h2>
<p>旋转位置编码 (RoPE) 是一种新颖的、基于相对位置的编码方法，它被设计用于提高 Transformer 模型处理长序列的能力，同时保持计算效率。与传统的绝对位置编码 (如正弦/余弦位置编码) 或直接的相对位置编码 (如 T5 中使用的相对偏置) 不同，RoPE 将位置信息集成到自注意力机制的 Q 和 K 的表示中，使得 Q 和 K 的点积自然地编码了<strong>相对位置信息</strong>。</p>
<p>RoPE 的核心思想是，通过对查询和键向量应用特定的旋转操作，使得两个向量的点积结果只依赖于它们之间的相对距离，而不是它们的绝对位置。这使得模型能够更好地泛化到更长的序列，并且在处理位置信息时更加高效。</p>
<p><strong>RoPE 的主要优点包括：</strong></p>
<ul>
<li><strong>编码相对位置信息：</strong> 自然地将相对位置信息融入到注意力分数中。</li>
<li><strong>长序列外推能力：</strong> 提高了模型在训练时未见过的更长序列上的性能。</li>
<li><strong>与自注意力机制的兼容性：</strong> 无缝集成到 QKV 点积注意力中。</li>
<li><strong>简单且高效：</strong> 实现相对简单，且不会显著增加计算复杂度。</li>
</ul>
<h2 id="formular">Formular</h2>
<p>RoPE 的主要思想是通过对查询 $q$ 和键 $k$ 应用一个旋转矩阵 $R_t$ (取决于其绝对位置 $t$) ，使得点积 $q_m^T k_n$ 能够通过某种方式转化为只依赖于相对位置 $m-n$ 的函数。</p>
<p>对于一个向量 $x \in \mathbb{R}^d$ 在位置 $m$ 处，RoPE 的变换函数 $f(x, m)$ 可以定义如下：</p>
<p>如果向量维度是偶数 $d$，我们可以将其分成 $d/2$ 对，每对执行一个二维旋转。
对于向量 $x = [x_0, x_1, \ldots, x_{d-1}]^T$，RoPE 对其每个维度对 $(x_{2i}, x_{2i+1})$ 应用旋转：</p>
$$
f(x, m)_{2i} = x_{2i} \cos(m\theta_i) - x_{2i+1} \sin(m\theta_i) \\
f(x, m)_{2i+1} = x_{2i} \sin(m\theta_i) + x_{2i+1} \cos(m\theta_i)
$$<p>其中 $\theta_i$ 是预设的频率，通常定义为 $\theta_i = 10000^{-2i/d}$. $i=0, \dots, d/2 - 1$ 是维度对的索引。</p>
<p><strong>用矩阵形式表示：</strong>
我们可以将这种旋转操作表示为一个稀疏的块对角矩阵 $R_m^d$，其形式为：
</p>
$$R_m^d = \begin{pmatrix}
\cos(m\theta_0) & -\sin(m\theta_0) & 0 & 0 & \cdots \\
\sin(m\theta_0) & \cos(m\theta_0) & 0 & 0 & \cdots \\
0 & 0 & \cos(m\theta_1) & -\sin(m\theta_1) & \cdots \\
0 & 0 & \sin(m\theta_1) & \cos(m\theta_1) & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}$$<p>
那么，经过 RoPE 编码的查询和键可以表示为：
</p>
$$\mathbf{q}_m = R_m^d \mathbf{q}$$<p>
</p>
$$\mathbf{k}_n = R_n^d \mathbf{k}$$<p>
其中 $\mathbf{q}$ 和 $\mathbf{k}$ 是原始的查询和键向量 (不含位置信息) ，$\mathbf{q}_m$ 和 $\mathbf{k}_n$ 是经过 RoPE 处理后的查询和键向量。</p>
<p><strong>RoPE 的关键特性：点积与相对位置</strong>
经过 RoPE 变换后，注意力机制中的点积可以分解为：
</p>
$$\mathbf{q}_m^T \mathbf{k}_n = (R_m^d \mathbf{q})^T (R_n^d \mathbf{k})$$<p>
由于 $R_m^d$ 是正交矩阵，其逆矩阵等于其转置，即 $(R_m^d)^T = (R_m^d)^{-1} = R_{-m}^d$. 因此有
</p>
$$\mathbf{q}_m^T \mathbf{k}_n = \mathbf{q}^T (R_m^d)^T R_n^d \mathbf{k} = \mathbf{q}^T R_{-m}^d R_n^d \mathbf{k} = \mathbf{q}^T R_{n-m}^d \mathbf{k}$$<p>
这个最终结果 $\mathbf{q}^T R_{n-m}^d \mathbf{k}$ 表明，两个向量的点积只依赖于它们的<strong>相对位置差 $n-m$</strong>，而与它们的绝对位置 $n$ 和 $m$ 无关。这就是 RoPE 能够编码相对位置信息的数学基础。</p>
<h1 id="workflow">Workflow</h1>
<h2 id="notation">Notation</h2>
<ul>
<li>$d$: embedding 维度</li>
<li>$d_h$: 每个注意力头的维度</li>
<li>$\mathbf{h}_t\in\mathbb{R}^d$: 某个 attention 层第 t 个 token 的输入。</li>
</ul>
<h2 id="kv-compression">KV Compression</h2>
$$
\textcolor{red}{c_t^{KV}} = W^{DKV}h_t  \tag{1}
$$<p>
</p>
$$
[k_{t,1}^{C}, k_{t,2}^{C}, \ldots, k_{t,n_h}^{C}] = k_t^C = W^{UK}c_t^{KV}  \tag{2}
$$<p>
</p>
$$
\textcolor{red}{k_t^R} = \text{RoPE}(W^{KR}h_t)  \tag{3}
$$<p>
</p>
$$
k_{t,i} = [k_{t,i}^C, k_{t}^R] \tag{4}
$$<p>
</p>
$$
[v_{t,1}^C, v_{t,2}^C, \ldots, v_{t,n_h}^C] = v_t^C = W^{UV}c_t^{KV} \tag{5}
$$<ul>
<li>$c_t^{KV} \in \mathbb{R}^{d_c}$: 压缩后的 KV 潜在向量。</li>
<li>$d_c (\ll d_h n_h)$: KV 压缩到的维度。</li>
<li>$W^{DKV} \in \mathbb{R}^{d_c \times d}$: KV 降维投影矩阵。</li>
<li>$W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ 分别是 K &amp; V 的升维投影矩阵。</li>
<li>$W^{KR} \in \mathbb{R}^{d_h^R \times d}$: 用于生成携带 RoPE 的解耦键的矩阵 (Su et al., 2024)</li>
</ul>
<p>红色的是需要缓存的向量，后续说明原因。注意到对 K 进行 RoPE 之前是对输入向量乘以了个投影再进行的。而且 K 的每个注意力头被拼接的都是同一个 $k_{t}^R$，有点类似于 MQA.</p>
<h2 id="q-compression">Q Compression</h2>
$$c_t^Q = W^{DQ}h_t \tag{6}$$<p>
</p>
$$[q_{t,1}^C, q_{t,2}^C, \ldots, q_{t,n_h}^C] = q_t^C = W^{UQ}c_t^Q \tag{7}$$<p>
</p>
$$[q_{t,1}^R, q_{t,2}^R, \ldots, q_{t,n_h}^R] = q_t^R = \text{RoPE}(W^{QR}q_t^C) \tag{8}$$<p>
</p>
$$q_{t,i} = [q_{t,i}^C, q_{t,i}^R] \tag{9}$$<ul>
<li>$c_t^Q \in \mathbb{R}^{d_c'}$: Q 压缩后的潜在向量。</li>
<li>$d_c'(\ll d_h n_h)$ 表示 Q 压缩后的维度。</li>
<li>$W^{DQ} \in \mathbb{R}^{d_c' \times d}, W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}$: 分别是 Q 的降维和升维矩阵。</li>
<li>$W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c'}$ 是用于生成携带 RoPE 的解耦 Q 的矩阵。</li>
</ul>
<p>注意到对 Q 的 RoPE 是在压缩后进行的，即为每个注意力头都生成了一个位置编码信息后进行拼接。</p>
<h2 id="attention-computation">Attention Computation</h2>
<p>最终 $q_{t,i}$, $k_{j,i}$, $v_{j,i}^C$ 被组合起来以生成最终的注意力输出 $u_t$</p>
$$\mathbf{o}_{t,i} = \sum_{j=1}^{t} \text{Softmax}\left(\frac{q_{t,i}^T \mathbf{k}_{j,i}}{\sqrt{d_h + d_R}}\right)v_{j,i}^C \tag{10}$$<p>
</p>
$$\mathbf{u}_t = W^O[\mathbf{o}_{t,1}, \mathbf{o}_{t,2}, \ldots, \mathbf{o}_{t,n_h}] \tag{11}$$<ul>
<li>$W^O \in \mathbb{R}^{d \times d_h n_h}$: 输出投影矩阵。</li>
</ul>
<h1 id="why-decoupled-rope">Why Decoupled RoPE</h1>
<p>假设不加 RoPE 的情况下进行 $q_{t,i}$, $k_{j,i}$ 的内积则有</p>
$$
q_{t,i}^{T}\times k_{j,i}=(W_{(i)}^{UQ}c_{t}^{Q})^{T}\times W_{(i)}^{UK}c_{j}^{KV}=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}W_{(i)}^{UK}\times c_{j}^{KV} \tag{12}
$$<p>RoPE 通过对向量应用一个<strong>位置依赖的旋转变换</strong>来注入相对位置信息。对于一个向量 $X$ 在位置 $t$，RoPE 可以被表示为一个旋转矩阵 $R_t$ 乘以 $X$：
</p>
$$\text{RoPE}(X, t) = R_t X$$<p>
这里的 $R_t$ 是一个正交旋转矩阵，它取决于位置 $t$.</p>
<p>如果直接对压缩后 $k_t^C$ 的 使用 RoPE 那么情况会变成</p>
$$
\begin{aligned}
q_{t,i}^{T}\times k_{j,i}&=(\mathcal{R}_{t}W_{(i)}^{UQ}c_{t}^{Q})^{T}\times\mathcal{R}_{j}W_{(i)}^{UK}c_{j}^{KV} \\
&=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}\mathcal{R}_{t}^{T}\mathcal{R}_{j}W_{(i)}^{UK}\times c_{j}^{KV}\\
&=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}\mathcal{R}_{t-j}W_{(i)}^{UK}\times c_{j}^{KV}
\end{aligned} \tag{13}
$$<p>中间的矩阵与相对位置有关，无法提前计算出来。因此文中就是对所有头都使用同一个 k 和计算 RoPE. 拼接后的向量再计算时</p>
$$
q_{t,i}^T\times k_{j,i}=[q_{t,i}^C;q_{t,i}^R]^T\times[k_{j,i}^C;k_t^R]=(q_{t,i}^C,k_{j,i}^C)+(q_{t,i}^R,k_t^R) \tag{14}
$$<p>前一部分按照公式 (12) 进行计算，后一部分按照 MQA 方式计算。因此只用缓存 $c_t^{KV}$ 和 $k_t^R$.</p>
<h1 id="source-code">Source Code</h1>
<p><a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/f6e34dd26772dd4a216be94a8899276c5dca9e43/inference/model.py#L393-L494">DeepSeek-V3 MLA</a> 对应的源码位置</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MLA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Multi-Head Latent Attention (MLA) Layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Attributes:
</span></span></span><span class="line"><span class="cl"><span class="s2">        dim (int): Dimensionality of the input features.
</span></span></span><span class="line"><span class="cl"><span class="s2">        n_heads (int): Number of attention heads.
</span></span></span><span class="line"><span class="cl"><span class="s2">        n_local_heads (int): Number of local attention heads for distributed systems.
</span></span></span><span class="line"><span class="cl"><span class="s2">        q_lora_rank (int): Rank for low-rank query projection.
</span></span></span><span class="line"><span class="cl"><span class="s2">        kv_lora_rank (int): Rank for low-rank key/value projection.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_head_dim (int): Total dimensionality of query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        v_head_dim (int): Dimensionality of value projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        softmax_scale (float): Scaling factor for softmax in attention computation.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">ModelArgs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算当前进程（卡）负责的注意力头数量，用于模型并行</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">//</span> <span class="n">world_size</span> 
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">q_lora_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">kv_lora_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_nope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># QK 头总维度 = 非 RoPE 部分 + RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">v_head_dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 查询投影 (wq) 的 LoRA 实现</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果 q_lora_rank 为 0，表示不使用 LoRA，直接进行全秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 dim 维度的输入投影到 n_heads * qk_head_dim 维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果 q_lora_rank &gt; 0，使用 LoRA 结构进行低秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wq_a: dim -&gt; q_lora_rank (低秩投影的第一步)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># q_norm: RMSNorm 应用于低秩维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wq_b: q_lora_rank -&gt; n_heads * qk_head_dim (低秩投影的第二步)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq_b</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 键值投影 (wkv) 的 LoRA 实现</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># wkv_a: dim -&gt; kv_lora_rank + qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应图中的 W^{DKV} 投影到低秩 KV 潜在空间 (kv_lora_rank) 和解耦的 RoPE 键 (qk_rope_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 kv_lora_rank 对应公式中的 d_c</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 qk_rope_head_dim 对应公式中的 d_h</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wkv_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># kv_norm: RMSNorm 应用于低秩维度</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># wkv_b: kv_lora_rank -&gt; n_heads * (qk_nope_head_dim + v_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应图中的 W^{UK} 和 W^{UV} 的组合</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 它将压缩后的 KV 潜在向量 (kv_lora_rank) 投影回非 RoPE 键 (qk_nope_head_dim) 和值 (v_head_dim) 的高维度空间</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 输出投影 (wo)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">RowParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Softmax 缩放因子，用于注意力分数的缩放，防止内积过大</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果序列长度超过原始训练长度，根据 RopeFactor 进行额外缩放，用于处理长序列外推问题</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">original_seq_len</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">mscale</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">mscale</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">rope_factor</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">*</span> <span class="n">mscale</span> <span class="o">*</span> <span class="n">mscale</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 根据注意力实现方式（naive 或 optimized）选择不同的 KV 缓存结构</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># naive 实现直接缓存完整键 K 和值 V</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_cache: (max_batch_size, max_seq_len, n_local_heads, qk_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;k_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># v_cache: (max_batch_size, max_seq_len, n_local_heads, v_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;v_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># optimized 实现缓存压缩后的 KV 潜在向量和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># kv_cache: (max_batch_size, max_seq_len, kv_lora_rank) - 对应论文中的 c_t^{KV}</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;kv_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># pe_cache: (max_batch_size, max_seq_len, qk_rope_head_dim) - 对应论文中的 k_t^R</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;pe_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Forward pass for the Multi-Head Latent Attention (MLA) Layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).
</span></span></span><span class="line"><span class="cl"><span class="s2">            start_pos (int): Starting position in the sequence for caching.
</span></span></span><span class="line"><span class="cl"><span class="s2">            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s2">            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">            torch.Tensor: Output tensor with the same shape as the input.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_pos</span> <span class="o">=</span> <span class="n">start_pos</span> <span class="o">+</span> <span class="n">seqlen</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. 查询 (Q) 的生成</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 全秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># LoRA 投影：x -&gt; wq_a -&gt; q_norm -&gt; wq_b</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wq_a</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape Q</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 分离 Q 的非 RoPE 部分和 RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># q_nope 对应论文中的 q_{t,i}^C (非位置信息查询)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># q_pe 对应论文中的 q_{t,i}^R (携带 RoPE 的解耦查询)</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_nope</span><span class="p">,</span> <span class="n">q_pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 对 Q 的 RoPE 部分应用旋转位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应论文中的 q_t^R = RoPE(W^{QR}c_t^Q) 的 RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_pe</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">q_pe</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. 键值 (KV) 的生成</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 将输入 x 投影到低秩 KV 潜在空间和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应论文中的 c_t^{KV} 和 k_t^R</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 分离出 KV 潜在向量和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># kv 对应论文中的 c_t^{KV}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># k_pe 对应论文中的 k_t^R (RoPE 解耦键)</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span><span class="p">,</span> <span class="n">k_pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 对 K 的 RoPE 部分应用旋转位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 注意 k_pe.unsqueeze(2) 是因为 apply_rotary_emb 期望 (..., seq_len, head_dim) 结构</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 k_pe 可能是 (bsz, seqlen, qk_rope_head_dim)，需要添加一个 head 维度</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_pe</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">k_pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">freqs_cis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. 注意力计算：根据实现方式 (naive 或 optimized)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Naive 实现直接拼接 Q 的 RoPE 和非 RoPE 部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">q_nope</span><span class="p">,</span> <span class="n">q_pe</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q 恢复为 (bsz, seqlen, n_local_heads, qk_head_dim)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 对 KV 潜在向量应用归一化，并进行第二阶段投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中将 c_t^{KV} 投影到非 RoPE 键和值的部分 (k_t^C 和 v_t^C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span><span class="p">(</span><span class="n">kv</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 KV 结果重塑为 (batch_size, seq_len, n_local_heads, qk_nope_head_dim + v_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 分离出非 RoPE 键和值</span>
</span></span><span class="line"><span class="cl">            <span class="n">k_nope</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 拼接非 RoPE 键和 RoPE 键，组成完整的键 K</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_pe 之前是 (bsz, seqlen, 1, qk_rope_head_dim)，需要 expand 到 n_local_heads</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k_nope</span><span class="p">,</span> <span class="n">k_pe</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 K 和 V 缓存 (在推理时用于自回归生成)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">k_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">v_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算注意力分数 (Q @ K^T)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># scores: (batch_size, q_seq_len, n_local_heads, k_seq_len)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 使用整个缓存中的键进行计算</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshd,bthd-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># optimized 实现</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 获取 wkv_b 权重，如果使用了量化则进行反量化</span>
</span></span><span class="line"><span class="cl">            <span class="n">wkv_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">weight</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">weight_dequant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 wkv_b 重塑为 (n_local_heads, head_dim, kv_lora_rank) 以便进行逐头的操作</span>
</span></span><span class="line"><span class="cl">            <span class="n">wkv_b</span> <span class="o">=</span> <span class="n">wkv_b</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">)</span> <span class="c1"># (n_heads, (qk_nope+v), kv_rank)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算 Q_nope 与 K_nope 的点积 (通过 kv 缓存)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># q_nope: (bsz, seqlen, n_local_heads, qk_nope_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wkv_b[:, :self.qk_nope_head_dim] 是 W^{UK} 的部分</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 这对应论文中的 Softmax(q_{t,i}^C @ c_{j,i}^{KV}) 的第一项</span>
</span></span><span class="line"><span class="cl">            <span class="n">q_nope</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshd,hdc-&gt;bshc&#34;</span><span class="p">,</span> <span class="n">q_nope</span><span class="p">,</span> <span class="n">wkv_b</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 KV 缓存 (kv_cache 对应论文中的 c_t^{KV})</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span><span class="p">(</span><span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 PE 缓存 (pe_cache 对应论文中的 k_t^R)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_pe 之前是 (bsz, seqlen, 1, qk_rope_head_dim)，squeeze 掉那个 1 维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">pe_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_pe</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算注意力分数</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第一项: 非 RoPE 查询 q_nope 与缓存的 kv_cache (压缩键) 的点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(q_{t,i}^C @ c_{j,i}^{KV}) 的第一部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshc,btc-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q_nope</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span> <span class="o">+</span> \
</span></span><span class="line"><span class="cl">                      <span class="c1"># 第二项: RoPE 查询 q_pe 与缓存的 pe_cache (解耦 RoPE 键) 的点积</span>
</span></span><span class="line"><span class="cl">                      <span class="c1"># 对应论文中的 Softmax(q_{t,i}^R @ k_{j,i}^R) 的第二部分</span>
</span></span><span class="line"><span class="cl">                      <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshr,btr-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q_pe</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="c1"># 应用缩放因子</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 应用注意力掩码 (如因果掩码，防止看到未来信息)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">+=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># unsqueeze(1) 广播到 heads 维度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 对分数应用 Softmax 得到注意力权重</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. 值 (V) 的加权求和</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Naive 实现直接与 V 缓存进行点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 sum(Softmax(...) * v_{j,i}^C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bsht,bthd-&gt;bshd&#34;</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># optimized 实现</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># optimized 实现通过 wkv_b 的值部分将加权后的压缩 KV 还原为 V</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第一步: 将注意力权重与缓存的 kv_cache (压缩值) 进行点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(...) * c_{j,i}^{KV} 的第一部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bsht,btc-&gt;bshc&#34;</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第二步: 将加权后的压缩值通过 wkv_b 的值投影部分还原为最终的值向量</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wkv_b[:, -self.v_head_dim:] 是 W^{UV} 的部分</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(...) * v_{j,i}^C 的第二部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshc,hdc-&gt;bshd&#34;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wkv_b</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 将所有头的结果展平并进行最终的输出投影</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># x.flatten(2) 将 (bsz, seqlen, n_local_heads, v_head_dim) 展平为 (bsz, seqlen, n_local_heads * v_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
  </channel>
</rss>
