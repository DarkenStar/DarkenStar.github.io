<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Paper Reading on WITHER</title>
    <link>http://localhost:57770/categories/paper-reading/</link>
    <description>Recent content in Paper Reading on WITHER</description>
    <generator>Hugo -- 0.147.7</generator>
    <language>en</language>
    <copyright>2024-2025 WITHER</copyright>
    <lastBuildDate>Thu, 12 Jun 2025 23:01:49 +0800</lastBuildDate>
    <atom:link href="http://localhost:57770/categories/paper-reading/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fast-dLLM</title>
      <link>http://localhost:57770/blogs/fast-dllm/</link>
      <pubDate>Thu, 12 Jun 2025 23:01:49 +0800</pubDate>
      <guid>http://localhost:57770/blogs/fast-dllm/</guid>
      <description>Paper Reading of Fast-dLLM</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Diffusion LLMs 被视为下一代文本生成技术的有力竞争者，其核心优势在于理论上可以并行生成多个 token，从而有望实现比自回归模型快几个数量级的推理速度。谷歌的 Gemini Diffusion 和 Inception Labs 的Mercury等模型已经展示了其惊人的潜力，宣称能达到每秒上千 token 的生成速度。</p>
<p>当前开源的扩散LLM (LLaDA、Dream) 在实际应用中的速度远远达不到预期，甚至比优化良好的自回归模型还要慢。这篇论文的工作，就是要拆掉阻碍扩散 LLM 起飞的两座大山。</p>
<ol>
<li>无法使用 KV Cache</li>
</ol>
<p>扩散LLM的注意力机制是双向的，即一个 token 的生成不仅依赖于它前面的内容，也依赖于它后面的内容（尽管后面可能是未知的 MASK token ）。这种特性使得过去的信息和未来的信息相互纠缠，无法像自回归模型那样简单地缓存和复用过去的信息。导致扩散LLM在每一步推理中都需要进行大量的重复计算，严重拖慢了速度。</p>
<p>Fast-dLLM 的第一个核心贡献，就是提出了一种分块近似 (block-wise approximate) KV Cache 机制。</p>
<blockquote class="quote"><p>While the bidirectional nature of attention in Diffusion LLMs precludes a fully equivalent KV Cache, our approximation closely resembles an ideal cache in practice.</p></blockquote>
<p>它将待生成的文本序列分成若干个块. 在生成某一个块 (比如Block 1) 时，它会提前计算并缓存其他所有块 (比如 Prompt 和 Block 0) 的 KV. 在这个块的内部生成过程中，这些缓存被反复利用。当这个块生成完毕后，再整体更新一次所有块的KV缓存 。</p>
<p>这个方法的近似在于，在一个块的生成过程中，缓存是固定的，而实际上随着块内 token 的不断去噪和清晰化，这些缓存理论上也应该随之微调。但论文通过可视化实验（图3）有力地证明，在相邻的推理步骤中，KV 激活值的 余弦相似度非常高，几乎接近于1. 这说明使用固定的近似缓存带来的误差微乎其微，完全可以用极小的精度损失换取巨大的速度提升。</p>
<p>论文还进一步提出了双缓存 (DualCache) 版本，不仅缓存了前面的“前缀”（prefix），还缓存了后面的“后缀”（suffix，通常是 MASK  token ） ，从而进一步压榨了计算优化的空间，实现了更快的速度。</p>
<ol start="2">
<li>并行解码带来的质量下降</li>
</ol>
<p>扩散LLM的另一大理论优势是 并行解码 (Parallel Decoding)，即一次性预测和生成多个 token  。然而，实践再次证明，当并行解码的 token 数量增多时，生成文本的质量会急剧下降 。</p>
<p>论文深刻地剖析了其根源：条件独立性假设 (conditional independence assumption) 的破坏 。在并行解码时，模型是独立地为每个待生成的 MASK 位置预测一个概率分布，然后从中采样。但实际上，一句话中的 token 之间存在着强烈的依赖关系。论文举了一个例子:</p>
<blockquote class="quote"><p>Consider an example from [30]: The list of poker hands that consist of two English words are: The subsequent two words could be, for instance, &ldquo;high card,&rdquo; &ldquo;two pair,&rdquo; &ldquo;full house,&rdquo; or &ldquo;straight flush.&rdquo; [&hellip;] However, the multi-token prediction procedure in MDMs first generates a probability distribution for each token and then samples from these distributions independently. This independent sampling can lead to undesirable combinations, such as &ldquo;high house.&rdquo;</p></blockquote>
<p>模型可能会独立地预测出 &ldquo;high&rdquo; 和 &ldquo;house&quot;这两个词，但把它们组合在一起就成了毫无意义的 high house. 这是因为模型在并行预测时忽略了 token 间的联合概率，而错误地直接使用了边缘概率的乘积。</p>
<p>为了解决这个问题，Fast-dLLM提出了第二个核心贡献：置信度感知并行解码 (Confidence-Aware Parallel Decoding) 策略 。这个想法非常直观且有效：我们只对那些模型非常有把握的 token 进行并行解码。</p>
<p>具体来说，在每一步解码时，模型会为每个待生成的 MASK 位置计算一个 置信度分数 (比如softmax概率的最大值). 然后，设定一个全局的置信度阈值 τ，只有那些置信度超过这个阈值的 token 才会被揭开，而置信度不足的 token 则继续保持 MASK 状态，留到下一步再做决策。为了避免无限循环，如果没有任何 token 的置信度达标，模型会强制解码置信度最高的那一个。</p>
<p>这个策略的精妙之处在于，它在理论上是站得住脚的。论文通过定理一从数学上证明了：当模型对一组 token 的预测置信度足够高时 (即 p&gt;1−ϵ，且 ϵ 足够小)，基于独立边缘概率的“贪心并行解码”与基于真实联合概率的“贪心串行解码”会得到完全相同的结果。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&amp;shareKey=c9e48ddb1e1f0600394ce8baa1d84426" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&amp;shareKey=c9e48ddb1e1f0600394ce8baa1d84426" alt="Effectiveness of Components of Fast-dLLM across Different Approaches">
    </a><figcaption>Effectiveness of Components of Fast-dLLM across Different Approaches</figcaption></figure></p>
<p>Fast-dLLM 的创新性体现在它是一种 training-free 的加速框架。它没有修改模型结构，也不需要重新训练，而是通过两项即插即用的推理策略——“分块近似KV缓存”和“置信度感知并行解码”，分别从减少重复计算和提升并行效率两个维度，精准地解决了当前开源扩散 LLM 面临的核心瓶颈。 实验结果在 LLaDA 和 Dream 等模型上，结合两种策略，实现了高达 27.6 倍的端到端吞吐量提升，同时在多个基准测试上几乎没有精度损失。</p>
<h1 id="2-preliminary">2. Preliminary</h1>
<h3 id="21-masked-diffusion-model">2.1. Masked Diffusion Model</h3>
<p>针对离散数据的扩散模型最早在 Argmax Flows and Multinomial Diffusion 和 Deep Unsupervised Learning using
Nonequilibrium Thermodynamics 中被探提出。随后 D3PM 提出了一个更通用的框架，通过特定的转移矩阵 $Q_{t}$ 定义了前向加噪过程的离散状态马尔可夫链，并通过最大化 ELBO 来学习反向过程的参数化模型 $p_{\theta}(x_{0}|x_{t})$. CTMC 进一步将 D3PM 扩展到连续时间，将其形式化为一个连续时间马尔可夫链 (CTMC) 框架。在另一种不同的方法中，SEDD 通过参数化似然比 $\frac{p_{t}(y)}{p_{t}(x)}$ 来学习反向过程，并采用去噪分数熵来训练该比率。</p>
<p>在各种离散扩散的噪声处理方式中，<strong>Masked Diffusion Models, MDMs</strong>，也被称为吸收状态离散扩散模型，获得了相当大的关注。MDMs 采用一种前向加噪过程，其中 token 被逐步替换为一个特殊的 MASK  token  。这个过程由以下转移概率定义：</p>
$$
q_{t|0}(x_{t}|x_{0})=\prod_{i=1}^{n}q_{t|0}(x_{t}^{i}|x_{0}^{i})=\prod_{i=1}^{n}Cat(x_{t}^{i};(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]})
$$<p> (1)</p>
<ul>
<li>$q_{t|0}(x_t|x_0)$: 表示给定原始序列 $x_0$，得到噪声序列 $x_t$ 的概率 。</li>
<li>$\prod_{i=1}^{n}$: 连乘符号，表示整个序列的噪声过程是序列中每个 token （token）独立进行噪声过程的概率乘积 。</li>
<li>$Cat(\cdot)$: 代表<strong>类别分布 (Categorical Distribution)</strong> 。</li>
<li>$t \in [0,1]$: 表示<strong>扩散时间</strong>或<strong>掩码级别</strong>。当 $t=0$ 时，序列完全是原始的；当 $t=1$ 时，序列被完全替换为 <code>[MASK]</code>  token 。</li>
<li>$(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}$: 在时间 <code>t</code>，第 <code>i</code> 个 token 有 $1-t$ 的概率保持其原始身份 $x_0^i$，有 $t$ 的概率变成 <code>[MASK]</code>  token 。<code>$\delta$</code> 是克罗内克函数，用于指定概率。</li>
</ul>
<p>最近，MDLM 和 RADD 的工作表明，对于 MDMs 不同的参数化是等价的。此外，他们证明了 MDMs 的训练目标可以被简化或直接从数据似然中推导出来 。这导出了以下目标函数，即 $log~p_{\theta}(x)$ 的一个 ELBO:</p>
$$
-log~p_{\theta}(x)\le\int_{0}^{1}\frac{1}{t}\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})]dt:=\mathcal{L}_{MDM}.
$$<p> (2)</p>
<ul>
<li>$-log~p_{\theta}(x)$: 模型的目标是最大化生成真实数据 $x$ 的对数似然，这等价于最小化它的负对数似然。这个公式给出了负对数似然的一个* ELBO.</li>
<li>$\int_{0}^{1}...dt$: 对所有可能的噪声级别 <code>t</code> (从0到1) 进行积分，意味着模型需要学会在任何噪声水平下都能很好地复原数据 。</li>
<li>$\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[...]$: 表示对所有可能的噪声样本求期望。在训练时，我们根据公式(1)随机生成一个带 <code>[MASK]</code> 的噪声序列 $x_t$.</li>
<li>$\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})$:
<ul>
<li>$\sum_{i:x_{t}^{i}=[MASK]}$: 对所有被 <code>[MASK]</code> 的位置 <code>i</code> 进行求和 。</li>
<li>$-log~p_{\theta}(x_{0}^{i}|x_{t})$: 这是交叉熵损失。它的意思是，给定带有 <code>[MASK]</code> 的序列 $x_t$，模型 $p_{\theta}$ 需要预测在位置 i 上的原始 token  $x_0^i$ 应该是什么。模型预测得越准，这个损失值就越小。</li>
</ul>
</li>
</ul>
<h3 id="22-mdms-的生成过程">2.2. MDMs 的生成过程</h3>
<p>对于公式1中定义的前向过程，其解析上的逆过程在生成时计算效率低下，因为它通常每步只修改一个 token 。一个常见的加速策略是采用 $\tau$-leaping 近似法来处理反向过程。在 MDMs 的背景下，这允许一个迭代式的生成过程，其中多个被掩码的 token 可以从一个噪声水平 t 近似地单步恢复到一个更早的水平 s &lt; t.</p>
$$
q_{s|t}(x_s|x_t)=\prod_{i=0}^{n-1}q_{s|t}(x_{s}^{i}|x_{t})
$$<p>其中</p>
$$
q_{s|t}(x_{s}^{i}|x_{t})=\begin{cases}1, & \text{if } x_{t}^{i}\ne[MASK], x_{s}^{i}=x_{t}^{i} \\ \frac{s}{t}, & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}=[MASK] \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}\ne[MASK]\end{cases}
$$<ul>
<li>$q_{s|t}(x_{s}^{i}|x_{t})$: 表示从 <code>t</code> 时刻的 token  $x_t^i$ 变为 <code>s</code> 时刻的 token  $x_s^i$ 的概率 。</li>
<li><strong>Case 1</strong>: 如果一个 token 在 <code>t</code> 时刻就不是 <code>[MASK]</code>，那么它在更早的 <code>s</code> 时刻也保持不变 。</li>
<li><strong>Case 2</strong>: 一个在 t 时刻是 <code>[MASK]</code> 的 token ，在更早的 s 时刻仍然是 <code>[MASK]</code>.</li>
<li><strong>Case 3</strong>: 这是关键的去噪步骤。如果一个 token 在 <code>t</code> 时刻是 <code>[MASK]</code>，模型会尝试在 s 时刻预测出一个具体的 token.
<ul>
<li>$\frac{t-s}{t}$: 代表一个在 <code>t</code> 时刻被掩码的 token，在 <code>s</code> 时刻被“揭示”出来的概率 。</li>
<li>$q_{0|t}(x_{s}^{i}|x_{t})$: 这是由神经网络模型给出的预测分布。模型会观察整个带有 <code>[MASK]</code> 的上下文 $x_t$，然后为当前位置预测一个最有可能的原始 token ，并给出一个在整个词汇表上的概率分布 。</li>
</ul>
</li>
</ul>
<p>在涉及条件数据的场景中，例如根据一个 propmt p 生成一个回应 $x_{0}$，MDM 的反向过程 (公式3所定义) 需要进行调整。具体来说，模型用于揭示一个 token  $x_{s}^{i}$ 的预测分布 $q_{0|t}(x_{s}^{i}|x_{t})$ 现在也需要以 prompt p 为条件，即 $q_{0|t}(x_{s}^{i}|x_{t},p)$ 。</p>
<p><strong>并行解码的诅咒</strong>
直接逆转公式1的前向过程来进行生成是缓慢的，通常每步只改变一个 token. 一个常见的加速策略是采用 $\tau$-leaping 近似法来处理反向过程。对于 MDMs，这意味着多个被掩码的 token 将在一个步骤中并行生成。然而，由于条件独立性假设，多 token 预测中出现了一个重大挑战。考虑一个例子：由两个英文单词组成的扑克手牌列表是：随后的两个词可能是，例如，high card，two pair，full house，或 straight flush. 值得注意的是，这两个词之间存在着关联。然而，MDMs 中的多 token 预测过程首先为每个 token 生成一个概率分布，然后独立地从这些分布中进行采样。这种独立采样可能导致不希望的组合，例如 high house.</p>
<p>为了将其形式化，考虑揭示两个 token 位置 i 和 j. 由于条件独立性假设，MDMs 从 $p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t})$ 中采样这些 token. 然而，真实的联合概率需要考虑它们之间的依赖关系：</p>
$$
p(x_{s}^{i},x_{s}^{j}|x_{t})=p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t},x_{s}^{i})
$$<p>
或者对称地，通过将 i 依赖于条件 j. 这种假设的独立生成与真实的依赖性数据分布之间的差异，会降低生成序列的质量和连贯性。当在单一步骤中同时揭示大量 token 时，这个问题会变得更加严重。</p>
<h1 id="3-methodology">3. Methodology</h1>
<h2 id="31-pipeline-overview">3.1. Pipeline Overview</h2>
<p><strong>Fast-dLLM</strong>，建立在 MDM 架构之上，以实现高效和高质量的序列生成。为了加速推理，整体流水线融合了两大关键策略：通过 KV Cache 实现的高效注意力计算，以及一个由预测置信度引导的 并行解码方案。具体来说，我们采用了分块解码设计的 KV Cache，它允许在不同步骤间复用注意力激活值，并显著减少了冗余计算。在每个块内部，进一步提出了置信度感知的并行解码，它能根据置信度分数选择性地更新 token ，从而在保持输出质量的同时提高效率。通过结合这些策略，Fast-dLLM 在对生成性能影响最小的情况下，显著加快了 MDM 的推理速度。整体流程在算法 1 中进行了总结。</p>
<h2 id="32-key-value-cache-for-block-wise-decoding">3.2. Key-Value Cache for Block-Wise Decoding</h2>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&amp;shareKey=8952caa17d664bd8bcc33b9ebcec321e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&amp;shareKey=8952caa17d664bd8bcc33b9ebcec321e" alt="Illustration of our Key-Value Cache for Block-Wise Decoding">
    </a><figcaption>Illustration of our Key-Value Cache for Block-Wise Decoding</figcaption></figure></p>
<p>如上图所示，我们采用了一种分块解码的策略来支持 KV Cache 的使用。一开始计算并存储 prompt 的 KV 缓存，这个缓存将在整个块 0的解码过程中被复用。在每个块的内部，相同的缓存会被多个解码步骤复用。在完成一个块的解码之后，更新<strong>所有</strong> token  (不仅仅是新生成的 token ) 的缓存。这个缓存更新可以与解码步骤联合执行，因此与不使用缓存相比，没有额外的计算开销。由于掩码扩散模型中使用的是完全注意力机制，这种方法导致了一个近似的解码过程。</p>
<p>我们的近似 KV 缓存方法的有效性，源于我们观察到 KV 激活值在相邻的推理步骤中表现出高度的相似性，如下图所示。图 a 中红色方框区域突显了块内的相似性分数，这些分数始终接近于1。这表明在分块解码期间，前缀 (prefix) 的键和值的差异可以忽略不计，使我们能够安全地复用缓存而不会有显著的准确率损失。 此外，我们实现了一个我们 KV 缓存机制的双向版本，名为 <strong>DualCache</strong>，它不仅缓存前缀 token ，还缓存后缀 (suffix)  token ，在我们的分块解码方案中，后缀完全由掩码 token 组成。如表3所示，DualCache 带来了进一步的加速。图 b 中的红色方框区域进一步证明，在分块解码期间，后缀的键和值的差异也可以忽略不计。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&amp;shareKey=6a5005c556aaa11edb4006a48b755b4a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&amp;shareKey=6a5005c556aaa11edb4006a48b755b4a" alt="Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA">
    </a><figcaption>Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA</figcaption></figure></p>
<h2 id="33-confidence-aware-parallel-decoding">3.3. Confidence-Aware Parallel Decoding</h2>
<p>尽管存在一些方法，例如使用辅助模型来显式地捕捉不同位置 token 之间的依赖关系，但它们通常会增加整个流水线的复杂性。与这些方法相反，我们提出了一个简单而有效的<strong>置信度感知解码算法</strong>，旨在缓解这种条件独立性问题。</p>
<p>在每次迭代中，我们不是冒然地使用它们独立的边缘概率来揭示所有被掩码的 token ，而是为每个 token 计算一个置信度分数 (例如最大的 softmax 概率). 只有那些置信度超过一个阈值的 token 才会在当前步骤被揭示；其余的则保持掩码状态，并在未来的步骤中重新考虑。如果没有 token 的置信度超过阈值，就揭示置信度最高的那一个，以确保过程能够进行并防止无限循环。这个策略在加速生成的同时，减少了由不确定或模糊预测引起的错误。</p>
<p>一个关键问题是</p>
<blockquote class="quote"><p><em>When is it theoretically justifiable to decode tokens in parallel using independent marginals, despite the true joint distribution potentially containing dependencies?</em></p></blockquote>
<p>以下结果来回答了在高置信度情况下，greedy parallel 解码等同于 greedy sequential 解码的条件，并量化了两种分布之间的差异。在给出定理之前，我们将定义其表述中使用的数学符号。</p>
<p>设 $p_{\theta}(\cdot|E)$ 表示一个 MDM 在给定 E (包括 prompt $p_{0}$ 和先前生成的 token) 的条件下给出的 PMF. 假设模型要为不在 E 中的位置 $i_{1},...,i_{n}$ 预测 n 个 token.</p>
<p>令 $X=(X_{i_{1}},...,X_{i_{n}})$ 是 n 个 token 的向量，其中每个 $X_{i_{j}}$ 在词汇表 V 中取值。设 $p(X|E)\equiv p_{\theta}(X_{i_{1}},...,X_{i_{n}}|E)$ 是模型给出的联合条件 PMF。设 $p_{j}(X_{i_{j}}|E)\equiv p_{\theta}(X_{i_{j}}|E)$ 是位置 $i_{j}$ 的边缘条件 PMF。并行解码使用边缘概率的乘积来生成 token ：$q(X|E)=\tilde{\prod}_{j=1}^{n}p_{j}(X_{i_{j}}|E)$。定理1的证明及相关讨论见附录A。</p>
<p><strong>定理 1 (高置信度下的并行解码).</strong> 假设存在一个特定的 token 序列 $x^{*}=(x_{i_{1}},...,x_{i_{n}})$，使得对于每个 $j\in\{1,...,n\}$，模型对 $x_{i_{j}}$ 都有很高的置信度：$p_{j}(X_{i_{j}}=x_{i_{j}}|E)>1-\epsilon$，对于某个很小的 $\epsilon>0$. 那么，以下结论成立：</p>
<ol>
<li><strong>贪婪解码的等价性</strong>：如果 $(n+1)\epsilon\le1$（即 $\epsilon\le\frac{1}{n+1}$），那么
$$
\text{argmax}_{z} p(z|E) = \text{argmax}_{z} q(z|E) = x^{*}.
$$ (4)</li>
</ol>
<p>这意味着 greedy parallel 解码 (选择 argmax q) 与贪婪序贯解码 (选择 argmax p) 产生相同的结果。  这个界是紧的：如果 $\epsilon > \frac{1}{n+1}$，则存在满足高置信度边缘假设的分布 $p(X|E)$，使得 argmax $p(z|E)$ ≠ argmax $q(z|E)$。</p>
<ol start="2">
<li><em>Distance and Divergence Bounds</em>：为简洁起见，将 $p(\cdot|E)$ 和 $q(\cdot|E)$ 表示为 p 和 q。</li>
</ol>
<p><strong>$L_p$ Distance ($p \ge 1$)</strong>: 对于 $n>1$，$D_{p}(p,q)<((n-1)^{p}+2n)^{1/p}\epsilon$。特别地，对于总变差距离 ($D_{TV}(p,q)=\frac{1}{2}D_{1}(p,q)$)，$D_{TV}(p,q)<\frac{3n-1}{2}\epsilon$。</p>
<p><strong>Forward KL Divergence</strong>: 对于 $n > 1$，$D_{KL}(p||q)<(n-1)(H_{b}(\epsilon)+\epsilon~ln(|\mathcal{V}|-1))$，其中 $H_{b}(\epsilon)=-\epsilon~ln~\epsilon-(1-\epsilon)ln(1-\epsilon)$ 是二元熵函数，而 $|\mathcal{V}|$ 是词汇表的大小。</p>
<h1 id="4-experiments">4. Experiments</h1>
<hr>
<h2 id="experimental-setup">Experimental Setup</h2>
<ul>
<li><strong>硬件与环境</strong> 🖥️: 所有实验均在单张 <strong>NVIDIA A100 80GB GPU</strong> 上进行，batch size=1.</li>
<li><strong>评测模型</strong> 🧠: <strong>LLaDA</strong>  和 <strong>Dream</strong>.</li>
<li><strong>评测基准</strong> 📊: 采用了四个广泛使用的基准数据集：<strong>GSM8K</strong>、<strong>MATH</strong>、<strong>HumanEval</strong> 和 <strong>MBPP</strong>.</li>
<li><strong>核心指标</strong> ⏱️:
<ul>
<li><strong>准确率 (Accuracy)</strong>: 衡量模型在具体任务上的表现。</li>
<li><strong>吞吐量 (Throughput)</strong>: 以 tokens/sec 为单位，反映端到端的真实解码速度。</li>
</ul>
</li>
<li><strong>超参数</strong> ⚙️:
<ul>
<li><strong>缓存块大小</strong>: 在 4 到 32 之间进行探索。</li>
<li><strong>置信度阈值</strong>: 在 0.5 到 1.0 之间进行探索。</li>
<li>实验默认使用 <strong>PrefixCache</strong>，块大小为 <strong>32</strong>，置信度阈值为 <strong>0.9</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="main-results-performance-and-speed">Main Results: Performance and Speed</h2>
<p>实验结果表明，Fast-dLLM 在各种任务和设置上都取得了显著的速度提升，同时对模型准确率的影响微乎其微 。</p>
<ul>
<li>加速效果:
<ul>
<li>单独引入 KV Cache 机制，通常能带来 <strong>2x-3.6x</strong> 的速度提升。</li>
<li>当 KV Cache 和并行解码两种策略结合使用时，性能提升更为显著。在 LLaDA 模型上，最 高可达 <strong>11.0x</strong> 的吞吐量提升；在 Dream 模型上，最高可达 <strong>7.8x</strong> 的提升 。</li>
</ul>
</li>
<li>极小的精度损失: 在所有基准测试中，加速后模型的准确率与原始基线模型的差距基本保持在 <strong>1-2个百分点</strong> 以内，有时甚至略有提高。</li>
<li>对长序列更友好: 实验还发现，在处理更长的文本序列时 (例如 few-shot 场景或长代码生成)，Fast-dLLM 的加速效果更为明显。</li>
</ul>
<h2 id="核心结果摘要">核心结果摘要</h2>
<p>下表以 GSM8K (5-shot) 任务为例，直观展示了 Fast-dLLM (即 +Cache+Parallel) 相较于基线模型的性能提升。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">模型</th>
          <th style="text-align: left">生成长度</th>
          <th style="text-align: left">配置</th>
          <th style="text-align: left">准确率 (%)</th>
          <th style="text-align: left">吞吐量 (tok/s)</th>
          <th style="text-align: left">相对加速</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>LLaDA</strong></td>
          <td style="text-align: left">256</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">79.3</td>
          <td style="text-align: left">6.7</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>78.5</strong></td>
          <td style="text-align: left"><strong>54.4</strong></td>
          <td style="text-align: left"><strong>8.1x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">512</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">77.5</td>
          <td style="text-align: left">3.2</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>77.2</strong></td>
          <td style="text-align: left"><strong>35.3</strong></td>
          <td style="text-align: left"><strong>11.0x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Dream</strong></td>
          <td style="text-align: left">256</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">75.0</td>
          <td style="text-align: left">9.1</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>74.8</strong></td>
          <td style="text-align: left"><strong>48.2</strong></td>
          <td style="text-align: left"><strong>5.3x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">512</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">76.0</td>
          <td style="text-align: left">7.7</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>74.0</strong></td>
          <td style="text-align: left"><strong>42.9</strong></td>
          <td style="text-align: left"><strong>5.6x</strong></td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="ablations-and-analysis">Ablations and Analysis</h2>
<p>为了深入理解各个组件的贡献，论文进行了一系列详细的消融实验。</p>
<ul>
<li>
<p><strong>输入与生成长度的影响</strong>:</p>
<ul>
<li>实验证明，更长的上下文 (prefill，如从 5-shot 增加到 8-shot) 和更长的生成长度，都能显著放大加速效果。</li>
<li>在 8-shot 和 1024 生成长度的设置下，<strong>DualCache</strong> 实现了 <strong>27.6x</strong> 端到端加速。</li>
</ul>
</li>
<li>
<p><strong>PrefixCache vs. DualCache</strong>:</p>
<ul>
<li><strong>DualCache</strong> 通常比只缓存前缀的 <strong>PrefixCache</strong> 实现更高的加速比，尤其是在长序列生成任务中 。</li>
</ul>
</li>
<li>
<p><strong>缓存块大小的影响</strong>:</p>
<ul>
<li><strong>small block size</strong>：准确率最高，但因频繁更新缓存导致开销较大，速度提升有限 。</li>
<li><strong>small block size</strong>：速度快，但可能因上下文不匹配导致准确率下降 。</li>
<li>实验发现，块大小为 <strong>32</strong> 时在速度和精度之间取得了最佳平衡。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&amp;shareKey=1e3a007e630de1a9cbf8b3d9f318f307" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&amp;shareKey=1e3a007e630de1a9cbf8b3d9f318f307" alt="Impact of Cache Block Size on Accuracy and Throughput">
    </a><figcaption>Impact of Cache Block Size on Accuracy and Throughput</figcaption></figure></p>
<ul>
<li><strong>动态阈值 vs. 固定步数策略</strong>:
<ul>
<li>论文提出的 <strong>置信度感知并行解码</strong> 策略，在性能上持续优于每步固定解码 K 个 token 的 baseline 方法。</li>
<li>在达到相似甚至更高准确率的同时，该动态策略能实现更高的平均每步解码 token 数，从而获得更高的吞吐量。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&amp;shareKey=88d29eb3e40615a74c4846d278413e5b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&amp;shareKey=88d29eb3e40615a74c4846d278413e5b" alt="Threshold VS Fxied Step">
    </a><figcaption>Threshold VS Fxied Step</figcaption></figure></p>
<h1 id="5-related-work">5. Related Work</h1>
<p>本章节回顾了与 Fast-dLLM 相关的两个核心领域：扩散语言模型的发展，以及大语言模型的通用加速技术。</p>
<hr>
<h2 id="51-diffusion-llm">5.1. Diffusion LLM</h2>
<p>扩散模型作为一种强大的生成范式，最初在图像和音频等连续数据领域取得了巨大成功，随后其影响力扩展到了 NLP. 特别是离散扩散模型的最新进展为大语言模型提供了一种替代自回归 (AR) 范式的可行方案 。</p>
<ul>
<li>
<p><strong>理论基础的发展</strong>:</p>
<ul>
<li>离散数据的扩散模型最早由 [29, 11] 探索 。</li>
<li><strong>D3PM</strong> 提出了一个更通用的框架，将前向加噪过程建模为离散状态马尔可夫链，并通过最大 ELBO 来学习反向过程。</li>
<li><strong>CTMC</strong> 将 D3PM 扩展到连续时间设定 。</li>
<li><strong>SEDD</strong> 采用了不同的方法，通过参数化边际似然比来学习反向过程 。</li>
<li><strong>MDMs</strong> 近期受到了广泛关注，其中 <strong>MDLM</strong> 和 <strong>RADD</strong> 的研究表明，MDMs 的不同参数化方法是等价的，并且其训练目标可以被简化 。</li>
</ul>
</li>
<li>
<p><strong>与预训练语言模型的结合</strong>: 一个关键的突破是将离散扩散与现有的大语言模型架构相结合 。</p>
<ul>
<li><strong>Diffusion-NAT</strong> [40] 将离散扩散的去噪过程与 BART 的非自回归解码相结合，通过迭代式地优化被掩码的 token ，实现了比同类自回归 Transformer 快20倍的生成速度 。</li>
<li><strong>LLaDA</strong> [21]、<strong>DiffuLLaMA</strong> [7] 和 <strong>Dream</strong> [36] 等框架将扩散模型扩展到了 7B 参数的规模，通过在扩散时间步上进行递归式的 token 预测，展现了与 LLaMA3 等主流自回归模型相匹敌的性能 。</li>
</ul>
</li>
</ul>
<h2 id="52-llm-acceleration">5.2. LLM Acceleration</h2>
<h3 id="kv-cache">KV Cache</h3>
<p>由于 LLaDA 等扩散语言模型采用的是 <strong>full attention</strong>，将 KV 缓存直接应用于这类模型并非易事。 一篇相关的研究 <strong>Block diffusion</strong>  通过<strong>分块生成 (block-by-block)</strong> 的方式，克服了先前扩散语言模型的局限，使得缓存和复用先前已解码块的键和值成为可能 。</p>
<h3 id="non-autoregressive-generation">Non-Autoregressive Generation</h3>
<p>非自回归 (NAR) 生成标志着一种根本性的转变，它通过同时生成多个 token 来显著加速推理过程。NAR 方法最初被用于神经机器翻译，现已扩展到语法纠错、文本摘要和对话系统等多种任务
。
尽管 NAR 在速度上优势巨大，但它通常以牺牲一定的生成质量为代价。扩散语言模型是 NAR 领域一个新兴的范式；然而，先前的工作（如 LLaDA）在实践中难以实现预期的加速，因为并行生成会导致输出质量显著下降。</p>
<h1 id="weakness">Weakness</h1>
<p>近似缓存的误差累积效应：论文证明了在相邻步骤中，KV激活值的差异很小 。但随着生成块的增多，这种“近似”带来的微小误差是否会累积，并在生成非常长的文本（如数万 token 的小说）时导致语义漂移或一致性下降？论文的最长测试序列为1024 ，对于更长的序列，其鲁棒性有待进一步验证。</p>
<p>对模型能力的依赖：“置信度感知解码”策略的有效性，隐式地依赖于模型本身具有良好的“校准度”（calibration），即模型的置信度能够真实反映其预测的正确性。如果模型本身“过于自信”或“不够自信”，可能会导致该策略效果不佳。论文没有对所用模型的校准度进行分析。
定理一的理论与实践差距：论文坦诚地指出了定理一的局限性</p>
<blockquote>
<p>In practice, while MDM may not strictly satisfy this property, its behavior typically offers a close approximation.</p></blockquote>
<p>理论证明假设了一个“理想的”联合概率分布，而真实模型是否以及在多大程度上符合这个理想假设，是一个需要进一步探究的问题。理论和实践之间的差距可能在某些刁钻的（adversarial）或分布外（Out-of-Distribution）的场景下被放大。
超参数的敏感性与调优成本：尽管论文分析了块大小和阈值的影响，但并未提供一套系统性的方法来为新模型或新任务选择最佳超参数。在实际应用中，这可能意味着需要为每个特定用例进行成本不菲的网格搜索（grid search），增加了方法的应用门槛。
评估维度的局限性：论文主要使用了基于准确率的基准测试。但在开放式生成、对话等任务中，评估指标（如流畅度、一致性、多样性）更为复杂。Fast-dLLM是否会在这些“软”指标上引入不易察觉的负面影响，需要更全面的评估。</p>
]]></content:encoded>
    </item>
    <item>
      <title>LLaDA</title>
      <link>http://localhost:57770/blogs/llada/</link>
      <pubDate>Thu, 12 Jun 2025 13:43:16 +0800</pubDate>
      <guid>http://localhost:57770/blogs/llada/</guid>
      <description>Paper Reading of LLaDA</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>LLM 主要的思想是 <em>generative modeling</em> 的思想是通过最大似然估计来优化模型的分布 $\log p_\theta(\cdot)$ 来逼近数据的分布 $\log p_{\text{data}}(\cdot)$
</p>
$$
\underbrace{\max_\theta\mathbb{E}_{p_{\text{data}}(x)}\log p_\theta(x)\Leftrightarrow\min_\theta\operatorname{KL}(p_{\text{data}}(x)||p_\theta(x)).}_{\text{Generative modeling principles}} \quad(1)
$$<p>当前，几乎所有我们熟知的大语言模型，从GPT系列到LLaMA系列，都基于<em>autoregressice modeling</em> 来实现。这种范式的核心是 <strong>next-token prediction</strong> ，即根据已经生成的文本序列，逐 toekn 地预测下一个最有可能出现的 token.</p>
$$
\underbrace{p_\theta(x)=p_\theta(x^1)\prod_{i=2}^Lp_\theta(x^i\mid x^1,\ldots,x^{i-1})}_{\text{Autoregressive formulation}} \quad(2)
$$<p>这种单向、顺序的生成方式在处理需要双向推理的任务时表现不佳，一个典型的例子就是 <strong>Reversal Curse</strong> ——模型知道 A is B，却往往无法推断出 B is A.</p>
<p>LLM 能力的核心基石是生成式建模原理本身，即通过最大似然估计让模型学习真实世界的数据分布 ，而非自回归这一具体的实现形式。</p>
<blockquote class="quote"><p><strong>It is the generative modeling principles (i.e., Eq. (1)), rather than the autoregressive formulation (i.e., Eq. (2)) itself, that fundamentally underpin the essential properties of LLMs.</strong></p></blockquote>
<ol>
<li>
<p>大语言模型的可扩展性 (scalability) ——即模型越大、数据越多、效果越好的特性——并非自回归模型所独有 。相反，这种可扩展性来源于更底层的生成式建模原理，而这些原理恰好保证了<em>fisher consistency</em><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</li>
<li>
<p><em>instruction-following</em> 和 <em>in-context learning</em><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> 并非自回归模型所独有，而是所有设计得当的条件生成模型 (conditional generative models) 在处理结构化语言任务时都应具备的内在属性 。</p>
</li>
</ol>
<p>因此作者提出了<strong>LLaDA</strong> (<strong>L</strong>arge <strong>L</strong>anguage <strong>D</strong>iffusion with m<strong>A</strong>sking)，一个从零开始训练的、参数量达到 8B 的扩散语言模型。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&amp;shareKey=94170299ede39d5102cf1cf6e397c5c7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&amp;shareKey=94170299ede39d5102cf1cf6e397c5c7" alt="Zero&amp;Few-Shot Benchmarks">
    </a><figcaption>Zero&amp;Few-Shot Benchmarks</figcaption></figure></p>
<p>LLaDA 使用了 Masked Diffusion Model (MDM)，该方法结合了离散随机掩蔽过程，并训练了一个掩码预测器来近似其反向过程。</p>
<h1 id="2-approach">2 Approach</h1>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&amp;shareKey=0293b80db53bfd7b8a9ba03f15a6f802" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&amp;shareKey=0293b80db53bfd7b8a9ba03f15a6f802" alt="A Conceptual Overview of LLaDA">
    </a><figcaption>A Conceptual Overview of LLaDA</figcaption></figure></p>
<h2 id="21-probabilistic-formulation">2.1 Probabilistic Formulation</h2>
<p>与公式(2)中的自回归模型不同，LLaDA通过<strong>前向过程 (forward process)</strong> 和 <strong>反向过程 (reverse process)</strong> 来定义模型分布 $p_{\theta}(x_{0})$。</p>
<h3 id="forward-process">Forward Process</h3>
<p>逐步地、独立地 mask $x_{0}$ 中的 token，直到在 $t=1$ 时序列被完全 mask.</p>
<p>给定 $x_{0}$ 时 $x_{t}$ 的条件分布可以被分解为：</p>
$$
q_{t|0}(x_{t}|x_{0}) = \prod_{i=1}^{L} q_{t|0}(x_{t}^{i}|x_{0}^{i})
$$<p>对于 $t \in (0,1)$，序列 $x_{t}$ 是部分被掩码的，其中每个 token 有 $t$ 的概率被mask，或有 $1-t$ 的概率保持不变。</p>
$$
q_{t|0}(x_{t}^{i}|x_{0}^{i}) = \begin{cases} 1-t, & x_{t}^{i} = x_{0}^{i} \\ t, & x_{t}^{i} = M \end{cases}
$$<p>其中 M 表示掩码 token. 直观上，每个 token 要么保持不变，要么被掩码，而被掩码的概率随着 t 从 0 到 1 线性增加。在 $t=1$ 时，所有 token 都被 mask.</p>
<h2 id="reverse-process">Reverse Process</h2>
<p>反向过程则通过在 $t=1\rightarrow 0$ 从完全被掩码的序列中生成新数据。</p>
<p>对于 $0 \le s < t \le 1$，反向过程的条件分布分解为：</p>
$$
q_{s|t}(x_{s}|x_{t}) = \prod_{i=1}^{L} q_{s|t}(x_{s}^{i}|x_{t})
$$<p>其中每个 token 的条件分布为：</p>
$$
q_{s|t}(x_{s}^{i}|x_{t}^{i}) = \begin{cases} 1, & x_{t}^{i} \ne M, x_{s}^{i} = x_{t}^{i} \\ \frac{s}{t}, & x_{t}^{i} = M, x_{s}^{i} = M \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & x_{t}^{i} = M, x_{s}^{i} \ne M \\ 0, & \text{otherwise} \end{cases}
$$<p>需要估计的关键函数是条件分布 $q_{0|t}(x_{s}^{i}|x_{t})$，它在输入 $x_{t}$ 中对应位置被掩码的情况下，预测出原始的 token. 类似于连续扩散模型中的数据预测形式。如 (Ou et al., 2024) 所证明，可以推导出一个等价但无时间依赖的参数化形式</p>
$$
q_{0|t}(x_s^i|x_t)=p_{\text{data}}(x_0^i|x_t^\text{UM}),\quad\forall i\text{ such that }x_t^i=\mathbf{M}
$$<p>其中 $x_{t}^{\text{UM}}$ 表示 $x_{t}$ 中未被掩码 token 的集合，它与原始数据 $x_{0}$ 中对应的 token 相同，因为未掩码的 token 仅由 $x_{0}$ 决定且与时间 t 无关 。直观上，这意味着估计数据预测函数等同于估计在干净数据上的条件分布，而后者是时不变的。因此，时间 t 不需要作为输入提供给参数化模型 。</p>
<p>尽管 MDM 的推导过程不简单，但其实现是直接的。我们首先引入<strong>掩码预测器</strong>，一个参数化模型 $p_{\theta}(\cdot|x_{t})$ (例如一个没有因果掩码的 Transformer)，它将任意 t 时刻的 $x_{t}$ 作为输入，并同时预测所有被 mask 的 token. 然后，我们如下定义模型分布 $p_{\theta}(x_{0})$：从一个被完全 mask 序列的 $x_{1}$ 开始，从 $t=1$ 到 0 模拟一个由 $p_{\theta}(\cdot|x_{t})$ 参数化的近似反向过程。在 $t=0$ 时刻推导出的边缘分布即代表了模型分布 $p_{\theta}(x_{0})$ 。</p>
<p>掩码预测器将 $x_{t}$ 作为输入并同时预测所有被掩码的 token (表示为 M). 它通过一个仅在被掩码 token 上计算的交叉熵损失进行训练：</p>
$$
\mathcal{L}(\theta)\triangleq-\mathbb{E}_{t,x_{0},x_{t}}[\frac{1}{t}\sum_{i=1}^{L}I[x_{t}^{i}=M]log~p_{\theta}(x_{0}^{i}|x_{t})], \quad(3)
$$<p>其中，$x_{0}$ 从训练数据中采样，$t$ 从<code>[0, 1]</code>中均匀采样<span class="sidenote-number"><small class="sidenote">Notably, LLaDA employs a masking ratio that <em>varies randomly</em> between 0 and 1 while masked language models (Devlin, 2018) use a fixed ratio.</small></span>
，$x_{t}$ 从前向过程中采样。指示函数 $I[\cdot]$ 确保损失仅针对被掩码的 token 计算。一旦训练完成，便可以模拟一个由该掩码预测器参数化的反向过程（详见2.4节），并将模型分布 $p_{\theta}(x_{0})$ 定义为该过程的边缘分布。</p>
<p>总的来说该方法通过在正向过程中逐步屏蔽令牌并在反向过程中学习恢复数据分布来训练生成模型，所有这些都在（近似）最大似然估计框架下。</p>
<h2 id="pretraining">Pretraining</h2>
<ul>
<li>
<p>LLaDA 8B 模型在一个包含 2.3T tokens 的高质量、多源数据集上从零开始进行预训练。该数据集覆盖了通用文本、代码、数学和多语言内容 。</p>
</li>
<li>
<p>训练总共消耗了 0.13M H800 GPU小 hours. 训练序列长度固定为4096. 其核心训练步骤是：对每个序列随机采样一个掩码率 t，并独立地以该概率掩码每个 token，然后让模型去预测被掩码的部分 。</p>
</li>
<li>
<p><strong>架构调整</strong> 相较于LLaMA3 8B，LLaDA 8B在架构上做了一些必要调整，如使用标准的 MHA 而非 GQA，并相应地调整了 FFN 的维度以保持模型总参数量相当 。</p>
</li>
<li>
<p><strong>优化器与学习率</strong> 训练使用了 AdamW 优化器和一个特殊的 Warmup-Stable-Decay 学习率调度策略。整个8B模型的训练实验只执行了一次，没有进行任何超参数调优。</p>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center">Our ARM Baseline 1B</th>
          <th style="text-align: center">LLaDA IB</th>
          <th style="text-align: center">Our ARM Baseline 7B</th>
          <th style="text-align: center">LLaDA 8B</th>
          <th style="text-align: center">LLaMA3 8B</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>Layers</strong></td>
          <td style="text-align: center">22</td>
          <td style="text-align: center">22</td>
          <td style="text-align: center">28</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Model dimension</strong></td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">4096</td>
          <td style="text-align: center">4096</td>
          <td style="text-align: center">4096</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Attention heads</strong></td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Vocabulary size</strong></td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126.464</td>
          <td style="text-align: center">128,000</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>FFN dimension</strong></td>
          <td style="text-align: center">5634</td>
          <td style="text-align: center">5634</td>
          <td style="text-align: center">13.440</td>
          <td style="text-align: center">12,288</td>
          <td style="text-align: center">14,336</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Key/Value heads</strong></td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">8</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">8</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Total parameters</strong></td>
          <td style="text-align: center">1.49 B</td>
          <td style="text-align: center">1.49 B</td>
          <td style="text-align: center">6.83 B</td>
          <td style="text-align: center">8.02 B</td>
          <td style="text-align: center">8.03 B</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Non-embedding parameters</strong></td>
          <td style="text-align: center">0.97 B</td>
          <td style="text-align: center">0.97 B</td>
          <td style="text-align: center">5.80 B</td>
          <td style="text-align: center">6.98 B</td>
          <td style="text-align: center">6.98 B</td>
      </tr>
  </tbody>
</table>
<h2 id="supervised-fine-tuning">Supervised Fine-Tuning</h2>
<p>我们通过使用配对数据 $(p_{0}, r_{0})$ 进行监督微调 (SFT)来增强LLaDA遵循指令的能力，其中 $p_{0}$ 是 prompt，$r_{0}$ 表示响应(response). 这是针对LLM最简单、最基础的 post-training 方法。从技术上讲，这要求模型对条件分布 $p_{\theta}(r_{0}|p_{0})$ 进行建模，而非预训练中的 $p_{\theta}(x_{0})$。</p>
<p>其实现方式与预训练类似。如图2(b)所示，保持 prompt 部分不变，并像处理 $x_{0}$ 一样，独立地 mask response 中的 token. 然后，将提示和被掩码的响应 $r_{t}$ 一同送入预训练好的掩码预测器，以计算用于 SFT 的损失</p>
$$
-\mathbb{E}_{t,p_{0},r_{0},r_{t}}[\frac{1}{t}\sum_{i=1}^{L^{\prime}}I[r_{t}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{t})] \quad (5)
$$<p>其中，$L^{\prime}$ 表示稍后指定的动态长度。这种方法与预训练是完全兼容的。本质上，将 $p_{0}$ 和 $r_{0}$ 拼接起来可以被视为干净的预训练数据 $x_{0} $，而将 $p_{0}$ 和 $r_{t}$ 拼接起来则可作为其被掩码后的版本 $x_{t}$. 这个过程与预训练完全相同，唯一的区别在于所有被掩码的 token 恰好都出现在 $r_{0}$ 部分。</p>
<p>LLaDA 8B 模型在一个包含 4.5M 对样本的数据集上进行了 SFT. 与预训练过程一致，数据准备和训练都遵循了现有LLM (Chu et al., 2024; Yang et al., 2024) 中使用的 SFT 协议，没有引入任何额外的技术来优化 LLaDA 的性能。该数据集涵盖了多个领域，包括代码、数学、指令遵循和结构化数据理解。我们在每个 mini-batch 中的短样本对末尾附加 EOS token，以确保所有数据长度相等。在训练期间将 EOS视为一个普通 token ，并在采样时将其移除，使得LLaDA能够自动控制响应的长度。</p>
<p>我们在SFT数据上训练了 3 个 epoch，其调度策略与预训练阶段相似。学习率在最初 50 次迭代中从 0 线性增加到 $2.5 \times 10^{-5}$，然后保持不变。在最后 10% 的迭代中，学习率性降低到 $2.5 \times 10^{-6}$. 此外，我们将权重衰减设置为 0.1，全局 batch size 设置为 256，每个 GPU 的本地 batch size 设置为 2. SFT实验只执行了一次，没有进行任何超参数调优。</p>
<h2 id="inference">Inference</h2>
<p>作为一个生成式模型，LLaDA既能 <strong>采样 (sampling)</strong> 新文本，也能 <strong>评估 (evaluating)</strong> 候选文本的似然。</p>
<p>我们先从采样说起。如图 2(c) 所示，给定一个 prompt $p_{0}$，我们通过离散化反向过程来从模型分布 $p_{\theta}(r_{0}|p_{0})$ 中进行采样，这个过程从一个被完全掩码的 response 开始。总的采样步数是一个超参数，为 LLaDA 提供了一个在效率和样本质量之间的权衡（详见3.3节分析）。我们默认使用均匀分布的时间步。此外，生成长度也被视为一个超参数，它指定了采样过程开始时完全被掩码句子的长度。如附录B.4所述，由于预训练和SFT都是在可变长度的数据集上进行的，最终结果对这个长度超参数不敏感。</p>
<p>在一个从时间 $t \in (0, 1]$ 到 $s \in [0, t)$的中间步骤中，我们将 $p_{0}$ 和 $r_{t}$ 同时送入掩码预测器，并一次性预测所有被掩码的 token. 随后 <em>remask</em> $\frac{s}{t}$ 比例的已预测 token. 得到$r_{s}$，从而确保反向过程的转换与前向过程保持一致，以实现准确采样。</p>
<p>受 LLM 采样中退火技巧的启发，我们探索了两种确定性但有效的重掩码策略。</p>
<ul>
<li><strong>low-confidence remasking</strong>: remask 那些基于预测置信度最低的 $\frac{s}{t}$ 比例的 token.</li>
<li><strong>semi-autoregressive remasking</strong>: 对于经过 SFT 的 LLaDA 模型，将序列分成几个块，并从左到右地生成. 在每个块内部，采用反向过程进行采样。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&amp;shareKey=838350c5b31c7e78112324263cdf5621" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&amp;shareKey=838350c5b31c7e78112324263cdf5621" alt="A Conceptual Overview of the Semi-autoregressive Sampling">
    </a><figcaption>A Conceptual Overview of the Semi-autoregressive Sampling</figcaption></figure></p>
<p>对于条件似然评估，我们自然可以利用公式(5)中的上界。然而，我们发现下面这个等价形式（公式6）表现出更低的方差，在评估时更为稳定：</p>
$$
-\mathbb{E}_{l,r_{0},r_{l}}[\frac{L}{l}\sum_{i=1}^{L}I[r_{l}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{l})] \quad(6)
$$<p>其中，$l$ 从 ${1, 2, ..., L}$ 中均匀采样，$r_{l}$ 是通过从 $r_{0}$ 中不放回地均匀采样 $l$ 个没被 mask 的 token 得到的。此外，我们还采用了 unsupervised classifier-free guidance.</p>
<p>虽然这两个形式的期望值相同，但它们的方差不同。直观上，在公式 (5) 中，我们期望 $x_{t}=[p_0,r_t]$ 有 $t$ 比例的 token 被掩码。然而，前向过程的随机性常常会导致偏差，尤其当 $x_{t}$ 包含的 token 很少时。相比之下，在公式 (6) 中，$r_{l}$ 中被掩码 token 的比例 $\frac{l}{L}$ 是确定的。</p>
<p>虽然理论分析取决于数据分布，但经验结果表明，公式 (5) 需要超过 1000 次蒙特卡洛估计才能得到稳定结果，而公式 (6) 仅需 128 次估计即可达到稳定。</p>
<p>Any-order autoregressive models (AO-ARM)  通过对 L 个变量所有可能的排列顺序进行自回归来描述联合分布。为了学习这样的分布，AO-ARM 利用一个权重共享的神经网络来为所有单变量条件概率建模，并使用掩码 token 来表示缺失的变量。在训练期间，模型会最小化在所有顺序的均匀分布 $U_{\pi}$ 上的期望负对数似然：</p>
$$
-\mathbb{E}_{x_{0},\pi \sim U_{\pi}}[\sum_{i=1}^{L}log~p_{\theta}(x_{0}^{\pi(i)}|x_{0}^{\pi(<i)}; \pi)]
$$<p> (15)</p>
<p>直观上，$x_{0}^{\pi(<i)}$ 可以被理解为一个被掩码的 token 序列 $x_{t}$，其中索引在 $\pi(\ge i)$ 的 token 被掩码 。可以进一步证明，公式 (15) 等价于公式 (12) 。这种联系解释了 LLaDA 的双向推理能力，即使它在推理过程中从未被显式使用 。</p>
<p>Nie et al. (2024) 引入了无监督的无分类器指导，这是一种即插即用的技术，可以平衡与提示的对齐度和文本多样性 。具体来说，无监督的无分类器指导在推理时采用以下修改过的掩码预测器 ：</p>
$$
\tilde{p}_{\theta}(r_{0}|p_{0},r_{t}) \propto \frac{p_{\theta}(r_{0}|p_{0},r_{t})^{1+w}}{p_{\theta}(r_{0}|m,r_{t})^{w}}
$$<p> (16)</p>
<p>其中，$m$ 是一个与 $p_{0}$ 长度相同的掩码序列，$w$ 是一个控制 $p_{0}$ 强度的超参数 。我们在下游任务中采用了无监督的无分类器指导，详见附录 B.5 。</p>
<h1 id="3-experiment">3 Experiment</h1>
<p>实验主要围绕以下三个核心方面展开：</p>
<ol>
<li>
<p>可扩展性 (Scalability)：研究 LLaDA 的性能是否随着计算资源和模型规模的增加而稳定提升。通过与自建的自回归模型 (ARM) 基线在相同数据上进行对比，结果显示 LLaDA 表现出强大的可扩展性，其性能增长趋势与 ARM 相当，甚至在 MMLU 和 GSM8K 等任务上更具优势。</p>
</li>
<li>
<p>基准测试结果 (Benchmark Results)：将 8B 规模的 LLaDA 与 LLaMA3 8B、LLaMA2 7B 等主流模型在涵盖通用，数学，代码和中文四大类的 15 个标准基准上进行对比。</p>
<ul>
<li>
<p>预训练模型：LLaDA 8B Base 模型的性能全面超越 LLaMA2 7B，并与 LLaMA3 8B 整体上具有竞争力，尤其在数学和中文任务上表现突出。</p>
</li>
<li>
<p>微调模型：仅经过 SFT 的 LLaDA 8B Instruct 模型，在未进行强化学习对齐的情况下，其性能在多数任务上得到提升 ，并展现出令人印象深刻的 Instruction Follow 能力。</p>
</li>
</ul>
</li>
<li>
<p>反向推理 (Reversal Reasoning)：为了量化模型克服“反转诅咒”的能力，实验在一个中文古诗补全任务上进行了测试。结果表明，LLaDA 在正向和反向任务上表现均衡，一致性强，而 GPT-4o 等模型则在反向任务上表现出显著的性能下降。</p>
</li>
</ol>
<h1 id="reference">Reference</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>简单来说就是拥有无限数据、一个足够大的网络和最优训练的理想条件下，模型有能力恢复出真实的数据分布。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>在不更新其自身参数的情况下，仅通过在 Prompt 中提供少量示例 (few-shot) 或任务描述 (zero-shot)，就能当场学会并执行一个新任务的能力。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Transformer Family</title>
      <link>http://localhost:57770/blogs/transformerfamily/</link>
      <pubDate>Sat, 07 Jun 2025 21:24:13 +0800</pubDate>
      <guid>http://localhost:57770/blogs/transformerfamily/</guid>
      <description>Introduction of Transformer Family</description>
      <content:encoded><![CDATA[<h1 id="origin-of-transformer">Origin of Transformer</h1>
<p>Transformer 由谷歌研于 2017 年在一篇名为 <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> 的论文中提出。与 RNN 的输入仅为一个 token 不同，Transformer 一次性可以输入一整个完整的序列。总体结构如下图所示，包含一个 Encoder 和一个 Decoder.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBd293bc1a46904e1af31ce993b83c68f1?method=download&amp;shareKey=47cf357e488e7da5483a1b98f3257ab1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBd293bc1a46904e1af31ce993b83c68f1?method=download&amp;shareKey=47cf357e488e7da5483a1b98f3257ab1" alt="Transformers Architecture">
    </a><figcaption>Transformers Architecture</figcaption></figure></p>
<h2 id="embedding">Embedding</h2>
<p>Embedding 是一种将离散的、稀疏的输入 (如词语、字符、类别标签&hellip;) 转换为连续的、密集的向量表示的技术，核心是通过一个映射函数将离散的输入符号 (如单词) 映射到一个低维向量空间中。假设我们有一个包含 V 个单词的 Vocabulary，维度为 d，那么 Embedding Matrix 将是一个大小为 V×d 的矩阵，其中每一行是一个单词的向量表示。通过嵌入层，输入的词索引 (通常是整数) 就会被映射到该矩阵的对应行，从而得到词的向量表示。常见的预训练词嵌入方法包括：</p>
<ul>
<li>Word2Vec：通过上下文预测词语的方式学习词向量。</li>
<li>GloVe：通过统计词共现信息来学习词向量。</li>
<li>FastText：考虑了子词信息的词嵌入方法，能更好地处理词形变化。</li>
</ul>
<p>在 PyTorch 和 TensorFlow 等框架中，通常有专门的 Embedding 层，Hugging Face 也有 tokenizer 将句子划分成单词并转换成对应的索引：</p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Positional Encoding 作用是为输入的序列中的每个元素提供位置信息。由于 Transformer 架构并没有使用递归或卷积结构，本身无法捕捉输入序列中元素的相对位置关系，因此需要通过位置编码来显式地引入这些位置信息。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>Transformer 的主要优势是通过 Self-Attention 并行处理序列中的每个元素，但是这也意味着它没有自带顺序感知能力，它并不会自动知道一个单词是在句子的开头还是结尾，因此需要额外的机制来编码每个元素在序列中的位置。</p>
<p>位置编码 通过将每个单词的位置信息 (即它在序列中的位置) 编码为一个向量，并将该向量添加到单词的嵌入表示中，从而让模型能够感知每个元素的相对或绝对位置。</p></div>

<p>经典的 Transformer 位置编码使用 正弦和余弦函数的组合，为每个位置生成的向量在不同维度上具有不同的周期性，这能够捕捉到不同级别的相对位置关系。假设输入的序列中有 N 个单词，每个单词的嵌入维度为 d，那么 Positional Encodin(PE) 的计算公式如下:</p>
$$
\begin{aligned}
&PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{\frac{2i}d}}\right)\\
&PE_{(pos,2i+1)}=\cos\left(\frac{pos}{10000^{\frac{2i}d}}\right)
\end{aligned}
$$<p>其中：</p>
<ul>
<li>pos 是单词在序列中的位置索引 (位置从 0 开始).</li>
<li>i 是位置编码的维度索引，表示该位置编码向量中的第 i 个元素。</li>
<li>d 是 Embedding 的维度</li>
</ul>
<p>这些位置编码与单词的词嵌入 (Word Embedding) 相加，最终形成输入模型的向量表示。</p>
<h2 id="masked-multi-head-attention">(Masked) Multi-Head Attention</h2>
<p>Multi-Head Attention (MHA) 的目的是通过并行地计算多个注意力头 (Attention Head)，从多个子空间中学习输入序列的不同表示。经过 Word Embedding 后的输入 X 形状为 Nxd. 计算步骤如下</p>
<ol>
<li>
<p>通过学习的变换矩阵将 X 映射到查询 (Q)、键 (K) 和值 (V) 空间。
</p>
$$
\begin{aligned}&Q=XW^{Q}\\&K=XW^{K}\\&V=XW^{V}\end{aligned}
$$<p>
其中 $W^{Q},W^{K}\in\mathbb{R}d_{model}\times d_{k},W^{Q},W^{V}\in\mathbb{R}d_{model}\times d_{v}$</p>
</li>
<li>
<p>根据 QKV 计算 Attention
每个查询向量会与所有键向量进行相似度计算 (一般采用 scaled inner product)，从而获得权重，然后利用这些权重对所有值向量进行加权求和。</p>
</li>
</ol>
$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p><br>在多头注意力中，为了增加模型的表达能力，通常将 Q、K 和 V 通过多个不同的线性变换矩阵进行多次计算，得到多个注意力头 (Attention Heads). 每个头的计算是独立的，但它们的结果会在最后进行拼接并经过线性变换。最终的 Multi-Head Attention 公式为：</p>
$$
\text{MultiHead}(Q,K,V)=\text{Concat}(head_1,head_2,\ldots,head_h)W^O
$$<p><br>每个头 $head_i$ 计算公式为</p>
$$
\text{MultiHead}(Q,K,V)=\text{Concat}(head_1,head_2,\ldots,head_h)W^O
$$<p><br>这里的 $W^{Q}_{i},W^{K}_{i},W^{V}_{i}$ 是为每个头学习到的不同权重矩阵，$W^O$ 是输出的线性变换矩阵。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB85e0bf86b5d9f2c649bbc3f08c03d203?method=download&amp;shareKey=b5e662d324237709f786beb08c27b774" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB85e0bf86b5d9f2c649bbc3f08c03d203?method=download&amp;shareKey=b5e662d324237709f786beb08c27b774" alt="Multi-Head Attention">
    </a><figcaption>Multi-Head Attention</figcaption></figure></p>
<p>Decoder 中的 Masked MHA 确保模型只能在解码序列的当前位置及其之前的位置上操作，而不能 “看到” 将要生成的未来信息。与标准的 MHA 相同，注意力分数 $\mathrm{Attention Scores}=\frac{QK^T}{\sqrt{d_k}}$ 是通过 Q 和 K 的点积计算得到的。计算完成后我们给其加上一个下三角元素 (包含主对角线) 为 0，上三角元素为 —∞ 的 mask，这样未来的信息经过 Softmax 后的权重为 0，被完全屏蔽。</p>
<h2 id="grouped-query-attentiongqa-multi-query-attention-mqa">Grouped Query Attention（GQA）&amp; Multi-query Attention (MQA)</h2>
<p><a href="https://arxiv.org/pdf/2305.13245">GQA</a> 将多个 Q 分成若干组，每一组共享相同的权重矩阵。这使得每组查询可以共同处理同一个 K 和 V，降低了计算量和内存需求。在 MHA 中，所有的头共享相同的输入 X，但使用不同的投影矩阵来生成 K 和 V. GQA 中 K 和 V 通常是对输入 X 进行一次性线性变换，并在所有同一分组中的 Q 共享。MQA 更为极端，所有的 Q 共享一个 K 和 V.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3c7dc003db55abf4b8a1ebeb4aabd667?method=download&amp;shareKey=f1570d975432b38d6f74742e9bb4cf6e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3c7dc003db55abf4b8a1ebeb4aabd667?method=download&amp;shareKey=f1570d975432b38d6f74742e9bb4cf6e" alt="Overview of MHA, GQA &amp; MQA">
    </a><figcaption>Overview of MHA, GQA &amp; MQA</figcaption></figure></p>
<h2 id="multi-head-cross-attention">Multi-Head Cross Attention</h2>
<p>Multi-Head Cross Attention 是 Transformer Decoder 中的一个核心组件。与 Self-Attention 不同，Cross Attention 负责将解码器的隐藏状态与编码器的输出上下文信息进行交互，允许解码器的每一个解码时间步的状态 <strong>查看整个编码器的输出</strong>。每个解码的时间步 t，Decoder 的隐藏状态作为 Q，Encoder 的输出作为 K 和 V，计算过程与 标准的 Self-Attention 相同。</p>
<h1 id="evolution-tree-of-transformer">Evolution Tree of Transformer</h1>
<p>后续的研究逐渐把 Encoder 和 Decoder 分离开来，形成 Encoder-Only 和 Decoder-Only 的模型。如下图所示</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa2db49ee75b563db2d846dab14947060?method=download&amp;shareKey=12514a3314f3bb4c5e30936c2d634650" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa2db49ee75b563db2d846dab14947060?method=download&amp;shareKey=12514a3314f3bb4c5e30936c2d634650" alt="Transformer Evolution Tree">
    </a><figcaption>Transformer Evolution Tree</figcaption></figure></p>
<h2 id="feed-forward-network">Feed Forward Network</h2>
<p>FFN 是一个两层的前馈全连接网络，中间有一个非线性激活函数。第一层全连接将 $d_model$ 映射到 $4d_model$ ，经过非线性激活函数后，第二层全连接再重新映射回 $d_model$.</p>
<h1 id="decoder-only-transformer">Decoder-Only Transformer</h1>
<p>Decoder-Only 删除了原先 Transformer Encoder 的部分以及 Encoder 和 Decoder 进行 Cross Attention 的部分。它具有三个必要的特征:</p>
<ol>
<li>在给定编码器输入作为上下文的情况下基于迄今为止生成的 token 自动回归预测下一个。</li>
<li>在评估对输入序列的 Q 时看不到未来值。这就是为什么仅解码器的模型通常被称为 Casual Language Model (CLM).</li>
<li>训练模型以在给定当前输入序列的情况下预测下一个 token. 这种训练方法与回归相结合，允许模型自回归生成任意长 (最高达输入序列的最大长度) 的序列。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa6c37075488053053efa01808163d0ba?method=download&amp;shareKey=5542015805dbda24ff7ab5dbf44a368b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa6c37075488053053efa01808163d0ba?method=download&amp;shareKey=5542015805dbda24ff7ab5dbf44a368b" alt="Decoder-only (left) and Encoder-only (right) Transformer Architectures">
    </a><figcaption>Decoder-only (left) and Encoder-only (right) Transformer Architectures</figcaption></figure></p>
<h1 id="llama-transformer-architecture">LLaMA Transformer Architecture</h1>
<p>LLaMA Transformer 结构如下，主要有以下变化</p>
<ol>
<li>使用 RoPE (Rotary Position Embedding) 替代传统的位置编码。</li>
<li>RMSNorm 替代 LayerNorm</li>
<li>引入 Gated Linear Unit (GLU)</li>
</ol>
<h2 id="rotary-position-embedding">Rotary Position Embedding</h2>
<p>传统的 Transformer 模型使用可学习的绝对位置编码 (如 sinusoidal position embedding)，但 RoPE 采用了旋转矩阵的思想，将位置编码与输入的 token 表示直接结合，而不依赖于额外的可学习参数。</p>
<p>输入向量的旋转角度为 $\theta(p,i)=p\cdot10000^{-2i/d}$. p 表示位置索引，i 表示维度索引，d 为向量的总维度。对于输入的 token 向量 x 中的每一对偶数和奇数维度 $(x_{2i},x_{2i+1})$，旋转操作可以用 2D 旋转矩阵表示为</p>
$$\begin{bmatrix}x_{2i}^{\prime}\\x_{2i+1}^{\prime}\end{bmatrix}=\begin{bmatrix}\cos(\theta)&-\sin(\theta)\\\sin(\theta)&\cos(\theta)\end{bmatrix}\cdot\begin{bmatrix}x_{2i}\\x_{2i+1}\end{bmatrix}$$<p><br>对于输入的 token 向量 $\mathbf{x}\left[x_{0},x_{1},x_{2},x_{3},\cdots,x_{d-1}\right]$, RoPE 将其两两一组配对，每一组都会与位置相关的旋转角度 θ 对应地应用旋转操作。这个过程的本质是对输入 token 的表示做了旋转变换，使得这些特征不仅依赖于输入的特征，还隐含了该 token 在序列中的位置。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf24aca24d7ff8bc2901ca4983cbf6c47?method=download&amp;shareKey=9ac054d415fe2e172bb8a719935d4793" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf24aca24d7ff8bc2901ca4983cbf6c47?method=download&amp;shareKey=9ac054d415fe2e172bb8a719935d4793" alt="RoPE">
    </a><figcaption>RoPE</figcaption></figure></p>
<h2 id="rmsnorm">RMSNorm</h2>
<p>RMSNorm 相对于 LayerNorm 去掉了均值计算，仅基于输入的均方根进行归一化 $\mathrm{RMSNorm}(\mathbf{x})=\frac{\mathbf{x}}{\mathrm{RMS}(\mathbf{x})+\epsilon}\cdot\gamma$</p>
<p>其中</p>
<ul>
<li>$\mathrm{RMS}(\mathbf{x})=\sqrt{\frac1d\sum_{i=1}^dx_i^2}$ 为输入的均方根。</li>
<li>$\gamma{:}$ 为可学习的缩放参数。</li>
<li>$\epsilon{:}$ 为防止除以 0 的小数。</li>
</ul>
<h2 id="silu">SiLU</h2>
<p>SiLU (Sigmoid Linear Unit) 是一种激活函数，也称为 Swish，其定义为输入 x 和 Sigmoid 函数输出的乘积。其定义为
</p>
$$\mathrm{SiLU}(x)=x\cdot\sigma(x)$$<p>
其中 $\sigma(x)=\frac1{1+e^{-x}}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB552a846c520bf2b5194c621e7b8e224e?method=download&amp;shareKey=519f3a1e4cce59da1895fa7bc2bcc842" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB552a846c520bf2b5194c621e7b8e224e?method=download&amp;shareKey=519f3a1e4cce59da1895fa7bc2bcc842" alt="SiLU">
    </a><figcaption>SiLU</figcaption></figure></p>
]]></content:encoded>
    </item>
    <item>
      <title>ZeRO, ZeRO-Offload, ZeRO-Infinity</title>
      <link>http://localhost:57770/blogs/zero/</link>
      <pubDate>Sat, 07 Jun 2025 21:11:32 +0800</pubDate>
      <guid>http://localhost:57770/blogs/zero/</guid>
      <description>Paper reading of ZeRO.</description>
      <content:encoded><![CDATA[<h1 id="zero">ZeRO</h1>
<p>Zero 用于优化内存，极大地提高了训练速度，同时增加了可以训练的模型大小。ZeRO 消除了数据和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度，能够以持续的高效率按设备数量等比例扩展可训练模型的大小。</p>
<h2 id="introduction">Introduction</h2>
<p>ZeRO 首先总结了下当前并行方法存在的问题</p>
<ul>
<li>Basic DP: 没有减少每个设备的内存，在 32GB 内存的 GPU 上训练超过 1.4B 参数的模型便会 OOM.</li>
<li>Model Parallelsim (MP): 切分了每一层的计算和激活到每个设备上，但引入了大量的通信 (前向和反向都需要 2xAll-Reduce)，因此扩展性差，通常只在一个节点内的高带宽连接的 GPU 中进行。在 DGX-2 节点训练 40B 参数的模型每个 V100 GPU 仅能达到硬件峰值的 5% 算力 (5T flops).</li>
</ul>
<p>模型状态通常占据了训练时的大部分内存，但 DP 在所有数据并行进程中保存一份模型状态，导致冗余内存消耗；而 MP 对这些状态进行切分以获得高内存效率，但通常会导致过于细粒度的计算和昂贵的通信，扩展效率较低。此外，这些方法都静态地维护整个训练过程所需的<strong>整个模型状态</strong>。</p>
<p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO-DP</a> 通过在数据并行过程中划分模型状态 (参数、梯度和优化器状态) 消除了数据并行过程中的内存冗余。</p>
<p>结论：如下图所示 ZeRO-DP 有三个主要的优化阶段，它们对应于优化器状态、梯度和参数的划分。
对于使用 FP16 的模型，内存占用包括参数 (FP16)、梯度 (FP16)、Adam 优化器状态 (动量 (FP32)，方差 (FP32) 以及更新后的参数 (FP32), 因此 K=12).</p>
<ol>
<li>优化器状态划分 (Pos) —— 内存减少 4 倍，需要对梯度进行 reduce-scatter，用各自的优化器状态更新梯度后进行 All-gather 使所有设备都有最新的梯度，通信量与数据并行性相同 (对 Loss 进行一次 All-reduce).</li>
<li>添加梯度划分  (Pos+g) &ndash; 内存减少 8 倍，每个设备需要将自己的梯度 scatter 到负责更新那部分参数的设备上，然后使用 Gather 将其他设备更新后的模型参数同步到自己上面，通信量与数据并行性相同。</li>
<li>添加参数划分 (Pos+g+p) &ndash; 内存减少与数据并行度 Nd 呈线性关系。通信量增加了50%，因为在前向/反向传播中需要每个设备需要额外广播自己存储的模型参数 <code>2*(N-1)/N*P</code>，反向传播时需要对发送梯度到对应的设备上 <code>(N-1)/N*P</code>.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBcfab82173b0f76eb5b3c8396e81e238a?method=download&amp;shareKey=1b8bb86256be5b15bec039beecee062b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBcfab82173b0f76eb5b3c8396e81e238a?method=download&amp;shareKey=1b8bb86256be5b15bec039beecee062b" alt="Memory Savings and Communication Volume for the 3-stage of ZeRO">
    </a><figcaption>Memory Savings and Communication Volume for the 3-stage of ZeRO</figcaption></figure></p>
<p>激活、临时缓冲区和不可用的内存片段会成为次要内存瓶颈。作者开发了 ZeRO-R 优化了这三个因素分别消耗的剩余内存。</p>
<ol>
<li>对于激活 (在前向传播中存储，反向传播中使用)，仅仅使用激活检查点是不够的。ZeRO-R 通过激活划分识别和删除现有 MP 方法中重复存储的激活，并且在适当时候将激活存储在 CPU 中。</li>
<li>ZeRO-R 定义了适当大小的临时缓冲区，以实现内存和计算效率的平衡。</li>
<li>由于不同张量的寿命存在差异，ZeRO-R 根据张量的不同生命周期主动管理内存，防止内存碎片。</li>
</ol>
<p>在某些情况下，MP 仍可以和 ZeRO 一起使用：i）当与 ZeRO-R 一起使用时，MP 可以减少超大模型的激活内存占用。ii）对于较小模型，当单独使用 DP 的 batchsize 太大而无法实现良好的收敛时，MP 也可以带来好处。</p>
<h2 id="where-did-all-the-memory-go">Where Did All the Memory Go?</h2>
<p>在模型训练期间，大部分内存被模型状态消耗 (优化器状态、梯度和参数). 除了这些模型状态之外，剩余的内存被激活、临时缓冲区和碎片内存所消耗，称之为剩余状态。</p>
<h3 id="model-states-optimizer-states-gradients-and-parameters">Model States: Optimizer States, Gradients and Parameters</h3>
<p>Adam 优化器需要存储两个优化器状态：时间平均动量和梯度方差来计算更新后的参数。此外，还需要有足够的内存来存储梯度和权重本身。</p>
<p><strong>混合精度训练 (Mixed-Precision Training)</strong> 中参数和激活以 fp16 格式存储并且在前向和反向传播中也使用 fp16 格式的权重和激活。Adam 优化器存储 fp32 格式的参数副本、动量和方差以保证更新的精度。</p>
<p>假设模型参数量为 ψ，模型参数需要占用 2ψ 字节的内存，反向传播中产生的 fp16 梯度需要占用 2ψ 字节的内存。Adam 优化器存储 fp32 格式的参数副本、动量和方差每个都需要占用 4ψ 字节的内存。因此训练时总共需要 16ψ 字节的内存，为存储模型参数的 8x.</p>
<h3 id="residual-memory-consumption">Residual Memory Consumption</h3>
<p>在训练过程中，<strong>激活</strong>会占用大量的内存。基于 transformer 的模型的激活内存占用与层数×隐藏维度×序列长度×批大小成正比。对于类似 GPT-2的结构，总激活约为 12×隐藏亮度×批大小×序列长度×变层数 (<code>QKV(h*3h) + O(h*h) + MLP(h*4h+4h*h)=12h*h</code>，没有考虑 mask). 激活重计算可以以 33% 的额外计算开销 (之前是一次前向，一次反向，反向因为需要对输入和参数都进行求导所以计算量是前向的两倍，现在多了一次前向) 换取接近原先激活大小平方级别的内存占用。</p>
<p>对于大型模型，用于存储中间结果的<strong>临时缓冲区</strong>会消耗大量内存。对梯度进行 All-Reduce 或梯度归一化计算等操作倾向于在操作之前将所有梯度融合到单个扁平缓冲区中，以提高吞吐量。</p>
<p><strong>碎片化内存</strong>会导致即使有足够的内存但没有足够大的连续块进行分配时的 OOM，作者观察到极端情况下在有 30% 剩余内存时也会产生 OOM.</p>
<h2 id="zero-insight-and-overview">ZeRO: Insight and Overview</h2>
<p>ZeRO有两组优化：ZeRO-DP 旨在减少模型状态的内存占用；ZeRO-R 旨在减少剩余内存消耗。</p>
<p>ZeRO-DP 基于三个关键见解：</p>
<ol>
<li>DP 比 MP 具有更好的扩展效率，因为 MP 减少了计算的粒度，同时也增加了通信开销。</li>
<li>DP 内存效率低下，因为模型状态被在所有数据并行进程中都存有一份。</li>
<li>DP 和 MP 都保留了整个训练过程中所需的所有模型状态，但并非所有状态在整个训练期间都需要。</li>
</ol>
<p>ZeRO-DP 划分模型状态，并使用动态通信调度利用模型状态的内在的暂时性，同时最小化通信量。</p>
<p>ZeRO-R 基于两个关键见解：</p>
<ol>
<li>MP 对模型状态进行切分，但通常需要重复存储激活。</li>
<li>对于GPT-2或更大的模型，算术强度 (每次迭代计算量与激活检查点数量的比值) 非常大 (≥10K)，并且随着隐藏维数的增加而线性增加，即使在带宽较低的情况下，也可以隐藏激活检查点的数据移动成本。</li>
</ol>
<p>ZeRO 通过跨 GPU 划分激活检查点来消除 MP 中的内存冗余，并根据需要使用 All-Gather 来重建；使用恒定大小的缓冲区来避免临时缓冲区随着模型大小的增加而爆炸；通过将激活检查点和梯度移动到预分配的连续内存缓冲区来执行动态内存碎片整理。</p>
<h2 id="deep-dive-into-zero-dp">Deep Dive into ZeRO-DP</h2>
<p>下表显示了逐渐切分 (1) 优化器状态、(2) 梯度和 (3) 参数冗余后的内存占用。称为ZeRO-DP的三个优化阶段：Pos， Pg和Pp，将在下面详细说明。</p>
<table border="1" cellspacing="0" cellpadding="5">
  <thead>
    <tr>
      <th rowspan="2">DP</th>
      <th colspan="3">7.5B Model (GB)</th>
      <th colspan="3">128B Model (GB)</th>
      <th colspan="3">1T Model (GB)</th>
    </tr>
    <tr>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>120</td>
      <td>120</td>
      <td>120</td>
      <td>2048</td>
      <td>2048</td>
      <td>2048</td>
      <td>16000</td>
      <td>16000</td>
      <td>16000</td>
    </tr>
    <tr>
      <td>4</td>
      <td>52.5</td>
      <td>41.3</td>
      <td><b>30</b></td>
      <td>896</td>
      <td>704</td>
      <td>512</td>
      <td>7000</td>
      <td>5500</td>
      <td>4000</td>
    </tr>
    <tr>
      <td>16</td>
      <td>35.6</td>
      <td><b>21.6</b></td>
      <td>7.5</td>
      <td>608</td>
      <td>368</td>
      <td>128</td>
      <td>4750</td>
      <td>2875</td>
      <td>1000</td>
    </tr>
    <tr>
      <td>64</td>
      <td><b>31.4</b></td>
      <td>16.6</td>
      <td>1.88</td>
      <td>536</td>
      <td>284</td>
      <td><b>32</b></td>
      <td>4187</td>
      <td>2218</td>
      <td>250</td>
    </tr>
    <tr>
      <td>256</td>
      <td>30.4</td>
      <td>15.4</td>
      <td>0.47</td>
      <td>518</td>
      <td>263</td>
      <td>8</td>
      <td>4046</td>
      <td>2054</td>
      <td>62.5</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>30.1</td>
      <td>15.1</td>
      <td>0.12</td>
      <td>513</td>
      <td>257</td>
      <td>2</td>
      <td>4011</td>
      <td>2013</td>
      <td><b>15.6</b></td>
    </tr>
  </tbody>
</table>
<h3 id="pos-optimizer-state-partitioning">Pos: Optimizer State Partitioning</h3>
<p>设 DP 并行度为 Nd, 每个数据并行进程只需要存储和更新总优化器状态的 1/Nd，然后只更新参数的 1/Nd. 在每个训练步骤结束时，在数据并行进程中执行一次 All-Gather，以获得所有数据并行过程中完全更新的参数。这使得每个设备上保存模型状态需要的内存从 4ψ+Kψ 变成 4ψ+Kψ/Nd，当使用 Adam 优化器 (K=12) 并且 Nd 很大时，内存需求可以降低接近 4x.</p>
<h3 id="pg-gradient-partitioning">Pg: Gradient Partitioning</h3>
<p>由于每个数据并行进程只用更新自己被分配的参数，因此他也只需要那部分参数 reduce 后的梯度。只在负责更新相应参数的数据并行过程中进行 reduce. 完成后它们的内存可以被释放。这使得了梯度所需的内存占用从 2Ψ 字节减少到 2Ψ/Nd. 更新后的参数再被 scatter 到其他进程。</p>
<p>通常为了效率，将需要 reduce 的梯度按照参数的分区划分成多个 buckets，每个 bucket 对应特定的一组参数，对每个 bucket 进行整体 reduce 操作，而不是对单个梯度进行操作。进一步划分梯度后，每个设备上保存模型状态需要的内存进一步减少到 2ψ+(K+2)ψ/Nd</p>
<p>蓝色箭头串起来的白色长方形代表的是 Transformer Block，蓝色的第一行代表 FP16 参数；橙色的第二行代表 FP16 梯度，反向传播时将用于更新参数；绿色的行代表优化器状态 (FP32 的梯度，动量，方差，以及更新后的参数)，其中在计算完 FP16 梯度以后不再需要保存 FP32 参数。同时也需要 buffer 来保存部分 transformer block 的输出激活。</p>
<h3 id="pp-parameter-partitioning">Pp: Parameter Partitioning</h3>
<p>更进一步我们可以将模型参数也进行划分，当设备所没有的参数需要进行向前和向后传播时，通过广播从其他的的数据并行进程接收。通过前面的分析可知这使得通信量变为原来的 1.5x， 但使得所有的模型参数都被划分到每个设备上，只需要 (4+K)/Nd 字节的内存。</p>
<h3 id="execution-steps-of-zero3">Execution Steps of ZeRO3</h3>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fd41e8b92bcd256a910ce757d4eea21?method=download&amp;shareKey=d82f49f0d59309d987c164a100966895" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fd41e8b92bcd256a910ce757d4eea21?method=download&amp;shareKey=d82f49f0d59309d987c164a100966895" alt="Overview of Memory Consumption">
    </a><figcaption>Overview of Memory Consumption</figcaption></figure></p>
<p>每个 GPU 只需要保存自己部分的 Pos+g+p. 前向传播时保存对应模型参数的 GPU 需要把参数广播到其他 GPU 中，其他 GPU 用自己部分的数据完成前向传播后就可以删除这部分参数 (最后一部分除外). <code>(N-1)/N*P</code></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB82b66a8a7b40a2fdf9512545189cc37a?method=download&amp;shareKey=37a4614d85a77d2ca00e6d5a4769e2f9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB82b66a8a7b40a2fdf9512545189cc37a?method=download&amp;shareKey=37a4614d85a77d2ca00e6d5a4769e2f9" alt="Broadcast of Model Parameters">
    </a><figcaption>Broadcast of Model Parameters</figcaption></figure></p>
<p>前向传播完成后，第一次反向传播可以利用最后一次正向传播已经广播了的模型参数，每个 GPU 计算自己部分的梯度，然后 Reduce 到存储对应模型参数的 GPU 中。之后和前向传播一样，每个 GPU 都需要广播自己的参数，然后其他 GPU 用自己的数据完成梯度计算以后 Reduce 到自己的梯度。<code>(N-1)/N*P + 1/N*G*(N-1)</code></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB575b3869e59814ae1449351cf1b18d01?method=download&amp;shareKey=10f29390f47227ca8eefbcc00f4fca6e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB575b3869e59814ae1449351cf1b18d01?method=download&amp;shareKey=10f29390f47227ca8eefbcc00f4fca6e" alt="Gradient Accumulation">
    </a><figcaption>Gradient Accumulation</figcaption></figure></p>
<p>反向传播结束以后，每个 GPU 使用优化器更新自己的 FP32 模型参数后转换成 FP16 格式。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB19cb21dfa63ab76437a2246ff52b00aa?method=download&amp;shareKey=ddb63d57f3bf976b1dce4596e77a2009" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB19cb21dfa63ab76437a2246ff52b00aa?method=download&amp;shareKey=ddb63d57f3bf976b1dce4596e77a2009" alt="Update Parameters Locally">
    </a><figcaption>Update Parameters Locally</figcaption></figure></p>
<h2 id="deep-dive-into-zero-r">Deep Dive into ZeRO-R</h2>
<h3 id="pa-partitioned-activation-checkpointing">Pa: Partitioned Activation Checkpointing</h3>
<p>一旦计算了模型的一层的前向传播，输入激活将在所有模型并行过程中进行划分，直到在反向传播期间再次需要输入激活。此时，ZeRO 使用一个 All-Gather 操作来重新实现激活的复制副本。称这个优化为 Pa. 将 Pa 与激活检查点结合，只存储分区的激活检查点，这样使得激活占用空间的减少与 MP 并行度成正比。</p>
<h3 id="cb-constant-size-buffers">CB: Constant Size Buffers</h3>
<p>通信的效率不仅仅与数据量相关，还受到固定启动开销和带宽利用率的影响。较大的输入更容易充分利用硬件的带宽和优化机制，因而能显著提高 All-Reduce 操作的效率。因此经常将需要进行通信的数据合并到一个缓冲器。然而，合并缓冲区的内存开销与模型大小成正比，模型过大时容易 OOM. 为了解决这个问题，<strong>当模型很大时，简单地使用一个性能高效的固定大小的合并缓冲区</strong>。</p>
<h3 id="md-memory-defragmentation">MD: Memory Defragmentation</h3>
<p>前向传播中只需要保存检查点的激活而丢弃其他激活会产生碎片化内存。同样的反向传播中只需要保存参数的梯度而丢弃激活的梯度也会产生碎片化内存。内存碎片导致两个问题: (1) 即使有足够的可用内存，由于缺乏连续内存导致 OOM. (2) 由于内存分配器花费大量时间搜索连续内存块以满足内存请求而导致效率低下。ZeRO 通过<strong>为激活检查点和梯度预分配连续内存块，并在它们产生时将它们复制到预分配的内存中</strong>，从而实时地进行内存碎片整理。</p>
<h2 id="communication-analysis-of-zero-dp">Communication Analysis of ZeRO-DP</h2>
<p>使用 Pos 和 Pg 时，ZeRO-DP 不会产生额外的通信，同时可以减少高达 8 倍的内存。使用 Pos+g+p 时，ZeRO-DP 最多会产生 1.5 倍的通信，同时减少内存占用为原来的 1/Nd.</p>
<p>在数据并行训练过程中，在计算下一步的更新之前，在反向传播结束时对所有数据并行进程的梯度使用 All-Reduce 进行平均，因此通信量为 2ψ. 使用 Pos+g 时每个设备需要将自己的梯度 scatter 到负责更新那部分参数的设备上，然后使用 Gather 将其他设备更新后的模型参数同步到自己上面，总通信量仍为 2ψ，与数据并行相同。使用 Pos+g+p 时负责该分区的数据并行进程将权重 brocast 给所有数据并行进程 (前向反向各一次)，最后仍需要 Gather 其他进程上更新好的参数，因此总通信量为 3ψ.</p>
<h2 id="communication-analysis-of-zero-r">Communication Analysis of ZeRO-R</h2>
<p>在使用激活检查点的 Megatron-LM 中，每个 transformer block 在前向传播中执行 2 次大小为 批大小×序列长度×隐藏维度的 All-Reduce 操作，反向传播中执行 2 次同样大小的 All-Reduce 操作，同时激活重计算也需要 2 次同样大小的 All-Reduce 操作。因此每个块的总通信量为 12×序列长度×隐藏维度。</p>
<p>当使用 ZeRO-R 划分激活检查点时，在对每个激活检查点上的反向传播进行前向重新计算之前，需要进行额外的一次 All-Gather 操作。因此，Pa的总通信开销相对于原先 MP 通信量增加了 1/12，但是使得激活内存占用减小到原来的 1/MP_degree.</p>
<p>如果使用了 Pa+cpu，则分区激活检查点将被存储到 CPU，对激活内存需求减少到几乎为零，而代价是与 Pa 相比，需要从 CPU 和内存之间的数据移动增加了 2 倍。</p>
<h1 id="zero-offload">ZeRO-Offload</h1>
<p>ZeRO-Offload 通过将数据和计算下放到 CPU 来实现大型模型训练。为了保持计算效率，它尽可能减少数据在 GPU 和 CPU 之间的移动，同时最大限度地减少 CPU 的计算时间，并最大限度地节省 GPU 上的内存。</p>
<h2 id="introduction-1">Introduction</h2>
<p>PP, MP 和 ZeRO 等并行技术都需要有足够的 GPU 设备，使得它们的内存之和能够容纳训练所需的模型状态的存储。目前基于注意力的大模型训练的主要内存瓶颈是模型状态，而不是激活。现有的异构训练在两个主要方面受到限制：(i) 几乎所有的训练都利用 CPU 内存，而不是 CPU算力。(ii) 它们主要是为单个 GPU 设计和评估的。</p>
<p>ZeRO-Offload 为了提高计算效率采取的设计原则有三条：(i) 它需要的 CPU 计算量与 GPU 相比减少了几个数量级。(ii) 它最小化了 CPU 和 GPU 之间的通信量，防止了通信成为瓶颈。(iii) 可以证明在实现最小通信量的同时最大限度地节省了 GPU 的内存。</p>
<p>ZeRO-Offload 将梯度，优化器状态和优化器计算卸载到 CPU，而将参数和前向和反向计算保留在 GPU上。这样 CPU 上的计算量为 O(M)，而 GPU 上的计算量则为 O(MB)，其中 M 和 B 分别为模型大小和 batchsize. 因为 CPU 只处理模型参数的更新，而不参与与 batch size 相关的梯度求平均的操作。在大多数情况下，batchsize 较大，因此 CPU 计算不是瓶颈。但是对于较小的 batchsize，CPU 计算可能会成为瓶颈。</p>
<h2 id="unique-optimal-offload-strategy">Unique Optimal Offload Strategy</h2>
<p>为了确定最佳的卸载策略，ZeRO-Offload 将 DL 训练建模为如下图所示的数据流，并有效地在 CPU 和 GPU 设备之间进行划分。GPU 和 CPU 之间的卸载策略可以使用该图的二分图来表示，这样一个分区中的计算节点将在拥有该分区的设备上执行，分区中的数据节点也存储在拥有该分区的设备上。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc55ea0bd058b2e603052658a6cb25aa6?method=download&amp;shareKey=e2474b8904eff6869bcdce3b09e545f6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc55ea0bd058b2e603052658a6cb25aa6?method=download&amp;shareKey=e2474b8904eff6869bcdce3b09e545f6" alt="The Dataflow of Fully Connected Neural Networks">
    </a><figcaption>The Dataflow of Fully Connected Neural Networks</figcaption></figure></p>
<p>由于 CPU 的算力远远低于 GPU，所以前向传播和反向传播 (它们的计算复杂度都是 O(MB)) 必须在 GPU上完成，而其余复杂度为 O(M) 的计算 (如归一化计算、权重更新等) 会被卸载到 CPU 上。</p>
<p>CPU 内存带宽 (100xGB) 至少比 CPU 和 GPU 之间的 PCIe 带宽 (10xGB) 快一个数量级，而 GPU 内存带宽比 CPU 内存带宽 (TB) 快另一个数量级。数据流中的每个节点都是环的一部分。因此，对该图进行任何划分都需要切割至少两条边，每条边的权值至少为 2M，从而总通信量至少 4M (通过仅卸载部分模型状态，可以进一步减少通信量). 因此，为了实现最小的通信量，所有卸载策略必须使得关于 fp32 模型状态操作的生产者和消费者相同。fp16 参数节点必须和 FWD-BWD 节点在一个子图中，因为这两个节点之间的边权值是 4M.</p>
<p>下表显示了最小化通信量情况下的所有有效分区策略所节省的内存。通过将 fp16 梯度和 Update Super 节点放到 CPU 可以实现 8x 的最大内存节省。</p>
<table>
  <thead>
      <tr>
          <th>FWD-BWD</th>
          <th>p16</th>
          <th>g16</th>
          <th>Update</th>
          <th>Memory</th>
          <th>Reduction</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>gpu</td>
          <td>gpu</td>
          <td>16M</td>
          <td>1x (baseline)</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>cpu</td>
          <td>gpu</td>
          <td>14M</td>
          <td>1.14x</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>4M</td>
          <td>4x</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>2M</td>
          <td>8x</td>
      </tr>
  </tbody>
</table>
<p>综上所述 ZeRO-Offload 在 CPU 上存储所有 fp32 模型状态以及 fp16 梯度，并且还在 CPU 上计算更新后的参数。fp16 的参数保存在 GPU 上，前向和反向计算也在GPU上完成。</p>
<h2 id="zero-offload-schedule">ZeRO-Offload Schedule</h2>
<p>在训练过程中，首先通过前向传播计算损失。由于 fp16 参数已经存放在GPU上，因此这部分计算不需要与 CPU 通信。在损失的反向传播过程中，不同设备计算不同参数的梯度。ZeRO-Offload 可以在计算完每个参数后，将这些梯度单独或分组传输到 CPU 内存中。由于梯度是逐层传输的，因此 GPU 上只需要很小的缓冲区来存放每一层的梯度。在反向传播之后，ZeRO-Offload 直接在 CPU 上更新 fp32 参数和优化器状态），并将更新后的 fp32 参数从 CPU 内存复制到 GPU 内存上的 fp16 参数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB810cb8d722c2c9e8e140b80084c47cbe?method=download&amp;shareKey=6ab97f8c087ad948e03121afde1c266a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB810cb8d722c2c9e8e140b80084c47cbe?method=download&amp;shareKey=6ab97f8c087ad948e03121afde1c266a" alt="ZeRO-Offload Training Process on a Single GPU">
    </a><figcaption>ZeRO-Offload Training Process on a Single GPU</figcaption></figure></p>
<p>在卸载之前进行如上一节所述的划分的主要好处是，对于具有超过 1 个 GPU 的系统，每个数据并行进程只负责更新参数的一个子集。所有数据并行进程的 GPU 到 CPU 的通信量总和保持不变，CPU 资源可以并行使用，共同计算单个权重更新。ZeRO-Offload 在不同 GPU 之间划分梯度和优化器状态，每个 GPU 将其拥有的部分卸载到 CPU 内存中，并在整个训练过程中将其一直保存在那里。在反向传播过程中，在 GPU上使用 reduce-scatter 计算普遍复核一遍梯度，每个 GPU 只将属于其那一部分的平均梯度卸载到 CPU 内存中。然后优化器状态将由每个数据并行进程直接在 CPU 上并行更新。更新后，参数被移回 GPU，然后在 GPU 上执行类似于 ZeRO-2 的 All-Gather 操作来获取所有更新后的参数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6e4ed2b9f7f37bf8a5f8f9326bb98971?method=download&amp;shareKey=0bf3ca60c8edb9a602a3ae93262c0967" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6e4ed2b9f7f37bf8a5f8f9326bb98971?method=download&amp;shareKey=0bf3ca60c8edb9a602a3ae93262c0967" alt="ZeRO-Offload Data Placement with Multiple GPUs">
    </a><figcaption>ZeRO-Offload Data Placement with Multiple GPUs</figcaption></figure></p>
<h2 id="optimized-cpu-execution">Optimized CPU Execution</h2>
<ol>
<li>作者使用高性能计算技术实现了一个加速版的 CPU Adam 优化器</li>
<li>开发了一个一步延迟参数更新计划，将 CPU 参数更新计算与 GPU 上的前向和反向计算重叠，隐藏了 CPU 执行时间。</li>
</ol>
<h3 id="implementing-the-cpu-optimizer">Implementing the CPU Optimizer</h3>
<p>作者使用三级并行性来提高 CPU 优化器的性能。</p>
<ol>
<li>SIMD 矢量指令，充分利用 CPU 架构的硬件并行性。</li>
<li>循环展开，一种提高指令级并行性的有效技术，能更好地利用内存带宽。</li>
<li>OMP 多线程，可以有效地并行利用 CPU 上的多个内核和线程。</li>
</ol>
<p>算法的输入为 β₁(动量系数), β₂(RMSProp 的平方梯度衰减系数), α(学习率)，以及梯度，动量，方差和 fp32 参数作为输入。我们还使用了一些特定于实现的参数，如 simd_width 和 unroll_width. Adam 优化器分别发送更新的方差、动量和参数的 fp16 和 fp32 格式到 GPU 和 CPU. 首先将数据读入矢量寄存器。然后，主循环中使用 Fused Multiplication Add 矢量操作。其他操作，如乘法、除法和平方根，也在矢量模式下运行。为了获得最佳性能，使用 AVX512 simd 指令集和基于自动调优结果的 unroll_width=8. 除了 CPU-Adam 优化器之外，还以分块的方式实现了 CPU 到 GPU 的 fp16 参数复制。通过并行化 Adam 计算并将参数复制到 GPU 来重叠 CPU 和 GPU 的执行。<strong>当在 CPU 上处理当前数据块的 Adam 计算时，将先前处理过的数据块的参数写回 GPU.</strong></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7e25392bc19670dfd7b80cc9a84a5d73?method=download&amp;shareKey=ac89809b9f51717d9a5429d1cd5d9865" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7e25392bc19670dfd7b80cc9a84a5d73?method=download&amp;shareKey=ac89809b9f51717d9a5429d1cd5d9865" alt="CPU-ADAM Optimizer">
    </a><figcaption>CPU-ADAM Optimizer</figcaption></figure></p>
<h3 id="one-step-delayed-parameter-update">One-Step Delayed Parameter Update</h3>
<p>下图展示了 Delayed Parameter Update(DPU) 的 ZeRO-Offload 训练的工作流程。</p>
<ol>
<li>前 N−1 步不使用 DPU 进行训练，避免在梯度变化迅速的早期阶段破坏训练的稳定性。</li>
<li>在第 N 步中，从 GPU 获取梯度，但跳过 CPU 优化步骤，也不更新 GPU 上的 fp16 参数。</li>
<li>在第 N+1 步中，我们使用第 N 步的梯度计算 CPU 上的参数更新，同时使用第 N-1 步更新的参数并行计算 GPU 上的前向和反向。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc56f9c814071e1766b9f33833024d829?method=download&amp;shareKey=d5be7d41c0247d8243bddb3452eba75b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc56f9c814071e1766b9f33833024d829?method=download&amp;shareKey=d5be7d41c0247d8243bddb3452eba75b" alt="Delayed Parameter Update During the Training Process">
    </a><figcaption>Delayed Parameter Update During the Training Process</figcaption></figure></p>
<h1 id="zero-infinity">ZeRO-Infinity</h1>
<p>ZeRO-Infinity 是一种新的异构系统技术，它利用 GPU, CPU 和 NVMe 内存，在有限的资源上实现前所未有的模型扩展，并且不需要模型代码重构。</p>
<p>目前大型模型训练技术中最先进的是三维并行 (3D parallelism)，它将模型（张量切片）和流水线并行与数据并行相结合。但是 GPU 内存跟不上模型大小的增长。</p>
<p>ZeRO-Infinity 的优势如下</p>
<ol>
<li>通过同时利用 CPU 和 NVMe 内存，在有限的 GPU 资源上支持大模型训练。</li>
<li>引入了一种称为 <em>memory-centric tiling</em> 的 GPU 内存优化技术，以应对 GPU 内存无法一次放下的超大 block 情况。</li>
<li>引入了一种称作 <em>bandwidth-centric partitioning</em> 的数据分区策略，用于利用所有设备上的内存带宽，并将其与重叠通信与计算的技术结合。</li>
</ol>
<h2 id="memory-requirements">MEMORY REQUIREMENTS</h2>
<p><strong>Memory for Model States:</strong> 基于 Transformer 的模型中的参数总数主要取决于隐藏维度 (hd) 和 Transformer 层数 (nl). Transformer block 中的几乎所有参数都来自四个线性层，大小分别为：QKV_Linear(nd,3nd), O_Linear(hd, hd),MLP(hd, 4hd)+(4hd, hd). 因此一个 Transformer block 的参数量约为 <strong>12 x nl x (hd)²</strong>，因此占用的内存大小为 192 x nl x (hd)² 字节。</p>
<p><strong>Memory for Residual States:</strong> 剩余状态主要由激活内存组成，它取决于模型架构、批处理大小 (bsz) 和序列长度 (seq). 存储激活检查点所需的内存估计为 <strong>2×bsz×seq×hd×nl/ci</strong>，其中 ci(checkpoint interval) 是两个激活检查点之间的 Transformer block 的数量。</p>
<p><strong>Model State Working Memory (MSWM)</strong> 是在所有模型状态被卸载到 CPU 或 NVMe 之后，在模型中最大的单个算子上执行前向或反向传播所需的最小 GPU 内存。对于基于 Transformer 的模型，最大的算子是将隐藏维度从 hd 转换为 4hd 的线性层，因此 fp32 格式下需要 <strong>4xhdx4hd</strong> 字节的内存。</p>
<p><strong>Activation Working Memory (AWM):</strong> 是在执行实际的反向传播之前重新计算激活所需的内存，即两个连续激活检查点之间的激活大小 bsz × seq × ci × (16 × hd + 2 × attn_heads × seq) 字节。</p>
<h2 id="bandwidth-requirements">BANDWIDTH REQUIREMENTS</h2>
<p>假设没有任何计算和通信重叠的工作负载执行，我们可以使用峰值计算吞吐量 (peaktp)，数据移动带宽 (bw) 及其算术强度 (ait) 来估计训练效率。需要注意 peaktp 不是理论上的硬件峰值，而是在没有任何通信瓶颈的情况下可以达到的峰值。</p>
<p>算术强度 (AIT) 是总计算量与计算所需数据量之比。它描述了每次数据移动的计算量。</p>
<ol>
<li>compute_time = total_computation / peaktp</li>
<li>ait = total_computation / total_data_movement</li>
<li>communication_time = total_data_movement / bw = total_computation / (ait × bw)</li>
<li>efficiency = compute_time / (compute_time + communication_time) = ait x bw / (ait x bw + peaktp)</li>
</ol>
<h3 id="quantifying-ait-in-dl-training">Quantifying AIT in DL training</h3>
<p>Transformer block 中一次前向传播中的计算量可以近似为输入乘以参数大小 2 × bsz × seq × params. 反向传播则为其 2 倍。如果使用激活检查点则还需要一次额外的前向传播，因此每次迭代的总计算量为 computation_per_iter = 2 × 4 × bsz × seq × parameters = 2 × 4 × 12 × bsz × seq × nl × (hd)²</p>
<p><strong>AIT w.r.t. Parameters and Gradients:</strong> 前向和反向过程中模型参数必须从存储位置位置加载到 GPU 寄存器各次。在使用激活检查点的情况下，还需要加载一次，以便在反向传播期间重新计算。此外，梯度必须从 GPU 寄存器存储到其最终位置至少一次。因此总共要移动模型参数 4 次，总计 2 x 4 x parameters 字节。因此关于参数和梯度的计算强度为 seq x bsz.</p>
<p><strong>AIT w.r.t. Optimizer States:</strong> 优化器状态必须至少读取和写入一次。所以总的数据移动是 2 × optimizer_states，总计 2 × 16 × parameters 字节。因此关于优化器状态的计算强度为 seq x bsz / 4.</p>
<p><strong>AIT w.r.t. Activation Checkpoints:</strong> 前向传播时必须将激活检查点保存到它们的最终位置，然后在反向传播期间加载激活检查点。因此总数据移动量为 4 × nl/ci × hd × seq × bsz 字节。因此关于激活检查点的计算强度为 24 × hd × ci.</p>
<h3 id="bandwidth-requirements-1">Bandwidth Requirements</h3>
<p>通过前面的分析可知模型状态的计算强度仅取决于批大小和序列长度，激活检查点的计算强度仅取决于存储间隔和模型的隐藏维度大小。下图 a 说明当传输参数和梯度的带宽超过 70 GB/s 时，即使是最小的批处理大小，也可以实现超过 50% 的效率。图 b 说明，传输优化器状态需要近 4 倍的带宽才能达到 50% 的效率。并且<strong>优化器状态更新需要等待所有前向和反向传播结束，不能与计算重叠</strong>。图 c 说明，启用激活检查点后，即使隐藏大小为2K，2 GB/s 的带宽也能够保持 50% 以上的效率。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfc095fefa5837b449dcafbd0b9441d63?method=download&amp;shareKey=42ab0064e0986bfde564af6e879f5cd6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfc095fefa5837b449dcafbd0b9441d63?method=download&amp;shareKey=42ab0064e0986bfde564af6e879f5cd6" alt="Impact of Bandwidth on Efficiency with 70 TFlops of single GPU Peak Throughput">
    </a><figcaption>Impact of Bandwidth on Efficiency with 70 TFlops of single GPU Peak Throughput</figcaption></figure></p>
<h2 id="zero-infinity-design-overview">ZERO-INFINITY DESIGN OVERVIEW</h2>
<p>GPU 集群采用异构内存存储，除了 GPU 内存还拥有 CPU 内存以及比 GPU 内存大 50x, 比 CPU 内存大近 20x 的大规模 NVMe 存储。下图为 ZeRO-Infinity 架构，描述了第一层的反向传递的通信。将划分后的参数从慢速内存移动到 GPU，然后 All-Gather 以形成完整的层。在计算梯度之后，参数被聚合和重新划分，然后卸载到慢速内存中。层用下标表示，DP rank 用上标表示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB07d94538fa97fe4b478f44d44eb19e79?method=download&amp;shareKey=82f65d6660819491e1bec3688d2715ae" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB07d94538fa97fe4b478f44d44eb19e79?method=download&amp;shareKey=82f65d6660819491e1bec3688d2715ae" alt="A Snapshot of ZeRO-Infinity Training a Model with 2 Layers on 4 DP Ranks">
    </a><figcaption>A Snapshot of ZeRO-Infinity Training a Model with 2 Layers on 4 DP Ranks</figcaption></figure></p>
<p><strong>Efficiency w.r.t Parameter and Gradients:</strong> 现有的异构解决方案 (例如 ZeRO-Offload) 要求先将参数从 CPU 移动到拥有这些参数的 GPU，然后再进行广播。这种方式需要在每个 GPU 上使用足够大的 batchsize，以确保通信能被计算掩盖。但这带来了两个问题：</p>
<ol>
<li>对于超大规模模型，激活的内存占用会过大，甚至超过 CPU 的内存容量。</li>
<li>当扩展到数百甚至上千个 GPU 时，为了实现有效的收敛，实际的 batchsize 会变得过大。</li>
</ol>
<p><strong>Efficiency w.r.t Optimizer States:</strong> 与在前向和反向传播期间参数和梯度的产生有先后顺序不同，优化器状态可以同时更新。ZeRO-Infinity 建立在 ZeRO-3 之上，因此在将优化器状态卸载到 CPU 内存时，它还可以利用所有的 GPU 和 CPU 内存带宽以及所有 CPU 算力用于优化器状态更新。然而，使用 NVMe 卸载，需要将数据从 NVMe 传入到 CPU 内存中，再从 CPU 内存返回。由于 CPU 内存有限，必须将数据分块从 NVMe 加载到 CPU 内存，进行计算后再写回 NVMe.</p>
<p><strong>Efficiency w.r.t Activations:</strong> 在一台 DGX-2 节点上，每个 GPU 可以通过 PCIe 接口以大约 3 GB/s 的速度并行读写数据到 CPU 内存。这使得在隐藏层大小为 8K 或更大时，可以将激活检查点卸载到 CPU 内存的同时保持超过 80% 的效率。</p>
<h2 id="efficiency-optimizations">EFFICIENCY OPTIMIZATIONS</h2>
<h3 id="bandwidth-centric-partitioning">Bandwidth-Centric Partitioning</h3>
<p>在 ZeRO-3 和 ZeRO-Offload 中每层的参数为单个数据并行进程拥有，在需要时将它们广播给其他进程，ZeRO-Infinity 在所有数据并行进程中划分单个参数，并在需要参数时使用 All-Gather. 相较于广播只用到了单个 PCIe 链路将参数从存储位置加载到 GPU，All-Gather 同时使用所有的 PCIe 链路，每条链路传输 1/dp 的参数。</p>
<h3 id="overlap-centric-design">Overlap Centric Design</h3>
<p>访问 NVMe 内存需要三个步骤：(i) 从 NVMe 读取数据到CPU内存 (nc-transfer). (ii) 将数据从 CPU 内存复制到 GPU 内存 (cg-transfer). (iii) 执行 All-Gather 以在所有 GPU 上获得完整参数 (gg-transfer).</p>
<p>ZeRO-Infinity 的通信重叠有两个组件</p>
<ol>
<li>一个 dynamic prefetcher，在每次迭代期间，跟踪其在算子序列中的位置，并预取未来算子所需的参数。在执行第 i 个操作符之前，prefetcher 可以分别对第 i+3，第 i+2 和第 i+1 个算子所需的参数调用 nc, cg 和 gg-transfer.</li>
<li>一个通信和卸载重叠机制，用于并行执行梯度所需的数据移动和反向计算。将第 i+1 个算子中参数梯度的 Reduce-Scatter 与第 i 个算子的计算重叠，同时将第 i+2 个算子 Reduce-Scatter 划分的梯度传输给 CPU 或 NVMe.</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>All2All Communication Cost</title>
      <link>http://localhost:57770/blogs/all2allcommcost/</link>
      <pubDate>Sun, 12 Jan 2025 16:05:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/all2allcommcost/</guid>
      <description>Introduction of Transformer Family</description>
      <content:encoded><![CDATA[<p>在 All2All 通信中，每个设备给其他设备发送大小为 m 的不同的消息。此操作相当于使用一维数组分区对分布在 p 个进程中的二维数据数组进行转置，因此也被称作全交换 (<strong>total exchange</strong>)</p>
<h2 id="ring--bidirectional-linear-array">Ring / Bidirectional Linear Array</h2>
<p>线性数组拓扑结构的 All2All 通信中，每个设备需要发送 p-1 份大小为 m 的数据。用 {i,j} 表示消息需要从设备 i 发送到设备 j. 首先，每个节点将所有要发送的数据作为一个大小为 m(p-1) 的合并消息发送给它邻居 (假设所有设备通信方向相同)。当邻居收到这个消息后提取他所需要的那一部分，发送剩下的大小为 m(p-2). 每个设备一共发送 p-1 次，每次要发送的消息大小减少 m.</p>
<p>由此可以得出在 p 个设备组成的线性数组拓扑上进行 All2All 每个设备需要向相邻设备通信 p-1 次，第 i 次通信的消息大小为 m(p-i). 如果向两个方向都进行发送，那么每个方向都只用发送原先一半的数据。</p>
$$
\begin{aligned}T_{ring}&=\quad\sum_{i=1}^{p-1}(t_{s}+t_{w}m(p-i))\\&=\quad t_{s}(p-1)+\sum_{i=1}^{p-1}it_{w}m\\&=\quad(t_{s}+t_{w}mp/2)(p-1).\end{aligned}
$$<p>环状网络中每份消息的平均传输跳数是 $\frac{\sum_{d=1}^{p-1}i}{p-1} = p/2$，因此 p 个节点总共的通信量之和为  $p\times m(p-1)\times\frac p2$  环状网络中总的链路数目为 p. 因此负载平均的情况下，最少需要的时间为 $\frac{m(p-1)\times\frac p2\times p}p = m(p-1)\frac p2$ ，因此算法时间为最优的。</p>
<p>跳数为 d 的消息数量对应于相距 d 的节点对 (i, j)，其中 |i-j|=d</p>
<ul>
<li>(0, d),(1, d+1), \ldots,(p-1-d, p-1)，即 i 从 0 到 p-1-d, j=i+d ，共有 p-d 对。</li>
<li>(d, 0),(d+1,1), \ldots,(p-1, p-1-d)，即  i  从  d  到  p-1, ~ j=i-d  ，也有 p-d 对。
总共有 2(p-d) 条消息的跳数为 d</li>
</ul>
<p>总跳数</p>
$$
\begin{aligned}
\text { 总跳数 } & =\sum_{d=1}^{p-1} d \times 2(p-d) \\
& =2 \sum_{d=1}^{p-1} d(p-d)=2\left(p \sum_{d=1}^{p-1} d-\sum_{d=1}^{p-1} d^{2}\right) \\
& = p \cdot \frac{(p-1) p}{2}-\frac{(p-1) p(2 p-1)}{6} \\
& = =\frac{(p-1) p(p+1)}{6}
\end{aligned}
$$<p>因此平均跳数 =$\frac{\text { 总跳数 }}{\text { 总消息数 }}=\frac{\frac{(p-1) p(p+1)}{3}}{p(p-1)}=\frac{p+1}{3}$</p>
<h2 id="mesh">Mesh</h2>
<p>若 p 个设备组成大小为 $\sqrt{p} \times \sqrt{p}$ 的 mesh 进行 All2All 通信，每个设备首先将其 p 个数据按照目的设备的列进行分组，即分成 $\sqrt{p}$ 组，每组包含大小为 $m\sqrt{p}$ 的消息。假设 3x3 的 mesh，则第一组消息的目的节点为 {0,3,6}，第二组消息的目的节点为 {1,4,7}，第三组消息的目的节点为 {2,5,8}</p>
<p>首先同时分别在每一行中进行 All2All 通信，每一份数据大小为 $m\sqrt{p}$. 通信结束后每个设备拥有的是该行目的设备为所在列的所有数据。然后将数据按照目的设备所在的行进行分组。即设备 {0,3,6} 第一组消息的目的节点为 0，第二组消息的目的节点为 3，第三组消息的目的节点为 6. 然后同时分别在每一列中进行 All2All 通信。</p>
<p>我们只需要将 Linear Array 拓扑结构中的公式的 p 换成 $\sqrt{p}$ ，m 换成 $m\sqrt{p}$，再乘以 2 就得到在 mesh 上进行 All2All 的时间</p>
$$
T_{mesh}=(2t_{s}+t_{w}mp)(\sqrt{p}-1).
$$<h2 id="hypercube">Hypercube</h2>
<p>超立方体拓扑在每个维度上都有两个节点，一共有 $\log{p}$ 个维度。在一共有 p 个节点超立方体中，在某个维度 $d$ 上，超立方体可以被划分为两个 (n−1) 维的子立方体，这两个子立方体通过维度 d 上的 p/2 条链路相连。</p>
<p>在 All2All 通信的任何阶段，每个节点都持有 $p$ 个大小为 $m$ 的数据包。当在特定维度上通信时，每个节点发送 $p/2$ 个数据包 (合并为一条消息)。这些数据包的目的地是由当前维度的链路连接的另一个子立方体包含的节点。在上述过程中，节点必须在每个 $\log{p}$ 通信步骤之前在本地重新排列消息。</p>
<p>$\log{p}$ 步中的每一步，每个设备沿当前维度的双向链路交换大小为 mp/2 的数据。因此在 hypercube 上进行 All2All 的时间为</p>
$$
T_{hcube}=(t_{s}+t_{w}mp/2)\log p.
$$<p>值得注意的是与 ring 和 mesh 算法不同，超立方体算法不是最优的。每个设备发送和接收大小为 m(p- 1) 的数据，超立方体上任意两个节点之间的平均距离为 $\log{p}/2$ . 因此，网络上的总数据流量为 $p\times m(p - 1)\times(\log{p})/2$. 每个超立方体一共有 $p\log{p}/2$  条双向链路，如果流量能够被平分，则通信用时下界应该为</p>
$$
\begin{aligned}T_{min}&=\frac{t_{w}pm(p-1)(\log p)/2}{(p\log p)/2}\\&=t_{w}m(p-1).\end{aligned}
$$<h2 id="optimal-algorithm-in-hypercube">Optimal Algorithm in Hypercube</h2>
<p>在超立方体上，执行 All2All 的最佳方法是让每一对节点彼此直接通信。因此，每个节点只需执行 p-1 次通信，每次与不同设备交换大小为 m 的数据。设备必须在每次通信中选择不会出现拥塞的通信对象。在第 j 次通信中，节点 i 与节点 $i \oplus j$ 交换数据。在超立方体上，从节点 i 到节点 j 的消息必须经过至少 l 条链路，其中 l 是 i 和 j 之间的汉明距离 (即 $i \oplus j$ 的二进制表示中的非零比特数). 我们通过 E-cube 路由来选择路径：</p>
<ol>
<li>将当前节点地址 C 与目标节点地址 D 进行 XOR 操作，得到 $R=C\oplus D$.</li>
<li>找到 R 的最低有效非零位，决定下一步跳转的维度。</li>
<li>沿选定维度跳转到下一个节点，更新当前节点地址。</li>
<li>重复上述步骤，直到 R=0， 即到达目标节点。
对于节点i和节点j之间的消息传输，该算法保证每一步的通信时间为 t_s + t_wm，因为在节点 i 和节点 j 之间的链路上沿着同一方向传播的任何其他消息都不存在竞争，切每一步只切换一个维度，通信距离为 1. 整个 All2All 的总通信时间为</li>
</ol>
$$T_{xor}=(t_{s}+t_{w}m)(p-1).$$<h1 id="bruck-algorithm-in-full-connected-network">Bruck Algorithm in Full-connected Network</h1>
<p>Bruck是一种存储-转发 (store-and-forward) 算法，需要 log(P) 次通信步骤。这意味着发送缓冲区 S 和接收缓冲区 R 都用于在中间通信轮次中发送、接收和存储数据。因为某些接收到的数据块必须在后续通信步骤中使用。这种存储-转发的特性对通信轮次的顺序提出了约束。与线性步骤实现不同，Bruck 必须保持明确的通信顺序，其中第 i+1 次迭代必须在第 i 次迭代之后物理时间上发生。

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1b5aaffb71ec91ead2f725d9249728f1?method=download&amp;shareKey=e4eea75f3b72f77982d47b17590c24b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1b5aaffb71ec91ead2f725d9249728f1?method=download&amp;shareKey=e4eea75f3b72f77982d47b17590c24b3" alt="Bruck">
    </a><figcaption>Bruck</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">Algorithm 2 NCCL Bruck algorithm
</span></span><span class="line"><span class="cl">P ← total number of processes.
</span></span><span class="line"><span class="cl">for i ∈ [0, P] do
</span></span><span class="line"><span class="cl">   R[i] = S[(p+i) % P] // S and R are send and receive buffers, and p is rank id of each process;
</span></span><span class="line"><span class="cl">end for
</span></span><span class="line"><span class="cl">allocate temporary buffer T with SC × (P+1) / 2 elements; // SC is number of elements per data-block.
</span></span><span class="line"><span class="cl">for k = 1; k &lt; P; k &lt;&lt;= 1 do
</span></span><span class="line"><span class="cl">   allocate send indexes array SB with (P+1) / 2 integers;
</span></span><span class="line"><span class="cl">   number of send data-blocks NB ← 0;
</span></span><span class="line"><span class="cl">   for i ∈ [k, P] do
</span></span><span class="line"><span class="cl">      if i &amp; k then
</span></span><span class="line"><span class="cl">            SB[NB] ← i;
</span></span><span class="line"><span class="cl">            copy R[i] into T[NB];
</span></span><span class="line"><span class="cl">            NB ← NB + 1;
</span></span><span class="line"><span class="cl">      end if
</span></span><span class="line"><span class="cl">      sendproc ← (p + k) % P;
</span></span><span class="line"><span class="cl">      recvproc ← (p - k + P) % P;
</span></span><span class="line"><span class="cl">      ncclGroupStart()
</span></span><span class="line"><span class="cl">      send data in T to sendproc;
</span></span><span class="line"><span class="cl">      receive data from recvproc into S;
</span></span><span class="line"><span class="cl">      ncclGroupEnd()
</span></span><span class="line"><span class="cl">      for i ∈ [0, SB] do
</span></span><span class="line"><span class="cl">            copy T[i] into R[SB[i]];
</span></span><span class="line"><span class="cl">      end for
</span></span><span class="line"><span class="cl">   end for
</span></span><span class="line"><span class="cl">   for i ∈ [0, P] do
</span></span><span class="line"><span class="cl">      R[i] = R[(p - i + P) % P] // final rotation;
</span></span><span class="line"><span class="cl">   end for
</span></span><span class="line"><span class="cl">end for
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>line(2-4): 将每个设备发送缓冲区 S 中的数据按照 rank 偏移重新排列拷贝到接收缓冲区 R 中。</li>
<li>line(5): 为通信阶段准备一个临时缓冲区 T</li>
<li>line(6): 通信步开始 k 以指数方式增长 (1, 2, 4, &hellip;)，总共执行 logP 次迭代
<ul>
<li>line(7-14): 用索引数组 SB，记录需要发送的数据块位置。遍历 k~P-1 同通过对 i&amp;k 判断哪些数据块需要在此轮发送. (若 P 是 2 的指数幂，因为 k 是 2 的指数幂，因此只有一位为 1，那么就是每轮发送 p/2 个数据块) 将接收缓冲区 R 中满足条件的数据拷贝到临时缓冲区 T，并记录索引。</li>
<li>line(15-16): 确定要接收和发送的目标。</li>
<li>line(17-20): 进行通信操作，将数据发送到目标的发送缓冲区。</li>
<li>line(21-23): 更新接收缓冲区。</li>
<li>line(25-27): 反向调整接收缓冲区数据的位置。</li>
</ul>
</li>
</ul>
<p>总共 log(p) 步骤每步发送 m 消息。</p>
<h1 id="tree-based">Tree-based</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6ed5f5f2681e4f2c3a57bfb7b901515a?method=download&amp;shareKey=7aafd92596dbc981100138525e0f6d09" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6ed5f5f2681e4f2c3a57bfb7b901515a?method=download&amp;shareKey=7aafd92596dbc981100138525e0f6d09" alt="Tree">
    </a><figcaption>Tree</figcaption></figure></p>
<p>采用先在行上进行 All-gather, 再在列上进行 Scatter. 也需要 log(p) 步，其中 gather 阶段第一步通信量为 m(p-1)，一共进行 0.5log(p) 步每一步通信量翻倍，跳数也翻倍；scatter阶段则是相反，因此两步的通信时间相同总共 t_s*log(p) + m(p-1)^2/3</p>
]]></content:encoded>
    </item>
    <item>
      <title>DistriFusion</title>
      <link>http://localhost:57770/blogs/distrifusion/</link>
      <pubDate>Wed, 23 Oct 2024 14:28:37 +0800</pubDate>
      <guid>http://localhost:57770/blogs/distrifusion/</guid>
      <description>Paper reading about DistriFusion.</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>DistriFusion 将模型输入分割成多个 patch 后分配给 GPU。但是直接实现这样的算法会破坏 patch 之间的交互并失去保真度，而同步 GPU 之间的激活将产生巨大的通信开销。为了克服这一困境，根据观察到的相邻扩散步输入之间的高度相似性提出了 <strong>displaced patch parallelism</strong>，该方法通过重用前一个时间步骤中预先计算的 feature map 来利用扩散过程的顺序性，为当前步提供 context. 该方法支持异步通信，可以通过计算实现流水线化。</p>
<h1 id="introduction">Introduction</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" alt="Original, Navie Patch &amp; DistriFusion">
    </a><figcaption>Original, Navie Patch &amp; DistriFusion</figcaption></figure></p>
<p>加速扩散模型推理主要集中在两种方法上：减少采样步骤和优化网络推理。随着计算资源的快速增长，利用多个 GPU 来加速推理是很有吸引力的。例如在 NLP 中， LLM 已经成功地利用了 GPU 之间的张量并行性，从而显著降低了延迟。然而，对于扩散模型，由于激活尺寸大，张量并行这样的技术不太适合扩散模型。多个 GPU 通常只用于 batch 推理，当生成单个图像时，通常只涉及一个GPU.</p>
<blockquote>
<p>Techniques like tensor parallelism are less suitable for diffusion models due to the large activation size, as communication costs outweigh savings from distributed computation.</p></blockquote>
<p>自然而然的一种方法是将图像分成几个 patch 后分配给不同的设备进行生成。由于各个 patch 之间缺乏相互作用，它在每个 patch 的边界处都有一个清晰可见的分界线。</p>
<p>DistriFusion 也是基于 patch parallelism. 关键在于扩散模型中相邻去噪步骤的输入是相似的，因此，只在第一步采用同步通信。后续步骤重用前一步中预先计算的激活，为当前步骤提供全局上下文和 patch 交互。通过异步通信有效地隐藏了计算中的通信开销。并且还稀疏地在指定的区域上进行卷积和注意力计算，从而按比例减少每个设备的计算量。</p>
<h1 id="method">Method</h1>
<h2 id="displaced-patch-parallelism">Displaced Patch Parallelism.</h2>
<p>在预测 $\epsilon_{\theta}(\mathbf{x}_{t})$ 时 (忽略条件 c 和时间步 t 的输入) ，首先将 $\mathbf{x}_{t}$ 分割成多个 patch $\mathbf{x}_t^{(1)},\mathbf{x}_t^{(2)},\ldots,\mathbf{x}_t^{(N)}$ ，对于每一层 l 和设备 i，在获得输入激活 patch $\mathbf{A}_{t}^{l,(i)}$ 后异步处理两个操作：首先，对于设备i， 激活 $\mathbf{A}_{t}^{l,(i)}$ 首先 scatter 到上一步旧的激活 $\mathbf{A}_{t+1}^{l}$ 中。然后将此分散操作的输出送入稀疏算子 Fl (线性、卷积或注意层)，该算子专门对新区域执行计算并产生相应的输出。同时，对 $\mathbf{A}_{t}^{l,(i)}$ 执行 AllGather 操作，为下一步的全尺寸激活 $\mathbf{A}_{t}^{l}$ 做准备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" alt="Overview of DistriFusion">
    </a><figcaption>Overview of DistriFusion</figcaption></figure></p>
<p>我们对除第一层 (采用同步通信获得其他设备上的输入) 外的每一层重复这个过程。然后将最终输出 Gather 在一起以近似 $\epsilon_{\theta}(\mathbf{x}_{t})$，用于计算 $\mathbf{x}_{t-1}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" alt="Timeline Visualization on Each Device">
    </a><figcaption>Timeline Visualization on Each Device</figcaption></figure></p>
<h2 id="sparse-operations">Sparse Operations</h2>
<p>对于每一层 l，如果原始算子 Fl 是一个卷积层、线性层或交叉注意层，调整使其专门作用于新激活的区域。这可以通过从 scatter 输出中提取最新部分并将其输入到 Fl 中来实现。对于 self-attention，将其转换为 cross-attention，仅在设备上保留来自新激活的 Q，而 KV 仍然包含整个特征图。</p>
<h2 id="corrected-asynchronous-groupnorm">Corrected Asynchronous GroupNorm</h2>
<p>仅对新 patch 进行归一化或重用旧特征都会降低图像质量。同步 AllGather 所有均值和方差将产生相当大的开销。为了解决这一困境，DistriFusion 在陈旧的统计数据中引入了一个校正项。计算公式如下</p>
$$
\mathbb{E}[\mathbf{A}_t]\approx\underbrace{\mathbb{E}[\mathbf{A}_{t+1}]}_{\text{stale global mean}}+\underbrace{\mathbb{E}[\mathbf{A}_t^{(i)}]-\mathbb{E}[\mathbf{A}_{t+1}^{(i)}]}_{\text{correction}}
$$<p>同样对二阶矩 $\mathbb{E}[\mathbf{A}^2_t]$ 也采用这种计算方式，然后通过 $\mathbb{E}[\mathbf{A}^2_t] - \mathbb{E}[\mathbf{A}_t]^2$ 来计算方差。对于方差结果为负的部分，将使用新鲜 patch 的局部方差代替。</p>
<h1 id="code-implementation">Code Implementation</h1>
<p>Distrifusion 中主要就是将 <a href="https://github.com/huggingface/diffusers/blob/9366c8f84bfe47099ff047272661786ebb54721d/src/diffusers/models/unets/unet_2d_condition.py#L71">UNet2DConditionModel</a> 中的 Conv2d, Attention 和 GroupNorm 替换成对应的 patch 实现的网络结构 <a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L15">DistriUNetPP</a>. 这里继承的 BaseModel 类为集成了 PatchParallelismCommManager 类 (介绍见后文) 的网络。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" alt="UNet2DConditionModel">
    </a><figcaption>UNet2DConditionModel</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriUNetPP</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>  <span class="c1"># for Patch Parallelism</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">UNet2DConditionModel</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">UNet2DConditionModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39; 
</span></span></span><span class="line"><span class="cl"><span class="s1">                Substitute Conv2d, Attention, GroupNorm with DistriConv2dPP, DistriSelfAttentionPP, DistriCrossAttentionPP, DistriGroupNorm 
</span></span></span><span class="line"><span class="cl"><span class="s1">                &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">subname</span><span class="p">,</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>  
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">kernel_size</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriConv2dPP</span><span class="p">(</span>  
</span></span><span class="line"><span class="cl">                            <span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="o">=</span><span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;conv_in&#34;</span>
</span></span><span class="line"><span class="cl">                        <span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">Attention</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn1&#34;</span><span class="p">:</span>  <span class="c1"># self attention</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="k">else</span><span class="p">:</span>  <span class="c1"># cross attention</span>
</span></span><span class="line"><span class="cl">                            <span class="k">assert</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn2&#34;</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriCrossAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriGroupNorm</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriUNetPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchparallelismcommmanager">PatchParallelismCommManager</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/utils.py#L112">PatchParallelismCommManager</a> 类主要处理异步通信的部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchParallelismCommManager</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span> <span class="o">=</span> <span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 已经注册的张量的累计总元素数量</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel_dict</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># 记录每个 layer_type 所注册的张量的累计元素数量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 在每个设备上存储所有注册张量的数据，通信所用的 buffer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">starts</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的起始位置 (在 buffer_list 中的起始索引)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ends</span> <span class="o">=</span> <span class="p">[]</span>    <span class="c1">#                 结束                       结束</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shapes</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的 shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">idx_queue</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 需要进行通信的张量索引的队列</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 存储每个设备通信操作的句柄的 list, 用于检查通信是否完成</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>成员函数功能介绍如下</p>
<ol>
<li>
<p><code>register_tensor(self, shape: tuple[int, ...] or list[int], torch_dtype: torch.dtype, layer_type: str = None) -&gt; int</code>: 用于注册张量的形状和数据类型，同时计算并记录张量在缓冲区中的起始位置和结束位置。</p>
<ul>
<li>如果尚未指定 <code>torch_dtype</code>，则将传入的 <code>torch_dtype</code> 设为类成员的默认数据类型。</li>
<li>计算传入张量形状的总元素数 <code>numel</code>，并更新 <code>starts</code>、<code>ends</code> 和 <code>shapes</code> 列表。</li>
<li>如果指定了 <code>layer_type</code>，更新 <code>numel_dict</code> 中该层类型对应的元素数目。</li>
</ul>
</li>
<li>
<p><code>create_buffer(self)</code> : 每个设备上为所有注册的张量创建一个统一的缓冲区。</p>
<ul>
<li>为每个设备创建一个形状为 <code>(numel,)</code> 的张量，并将其放入 <code>buffer_list</code> 中。</li>
<li>输出在各设备上创建的缓冲区总参数量。</li>
</ul>
</li>
<li>
<p><code>get_buffer_list(self, idx: int) -&gt; list[torch.Tensor]</code>: 返回每个设备上对应于指定索引 <code>idx</code> 的缓冲区张量。</p>
<ul>
<li>根据 <code>starts</code> 和 <code>ends</code> 信息，从 <code>buffer_list</code> 中提取指定索引 <code>idx</code> 的张量片段并调整其形状。</li>
</ul>
</li>
<li>
<p><code>communicate(self)</code>: 调用 <code>dist.all_gather</code> 将缓冲区中的张量在不同设备间进行广播。</p>
<ul>
<li>确定当前需要通信的张量范围 (根据 <code>idx_queue</code> 中的索引).</li>
<li>调用 <code>dist.all_gather</code> 在设备组内进行异步广播通信，并将句柄存储在 <code>handles</code> 中。</li>
</ul>
</li>
<li>
<p><code>enqueue(self, idx: int, tensor: torch.Tensor)</code>: 将指定索引 <code>idx</code> 处的张量数据复制到 <code>buffer_list</code> 中，并将索引添加到通信队列 <code>idx_queue</code>。</p>
<ul>
<li>如果通信队列不为空且索引为 0，则先执行一次通信操作。</li>
<li>将张量数据复制到 <code>buffer_list</code> 中的对应位置。</li>
<li>当通信队列长度达到 <code>distri_config</code> 中设定的通信检查点值时，进行通信。</li>
</ul>
</li>
<li>
<p><code>clear(self)</code>: 执行一次所有待通信张量的通信，并等待所有异步操作完成。</p>
<ul>
<li>如果通信队列不为空，则进行通信操作。</li>
<li>遍历所有句柄，等待所有异步操作完成后，将句柄设为 <code>None</code>.</li>
</ul>
</li>
</ol>
<h2 id="districonv2dpp">DistriConv2dPP</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L10">DistriConv2dPP</a> 计算自己负责 patch 部分的卷积，需要通信其他设备需要自己负责 patch 的上下 padding 部分。</p>
<ul>
<li><code>__init__</code>：构造函数，初始化成员变量，设置是否为第一层卷积。</li>
<li><code>naive_forward</code>：执行标准的前向传播，不进行任何切片操作。这是单个设备处理时的普通卷积操作。</li>
<li><code>sliced_forward</code>：处理输入张量的切片操作。根据当前设备索引 (<code>split_idx</code>) 计算输入张量在高度方向的起始和结束位置，并在必要时为切片后的张量添加 padding 后进行卷积操作。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriConv2dPP</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriConv2dPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_layer</span> <span class="o">=</span> <span class="n">is_first_layer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">naive_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#  x: [B, C, H, W]</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sliced_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;...&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 等待上一步通信完成</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">boundary_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># buffer_list 存储的是每个 devive 进行卷积所需要的其他 devive 的数据</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;conv2d&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">            <span class="k">def</span> <span class="nf">create_padded_x</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39;拼接接收到的数据&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># rank 0</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">elif</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># rank n-1</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>  <span class="c1"># other ranks</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="p">[</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                            <span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">padded_x</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 提取当前输入张量需要发送给其他设备的部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">boundary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">boundary_size</span><span class="p">,</span> <span class="p">:],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">boundary_size</span><span class="p">:,</span> <span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 直接用上一步的 buffer 拼接</span>
</span></span><span class="line"><span class="cl">            <span class="n">padded_x</span> <span class="o">=</span> <span class="n">create_padded_x</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">padded_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">boundary</span><span class="p">)</span>  <span class="c1"># 插入自己要发送的数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distriselfattentionpp">DistriSelfAttentionPP</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/attn.py#L107">DistriSelfAttentionPP</a> 只负责计算自己 patch 的输出，需要完整的 KV，将 self attention 运算变成 cross-attention 计算。需要通信自己的 KV.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">DistriAttentionPP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriSelfAttentionPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>  <span class="c1"># 获取 Attention 模块</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>  <span class="c1"># 残差连接</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">args</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">USE_PEFT_BACKEND</span> <span class="k">else</span> <span class="p">(</span><span class="n">scale</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Q Projection</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_kv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>  <span class="c1"># KV Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># 如果缓冲区未创建</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">buffer</span> <span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span> <span class="o">=</span> <span class="n">kv</span>
</span></span><span class="line"><span class="cl">        <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_buffer_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将 full_kv 分割为 key 和 value</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">full_kv</span><span class="p">,</span> <span class="n">full_kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># multi-head attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># O Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># Dropout</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">hidden_states</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distrigroupnorm">DistriGroupNorm</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/groupnorm.py#L9">DistriGroupNorm</a> 根据上一步全特征图的以及当前步 patch 的均值和二阶矩近似当前步的全特征图均值和方差。需要通信 patch 均值和二阶矩。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriGroupNorm</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriGroupNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl">        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_groups</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="n">group_size</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>  <span class="c1"># register for E[x], E[x^2]</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;gn&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算 patch 均值和二阶矩</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x2_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x2_mean</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="n">slice_mean</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Equation 2 in the paper E[A_t] = E[A_(t+1)] + (E[A^i_t] - E[A^i_(t+1)]), same for E[A^2_t]</span>
</span></span><span class="line"><span class="cl">            <span class="n">correction</span> <span class="o">=</span> <span class="n">slice_mean</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">)</span> <span class="o">/</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">+</span> <span class="n">correction</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">slice_mean</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">full_x_mean</span><span class="p">,</span> <span class="n">full_x2_mean</span> <span class="o">=</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">full_x2_mean</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算方差</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_x_mean</span><span class="p">,</span> <span class="n">slice_x2_mean</span> <span class="o">=</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_var</span> <span class="o">=</span> <span class="n">slice_x2_mean</span> <span class="o">-</span> <span class="n">slice_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">var</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">slice_var</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>  <span class="c1"># Correct negative variance</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">num_elements</span> <span class="o">=</span> <span class="n">group_size</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scale and shift</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>DeepSpeedUlysses</title>
      <link>http://localhost:57770/blogs/deepspeedulysses/</link>
      <pubDate>Mon, 21 Oct 2024 11:09:12 +0800</pubDate>
      <guid>http://localhost:57770/blogs/deepspeedulysses/</guid>
      <description>Paper reading of Deepseed Ulysses.</description>
      <content:encoded><![CDATA[<h1 id="deepspeed-ulysses-core-design">DeepSpeed-Ulysses Core Design</h1>
<h2 id="system-design">System Design</h2>
<p>原理如下图所示，假设设备数 P 等于多头注意力的头数 hc. 输入 <code>x[N,d]</code> 被切分到每个设备上 <code>[N/p, d]</code>，之后进行 QKV Projection，随后将 K 进行转置后进行一次 all-to-all 通信，这样每个设备上就有 <code>Q[N, d/P], K[d/P, N], V[N, d/P]</code>, 再执行标准的 attention 计算 $Outputcontext=Softmax((QK^T)/\sqrt{d})V$. 再进行一次 all-to-all 通信使得每个设备上有 <code>[N, d/P]</code> 结果再进行后续操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB06300727bd2f239239db47091e81223c?method=download&amp;shareKey=6abbc645b3fa3a039b464dd405f96d4a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB06300727bd2f239239db47091e81223c?method=download&amp;shareKey=6abbc645b3fa3a039b464dd405f96d4a" alt="DeepSpeed Sequence Parallelism (DeepSpeed-Ulysses) Design">
    </a><figcaption>DeepSpeed Sequence Parallelism (DeepSpeed-Ulysses) Design</figcaption></figure></p>
<h2 id="communication-analysis">Communication Analysis</h2>
<p>在采用节点内 NVSwitch 互连和节点间 fat tree IB 拓扑的集群中，对于总消息大小为 M 的 all-to-all 通信，每条链路通过 P 个 gpu 传输的通信量为 M/P。对于隐藏层大小为 h、序列长度为 N、并行度为 P 的 transform 模型，DS-Sequence 对注意力计算前总消息大小为 3Nh 的 QKV Projection 执行 all-to-all 通信，对每个 transformer block 的输出执行 all-to-all 通信，大小为 Nh. 因此，DeepSpeed 序列下每条链路的总通信量为 4Nh/P (或复杂度为 O(N/P)). 也就是说当 N 和 P 按比例增加时，该通信量是恒定的。</p>
<h2 id="comparison-of-other-works">Comparison of Other Works</h2>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBff8d584feabe45900c3a57eea94a78a0?method=download&amp;shareKey=7bae2e87b18707dabcd5e5ae7976e644" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBff8d584feabe45900c3a57eea94a78a0?method=download&amp;shareKey=7bae2e87b18707dabcd5e5ae7976e644" alt="Comparison of DS-Ulysses to Other Sequence Parallelism Methods">
    </a><figcaption>Comparison of DS-Ulysses to Other Sequence Parallelism Methods</figcaption></figure></p>
<ul>
<li>ColAI-SP 发明了 Ring-Attention，Q 存储在本地 而 KV 以环形方式传输以计算全局注意力，导致通信复杂度与消息大小 M 呈线性关系。</li>
<li>Megatron-LM 序列并行方法与 Megatron 张量并行紧密集成。Megatron-LM 沿着序列维度划分序列，并应用 all gather 和 reduce scatter 来聚合 QKV 注意力计算的投影。并行通信量随消息大小 M 线性增加。</li>
<li>DeepSpeed-Ulysses 通过增加与序列长度成比例的设备数来保持通信量恒定。同时将 Zero3 扩展到数据并行和序列并行的组合。ZeRO 跨序列和数据并行组划分模型状态，并在需要时使用 allgather 收集每个 rank 的部分。</li>
</ul>
<h2 id="general-and-attention-agnostic-solution">General and Attention Agnostic Solution</h2>
<p>DeepSpeed-Ulysses 的优势在于一种以注意力为中心的序列并行设计。在注意力计算是 N/P 划分的序列并行之前，注意力计算是头并行，每个头的注意力都是完整的，但只有较少的头，因此注意力计算可以被任何类型的注意机制所取代，例如 dense attention 和各种形式的 sparse attention.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Efficient Large-Scale Language Model Training on GPU</title>
      <link>http://localhost:57770/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/</link>
      <pubDate>Sat, 05 Oct 2024 10:09:35 +0800</pubDate>
      <guid>http://localhost:57770/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/</guid>
      <description>Paper reading about Efficient Large-Scale Language Model Training on GPU Clusters.</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>本文展示了如何将张量、流水线和数据并行性组合起来以扩展到数千个gpu。我们提出了一种新的交错流水线调度，可以在内存占用与现有方法相当的同时将吞吐量提高 10%.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb0e290b7248b1bb11233e55661bbb736?method=download&amp;shareKey=c1cd9d017d6f99b06b1fc2635f48fc19" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb0e290b7248b1bb11233e55661bbb736?method=download&amp;shareKey=c1cd9d017d6f99b06b1fc2635f48fc19" alt="Trend of Sizes of SOTA NLP Models">
    </a><figcaption>Trend of Sizes of SOTA NLP Models</figcaption></figure></p>
<h1 id="introduction">Introduction</h1>
<p>张量（层内）模型并行对于较大的模型会崩溃。较大的模型在多个多 GPU 服务器上进行切分会导致两个问题：</p>
<ol>
<li>张量并行所需的 all-reduce 通信需要通过服务器间链路进行，这比多 GPU 服务器内可用的高带宽 NVLink 要慢</li>
<li>高度模型并行会产生小规模的矩阵乘法（GEMM），从而可能降低 GPU 利用率。</li>
</ol>
<p>流水线模型并行化是指模型的各层在多个 GPU 上进行条带化处理。batch 被拆分成更小的 microbatch ，并在这些 microbatch 之间流水线执行。无论进度如何，为了保持严格的优化器语义，优化器步骤需要跨设备同步，从而在每个 batch 结束时进行流水线刷新 (<em>pipeline flush</em>)，允许 microbatch 完成执行 (不再注入新的 microbatch). microbatch 数量与流水线级数的比例越大，流水线刷新所花费的时间就越少。</p>
<p>我们展示了如何结合流水线、张量和数据并行性，我们称之为PTD-P. 配置分布式训练的指导原则如下:</p>
<ul>
<li>不同形式的并行性以不同的方式相互作用: 并行策略对通信量、执行内核的计算效率以及由于流水线冲洗 (流水线气泡) 而花费的等待计算的空闲时间有影响。</li>
<li>用于流水线并行性的调度对通信量、流水线气泡大小和用于存储激活的内存有影响。</li>
<li>超参数 (如 microbatch 大小) 的值会影响内存占用、在工作线程上执行的内核的算术效率和流水线气泡大小。</li>
<li>随着规模扩展分布式训练是通信密集型的。使用较慢的节点间互连或更密集的通信分区会影响扩展性能。</li>
</ul>
<h1 id="model-parallelism">Model Parallelism</h1>
<p>本节中将讨论有助于不适合单个 GPU 内存的大模型的并行训练方法。我们将流水线模型并行和张量模型并行 (如图 2 所示的组合) 与数据并行结合起来，简称为PTD-P.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB09f440d2c0b5bd3b923d2d09dbf47eb5?method=download&amp;shareKey=59701400f477d8dd03c4ea43138c933f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB09f440d2c0b5bd3b923d2d09dbf47eb5?method=download&amp;shareKey=59701400f477d8dd03c4ea43138c933f" alt="Combination of Tensor and Pipeline Model Parallelism (MP)">
    </a><figcaption>Combination of Tensor and Pipeline Model Parallelism (MP)</figcaption></figure></p>
<h2 id="data-parallelism">Data Parallelism</h2>
<p>使用数据并行时，每个 worker 都有一个完整模型的副本，输入数据集被分片， worker 定期汇总他们的梯度，以确保所有 worker 看到一个一致的权重版本。</p>
<h2 id="pipeline-parallelism">Pipeline Parallelism</h2>
<p>通过流水线并行，模型的层被分散到多个设备上。一个 batch 被分成更小的 microbatch. 在 microbatch 之间进行流水线执行。为了准确地保持优化器语义，我们引入了定期的流水线刷新，以便在设备之间同步优化器步骤。在每个 batch 处理的开始和结束时，设备都是空闲的。我们称这段空闲时间为流水线气泡 (<em>pipeline bubble</em>).</p>
<h3 id="default-schedule">Default Schedule</h3>
<p>GPipe 提出了一个调度方案，如图 3 所示 (假设<strong>反向传播的时间是前向传播的两倍</strong>，管道调度的效率并不取决于这个因素)，首先执行一个 batch 中所有 microbatch 的前向传播，然后执行所有 microbatch 的反向传播。设 GPipe 流水线气泡的大小为 t_pb，microbatch 的数量为 m，流水线阶段数量 (用于流水线并行的设备数量) 表示为 p，每次迭代的理想时间表示为 t_id (假设理想缩放)，执行单个 microbatch 的向前和反向传播的时间表示为 t_f 和 t_b. 在该调度中，流水线气泡由批处理开始时的 p−1 个前向传播和 p−1 个反向传播组成。则流水线气泡总时间为 t_pb=(p−1)·(t_f+t_b). batch 的理想执行时间为 t_id=m·(t_f+t_b)。因此，在流水线气泡中花费与理想计算时间的比例为:</p>
<p>流水线气泡占比 = t_pb / t_id = (p−1) / m.</p>
<p>为了使流水线气泡占比小，我们需要 m 远大于 p. 然而 m 非常大时这种方法的内存占用很高，因为它需要在训练一次迭代时间内为所有 m 个 microbatch 保存中间激活.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB04e4031b886573061f614e73854d1f43?method=download&amp;shareKey=97391411ed3d06fc2c6de5de5f20d1d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB04e4031b886573061f614e73854d1f43?method=download&amp;shareKey=97391411ed3d06fc2c6de5de5f20d1d0" alt="GPipe Pipeline Schedule">
    </a><figcaption>GPipe Pipeline Schedule</figcaption></figure></p>
<h3 id="schedule-with-interleaved-stages">Schedule with Interleaved Stages</h3>
<p>为了缩小流水线气泡的大小，每个设备都可以对多个层的子集（称为模型块）进行计算，流水线中的每个设备都被分配了多个流水线阶段（与之前相比，每个流水线阶段的计算量更少），而不是单个连续的层。</p>
<details class="custom-details">
    <summary class="custom-summary">An Example</summary>
    <div>例如，如果每个设备之前被分配 4 层 (即设备 1 有 1 - 4 层，设备 2 有 5 - 8层&hellip;)，我们可以让每个设备为两个模型块执行计算 (每个模型块被分配 2 层)，即设备 1 有 1、2、9、10 层; 设备 2 具有第3、4、11、12层&hellip;</div>
</details><br>
<p>和上一小节一样，我们可以执行完所有 microbatch 的前向传播然后执行所有反向传播 (all-forward, all-backward)，但这将占用大量内存 (与 m 成正比). 因此如图 4 所示，我们设计了一个适配于之前的内存高效 1F1B 的交错调度。它要求 <strong>microbatch 数量是流水线并行度 (流水线中的设备数量) 的整数倍</strong>。</p>
<p>如果每个设备都有 v 个阶段 (模型块)，那么每个阶段 microbatch 的前向和反向传播的时间分别为 t_f/v 和 t_b/v. 流水线气泡时间因此减少到 𝑡^int_pb=(p−1)·(tf+tb)/v，</p>
<p>流水线气泡占比为 𝑡^int_pb / t_id = (p−1) / (m·v).</p>
<p>这意味着该调度减少气泡时间到原先的 1/v，但该计划需要额外的通信，因此通信量也为原来的 v 倍。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb74cfaaf752e14cf2e44f4abd7e3e7bf?method=download&amp;shareKey=4558481494c1d4b22574e739743b123d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb74cfaaf752e14cf2e44f4abd7e3e7bf?method=download&amp;shareKey=4558481494c1d4b22574e739743b123d" alt="Default and Interleaved 1F1B Pipeline Schedules">
    </a><figcaption>Default and Interleaved 1F1B Pipeline Schedules</figcaption></figure></p>
<h2 id="tensor-model-parallelism">Tensor Model Parallelism</h2>
<p>详情见 Megatron-LM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB02b1b15f4d736bfee41738f3c3ee72b3?method=download&amp;shareKey=b1a7e4a6171585f4f0a8a39fb4b2d8b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB02b1b15f4d736bfee41738f3c3ee72b3?method=download&amp;shareKey=b1a7e4a6171585f4f0a8a39fb4b2d8b3" alt="Blocks of Transformer Model Partitioned with Tensor Model Parallelsim">
    </a><figcaption>Blocks of Transformer Model Partitioned with Tensor Model Parallelsim</figcaption></figure></p>
<h1 id="performance-analysis-of-parallelization-configurations">Performance Analysis of Parallelization Configurations</h1>
<p>首先定义下符号含义</p>
<ul>
<li>(p,t,d): 并行化维度。p 表示流水线模型并行大小，t 表示张量模型并行大小，d 表示数据并行大小。</li>
<li>n: GPU 数量，要求 ptd=n.</li>
<li>B: 全局批大小 (作为输入提供)</li>
<li>b: microbatch 大小。</li>
<li>m = B/(db): 一个 batch 中每个流水线中的 microbatch 的数量。</li>
</ul>
<h2 id="tensor-and-pipeline-model-parallelism">Tensor and Pipeline Model Parallelism</h2>
<p>如前所述，使用带有周期性冲洗的流水线并行会产生大小为 (p−1)/m 的流水线气泡. 固定 d=1，则 tp=n，气泡大小可以用 t 表示为</p>
<p>(p−1)/m=(n/t-1)/m.</p>
<p>GPU 之间的通信量也受 p 和 t 大小的影响。流水线模型并行的特点是更便宜的点对点通信，每个 microbatch 的每对连续设备之间 (前向或后向传递) 需要执行的通信总量为 bsh. 张量模型并行则使用 all-reduce 通信，总大小为 bsh 的张量需要在每层的前向和后向传递中，在 t 个模型副本之间进行两次 all-reduce，因此每个 microbatch 每层每个设备的总通信量为 4bsh(t-1)/t. 每个设备通常有多个层，则每个设备上每个 microbatch 的张量并行通信总量为 l^stage4bsh(t-1)/t, 其中 l^stage 为流水线阶段的层数。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 1: 当 t 大于单个节点中的 GPU 数量时，在较慢的节点间链路上执行张量模型并行的开销非常大。在考虑不同形式的模型并行时，使用 g-GPU 服务器时张量模型并行度一般为 g (all-reduce 通信量大，NVLink 带宽高)，然后可以使用流水线模型并行来扩展到跨服务器的更大模型 (P2P 通信量小，PCIe 带宽低).</p></div>

<h2 id="data-and-model-parallelism">Data and Model Parallelism</h2>
<p>管道模型并行性。设 t=1，每个管道的 microbatches 数量 m=𝐵/(db)=b&rsquo;/d, b&rsquo;:=B/b. 设 GPU 总数为 n ，流水线阶段数为 p=n/d，气泡大小为</p>
<p>(p−1)/m=(n/d-1)/(b&rsquo;/d)=(n-d)/b'</p>
<p>管道气泡随着 d 变大而变小。如果数据并行所需的 all-reduce 通信不会随着 d 的变大而急剧增加，那么总体吞吐量将会增加，因为基于环的实现的通信时间随着 d 的变化为 (d−1)/d=1−1/d.同样对于给定的并行配置，随着批量大小的增加，b&rsquo; = B/b 增加，因此吞吐量上升。同时数据并行所需的 all-reduce 通信频率也下降，进一步提高了吞吐量。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB27b82e198a75ad564ac917d6d560dec1?method=download&amp;shareKey=cfef60f8f216007803bdafe8b5a5e64c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB27b82e198a75ad564ac917d6d560dec1?method=download&amp;shareKey=cfef60f8f216007803bdafe8b5a5e64c" alt="Fraction of Time Spent Idling due to Pipeline Flush">
    </a><figcaption>Fraction of Time Spent Idling due to Pipeline Flush</figcaption></figure></p>
<p>在张量模型并行下，每个 microbatch 都需要进行 all-reduce 通信，这在多 GPU 服务器上开销很大；而数据并行只需要在每个 batch 中执行一次的 all-reduce通信。此外，使用张量模型并行，每个设备计算每层的一部分，因此对于不够大的层， GPU 可能无法以峰值效率执行这些子矩阵计算。</p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 2：在使用数据和模型并行时，应使用 M=tp 的总模型并行大小，以便模型参数和中间数据满足 GPU 内存限制；数据并行可用于将训练扩展到更多 GPU.</p></div>

<h2 id="microbatch-size">Microbatch Size</h2>
<p>给定函数 t_f(b) 和 t_b(b)，将 microbatch 大小映射为单个 microbatch 的前向和反向计算时间，计算一个 batch 所花费的总时间 (忽略通信成本) 为</p>
<p>(b&rsquo;/b+p-1)·(t_f(b)+t_b(b)).</p>
<p>microbatch 大小因此既影响运算的算术强度，也影响管道气泡大小。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe3b16b74c3ca1aedb0f81939501da9de?method=download&amp;shareKey=cd1c360e2597b37f705bd5f4906d64b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe3b16b74c3ca1aedb0f81939501da9de?method=download&amp;shareKey=cd1c360e2597b37f705bd5f4906d64b3" alt="Per-GPU Throughput versus Microbatch Size for a GPT Model">
    </a><figcaption>Per-GPU Throughput versus Microbatch Size for a GPT Model</figcaption></figure></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3de398053a7040a405722f3b8c929bf1?method=download&amp;shareKey=7014b2696bffc97d5646b4b2614bd3fb" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3de398053a7040a405722f3b8c929bf1?method=download&amp;shareKey=7014b2696bffc97d5646b4b2614bd3fb" alt="">
    </a><figcaption>Behavior of Throughput for the same GPT Model</figcaption></figure></p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 3：最佳 microbatch 大小 b 取决于模型的吞吐量和内存占用特征，以及流水线深度 p、数据并行大小 d 和批量大小 B.</p></div>

<h2 id="activation-recomputation">Activation Recomputation</h2>
<p>激活重计算通过在向后传递之前运行第二次正向传播 (并且仅存储给定流水线阶段的输入激活)，来权衡所执行的计算操作数量的增加对额外内存占用的影响。设 A^input 为一层的输入激活的大小，A^intermediate 为每层的中间激活的大小，一个模型阶段有 l 层， 激活保存点的数量为 c，那么总内存占用为 c·A^input + l/c·A^intermediate. 因此取 c = \sqrt(l·A^input·A^intermediate) 时内存占用最小。</p>
<h1 id="implementation">Implementation</h1>
<h2 id="communication-optimizations">Communication Optimizations</h2>
<p>使用流水线并行时，我们希望在正向和反向并行发送和接收张量。每台 DGX A100 都配备了 8 个 InfiniBand（IB）网卡。然而发送和接收都是点对点的，只发生在两台服务器上的一对 GPU 之间，因此很难充分利用所有网卡。对于流水线内的单次通信，每个 transformer 层的输出都会在张量并行的设备中复制。为了减少这种冗余，我们可以在发送端将张量分割成大小相等的块，然后使用每个 rank 自己的 InfiniBand 发送. 在接收端通过比 InfiniBand 互连快得多的 NVLink 执行 all-gather，重新组装整个张量。通过 scatter-gather 通信优化，将每对连续流水线阶段之间需要执行的通信总量减少为 bsh/t.</p>
<h2 id="computation-optimizations">Computation Optimizations</h2>
<p>将数据布局从 (b,s,a,h) 更改为 (s,b,a,h). 其次，使用 PyTorch JIT 为一系列元素操作 (bias+GeLU 和 bias+dropout+add) 生成融合算子。</p>
<h1 id="evaluation">Evaluation</h1>
<p>在 Selene 超级计算机上以混合精度运行。每个集群节点有</p>
<ul>
<li>8 个 NVIDIA 80GB A100 GPU，通过 NVLink 和 NVSwitch 互连。</li>
<li>8 个 NVIDIA Mellanox 200Gbps HDR Infiniband HCA 用于应用程序通信</li>
<li>额外有 2 个 HCA 用于专用存储。
节点以三级 (leaf, spine, core) 胖树拓扑结构连接，一共有 850个交换机。集群使用 all-NVME 共享并行文件系统进行高性能数据访问和存储。16 位精度的 A100 GPU 的峰值设备吞吐量为 312 teraFLOP/s.</li>
</ul>
<p>QKV 变换的线性层权重参数量均为 h^2, attention 后的线性层权重参数量为 h^2, 两层前馈网络每个线性层的权重参数量为 4h^2，因此每一个 transformer block 的所有线性层的参数量为 12h^2. 词嵌入的参数量为 Vh，位置编码的参数量为 sh.</p>
<p>一个 $A_{m\times k}\times X_{k\times n}$ 矩阵乘法需要 2mkn FLOPs( 2 是因为乘法和加法). transformer block 包含一个注意力块和一个两层前馈网络组成。对于注意力块，主要的 FLOP 来源于 QKV 转换 (6Bsh^2 次操作)、注意力矩阵计算 (2Bhs^2 次操作)、注意力乘 Value (2Bhs^2 次操作) 和 attention 后的线性层 (2Bsh^2 次操作). 前馈网络将隐藏维度放大到 4h，然后再减小到 1h，需要 16Bsh^2 次操作。将这些加在一起，每个 transformer block 一共有 24Bsh^2+4Bhs^2 FLOPs. 反向传播需要两倍的计算量，因为需要计算关于输入张量和权重张量的梯度。此外，使用激活重计算需要在反向传播之前进行额外的正向传播。因此，每一层的总计算量为 FLOPs 为 4*(24Bsh^2+4Bhs^2).</p>
<p>计算量另一方面来源于 head 的 logit 层，它将维度的特征 h 转换为词汇表维度的特征 V. 该操作所需的计算量为正向传播的 2BshV 和反向传播的 4BshV，总共 6BshV FLOPs.</p>
<h2 id="result">Result</h2>
<p>Pipeline-parallel 并行度增加降低 GPU 的计算效率，因为 bubble 变多了。
Batchsize 的增大可以减少 pipeline-parallel 并行度大小带来的影响。</p>
<p>Batch size增加有助于提高GPU的计算效率。
Interleaved schedules 能显著提高GPU的计算效率。</p>
<p>不使用激活重计算的话单位时间内的训练的吞吐是要高于使用重计算的，因为重计算在反向传播中引入额外的计算量。
由于重计算可以节省显存，batchsize 可以相应提高不少。由于 batchsize 的提高，训练吞吐量也得到了提高，从而达到了优化的效果。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Megatron-LM</title>
      <link>http://localhost:57770/blogs/megatronlm/</link>
      <pubDate>Wed, 02 Oct 2024 15:51:50 +0800</pubDate>
      <guid>http://localhost:57770/blogs/megatronlm/</guid>
      <description>Paper reading about Megatron-LM</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>我们的方法不需要新的编译器或更改库，与流水线模型并行 (<em>pipeline model parallelism</em>) 正交互补，并且可以通过在原生 PyTorch 中插入一些通信操作来实现。为了阐述我们的方法，使用 512 个 GPU 将基于 transformer 的模型扩展到 83 亿个参数。与可保持 39 TeraFLOPs (峰值 FLOPs 的 30%) 的强大单 GPU 基准相比，我们在整个应用中保持了 15.1 PetaFLOPs，扩展效率高达 76%.</p>
<h1 id="introduction">Introduction</h1>
<p>随着 LLM 变得越来越大，它们会超出现代处理器的内存限制，并需要如激活检查点 (activation checkpoint) 等额外的内存管理技术。广泛使用的优化算法 (如ADAM) 需要每个参数额外的内存来存储动量和其他优化器状态。这减少了可以有效训练的模型的大小。模型并行性的几种方法克服了这一限制，它们对模型进行分区，使权重及其相关的优化器状态不需要并发地驻留在处理器上。</p>
<details class="custom-details">
    <summary class="custom-summary">Activation Checkpoint</summary>
    <div>在深度学习模型的训练过程中，前向传播会计算并存储每一层的激活值，这些激活值在后向传播时被用来计算梯度。然而，对于深度很大的模型因为需要存储大量的激活值，可能会导致内存溢出。激活检查点技术通过在前向传播过程中只存储一部分的激活值来解决内存占用问题，如果在后向传播过程中需要没有存储的激活值就进行重新计算。</div>
</details><br>
<p>为了证明方法的可扩展性，通过在单个英伟达 V100 32GB GPU 上训练一个包含 12 亿个参数的模型来建立基准。训练该模型可维持 39 TeraFLOPs 的算力，是在 DGX-2H 服务器中配置的单个 GPU 理论峰值 FLOPS 的 30%. 在 512 个 GPU 上将模型扩展到 83 亿个参数，并采用 8 路模型并行，在整个应用中实现了高达 15.1 PetaFLOPs 的持续运行速度。与单 GPU 情况相比，扩展效率提高了 76%. 下图展示了更详细的扩展结果。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" alt="Model (blue) and model&#43;data (green) parallel FLOPS">
    </a><figcaption>Model (blue) and model&#43;data (green) parallel FLOPS</figcaption></figure></p>
<h1 id="background--chanllenges">Background &amp; Chanllenges</h1>
<h2 id="neural-language-model-pretraining">Neural Language Model Pretraining</h2>
<p>早期的预训练和传递语言神经表示的例子表明，与从头开始学习的词嵌入表相比，预训练的词嵌入表改善了下游任务的结果。目前的技术水平已经从传输单词嵌入表发展到传输整个数十亿参数的语言模型。这种方法的进步要求硬件、系统技术和框架能够高效地大规模运行。</p>
<h2 id="transformer-language-models-and-multi-head-attention">Transformer Language Models and Multi-Head Attention</h2>
<p>下图展示了使用的 transformer 模型的示意图。最近利用 transformer 进行语言建模的工作，如 BERT 和 GPT-2 根据需要分别只使用编码器和解码器。</p>
<blockquote>
<p>GPT-2 和 BERT 都对多头注意和 FFN 的输入使用 GeLU 非线性和层归一化，而原始 transformer 使用 ReLU 非线性并对输出进行层归一化。</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" alt="Transformer Architecture">
    </a><figcaption>Transformer Architecture</figcaption></figure></p>
<h2 id="data-and-model-parallelism-in-deep-learning">Data and Model Parallelism in Deep Learning</h2>
<p>将深度神经网络训练扩展到多硬件加速器有两种范式:</p>
<ul>
<li>Data Parallelism (DP): 将 batch 拆分到多个 worker</li>
<li>Model Parallelism (MP): 将模型的内存使用和计算分布在多个 worker 中。
<ul>
<li>Pipeline Parallelism (PP): 一组操作在一个设备上执行，然后将输出传递到流水线中的下一个设备执行另一组操作。</li>
<li>Distributed Tensor Computation: 将张量运算分割到多个设备上，以加速计算或增加模型大小。</li>
</ul>
</li>
</ul>
<p>然而，这些技术有一个基本的限制: 模型权重必须能加载进 worker. 我们的方法是利用模型并行性在多个加速器之间分割模型。</p>
<h1 id="model-parallel-transformers">Model Parallel Transformers</h1>
<p>我们利用 transformer 网络的结构 (self-attention 和 FFN (2*MLP) 组成)，通过添加一些同步原语，创建了一个简单的并行计算模型。下面分别阐述对 FFN 和 self-attention 的并行化。</p>
<p>FFN 第一个 MLP 由一个 GEMM，后跟一个 GeLU 非线性组成:</p>
$$
Y=\text{GeLU}(XA)
$$<p>并行化 GEMM 的一种选择是将权重矩阵 A 沿着行切分，并将 X 沿着其列切分:</p>
$$
X=[X_1,X_2], A=\begin{bmatrix}A_1\\A_2\end{bmatrix}
$$<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" alt="Row Split of Weight">
    </a><figcaption>Row Split of Weight</figcaption></figure></p>
<p>可以得出 $Y = X_1A_1+X_2A_2$. 由于 GeLU 是非线性函数，因此这种方法需要在 GeLU 函数之前进行同步。</p>
<p>另一个选择是沿着列切分 $A=\begin{bmatrix}A_1,A_2\end{bmatrix}$. 这样可以让 GeLU 独立地应用于每个 GEMM 的输出</p>
<p>$[Y_1, Y_2]=\begin{bmatrix}\text{GeLU}(XA_1),\text{GeLU}(XA_2)\end{bmatrix}$.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" alt="Column Split of Weight">
    </a><figcaption>Column Split of Weight</figcaption></figure></p>
<p>这种切分方式的优点是不需要进行同步操作。</p>
<p>如下图所示，以列并行方式切分第一个 GEMM，并沿着行切分第二个GEMM。然后，在将输出传递给 dropout 层之前，第二个GEMM 的输出在 GPU 之间进行 all-reduce 操作。这种方法将 FFN 中的两个 GEMM 拆分到多个 GPU 上执行，并且只需要在正向传播 (g 操作符) 和反向传播 (f 操作符) 中分别执行一次 all-reduce 操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" alt="Parallelism of MLP">
    </a><figcaption>Parallelism of MLP</figcaption></figure></p>
<p>如下图所示，利用多头注意力操作中本身存在的并行性，以列并行的方式划分与 QKV 相关的 GEMM，以便每个注意力头对应的矩阵乘法在一个 GPU 上独立完成。输出线性层的 GEMM 沿着其行并行化，并直接获取并行 attention 的输出。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" alt="Parallelism of Self-Attention">
    </a><figcaption>Parallelism of Self-Attention</figcaption></figure></p>
<p>如下图所示，这使能够仅在正向传播和反向传播中分别中使用两个 all-reduce 操作执行 transformer 中所有的 GEMM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" alt="Parallelism of Transformer Layer">
    </a><figcaption>Parallelism of Transformer Layer</figcaption></figure></p>
<p>基于 transformer 的语言模型的输出嵌入维度为隐藏层大小 (H) 乘以词汇表大小 (v). 我们沿着词汇表维度 $E = \begin{bmatrix}E_1,E_2\end{bmatrix}$ 并行化权重矩阵。每一块现在只包含嵌入表的一部分，输入嵌入后需要一个 all-reduce (g 算子).</p>
<p>对于输出嵌入，一种方法是通过并行 $\mathrm{GEMM} [Y_{1},Y_{2}]=[XE_{1},XE_{2}]$ 来获得 logits，并对结果 all-gather 后送入交叉熵损失函数。在这种情况下，all-gather 通信量为 bsv 个元素 (b 是批处理大小，s 是序列长度). 为了减小通信规模，我们将输出与交叉熵损失融合，这样通信量降为 bs.</p>
<p>我们在每个 GPU 上维护层归一化参数的副本，并在将这些张量作为输入送到下一个模型并行区域之前，在本地输出上进行 dropout 和残差连接。为了优化模型，我们允许每个模型并行 worker 优化自己的一组参数。因为所有的值要么是本地的，要么是在 GPU上 重复的，所以在这个公式中不需要通信更新的参数值。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Ring Attention Principle</title>
      <link>http://localhost:57770/blogs/ringattention/</link>
      <pubDate>Thu, 26 Sep 2024 22:59:35 +0800</pubDate>
      <guid>http://localhost:57770/blogs/ringattention/</guid>
      <description>This is a brief introduction to the Ring Attention Principle.</description>
      <content:encoded><![CDATA[<h1 id="background">Background</h1>
<p>如今 LLM 的 token 长度显著增加，从 GPT-3.5 的 16k 到 Claude 2 的 200k，现在 Gemini 1.5 Pro 甚至有 1M 的 token 长度。如此长的 token 在计算 attention 时对显存的需求非常大。<a href="https://arxiv.org/abs/2310.01889">Ring Attention</a> 便是为了并行计算 attention 而提出的一种方法<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<blockquote>
<p>Ring Attention 和 Flash Attention 可以同时使用。</p></blockquote>
<h1 id="attention-and-memory">Attention and Memory</h1>
<p>要计算 attention， 我们需要三个大小为 (s, d) 的矩阵：Q (query)、K (key)、V (value)，其中 s 为序列长度，d 为模型维度。attention 的计算公式为</p>
$$
Attention(Q, K, V) = softmax(QK^T / \sqrt{d})V
$$<p>忽略 sqrt(d) 项，我们记 Score Matrix 为 S = QK^T / \sqrt{d}，然后对 S 进行 softmax 归一化，得到 Attention Matrix. 可以发现它们占用显存大小是 O(s*s) 数量级。即使使用 <a href="https://arxiv.org/abs/2205.14135">Flash Attention</a>，显存占用量也是 O(s) 数量级。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe66e94e161b89a4ba25d05b67a47e393?method=download&amp;shareKey=742185dd412edbdb3266fa16ab91d787" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe66e94e161b89a4ba25d05b67a47e393?method=download&amp;shareKey=742185dd412edbdb3266fa16ab91d787" alt="Attention Compute Process">
    </a><figcaption>Attention Compute Process</figcaption></figure></p>
<p>我们希望如果在 N 个设备上并行计算 attention，每个设备的显存占用量为整个的 1/N, 因此就需要对 Q、K、V 的 sequence 长度进行切分。但是如果得到的最终 attention 矩阵需要在设备间进行集合通信组装每个的计算结果，通信量也和 sequence 长度成正比。Ring Attention 提出了一个巧妙的解决方案：在设备之间进行轮转，并行化所有计算而且完全隐藏通信的开销。</p>
<blockquote>
<p>We will rotate between devices to parallelize all computation and hide the communication overhead completely.</p></blockquote>
<h1 id="splitting-the-query">Splitting the Query</h1>
<p>假设我们有 N 个设备，我们将 Q 沿着 sequence 维度切分为 N 份，每份大小为 (s/N, d). 由于计算 Score 和 Attention 需要完整的 K 和 V，这样它们也被切分成 N 份，每份大小为 (s/N, d). 计算示意图如下。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB170087e68345309f813b8edee9487b92?method=download&amp;shareKey=f848ff8adb5676443347921c65a3b104" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB170087e68345309f813b8edee9487b92?method=download&amp;shareKey=f848ff8adb5676443347921c65a3b104" alt="Split Q">
    </a><figcaption>Split Q</figcaption></figure></p>
<h1 id="splitting-the-key-and-value">Splitting the Key and Value</h1>
<p>对 K 和 V 的切分并不能像 Q 那样直接。因为 softmax 的计算公式如下，要得到分母的值意味着我们需要对每一行进行计算。</p>
$$
softmax(s_i) = \frac{\exp(s_i)}{\sum_{j=i}^d{\exp(s_j)}}
$$<p>如果我们能对 K 和 V 进行切分并正确计算 softmax，那么计算过程可以由下图所示的那样完成 (忽略 softmax). 外循环遍历 Q 的所有分块，内循环遍历 K 和 V 的所有分块，一次计算一部分的 attention. Ring Attention 示意图如下所示，顾名思义所有设备组成一个环状，每个设备存储 Q 的一部分，每次迭代过程会传递 K 和 V 到下一个设备，最终每个设备将得到计算自己 Q 部分的 attention 矩阵所需要的 K 和 V. 每个设备被分配 Q 的一部分 (即一个外层循环索引)，并迭代计算每个 K 和 V 的分块 (内循环)。每个设备只需要跟踪形状为 (s/N, s/N) 的累积和 A_j。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbc9ef7d01431fe639ecf44842bce0e1a?method=download&amp;shareKey=03d587a38ca574ed1547f2594a45ab4c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbc9ef7d01431fe639ecf44842bce0e1a?method=download&amp;shareKey=03d587a38ca574ed1547f2594a45ab4c" alt="Attention Parallel Computation">
    </a><figcaption>Attention Parallel Computation</figcaption></figure></p>
<h1 id="online-softmax">Online Softmax</h1>
<p>在内循环的每次迭代中我们可以更新部分和为 $l^j = l^{j-1} + \sum_{k_t\in K_j}{\exp(Q_ik_t^T)}$. 在内循环结束后我们就可以获得每一行的指数和。归一化和与 V 的相乘顺序不会影响结果，我们可以先累加总和，并在所有其他计算完成后再执行实际的归一化操作。</p>
<p>因此，设备 i 除了计算当前的累计和 $A^j = A^{j-1} + \exp(Q_i K_j^T) V_j$ 外，还需要在内循环每次迭代中更新部分和 $l^j \in \mathbb{R}^{B_q}$ ，其中 $B_q$ 为 Q 的分块大小。</p>
<h1 id="safe-softmax">Safe softmax</h1>
<p>由于指数运算经常容易出现溢出，我们通常减去 max(s_i) 后进行指数运算，公式如下，这样并不会影响结果。</p>
$$
\mathrm{softmax}(s_{1:N})=\frac{\exp(s_{1:N})}{\sum_i\exp(s_i)}\cdot\frac{\exp(-s_{max})}{\exp(-s_{max})}=\frac{\exp(s_{1:N}-s_{max})}{\sum_i\exp(s_i-s_{max})}
$$<p>所以我们在内循环每次迭代中需要先更新当前的最大值 $m^{j+1}=\max(m^j,\max(Q_iK_{j+1}^T))$，然后更新之前迭代的计算结果 A_j 和 部分和 l_j. 最后再计算本次迭代的结果。</p>
$$
A^{j+1}=A^j\cdot\exp(m^j-m^{j+1})+\exp(Q_iK_{j+1}^T-m^{j+1})\cdot V_j
$$<p>更新部分和</p>
$$
l^{j+1}=l^j\cdot\exp(m^j-m^{j+1})+\exp(Q_iK_{j+1}^T-m^{j+1})
$$<h1 id="putting-it-together">Putting it Together</h1>
<p>Ring Attention 计算步骤如下：</p>
<ol>
<li>沿着 Q 的 sequence 长度拆分为一个独立的外循环。</li>
<li>应用 Online Safe Softmax，以便沿着 K 和 V 的sequence 长度拆分，从而在内层循环中累积计算注意力。</li>
</ol>
<p>这种并行化的方式是通过将每个设备分配一个 Q_i 块来实现的。因此，我们需要将 Q 拆分为 N 个相等的部分 (B_Q=N). 每个设备将分别计算它的输出块 $\text{Output}(Qi,K,V)= \text{softmax}(Q_i K^T)V ，通过在 K 和 V 块上执行内循环来迭代计算。难点挑战在于设备无法一次存储完整的 K 和 V 矩阵。</p>
<p>如果我们有 4 个 GPU，那么我们将把每个设备的 Q 按序列维度分成 4 个块，K 和 V 被分割成 B_K=B_Q=N 个块，并对设备进行初始化，使每个设备都持有一个 Qi 块、 一个 Kj 块和 一个 Vj 块。为简单起见，我们可以假设设备 i 在开始时持有 Qi, Ki 和 Vj 块。在设备计算完与其当前 vj kj 相对应的一个内循环步骤后，每个设备都需要接收下一个 Key 和 Value 块，以继续内循环。 我们将 N 个设备围成一个环，其中设备 i 可以向设备 i+1 以此类推，如图所示：</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5d0930a41cedf1d4e46af9baa5071f78?method=download&amp;shareKey=0e898be310f92f54f0b065a38771eb5f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5d0930a41cedf1d4e46af9baa5071f78?method=download&amp;shareKey=0e898be310f92f54f0b065a38771eb5f" alt="KV-overlap">
    </a><figcaption>KV-overlap</figcaption></figure></p>
<p>如果在设备 i 上计算内循环的一个步骤 Qi,Vj,Kj 的这段时间内，设备 i 还能向设备 i+1 发送其当前 Kj Vj，并同时从设备 i-1 接收 V_j-1,K_j-1，那么只要发送和接收密钥和值块的时间低于计算时间，那么发送和接收 Key 和 Value 块的延迟就会隐藏在执行实际计算时间之内。一个例子如下图所示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB935e1d2c0eb43c35c5c828abe8a44612?method=download&amp;shareKey=9ec41a1c178534620d9f7274ff2ce9d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB935e1d2c0eb43c35c5c828abe8a44612?method=download&amp;shareKey=9ec41a1c178534620d9f7274ff2ce9d0" alt="KV-rotate">
    </a><figcaption>KV-rotate</figcaption></figure></p>
<h1 id="memory-and-arithmetic-complexity">Memory and Arithmetic Complexity</h1>
<p>以深度学习中常用的 bfloat16 数据类型为例。GPU 或 TPU 等并行处理加速器通常以 FLOP:=F 来衡量，即设备理论上每秒可执行的浮点运算次数。我们假设硬件被完全利用。此外，我们设不同设备之间的连接带宽为:=B (Bytes/sec).</p>
<p>内存复杂度: 为了同时进行接收发送和计算，我们需要有用于接收新 KV 块的寄存器器。存储当前 KV  值块需要 2dc 浮点数或 4dc 字节。用于接收新的 KV 块的内存大小也是 2dc 浮点数或 4dc 字节。假设计算本身不需要更多内存 (利用 Flash Attention 或 Blockwise Attention)，计算当前步骤的输出需要 dc 个浮点数或 2dc 字节。此外，每个设备还需要存储其 Qi 块，这也需要 dc 个浮点数或 2dc 字节。总共需要 6dc 个浮点或 12dc 字节。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>Ring Attention 与 Flash Attention 是正交的，可以一起使用 (Flash Attention 实际上用于 Ring Attention 的内循环). Flash Attention 目标是不将整个 Score Matrix 加载到全局内存中，从而在序列长度上获得线性内存复杂度。Ring Attention 将 原始注意力方法和 Flash Attention 的内存复杂度至少降低了 N 倍，使用 N 个设备的内存复杂度至少降低 N 倍，因为它将所有矩阵都拆分为至少 N 个或更多部分 (将 QKV 分别分成 N 份，并将 Score Matrix 分成 N^2 分). 无论内存复杂度是由 QKV，还是由 Score Matrix 主导，Ring Attention 都能将内存成本降低至少 N 倍。</p></div>

<p>通信开销: 在内循环每一步中，每个设备需要通过带宽为 B 的信道向下一个设备发送 2⋅c_Q⋅d 浮点数。每个 bf16 大小为 2字节，因此，所需的时间约为 4⋅c⋅d/B.</p>
<p>运算强度： 一个内循环步骤，计算局部注意力需要 2⋅d⋅c^2 次浮点计算，计算 softmax，归一化向量和最大值向量需要 2⋅c⋅d 次浮点计算，计算局部注意力与 Vj 块的乘积需 2⋅d⋅c^2 次浮点计算。因此，总计算所需时间≈4⋅d⋅c^2/F.</p>
<p>为了重叠通信和计算 (隐藏通信开销)，我们需要 KV 块的传输时间小于等于计算本地 QKV 所需的时间：</p>
$$
4\cdot c\cdot d/B\leq4\cdot d\cdot c^2/F\iff B\geq F/c\iff s/N\geq F/B 
$$<h1 id="futher-optimization">Futher Optimization</h1>
<p>Ring Attention 的一个应用是用于因果 Transformal 模型时，加上三角形掩码用于注意力计算。这意味着有些 GPU 不需要对整个序列进行计算，导致它们大部分时间处于闲置状态。作为 Ring Attention 的扩展，<a href="https://arxiv.org/pdf/2311.09431.pdf">Stripe Attention</a> 解决了这一问题，并提供了一种分配计算更均匀的方案，从而使 Ring Attention 的计算速度更快。</p>
<p>除了 Ring Attention 和 Flash Attention 等使标准 Transformer 架构能有更长的上下文长度的技术外，人们还尝试使用 <a href="https://arxiv.org/abs/2312.00752">Mamba</a> 等具有线性注意力的状态空间模型（SSM）等模型架构。</p>
<h1 id="references">References</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://coconut-mode.com/posts/ring-attention/">https://coconut-mode.com/posts/ring-attention/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Comparsion of Parallelsim Metods in ViT</title>
      <link>http://localhost:57770/blogs/comparsion-of-parallelsim-metods-in-vit/</link>
      <pubDate>Mon, 13 Nov 2023 16:05:23 +0800</pubDate>
      <guid>http://localhost:57770/blogs/comparsion-of-parallelsim-metods-in-vit/</guid>
      <description>Paper reading of .</description>
      <content:encoded><![CDATA[<h1 id="basic-transformer-block">Basic Transformer Block</h1>
<p>符号含义表示如下</p>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Description</th>
          <th>Symbol</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>a</td>
          <td>注意力头数</td>
          <td>n</td>
          <td>并行度大小</td>
      </tr>
      <tr>
          <td>b</td>
          <td>batchsize</td>
          <td>s</td>
          <td>序列长度</td>
      </tr>
      <tr>
          <td>h</td>
          <td>隐藏层维度</td>
          <td>v</td>
          <td>词汇表大小</td>
      </tr>
      <tr>
          <td>L</td>
          <td>tranformer layer 层数数</td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>基本 transformer block 结构如下，输入是形状为 (b, s, h) 的三维张量，其中 b 为 batchsize. 每个变压器层由一个具有注意头的自注意块组成，随后是一个具有两层的 MLP，第一层将隐藏维度增加到 4h，然第二层将其减少到 h. 每个变压器层的输入和输出具有相同的形状.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6446c9e0a905932db1f9e39fa91c01ba?method=download&amp;shareKey=f26e075bcfc51b8c093388f69d39b40d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6446c9e0a905932db1f9e39fa91c01ba?method=download&amp;shareKey=f26e075bcfc51b8c093388f69d39b40d" alt="Basic Transformer Architecture">
    </a><figcaption>Basic Transformer Architecture</figcaption></figure>

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfbdc229aca70349939d6e3306e78c434?method=download&amp;shareKey=cff3f2903a8e16c5c46d607749a4b3c1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfbdc229aca70349939d6e3306e78c434?method=download&amp;shareKey=cff3f2903a8e16c5c46d607749a4b3c1" alt="Self-attention Block">
    </a><figcaption>Self-attention Block</figcaption></figure></p>
<h2 id="model-parameters">Model Parameters</h2>
<p>QKVO Linear 的权重形状均为 <code>h*h</code>, 偏置形状均为 <code>h*1</code>；MLP 两个 Linear 的权重形分别为 <code>h*4h</code> 和 <code>4h*h</code>，偏置形状分别为 <code>4h*1</code> 和 <code>h*1</code>. 因此每个模型的参数量为 <code>(12hh+13h)L</code>，占用大小还要 <code>x2</code>.</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在传统的 LLM 中最后还需要经过 logits layer，将隐藏层维度 <code>h</code> 转换成词汇表大小 <code>v</code>，参数量还要加上 <code>hv</code>.</p></div>

<h2 id="flops-calculation">FLOPs Calculation</h2>
<p>对于浮点数计算量 (FLOPs)，只考虑占主要部分的通用矩阵乘法 (GEMMs). 对于 Attention 部分，QKV Linear 的计算量为 <code>6bshh</code>，attention matrix (<a href="mailto:Q@K.T">Q@K.T</a>) 的计算量为 <code>2bssh</code>, attention@V 的计算量为 <code>2bssh</code>, O Linear 的计算量为 <code>2bshh</code>. MLP 的两个线性层的每一个计算量都为 <code>8shh</code>. 相加后得到正向传播中总计算量为 <code>(24bshh + 4bssh)L</code> bytes.</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在传统的 LLM 中最后还需要经过 logits layer，将隐藏层维度 <code>h</code> 转换成词汇表大小 <code>v</code>，其计算量为 <code>2bshv</code>.</p></div>

<p>反向传播因为要计算输入和权重的梯度，其计算量为正向传播的两倍，因此整个模型的计算量为 <code>72BLshh(1+s/(6h))</code>.</p>
<h1 id="activation-memory">Activation Memory</h1>
<p>激活的定义为在前向传播中产生并且需要在反向传播中进行梯度计算的张量，即不包括模型参数和优化器状态。并且不考虑相对非常小的激活。例如 LayerNorm 层的输入还需要张量每个通道的均值和方差 (大小均为 bs)，由于 h 大小通常超过 1k，因此只考虑输入张量所占激活的大小 bsh，忽略掉 2bs. 假设数据格式为 fp16/bf16，即每个数据占用 2 bytes 的存储空间，需要特殊处理的是 dropout 层的 mak，每个元素均为 unsigned int，只占用 1 byte.</p>
<p>Attention 部分激活占用如下 (共计 11bsh + 5bssa)</p>
<ul>
<li>QKV Linear: 三个线性层需要的输入相同，占用 2bsh bytes.</li>
<li><a href="mailto:Q@K.T">Q@K.T</a>: 需要存储 Q 和 K，占用 4bsh bytes.</li>
<li>Softmax: 需要存储大小为 2bssa bytes 的输入</li>
<li>Softmax droppot: 需要存储一个大小为 bssa bytes 的 mask.</li>
<li>attention@V: 需要存储 dropout 的输出和 V，分别占用 2bssa 和 2bsh bytes.</li>
<li>O Linear: 需要存储注意力的输出，占用 2bsh bytes.</li>
<li>O dropout 需要存储一个大小为 bsh bytes 的 mask;</li>
</ul>
<p>MLP (共计 18bsh): 第一层和第二层的输入分别占用 2bsh 和 8bsh bytes. GeLU 层需要第二层的输入用于反向传播，占用大小为 8bsh bytes. dropout 需要一个大小为 bsh bytes 的 mask.</p>
<p>LayerNorm (共计 4bsh): 需要存储该层的输入，占用 2bsh bytes. 一共有两个 LayerNorm.</p>
<p>加起来就可以得到每个 transformer block 需要激活大小为 bsh(34+5sa/h) bytes.</p>
<h1 id="tensor-parallelsim">Tensor Parallelsim</h1>
<p><a href="https://darkenstar.github.io/2024/10/02/MegatronLM/#Model-Parallel-Transformers">Megatron 张量并行</a> 的思想是将输入进行连续的两个矩阵乘法的第一个按列切分成 t 份，第二个按行切分成 t 份. 在 Transformer block 中体现为利用多头注意力本身的并行性将 Attention 计算中的 QKV 按列进行切分，O Linear 的权重按行进行切分；MLP 中第一个线性层的权重按列进行切分，第二个权重按行进行切分。</p>
<p>在这种并行方式下，前向传播和反向传播均需要进行 2 次 All-Reduce 通信，由于每次 All-Reduce 通信可以看作 Reduce-Scatter + All-Gather, 因此每次每个设备的通信量为 8αbsh bytes，其中 α=(n-1)/n.</p>
<p>对于激活，2*LayerNorm, QKV Linear 的输入, O dropout mask，MLP 第一层的输入和 MLP dropout 不会被切分，因此每个设备每个 block 要占用的激活为 bsh(10+24/n+5as/(hn))</p>
<p>2D Tensor Parallelsim</p>
<p>2D张量并行将激活第一个矩阵的列切分成 m*n 份，第二个权重 (权重形状为 he) 的行被切分成 m 份，列被切分成 n 份。以下图为例，Rank0-Rank2为通信组 x，Rank0-Rank1为 通信组 y. 第一个矩阵经过一次通信组 y 的 AllGather 后与本设备第二个矩阵进行矩阵乘积，得到的部分和经过一次通信组 x 间的ReduceScatter，计算出正确结果。第一次 AllGather 通信每个设备通信的大小为 bsh(n-1)/(mn). 第二次 ReduceScatter 通信每个设备通信的大小为 bse(m-1)/n.</p>
<h1 id="megatron-sequence-parallelsim">Megatron Sequence Parallelsim</h1>
<p>Megatron 张量并行中 LayerNorm 以及 O Linear 和 MLP 之后的 dropouts 在每个设备中都有一个副本。这些模块不需要大量的计算，但需要占用 10bsh bytes 大小的激活内存。<a href="">Megatron-SP</a> 沿着序列维度划分这些模块来减少激活内存，但需要配合 TP 一起使用，本质上是将 TP 中的 All-Reduce 拆成了在 TP 前进行 All-Gather 和在 TP 后进行 Reduce-Scatter. 但除去第一个 LayerNorm 外的每一个模块的激活都得到了切分。Megatron-SP 这里选择每个设备存储自己的部分并在反向传播中插入一次额外的 All-Gather 通信。因此通信量为 10bsh, 每个设备每个 block 需要占用的激活为 bsh/n*(34+5as/h)</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6800d68e35ee4215289de6aa75f01884?method=download&amp;shareKey=e67ffd54e4d1fe7cf3a10e81108af366" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6800d68e35ee4215289de6aa75f01884?method=download&amp;shareKey=e67ffd54e4d1fe7cf3a10e81108af366" alt="Transformer layer with Megatron-SP">
    </a><figcaption>Transformer layer with Megatron-SP</figcaption></figure></p>
<h1 id="pipeline-parallelsim">Pipeline Parallelsim</h1>
<p>流水线张量并行仅仅将 L 个 Transformer block 平均分到 p 个设备上，并没有划分激活所要占用的内存。在考虑 1F1B 策略下 batchsize 进一步被划分成 p 个 micro batch. 第一个 stage 必须存储 p 个 micro batch 的激活。每个 stage 包含 L/p 层，所以无论流水线并行大小 p 如何，第一个 stage 必须存储 p × L/p = L 层的激活值。在 Megatron-LM 中的 interleaving schedule 需要存储 L(1 + (p−1)/(pm)) 层的激活，其中 m 是 interleaving 的数量。</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在使用 output-tensor-deallocation 优化 (输出传到下一个 stage 后就释放) 的情况下，可以为为每个设备节省 bshr 内存，其中 r 是每个设备正在运行的 micro batch 的数量，在第一个 stage r=p 时达到峰值。</p></div>

<h1 id="deepseed-ulysses-sequence-parallel">Deepseed-Ulysses Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/10/21/Deepseed%20Ulysses/">DS-SP</a> 也是利用多头注意力的并行性，首先将输入按序列维度切分到每个设备上，每个设备占有的输入形状为 b*(s/n)*h. 在计算 Attention 之前对 QKV 进行 All-to-All 通信变成按隐藏层维度切分 ((a 要能整除 n))，通信量为 6αbsh/n bytes. 计算完 score@v 之后再进行一次 All-to-All 通信，通信量为 2αbsh/n bytes，总计通信量为 8αbsh/n bytes. 激活占用上 Attention 中 Softmax 及其 dropout mask 和 attention 没有被切分，激活占用量为 bsh(34/n+5sa/h). 因此，它不适合 GQA 和 MQA 情况, GQA 的并行度被限制在了组数，MQA 则完全没法使用。而且由于张量并行也需要在 a 维度上进行划分，SP-Ulysses 和 TP 是冲突的。</p>
<h1 id="ring-attention-sequence-parallel">Ring-Attention Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/09/26/Ring_Attention/#Putting-it-Together">Ring-SP</a> 实际上为环状的 FlashAttention，将输入沿着序列维度切分到每个设备上，在 Attention 计算过程中每个设备向相邻设备通信 KV 并更新自己的 Softmax 矩阵，通信量为 4bsh bytes. 激活占用和 DS-SP 一样为 bsh(34/n+5sa/h).</p>
<h1 id="unified-sequence-parallel">Unified Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/11/14/USP-A%20Unified%20Sequence%20Parallelism%20Approach%20for%20Long%20Context%20Generative%20AI/#Unified-Ulysses-Ring-Sequence-Parallelism">USP</a> 将 SP 进程组分割成两个正交的进程组：SP-Ring 进程组和 SP-Ulysses 进程组。可以将其视为一个 2D mesh ，每一列上运行 SP-Ring，每一行上运行 SP-Ulysses. 具体方法为 QKV 的切分 和 All-to-All 和 DS-Ulysses 相同，然后采用 Ring-Attention 的方式进行计算。如果遇到使用 casual mask 的情况需要加上 balance load 策略，把序列长度分为 2*(ring_degree) 大小，按照 0-&gt;1-&gt;&hellip;-&gt;(ring_degree-1)-&gt;(ring_degree-1)-&gt;&hellip;-&gt;0 的顺序进行分配。USP 消除了 SP-ulysses的头数限制。并且 USP可以通过调整 SP-Ulysses 进程组数目来更好的适应不同带宽的网络结构，可以让 All-to-All 操作在高带宽中运行，而异步 P2P 通信在低带宽部分运行。</p>
<h1 id="comparsion-of-different-parallelsim-in-training">Comparsion of Different Parallelsim in Training</h1>
<table border="1">
  <tr>
    <th rowspan="2"></th>
    <th colspan="4" style="text-align: center;">Communication (FWD+BWD)</th>
    <th rowspan="2">Split Dim</th>
    <th colspan="3" style="text-align: center;">Memory</th>
  </tr>
  <tr>
    <th>Param</th>
    <th>Cost</th>
    <th>Act</th>
    <th>Cost</th>
    <th>P/G</th>
    <th>OS</th>
    <th>Act</th>
  </tr>
  <tr>
    <td>DS-SP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>8*All2All</td>
    <td>(8/N)O(bsh)</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>Ring-SP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>P2P</td>
    <td>4O(bsh)</td>
    <td>L/L</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>DP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>0</td>
    <td>0</td>
    <td>b/b</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>ZeRO1</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>0</td>
    <td>0</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P/N </td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO1</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO2</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>P+(G/N)</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO3</td>
    <td>2*AllGather + ReduceScatter</td>
    <td>18O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>TP</td>
    <td>0</td>
    <td>0</td>
    <td>4*AllReduce</td>
    <td>8O(bsh)</td>
    <td>a/h</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>αA</td>
  </tr>
  <tr>
    <td>Megatron-SP</td>
    <td>0</td>
    <td>0</td>
    <td>6*AllGather + 4*ReduceScatter</td>
    <td>10O(bsh)</td>
    <td>a/h</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
</table>
<h1 id="analysis">Analysis</h1>
<ol>
<li>All2All 通信使得 DS-SP 的通信开销大于 DP. 使用 Ring-SP 时，尽管异步的 P2P 通信是可以重叠的，理想的性能也是只与 DP 相同。因此只有当批 batchsize 不足以进行切分时才考虑使用 SP.</li>
<li>Megatron-SP 通信量高于 DS-SP 和 Ring-SP. SP-Ring 对于 KV 的通信可以与计算重叠。Megatron-SP 的通信量不会随着并行度的增加而减少，而 DS-SP 可以做到。 DS-SP 和 Ring-SP 具有较低的激活通信成本，但需要同步梯度和参数。不过参数通信量相对于激活通信量较小，可以通过计算进行重叠。GQA/MQA 也可以降低它俩的通信成本，而 Megatron-SP 不受影响。</li>
<li>相同配置下使用 USP+Zero3 来代替 Megatron-SP 并不会增加可训练序列的长度。但与 Megatron-SP 相比，USP 能在通过提高并行度来增加可以训练的序列长度。</li>
<li>Megatron-SP 并行维度受限于注意力头数目。USP 可以通过提高 Ring-SP 的并行度来扩展，以在大规模配置下训练更大模型。</li>
</ol>
<h1 id="sora-inference-modeling-analysis-process">Sora Inference Modeling Analysis Process</h1>
<p>我们需要准备模型的输入：</p>
<ol>
<li>隐空间采样的噪声 z，形状与想生成的视频时常和分辨率相关。生成 1s 的视频为 25.5 frames，经过 VAE Encoder 后输出的通道数为 4，帧数会被压缩到 <code>num_frame*5//17</code>，分辨率的长宽分别被压缩到原来的 1/8. 因此 z 的形状应该为 <code>(B, 4, num_frame*5//17, img_size[0]//8, img_size[1]//8)</code>.</li>
<li>输入的 prompt 会经过 DeepFloyd/t5-v1_1-xxl 编码，该编码器最大的 token 数为 300，编码维度为 4096，文本长度不足时会填充到 300. 因此编码后的 prompt 的形状为 <code>(B, 1, 300, 4096)</code>.</li>
<li>当前去噪的时间步 t，形状为 <code>(B, )</code></li>
<li>生成视频的 fps，形状为 <code>(1, )</code></li>
</ol>
<p>还需要准备相关的模型配置，包括 mesh 形状，sub_mesh 的形状，并行策略以及 stage_ids. 如果需要将模型的 transformer block 切分成多段，则需要配置 sub_mesh 和 stage_ids.</p>
<ul>
<li>mesh_shape: (num_x, num_y)</li>
<li>submesh_shape: <code>[(num_x, num_y, loc_x, loc_y), ]</code></li>
<li>stage_ids: <code>[(submesh0_start, submesh0_end), ]</code></li>
<li>strategy: 并行策略</li>
</ul>
<p>然后初始化模型，Sora 的整体结构如下 我们初始化一个 Pipeline(包含整个流程的函数)，它会有一个或多个 Stage 用于保存模型的不同层，与 stage_ids 中对应。我们将模型分解成 Embedding_blocks(PatchEmbed3D, TimestepEmbedder, SizeEmbedder, Captionembedder, t_block), STDiT3_blocks 和 T2IFinalLayer. 将这个分解函数作为 Pipeline 的 sharding_func.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB9688e46ada2523df1ec522a7649be19a?method=download&amp;shareKey=991eba9aad6eca9f41599d2ad4f75c34" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB9688e46ada2523df1ec522a7649be19a?method=download&amp;shareKey=991eba9aad6eca9f41599d2ad4f75c34" alt="Open-Sora">
    </a><figcaption>Open-Sora</figcaption></figure></p>
<h2 id="init-pipeline">Init Pipeline</h2>
<p>我们需要根据配置以及 PipePatch 并行度和 SP 并行度初始化 Pipeline. 这其中会根据 stage_ids 分配每个 Stage 保存模型的哪些层以及对应的 submesh 大小。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">construct_stages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">submeshes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">],</span> <span class="n">stages_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># construct layers for each stage</span>
</span></span><span class="line"><span class="cl">    <span class="n">first_part</span><span class="p">,</span> <span class="n">module_list</span><span class="p">,</span> <span class="n">last_part</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stages_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">submesh</span> <span class="o">=</span> <span class="n">submeshes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage_id</span> <span class="o">=</span> <span class="n">stages_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get stage layers from user config stage ids in module list</span>
</span></span><span class="line"><span class="cl">        <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">module_list</span><span class="p">[</span><span class="n">stage_id</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">stage_id</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">first_part</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># concat module first part(if exists) bef module list to stage_0</span>
</span></span><span class="line"><span class="cl">            <span class="n">layers</span> <span class="o">=</span> <span class="n">first_part</span> <span class="o">+</span> <span class="n">layers</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">num</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">last_part</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># concat module last part(if exists) aft module list to last stage</span>
</span></span><span class="line"><span class="cl">            <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">last_part</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># deepcopy module for xla device tracing use</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage_module</span> <span class="o">=</span> <span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">Stage</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stage_module</span><span class="p">,</span> <span class="n">submesh</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">modules</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="write-sharding-function">Write Sharding Function</h2>
<p>要根据选择的不同的并行策略对每个 Stage 的模型权重，输入，输出进行切分。这里同样我们单独处理 Embedding_blocks, STDiT3_blocks 和 T2IFinalLayer. 让 stage0 包括对 Embedding_blocks 的处理，stage(N-1) 包括对 T2IFinalLayer 的处理。需要注意的是 DS-ulysses 我们需要对 <a href="mailto:Q@K.T">Q@K.T</a> 的结果 和 S@V 的结果也进行切分 SPMD 才会插入正确的 All2All，因此这部分只能放在网络的 forward 里面进行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_one_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># first 5 modules are embedding layers</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_embedding</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">total_len</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">    <span class="n">shard_sora_final</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_first_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_embedding</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_last_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_len</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># skip norm layer mark sharding</span>
</span></span><span class="line"><span class="cl">    <span class="n">shard_sora_final</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">total_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="construct-pipeline">Construct Pipeline</h2>
<p>然后为了处理多 stage 的情况，我们需要保存每个 stage 的输入和输出的形状。这一步相当于放到 cuda 上重走一遍整个模型的 forward，记录下每一层输入和输出的形状，保存为 json 一遍。实际上对于每个固定生成大小的视频进行一次就行，下次直接读取这个文件。因为现在都采用 <a href="https://facebookresearch.github.io/xformers/components/ops.html">xformers.ops.memory_efficient_attention</a>，需要输入张量在 cuda 上，我们需要手动在模型的 forward 函数中写一个 navie 的 attention 计算流程好让 torch_xla 能对张量进行跟踪。</p>
<h2 id="trace-mhlo-graph">Trace mhlo Graph</h2>
<p>根据上一步得到的每个 Stage 的输入形状，创建输入张量，放入 xla_device 上，执行 forward. 最后导出输出的 mhlo 计算图。这里需要注意第一个 stage 包含多个非连续的模块，因此需要单独处理，最后一个 stage 最后一层的输入与其他 block 不同，因此也要单独处理。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">trace_stage_mhlo_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">check_res</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    trace stage nn modules to mhlo graph
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># (NOTE): construct xla mesh before trace tensors generate,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># i.e., before any xla device call to avoid xla computation client construct</span>
</span></span><span class="line"><span class="cl">    <span class="n">xla_mesh</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">xla_mesh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_construct_stage_xla_mesh</span><span class="p">()</span>  <span class="c1"># create mesh from submesh info</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create xla device trace tensors, move module to xla device</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_trace_tensors</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">y_embedded</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">t_mlp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">t_mlp</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t_mlp</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">mod</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>  <span class="c1"># first load to cpu</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># get pipeline exec mode</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">exec_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">exec_mode</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># load lora cofifg</span>
</span></span><span class="line"><span class="cl">    <span class="n">lora_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">lora_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Enter trace mhlo graph for stage: &#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Trigger shard func to mark sharding the model</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_strategy</span><span class="p">,</span> <span class="n">xla_mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">exec_mode</span> <span class="o">==</span> <span class="n">EXEC_MODE</span><span class="o">.</span><span class="n">INFER</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># set stage name &amp; dump file path</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_set_stage_name_dump_file</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">exec_mode</span><span class="p">,</span> <span class="s2">&#34;fw&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_sampling_steps</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl">        <span class="n">timesteps</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">i</span> <span class="o">/</span> <span class="n">num_sampling_steps</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_timesteps</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_sampling_steps</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># FIXME: 原先是为每个stage单独生成trace_tensor, 现在要把上一个的结果传给下一个 stage</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#for i in range(30):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">pipeline_patches_height_list</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">pipeline_patches_height_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,:]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">xla_mesh</span><span class="p">)</span>  <span class="c1"># outputs is a list</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">,</span> <span class="n">xla_mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">check_res</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># check xla results compared to gpu results</span>
</span></span><span class="line"><span class="cl">            <span class="n">check_result_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># use torch xla _get_xla_tensors_hlo interface</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># to eliminate redundant live tensors as ret values</span>
</span></span><span class="line"><span class="cl">            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;XLA_DUMP_POST_OPTIMIZATIONS&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;true&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch_xla</span><span class="o">.</span><span class="n">_XLAC</span><span class="o">.</span><span class="n">_get_xla_tensors_hlo</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="analyze-mhlo-graph">Analyze mhlo Graph</h2>
<p>接下来我们要遍历上一步得出的 mhlo 图。</p>
<h3 id="opview">OpView</h3>
<p>从根节点的 ir 开始遍历上一步导出的整个计算图。根据传入 ir 的类型定义调用对应的 visit 函数读取其属性进行操作。主要通过 rsqrt 的位置来划分一个 Transformer block 中第几个 dot 和 dot_general 对应的是什么操作。对于 Sora 来说划分情况如下。这里需要注意的是 mhlo 图记录的是拓扑排序的顺序，不是程序顺序执行的顺序，因此第一个 block 会掺杂着 Embedding_blocks 的一些 dot 操作。因此我们从第二个 block 的第一个 rsqrt 位置开始统计。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">collect_rms_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">rms_collector</span> <span class="o">=</span> <span class="n">RMSCollector</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">rms_collector</span><span class="o">.</span><span class="n">visit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span> <span class="o">=</span> <span class="n">rms_collector</span><span class="o">.</span><span class="n">rms_locs</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># construct attention block &amp; ffn block ranges</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># exclude the rsqrt in T2IFinalLayer</span>
</span></span><span class="line"><span class="cl">  <span class="n">att_rm_locs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">att_rm_locs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># a block has 4 rsqrt, start from 2nd block to avoid embedding</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cro_attn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">att_rm_locs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># ORG: range(8, len(att_rm_locs), 4): </span>
</span></span><span class="line"><span class="cl">      <span class="n">start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">4</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">          <span class="n">end</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">end</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ffn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><table>
  <thead>
      <tr>
          <th>module</th>
          <th>operator</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td><code>RMSNorm(x)</code></td>
      </tr>
      <tr>
          <td><strong>Self Attention</strong></td>
          <td><code>dot(x, qkvLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(q)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(q, k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(s, v)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(attn, oLinear.weight)</code></td>
      </tr>
      <tr>
          <td><strong>Cross Attention</strong></td>
          <td><code>dot(x, qLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(y, kvLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(q, k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(s, v)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(attn, oLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(x) </code></td>
      </tr>
      <tr>
          <td><strong>Feed Forward Network</strong></td>
          <td><code>dot(x, upLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(x, downLinear.weight)</code></td>
      </tr>
  </tbody>
</table>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">visit_dot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">dot_lineno</span> <span class="o">=</span> <span class="n">_parse_loc_lineno</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">spt_att_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">cro_att_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cro_attn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">spt_qkv_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">ffn_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># lie in RMS ops closed attention block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">spt_att_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">spt_att_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#import pdb;pdb.set_trace()</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">cro_att_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">cro_att_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cro_att_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># lie ffn block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">spt_qkv_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">spt_qkv_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># pixart pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>                 
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Traversal of one block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_att_dot_cnt</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> \
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">attention_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># reset each block level counters</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_att_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">generic_visit</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>保存好一个 Transformer block 中每个 dot 或 dotgeneral 对应的是什么操作后，我们便可以访问这个 ir. 这里需要注意只要两个相乘的矩阵有一个是二维张量 (比如线性层的权重)，mhlo 都会将另一个 reshape 成二维张量。dot 算子 (<code>jaxlib.mlir.dialects._mhlo_ops_gen.DotOp</code>) 两个操作数都是二维的张量，qkvLinear 对应的是第一个 dot 操作。左操作数的 shape 为 <code>(BST,3C)</code>. 当两个相乘的矩阵都是 3 维及以上张量的时候就会生成 dot_general 该算子的两个相乘的矩阵都会被 reshape 成三维张量。Self-Attention 的第一个 dot_general 左操作数的 shape 为 <code>(BTN_A,S,C)</code>. 这样我们就可以得到 <code>BT=(BST)/S, N_A=(BTN_A)/(BT)</code>. 同样我们可以得到 OLinear, FFN 中 upLinear 和 downLinear 权重的形状. 以及 Cross-Attention 模块的对应信息。由于之前遍历是从第二个 block 开始的，因此总层数要 ＋1. 最后将得到的参数打包成一个字典返回。</p>
<h3 id="communication-view">Communication View</h3>
<p>我们以同样的方式定义各种集合通信算子的 visit 函数用于评估该算子的通信量，遍历到对应的 ir 后调用它。</p>
<p>AllReduce 将所有的数据通过规约操作集成到各个设备中。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6e4d9c026bc0632af5040321998fb3ab?method=download&amp;shareKey=f901430ac6bfa781d0b462f0170981d3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6e4d9c026bc0632af5040321998fb3ab?method=download&amp;shareKey=f901430ac6bfa781d0b462f0170981d3" alt="AllReduce">
    </a><figcaption>AllReduce</figcaption></figure></p>
<p>在 Ring-AllReduce 的 ReduceScatter 步骤中，每个进程发送 M 个元素 N-1 次，总共为 M(N-1). 在 AllGather 步骤中，每个进程发送它计算的块的结果。这是额外的 M 个元素发送了 N-1 次。总的通信量加起来是 2M(N-1).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB69d2b3957cd1863481bff0e785dc9a82?method=download&amp;shareKey=32e60903bafe5dbf240af91c67486e1b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB69d2b3957cd1863481bff0e785dc9a82?method=download&amp;shareKey=32e60903bafe5dbf240af91c67486e1b" alt="Ring-AllReduce">
    </a><figcaption>Ring-AllReduce</figcaption></figure></p>
<p>All-Gather 示意图如下，每个设备开始拥有初始的一部分数据，通信后每个设备都有一份完整的数据。总的通信量为 M(N-1).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe7e6e7a1230ed9ba7ba037556e489d51?method=download&amp;shareKey=5afdf2b669a500a6844aa9e281fe1ac3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe7e6e7a1230ed9ba7ba037556e489d51?method=download&amp;shareKey=5afdf2b669a500a6844aa9e281fe1ac3" alt="AllGather">
    </a><figcaption>AllGather</figcaption></figure></p>
<p>All2All 示意图如下，每个设备把自己的第 i 块数据发送给第 i 个设备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBddc785dcc80dd741fc1f469a85823cd4?method=download&amp;shareKey=085c0a5681116b4e1683d4d6ae5d080f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBddc785dcc80dd741fc1f469a85823cd4?method=download&amp;shareKey=085c0a5681116b4e1683d4d6ae5d080f" alt="All2All">
    </a><figcaption>All2All</figcaption></figure></p>
<p>基于 Bruck 算法的 All2All 流程如下</p>
<ol>
<li>局部循环移位 (Local Shift of Data-Blocks)
每个进程将其本地的数据块重新排列，进行初始的循环移位。对于进程 p 和数据块索引 i: R[i]=S[(p+i)%P]. 其中 S[i] 是进程本地初始的数据，R[i] 是移位后的数据。</li>
<li>全局通信 (Global Communication)
一共进行 log(P) 次通信。
每一步中每个进程将一部分数据发送给相邻的进程，并接收相邻进程发送的数据。若数据块索引 i 用 radix-2 表示的第 k 位为 1，则数据块会被发送到目标进程。
对于进程 p: 发送数据到进程 ((p + 2^k) % P)，接收来自进程 ((p - 2^k) % P) 的数据。
每次发送后，进程将接收到的数据更新到其本地数据中。</li>
<li>局部逆向移位 (Local Inverse Shift of Data-Blocks)
在完成所有全局通信之后，每个进程执行逆向移位，以恢复数据块的正确顺序。对于每个数据块索引 i: R[i]=R[(p−i+P)%P]</li>
</ol>
<p>在进程是 2 次幂的情况下每个设备每次要通信 M*P/2大小的数据，总共为 MPlog(P)/2.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa083a4c6002019e62b23c0b24b59a812?method=download&amp;shareKey=58f1b8055307d53e43ce86b9e1762989" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa083a4c6002019e62b23c0b24b59a812?method=download&amp;shareKey=58f1b8055307d53e43ce86b9e1762989" alt="Example of the Bruck Algorithm with 4 Processes">
    </a><figcaption>Example of the Bruck Algorithm with 4 Processes</figcaption></figure></p>
<h3 id="tflops-view">TFLOPS View</h3>
<p>计算量主要分成两种，element-wise 的操作计算量为元素个数。两个形状分别为 mxk 和 kxn 的矩阵相乘计算量为 2mkn. 被计入 element-wise 操作的算子有 add, subtract, multiply, divide, rsqrt, negate, exponential. 被计入矩阵乘法的算子有 dot, dot_general.</p>
<h2 id="performance-analysis">Performance Analysis</h2>
<p>我们根据提取出的 Transformer block 的信息送入性能分析器进行分析. tx8 的配置如下</p>
<table>
  <thead>
      <tr>
          <th>Parameter</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>TILE_NUM</td>
          <td>16</td>
      </tr>
      <tr>
          <td>SRAM (MB)</td>
          <td>3</td>
      </tr>
      <tr>
          <td>NOC BW (GB/s)</td>
          <td>128</td>
      </tr>
      <tr>
          <td>DRAM BW (GB/s)</td>
          <td>100</td>
      </tr>
      <tr>
          <td>DRAM LATENCY (us)</td>
          <td>0.1</td>
      </tr>
      <tr>
          <td>GEMM (TFLOPS)</td>
          <td>8</td>
      </tr>
      <tr>
          <td>VECTOR (TOPS)</td>
          <td>0.0625</td>
      </tr>
      <tr>
          <td>HOP LATENCY (us)</td>
          <td>0.01</td>
      </tr>
  </tbody>
</table>
<p>根据提取出的信息构建的 STDiT 的 spt_blk, tmp_blk, cross_blk 的参数字典如下.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                  <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_spt&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                  <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_tmp&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span><span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_cro&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;D_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;D_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FD&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FD&#34;</span><span class="p">]}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>根据这些参数再构建每个层的输入输出形状，计算类型和计算量，以 <code>Gate_ResAdd</code> 为例:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">GB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Gate_ResAdd</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">  Construct each op after MHSA on the config file
</span></span></span><span class="line"><span class="cl"><span class="s1">  &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># {name:{type:&#34;&#34;, size:&#34;&#34;, ishape:[], wshape:[]/None, oshape:[]}}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">construct_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">construct_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">GB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;D_O&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_weight_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;D_O&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_output_shape</span> <span class="o">=</span> <span class="n">ResAdd_input_shape</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_compute</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="n">GB</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="o">+</span><span class="s2">&#34;_&#34;</span><span class="o">+</span><span class="s2">&#34;ResAdd&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;ResAdd&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;Vector&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;ishape&#34;</span><span class="p">:</span> <span class="n">ResAdd_input_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;wshape&#34;</span><span class="p">:</span> <span class="n">ResAdd_weight_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;oshape&#34;</span><span class="p">:</span> <span class="n">ResAdd_output_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;compute&#34;</span><span class="p">:</span> <span class="n">ResAdd_compute</span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>就像这样构建整个 Transformer block 的所有操作</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">STDIT2_block</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># {name:{type:&#34;&#34;, size:&#34;&#34;, ishape:[], wshape:[]/None, oshape:[]}}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">construct_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">construct_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_spt&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_tmp&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span><span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_cro&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;D_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;D_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FD&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FD&#34;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cross_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cross_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">mlp_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;mlp&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block</span> <span class="o">=</span> <span class="n">FFN_block</span><span class="p">(</span><span class="n">cross_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">mlp_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;mlp&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">op_list</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">spatial_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="bp">self</span><span class="o">.</span><span class="n">temporal_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="bp">self</span><span class="o">.</span><span class="n">cross_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">op_dict</span> <span class="ow">in</span> <span class="n">op_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">op_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后就可以将构建好的 ops 放入 mapper 进行分析。刚才那些操作会被分成 3 类 <code>vector_mapper</code>, <code>gemm_auto_opt_mapper</code> 和 <code>flashatten_mapper</code>. 我们根据操作的类型送入对应的 mapper 进行分析，具体如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">STDIT2_mapper</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">QKV_fusion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">preset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</span></span><span class="line"><span class="cl">  <span class="n">Layers</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;L&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">ops</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">ops</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;=========================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Spatial Branch Mapping ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  =========================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">TmTn</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_RMSNorm&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_RMSNorm&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_ResAdd&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;==========================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Temporal Branch Mapping ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ==========================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>  <span class="c1"># 切分 30 份也无法满足SRAM要求</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_RMSNorm&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_RMSNorm&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_ResAdd&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;====================================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Cross Branch Mapping 2x per block ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ====================================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#mapping_result[&#39;spatial_RMSNorm&#39;]= vector_mapper(ops[&#39;spatial_RMSNorm&#39;],arch,splits=None,details=details)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_ResAdd&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">  <span class="c1"># HACK: Gate_ResAdd *2 了, cross 无gate 这里无 _2</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;====================================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Feed Forward Network 2x per block ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ====================================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;FFNup&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span><span class="n">fusion_op2</span><span class="o">=</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;SiLU&#39;</span><span class="p">],</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># mapping_result[&#39;FFNgate&#39;] = gemm_auto_opt_mapper(ops[&#39;FFNgate&#39;], arch, TmTn=TmTn, details=details)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># mapping_result[&#39;Hadamard&#39;] = vector_mapper(ops[&#39;Hadamard&#39;], arch, splits=None)</span>
</span></span><span class="line"><span class="cl">  <span class="n">TmTn</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>mapper 会遍历所有可能的切分策略放入 tx8 执行并选择最好的那一个。对于 vector 类型的算子只会沿着 sequence 维度切分；对于 GEMM 算子则会沿着 m, k, n 维度都进行切分；对于 flash-attention 的切分则与原算法相同，外循环遍历 K, V 的每一块，内循环遍历 Q 的每一块。这样就可以得到每个 tx8 上最优的切分方式对应的通信用时，计算用时和利用率。再用之前统计出的每个 die 上通信量除以 die2die 带宽得到通信用时，由此得到总的推理用时。</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
