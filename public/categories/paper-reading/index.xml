<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Paper Reading on WITHER</title>
    <link>http://localhost:1313/categories/paper-reading/</link>
    <description>Recent content in Paper Reading on WITHER</description>
    <generator>Hugo -- 0.148.1</generator>
    <language>en</language>
    <copyright>2024-2025 WITHER</copyright>
    <lastBuildDate>Fri, 19 Sep 2025 09:20:48 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/paper-reading/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>InternVideo2.5</title>
      <link>http://localhost:1313/blogs/internvideo2.5/</link>
      <pubDate>Thu, 10 Jul 2025 08:40:52 +0800</pubDate>
      <guid>http://localhost:1313/blogs/internvideo2.5/</guid>
      <description>Technical report reading of InternVideo2.5</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>这篇文章的核心目标是提升多模态大语言模型（MLLM）处理视频的能力，特别是处理长且信息丰富的视频上下文（Long and Rich Context, LRC）的能力 。目前，主流的 MLLM 在处理长视频时往往会遇到困难，要么因为计算资源不堪重负而内存溢出，要么在长时序中丢失关键的细节信息，导致理解和推理能力下降。这篇论文的工作旨在增强模型的处理长视频 (length) 的能力和捕捉精细细节 (fineness) 的能力。</p>
<p>文章提出了两大核心技术：Hierarchical Token Compression (HiCo) &amp; Task Preference Optimization (TPO).</p>
<p>HiCo 主要解决处理长视频的问题。视频中存在大量的冗余信息，比如相邻帧之间背景变化很小，或者在一个长镜头中语义信息是相似的。HiCo 剔除这些冗余，保留核心信息。它通过一个三步走的非学习性过程来实现：</p>
<ol>
<li>自适应时间采样：根据视频的长短和内容特性，动态调整采样频率。短视频（如动作片段）需要密集采样来捕捉细节，而长视频（如电影）则稀疏采样以把握事件脉络。</li>
<li>Spatiotemporal Token Merging: 它使用了一种名为ToMe (Token Merging) 的技术，该技术通过计算 token 之间的语义相似度，将相似的进行合并。把视频中意思相近的画面信息捏在一起，而不是像传统方法那样粗暴地丢弃或平均。论文特别指出，与需要大量额外参数和复杂训练的Q-Former 等压缩方法相比，ToMe 是即插即用的，效率极高。</li>
<li>Multimodal Token Dropout：在模型的深层，根据注意力权重动态丢弃那些与当前任务不太相关的视觉通证，进一步精简信息流，让模型能更专注于核心内容 。</li>
</ol>
<p>通过 HiCo，模型可以在不牺牲过多性能的前提下，处理更长的视频序列。实验结果极具说服力：在大海捞针（Needle-in-a-Haystack）测试中，基础模型 InternVL2.5 在处理 500 帧视频时就已经很吃力，超过1000 帧便会内存溢出 。而应用了 HiCo 的 InternVideo2.5，不仅能轻松处理超过 5000 帧的视频，还能在 3000 帧的长度内保持极高的信息检索准确率。可以说记住比原来长 6 倍以上的视频并非虚言。</p>
<p>TPO 主要解决信息丰富的问题，也就是提升模型对精细视觉细节的感知能力。其核心思想是让专家来教通才。通用的 MLLM 虽然能力全面，但在特定视觉任务上（如物体分割、时间定位）往往不如那些专门训练的专家模型。TPO通过 Direct Preference Optimization (DPO )技术，将这些专家模型对特定任务的偏好（即更准确的输出）注入到 MLLM 中。</p>
<p>具体来说，它为 MLLM 增加了专门的“任务头”（Task Head），比如用于时间定位的</p>
<p>Temporal Head和用于实例分割的Mask Head 。在训练时，不仅优化MLLM的基础对话能力，还利用特定任务的数据集（如分割、定位数据集）来优化这些任务头的表现。这样一来，MLLM就好像学会了在需要的时候“调用”这些专家能力。</p>
]]></content:encoded>
    </item>
    <item>
      <title>DeepSeek-V3 Technical Report</title>
      <link>http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/</link>
      <pubDate>Fri, 20 Jun 2025 16:37:06 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepseek/deepseek-v3technicalreport/</guid>
      <description>Paper Reading of DeepSeek-V3 Technical Report</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>DeepSeek-V3 (671B) 是 MoE 模型，每个 token 会激活 37B 的参数。采用 Multi-head Latent Attention (MLA) 和自创的 DeepSeek MoE 结构，在这两篇文章中已经做过讲解。同时采用了 auxiliary-loss-free 策略来实现专家负载平衡并且使用了 Multi-token Prediction (MTP) 来加速训练。整个预训练数据集一共有 14.8T tokens，通过 Suprvised Fine-Tuning (SFT) 和 强化学习来加强性能。训练时长为 2.788M H800 GPU 小时。</p>
<h1 id="1-introduction">1. Introduction</h1>
<p><strong>模型架构创新:</strong></p>
<ul>
<li>Multi-head Latent Attention (MLA): 加速推理。</li>
<li>DeepSeek MoE: 减少训练开销。</li>
</ul>
<p><strong>增强模型能力的策略:</strong></p>
<ul>
<li>auxiliary-loss-free: 实现负载平衡。</li>
<li>Multi-token Prediction (MTP): 增强模型表现。</li>
</ul>
<p><strong>提高训练效率的方法:</strong></p>
<ul>
<li>FP8 混合精度训练: 加速训练和减少 GPU 内存使用。</li>
<li>DualPipe 流水线并行算法: 减少气泡并且在训练时候通过计算掩盖了大部分通信。</li>
<li>新的节点间 All-to-all 通信算子: 更好地利用 InfiniBand (IB) and NVLink 带宽。</li>
<li>优化了内存后 DeepSeek-V3 训练没有使用 TP.</li>
</ul>
<p><strong>训练过程:</strong></p>
<ul>
<li>pre-training: 在 14.8T tokens 上进行。</li>
<li>stage 1: 扩展最大上下文长度到 32K.</li>
<li>stage 2: 扩展最大上下文长度到 128K.</li>
<li>post-training: 使用 Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) 并且蒸馏了 DeepSeek-R1 系列模型来获得推理能力。</li>
</ul>
<p>DeepSeek-V3 上训练每 1T token 只需要180K H800 GPU小时，即在 2048 个 H800 GPU 的集群上需要 3.7 天。</p>
<h1 id="2-architecture">2. Architecture</h1>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB4af4bca65c0e55a9290c2e4d808cb6b2?method=download&amp;shareKey=d8c9ce5c9b545f9d954d769f4e520fed" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB4af4bca65c0e55a9290c2e4d808cb6b2?method=download&amp;shareKey=d8c9ce5c9b545f9d954d769f4e520fed" alt="Illustration of the Basic Architecture of DeepSeek-V3">
    </a><figcaption>Illustration of the Basic Architecture of DeepSeek-V3</figcaption></figure></p>
<h2 id="21-mla">2.1 MLA</h2>
<p>在相关文章中已经介绍。</p>
<h2 id="22-deepseekmoe-with-auxiliary-loss-free-load-balancing">2.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing</h2>
$$
\begin{align}
\mathbf{h}'_t &= \mathbf{u}_t + \sum_{i=1}^{N_s} \text{FFN}_i^{(s)} (\mathbf{u}_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}_i^{(r)} (\mathbf{u}_t),  \\
g_{i,t} &= \frac{g'_{i,t}}{\sum_{j=1}^{N_r} g'_{j,t}},  \\
g'_{i,t} &= \begin{cases} s_{i,t}, & s_{i,t} \in \text{Topk}(\{s_{j,t}|1 \le j \le N_r\}, K_r), \\ 0, & \text{otherwise}, \end{cases} \\
s_{i,t} &= \text{Sigmoid}(\mathbf{u}_t^T \mathbf{e}_i),
\end{align}
$$<p><strong>Basic Architecture of DeepSeekMoE.</strong> 在相关文章中已经介绍，V3 和 V2 不同之处在于使用sigmoid函数来计算亲和分数，并在归一化所有选定的亲和分数之间来产生门控制值。</p>
<p><strong>Auxiliary-Loss-Free Load Balancing.</strong> 为每个专家引入一个偏差项 $b_i$，并将其与相应的亲和力分数$s_{i,t}$ 相加，以确定 top-K 路由</p>
$$
g'_{i,t} = \begin{cases} s_{i,t}, & s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \le j \le N_r\}, K_r), \\ 0, & \text{otherwise}. \end{cases} \tag{16}
$$<p><strong>这个偏置项仅用于路由</strong>，用于和 FFN 输出相乘的门控值还是来自于原先的原先的 $s_{i,t}$. 在每一步结束时，如果其对应的专家过载， DeepSeek-V3 将偏差项减少 $\gamma$ (一个超参数，被称作 bias update speed)，如果其对应的专家负载不足， DeepSeek-V3 将偏差项增加 $\gamma$.</p>
<p>V3 使用 sequence-wise balance loss，类似于 V2 中 Expert-Level Balance Loss。 不同之处在于使用归一化的亲和分数。</p>
$$
\begin{align}
\mathcal{L}_{\text{Bal}} &= \alpha \sum_{i=1}^{N_r} f_i P_i, \\
f_i &= \frac{N_r}{K_r T} \sum_{t=1}^{T} 1 (s_{i,t} \in \text{Topk}(\{s_{j,t}|1 \le j \le N_r\}, K_r)),  \\
s'_{i,t} &= \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}},  \\
P_i &= \frac{1}{T} \sum_{t=1}^{T} s'_{i,t} 
\end{align}
$$<p><strong>Node-Limited Routing.</strong> 对于每个 token 计算每个节点计算亲和度分数前 $\frac {K_r}M$ 的专家求和，选取前 $M$ 个作为路由节点。</p>
<p><strong>No Token-Dropping.</strong> 训练和推理中均不采用。Token Dropping指的是在 MoE 当路由到某个专家的 Token 数量超过了该专家的处理容量时，系统会故意丢跳过那些超出容量的 token，不让它们被这个专家处理。这些 token 通常会通过一个残差连接，将其输入时的状态直接传递到下一层。</p>
<h2 id="23-multi-token-prediction">2.3 Multi-Token Prediction</h2>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBcc8ba55fb1b39401e8287ae38a50d829?method=download&amp;shareKey=3fd271dd7007a3fc5b6b0939e165869f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBcc8ba55fb1b39401e8287ae38a50d829?method=download&amp;shareKey=3fd271dd7007a3fc5b6b0939e165869f" alt="Illustration of Multi-Token Prediction (MTP) implementation">
    </a><figcaption>Illustration of Multi-Token Prediction (MTP) implementation</figcaption></figure></p>
<p>与 Gloeckle 等人使用独立的输出头并行预测 D 个额外 token 不同， DeepSeek-V3 顺序预测额外 token 并在每个预测深度保持完整的因果链。</p>
<p><strong>MTP Modules.</strong> DeepSeek-V3 使用 D 个顺序模块来预测 D 个额外 token。第 k 个 MTP 模块由一个共享嵌入层 Emb(·)、一个共享输出头 OutHead(·)、一个 Transformer 块 TRM_k(·) 和一个投影矩阵 $M_k \in \mathbb{R}^{d \times 2d}$ 组成。对于第 <em>i</em> 个输入 token  $t_i$，在第 <em>k</em> 个预测深度， DeepSeek-V3 首先通过线性映射<strong>结合第 <em>i</em> 个 token 在第 (k−1) 个深度上的表示 $h_i^{k-1} \in \mathbb{R}^d$ 和第 (i+k) 个 token 的嵌入 $\text{Emb}(t_{i+k}) \in \mathbb{R}^d$</strong></p>
$$
h_t^k = M_k[\text{RMSNorm}(h_t^{k-1}); \text{RMSNorm}(\text{Emb}(t_{i+k}))], \tag{21}
$$<p>其中 [·;·] 表示拼接操作。特别地，当 k = 1 时，$h_t^{k-1}$ 指的是由主模型给出的表示。请注意，对于每个 MTP 模块，其嵌入层与主模型共享。合并后的 $h_t^k$ 作为第 k 个深度上 Transformer 块的输入，以在当前深度生成输出表示 $h_t^k$:
</p>
$$
h_{1:T-k}^k = \text{TRM}_k(h_{1:T-k}^k), \tag{22}
$$<p>
其中 T 代表输入序列长度，而 $_{1:T-k}$ 表示切片操作 (包含左右边界)。最后，将 $h_T^k$ 作为输入，共享输出头将计算第 k 个额外预测 token 的概率分布 $p_{t+k+1}^k \in \mathbb{R}^V$，其中 $V$ 是词汇表的大小:
</p>
$$
p_{t+k+1}^k = \text{OutHead}(h_T^k). \tag{23}
$$<p>
输出头 OutHead(·) 将表示线性映射到 logits，随后应用 Softmax 函数来计算第 k 个额外 token 的预测概率。此外，对于每个 MTP 模块，其输出头与主模型共享。DeepSeek-V3 维持预测因果链的原则与 EAGLE (Li et al., 2024b) 的原则相似，但其主要目标是推测解码 (Leviathan et al., 2023; Xia et al., 2023)，DeepSeek-V3 而 利用 MTP 来改进训练。</p>
<p><strong>MTP Training Objective.</strong> 对于每个预测深度，计算一个交叉熵损失 $\mathcal{L}_{\text{MTP}}^k$：</p>
$$
\mathcal{L}_{\text{MTP}}^k = \text{CrossEntropy}(P_{2+k:T+1}^k, t_{2+k:T+1}) = -\frac{1}{T}\sum_{i=2+k}^{T+1} \log p_i^k[t_i], \tag{24}
$$<ul>
<li>$T$: 输入序列长度</li>
<li>$t_i$: 第 i 个位置的真实 (ground-truth) token</li>
<li>$p_i^k[t_i]$: 代表第 $k$ 个 MTP 模块对于第 $i$ 个位置的预测中，赋给真实正确 token $t_i$** 的概率。</li>
</ul>
<p>最后计算所有深度的 MTP 损失的平均值，并乘以一个加权因子 $\lambda$ 来获得总体的 MTP 损失 $\mathcal{L}_{\text{MTP}}$，作为 DeepSeek-V3 的一个额外训练目标：</p>
$$
\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{\text{MTP}}^k. \tag{25}
$$<p><strong>MTP in Inference.</strong> MTP 策略主要旨在提升主模型的性能，因此在推理过程中可以直接丢弃 MTP 模块，主模型可以独立且正常地运作。此外，也可以重新利用这些 MTP 模块进行推测解码 (speculative decoding) ，以进一步改善生成延迟。</p>
<h1 id="3-infrastructure">3. Infrastructure</h1>
<h2 id="31-compute-clusters">3.1 Compute Clusters</h2>
<p>DeepSeek-V3 在 2048 NVIDIA H800 GPU 组成的集群上训练。每个节点有 8 张通过 NVLink 和 NVSwitch 连接的 H800. 节点之间通过 InfiniBand (IB) 连接。</p>
<h2 id="32-training-framework">3.2 Training Framework</h2>
<p>DeepSeek-V3 使用 16-way Pipeline Parallelism (PP), 横跨 8 个节点间的 64-way Expert Parallelism (EP) 以及 ZeRO-1 Data Parallelism (DP). 训练期间不使用 Tensor Parallelism (TP).</p>
<h3 id="321--dualpipe-and-computation-communication-overlap">3.2.1  DualPipe and Computation-Communication Overlap</h3>
<p>DeepSeek-V3 中专家并行导致的跨节点 All-to-all 通信所对应的计算通信比接近 1:1，效率很低。</p>
<p>DualPipe 的核心思想是在一对独立的 forward &amp; backword chunk 内部重叠计算和通信。具体来说，将每个 chunk 分为四个部分: attention, all-to-all dispatch， MLP 和 all-to-all combine. 特别地，对于 backword chunk, attention 和 MLP 都像在 ZeroBubble (Qi et al., 2023b) 中一样，被进一步拆分为两个部分：针对输入的反向传播和针对权重的反向传播。此外，还有一个流水线并行通信组件。如下图所示，对于一对 forward &amp; backword chunk，重排这些组件，并手动调整专用于通信与计算的 GPU SM 的比例。通过这种重叠策略，可以确保 all-to-all 和 PP 通信在执行期间都能够被完全隐藏。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBf348c55ccc87c5e4e388f2df2f18fb76?method=download&amp;shareKey=849010c74a6772b18fff8ca8d5550e8c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBf348c55ccc87c5e4e388f2df2f18fb76?method=download&amp;shareKey=849010c74a6772b18fff8ca8d5550e8c" alt="Overlapping Strategy for a Pair of Individual Forward and Backward Chunks">
    </a><figcaption>Overlapping Strategy for a Pair of Individual Forward and Backward Chunks</figcaption></figure></p>
<p>基于这种高效的重叠策略，完整的 DualPipe 调度方案如下图所示。它采用了一种双向流水线调度，即同时从流水线的两端送入 micro-batches，从而使得一大部分通信可以被完全重叠。这种重叠还确保随着模型规模的进一步扩大，只要保持恒定的计算与通信比率，仍然可以在节点间使用细粒度的专家 (fine-grained experts)，同时实现接近于零的all-to-all通信开销。具体的分析见相关文章。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" alt="DualPipe Schedule">
    </a><figcaption>DualPipe Schedule</figcaption></figure></p>
<h3 id="322-efficient-implementation-of-cross-node-all-to-all-communication">3.2.2 Efficient Implementation of Cross-Node All-to-All Communication</h3>
<p>DeepSeek-V3 定制了高效的跨节点 All-to-all 通信内核，以节省专用于通信的 SM 数量。内核的实现与MoE门控算法和 DeepSeek-V3 集群的网络拓扑共同设计。集群中跨节点 GPU 通过 IB(50 GB/s) 全连接，节点内通信通过 NVLink(160GB/s) 处理。为了有效地利用 IB 和 NVLink 的不同带宽，每个 token 限制最多被 dispatch 到 4 个节点以减少 IB 流量。</p>
<p>经过测试每个 token 在每个节点平均选择 3.2 个专家的同时不会产生额外的 NVLink 通信开销。意味着虽然 DeepSeek-V3 虽然实际上只选择 8 个路由专家，但它可以在保持相同通信成本的情况下最多选择 13 个专家 (4 节点x 3.2 专家/节点). 在这种通信策略下，仅 20 个 SMs 就足以充分利用 IB 和 NVLink 的带宽。详细地说，DeepSeek-V3 采用了 warp specialization 技术，并将 20 个 SMs 划分为 10 个通信通道。在 dispatch 过程中的通信链路为 (1)IB发送，(2) IB-to-NVLink 转发，(3) NVLink 接收由各自的 warp 处理。combine 过程则是相反的通信链路。</p>
<h3 id="323-extremely-memory-saving-with-minimal-overhead">3.2.3 Extremely Memory Saving with Minimal Overhead</h3>
<p>DeepSeek-V3 采取了如下技术来减少训练过程中的内存占用。</p>
<ul>
<li>重计算 RMSNorm 和 MLA 升维投影。</li>
<li>Exponential Moving Average (EMA) 参数被存放在 CPU 中并且异步更新。</li>
<li>MTP 的 Embedding 和输出头在 PP rank 相同的设备上是共享的。</li>
</ul>
<h2 id="33-fp8-training">3.3 FP8 Training</h2>
<p>低精度计算在累加过程中容易出现的问题有:</p>
<ol>
<li>溢出 (Overflow): 当许多数字相加时，它们的和很容易会超出 FP8 格式所能表示的最大值。</li>
<li>精度损失 (Precision Loss/Underflow): 在累加过程中，如果一个很大的中间和与一个很小的乘积相加，这个很小的乘积可能会因为精度限制而被吞掉，直接变成零，对最终结果毫无贡献。</li>
</ol>
<p>DeepSeek-V3 引入了一种细粒度的量化策略: $1\times N_c$ 元素的 tile 分组或 $N_cN_c\times N_c$ 元素的 block 分组。并且在其设计的高精度累加过程过程中，相关的反量化开销在很大程度上得到了缓解。此外，为了进一步减少 MoE 训练中的内存和通信开销，DeepSeek-V3 用 FP8 格式缓存和 dispatch 激活，以 BF16 格式存储低精度优化器状态。相较于 BF16 baseline, FP8 训练的相对误差低于 0.25%.</p>
<h3 id="331-mixed-precision-framework">3.3.1 Mixed Precision Framework</h3>
<p>如图中所示 Fprop(forward pass), Dgrad(activation backward pass) 以及 Wgrad(weight backward pass) GEMM 操作的输入是 FP8 格式，输出为 BF16 或者 FP32 格式。以 FP8 格式进行 Wgrad 允许激活也以 FP8 格式进行存储，减少了内存占用。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBa301c0983af1cc29da1165ca160c0d3e?method=download&amp;shareKey=92f1e462ac7dc2e7fd40cb3dff511153" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBa301c0983af1cc29da1165ca160c0d3e?method=download&amp;shareKey=92f1e462ac7dc2e7fd40cb3dff511153" alt="The Overall Mixed Precision Framework with FP8 Data Format">
    </a><figcaption>The Overall Mixed Precision Framework with FP8 Data Format</figcaption></figure></p>
<p>一些低开销的算子可以使用更高精度并且对训练开销的影响可以忽略不计。DeepSeek-V3 对这些模块使用原格式进行运算：Embedding，输出头，MoE 门控，归一化操作以及 Attention 操作。同时为了数值稳定性，以更高精度存储 master weights(FP32), weight gradients(FP32) &amp; optimizer states(BF16). 这些高精度部分带来的内存开销可以被 DP 减轻。</p>
<h3 id="332-improved-precision-from-quantization-and-multiplication">3.3.2 Improved Precision from Quantization and Multiplication</h3>
<p>DeepSeek-V3 使用了如下技术来提高低精度训练的准确性:</p>
<blockquote>
<p>As a standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes lowprecision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy.</p></blockquote>
<p><strong>Fine-Grained Quantization.</strong> 如下图 所示 DeepSeek-V3 采取更细粒度的方式对输入进行缩放到 FP8 的表示范围: (1) 对于激活以 1x128 tile 进行分组和缩放 (每个 token 的 128 通道为一组); (2) 对于权重以 128x128 进行分组和缩放 (每 128 个输入和输出通道为一组). 虽然原生的 FP8 GEMM 不支持对 reduction 维度进行按组缩放，但可以和下面介绍的 FP32 累加策略进行配合使用。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9db831a1951c8d26e446ee1588f8f55b?method=download&amp;shareKey=fab1571fc5238ec904095a783f855ef3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9db831a1951c8d26e446ee1588f8f55b?method=download&amp;shareKey=fab1571fc5238ec904095a783f855ef3" alt="Fine-grained Quantization">
    </a><figcaption>Fine-grained Quantization</figcaption></figure></p>
<p><strong>Increasing Accumulation Precision.</strong>  NVIDIA H800 GPU 上的 FP8 GEMM 累加精度被限制在 14 bits (远低于 FP32). 为在低精度计算中确保最终的数值精度，DeepSeek-V3 采用了一种结合 Tensor Cores 与 CUDA Cores 的混合计算流程。首先利用 Tensor Cores 的高吞吐量特性来执行 MMA (Matrix Multiply-Accumulate) 运算，中间结果在硬件原生的有限位宽累加器中进行阶段性累加。当累加操作进行 $N_c$ 次后，所产生的部分和将被立即复制到 CUDA Cores 上的 FP32 寄存器中，并与各自对应的细粒度量化缩放因子相乘，从而在执行全精度 FP32 最终累加的同时，高效地完成了反量化操作。这样能将反量化开销无缝融入到高精度累加步骤中，从而以最小的性能代价保证了最终结果的精确性。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB123812d18b874758b234db511dc40e22?method=download&amp;shareKey=982d72c6d03c9bf72d5461d643ad4c65" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB123812d18b874758b234db511dc40e22?method=download&amp;shareKey=982d72c6d03c9bf72d5461d643ad4c65" alt="Increasing Accumulation Precision">
    </a><figcaption>Increasing Accumulation Precision</figcaption></figure></p>
<p>在 H800 架构上，典型的情况是两个 WGMMA 同时存在，当一个 warp group 执行 promotion 到 CUDA Core 操作时，另一个 warp group 能够执行 MMA 操作。实验中取 $N_c=128$，对应于 4 个 WGMMA.</p>
<p><strong>Mantissa over Exponents.</strong> 对所有高精度的张量使用 4EM3 格式。</p>
<details class="custom-details">
    <summary class="custom-summary">How to Compute Float Point Value</summary>
    <div><p>一个常规浮点数 (即非零、非无穷大等特殊值) 的计算公式为：</p>
$$
\text{Value} = (-1)^S \times (1.M)_{\text{binary}} \times 2^{(E_{\text{decimal}} - \text{Bias})}
$$<p>要理解这个公式， DeepSeek-V3 需要拆解里面的三个关键部分：符号、尾数和指数。</p>
<hr>
<ol>
<li>
<p>确定符号 (Sign)</p>
<ul>
<li><code>S = 0</code>，则数值为正，$(-1)^0 = 1$</li>
<li><code>S = 1</code>，则数值为负，$(-1)^1 = -1$</li>
</ul>
</li>
<li>
<p>计算尾数的值 (Mantissa)</p>
</li>
</ol>
<p>不能直接使用 M 的二进制值。在常规浮点数中，标准规定尾数部分永远以 <code>1.</code> 开头。这样，这个 <code>1</code> 就不需要实际存储，从而可以节省一个比特位来提高精度。因此，尾数的实际值是 <code>(1.M)</code> 的二进制形式。</p>
<p>假设尾数位是 $m_1 m_2 m_3 \dots$，其代表的小数值为</p>
$$
m_1 \times 2^{-1} + m_2 \times 2^{-2} + m_3 \times 2^{-3} + \dots
$$<p>.</p>
<p>最终尾数项的值为 $1 + (m_1 \times 2^{-1} + m_2 \times 2^{-2} + m_3 \times 2^{-3} + \dots)$.</p>
<ol start="3">
<li>计算指数的值 (Exponent)</li>
</ol>
<p>指数部分也不能直接使用。为了能表示正、负指数，引入了偏置值 (Bias). 首先从 E 的二进制值计算出其十进制值 $E_{\text{decimal}}$ 后减去 Bias($2^{k-1} - 1$). 其中 k 是指数位的比特数。</p>
<ul>
<li>对于 <strong>E4M3</strong> (k=4)，Bias = $2^{(4-1)} - 1 = 2^3 - 1 = 7$.</li>
<li>对于 <strong>E5M2</strong> (k=5)，Bias = $2^{(5-1)} - 1 = 2^4 - 1 = 15$.</li>
</ul>
<ol start="4">
<li>特殊值说明
当指数 E 全为 0或全为 1 时，代表的是一些特殊值，计算规则也不同:</li>
</ol>
<ul>
<li>E 全为 0:
<ul>
<li>如果 M 也全为 0，代表零 (Zero).</li>
<li>如果 M 不为 0，代表<strong>非规格化数 (Subnormal Numbers)</strong>，计算公式变为 $(-1)^S \times (0.M) \times 2^{(1 - \text{Bias})}$，此时没有隐含的 1.</li>
</ul>
</li>
<li>E 全为 1:
<ul>
<li>如果 M 全为0，代表<strong>无穷大 (Infinity)</strong>。</li>
<li>如果 M 不为0，代表<strong>NaN(Not a Number)</strong>.</li>
</ul>
</li>
</ul>
</div>
</details><br>
<p><strong>Online Quantization.</strong> 采用 online 方式计算每个 1x128 激活 tile 和 128x128 权重 block 的最大绝对值。</p>
<h3 id="333-low-precision-storage-and-communication">3.3.3 Low-Precision Storage and Communication</h3>
<p><strong>Low-Precision Optimizer States.</strong> 用 BF16 格式存储 AdamW 优化器的一阶和二阶动量。优化器存储的 master weights 和 betch 的累加梯度仍以 FP32 格式存储。</p>
<p><strong>Low-Precision Activation.</strong> 大部分激活以 FP8 格式存储，但以下这些是例外。</p>
<ul>
<li><strong>Inputs of the Linear after the attention operator.</strong> 这些激活会在反向传播过程中作为 attention 的输入，对精度比较敏感，因此采用 E5M6 格式存储。量化过程的缩放因子被限制为 2 的整数次幂。</li>
<li><strong>Inputs of the SwiGLU operator in MoE.</strong> 以 FP8 格式存储 SwiGLU 的输入然后再反向传播中重计算。</li>
</ul>
<p><strong>Low-Precision Communication.</strong> 在 dispatch 之前对  MoE up-projections 的输入进行 FP8 量化。专家接收到 FP8 数据后，可以直接进行兼容的 FP8 前向传播。量化过程的缩放因子被限制为 2 的整数次幂。在反向传播进入 MoE down-projections 之前同样使用该策略。前向传播和反向传播 combine 后的结果以 FP16 格式存储。</p>
<h2 id="34-inference-and-deployment">3.4 Inference and Deployment</h2>
<p>为了同时保证 Service-Level Objective (SLO) 和高吞吐量, <em>prefilling</em> 和 <em>decoding</em> 阶段采用了不同的部署策略。</p>
<p>prefilling 阶段的部署单元为 4 个节点 (32 GPUs). 并行策略如下</p>
<ul>
<li>attention part: 采用带有 Sequence Parallel (SP) 的 4-way Tensor Parallel (TP4)，并且和 8-way Data Parallelism (DP8) 一起使用。</li>
<li>MoE part: 采用 32-way Expert Parallelism (EP32), shallow layer 不使用 TP.</li>
</ul>
<p>其他部署细节:</p>
<ul>
<li><em>redundant experts</em>: 部署 32 高负载的专家 (每十分钟统计一次进行调整) 副本。每个 GPU 除了有自己的 8 个专家之外还有 1 个高负载专家。</li>
<li>同时处理两个计算量差不多的 micro-batches，来掩盖 All-to-all 和 TP 的通信。即将一个 micro-batch 的 attention+MoE 和另一个 batch 的 dispatch+combine 重叠。</li>
<li><em>dynamic redundancy</em>: 每个 GPU 上放置 16 个专家，但每次只有 9 个被激活。</li>
</ul>
<p>decoding 阶段的部署单元为 40 个节点 (320 GPUs). 并行策略如下</p>
<ul>
<li>attention part: 采用带有 SP 的 TP4，并且和 DP80 一起使用。</li>
<li>MoE part: 采用 EP320. 256 GPU 被用来放置路由专家，64 GPU 被用来放置共享专家和冗余专家。</li>
</ul>
<p>All-to-all 通过 IB 进行点对点直接传输。同时利用 IBGDA 技术让网卡直接读写 GPU 内存。系统会根据流量统计周期性地判断哪些常规路由专家是当前最热门的，然后动态地让那 64 个GPU去扮演这些热门专家的副本。因为每个 GPU 只被放置一个专家，所以当需要更改冗余策略时系统只需要改变路由逻辑，不需要在物理上移动或重新加载模型权重。</p>
<p>在 decoding 过程中 attention 会耗费更多时间。因此将一个 micro-batch 的 attention 和另一个的 dispatch+MoE+combine 重叠。decoding 阶段每个 GPU 只需要加载一个专家的参数，因此可以分配更多的 SM 给 attention 部分来加速其计算。</p>
<h2 id="35-suggestions-on-hardware-design">3.5 Suggestions on Hardware Design</h2>
<p>基于 All-to-all 实现和 FP8 训练框架，DeepSeek-V3 对 AI 硬件厂商提出了一些建议。</p>
<h3 id="351-communication-hardware">3.5.1 Communication Hardware</h3>
<p>当前通信算子的实现依赖于 SM，DeepSeek-V3 使用了 20 个 H800 SMs (一共 132 个) 用于通信，但使用 SM 进行通信会导致 tensor core 利用率很低。</p>
<p>当前 SM 主要在 All-to-all 通信中执行以下任务:</p>
<ul>
<li>IB 和 NVLink 域之间的数据转发，将目的地为同一节点内多个不同 GPU 的流量，首先汇聚到单个代理GPU上。</li>
<li>在 RDMA 缓冲区 (已注册的 GPU 内存区域) 与模型的输入/输出缓冲区之间进行数据搬运。</li>
<li>为 All-to-all 通信的 combine 阶段执行 reduce 操作。</li>
<li>在需要跨越 IB 和 NVLink 网络域、向多个不同专家进行分块数据传输<span class="sidenote-number"><small class="sidenote">在一个 GPU上 的tokens，其中一些可能要去当前节点内的专家 (通过NVLink)，另一些则要去其他节点上的专家 (通过IB). 在发送之前，GPU必须在自己的内存里进行一次数据重排，把所有目的地是专家 A 的 tokens 打包成一个连续的内存块，所有去专家 B 的 tokens 打包成另一个内存块。</small></span>
的过程中，管理细粒度的内存布局。</li>
</ul>
<h3 id="352-compute-hardware">3.5.2 Compute Hardware</h3>
<p><strong>Higher FP8 GEMM Accumulation Precision in Tensor Cores.</strong> 在目前 NVIDIA Hopper 架构的 Tensor Core 实现中，FP8 GEMM 的累积精度有限。在根据最大指数右移对齐 32 个尾数乘积后，Tensor Core 只使用每个尾数乘积的最高 14 位进行加法，并截断超过此范围的位。将加法结果累加到寄存器中也采用 14 位精度。</p>
<p><strong>Support for Tile- and Block-Wise Quantization.</strong> 目前的 GPU 只支持逐张量量化，缺乏对细粒度量化的原生支持，比如 DeepSeek 的 tile 量化和 block 量化。在当前的实现中，当累加 $N_c$ 次时，部分结果将从 Tensor Core 复制到 CUDA Core，乘以缩放因子，并累加到 CUDA Core 上的FP32 寄存器。尽管与精确的 FP32 累加策略相结合，反量化开销显着减轻，但 Tensor Core 和 CUDA Core 之间频繁的数据移动仍然限制了计算效率。</p>
<p><strong>Support for Online Quantization.</strong> 当前情况下需要从 HBM 中读取 128 个 BF16 激活值 (之前计算的输出) 进行量化，然后将量化后的 FP8 值写回 HBM，然后再次读取以进行 MMA.</p>
<p><strong>Support for Transposed GEMM Operations.</strong> 在当前工作流程中，前向传播的激活被量化为 1x128 FP8 tile 并存储。在反向传播中，矩阵需要被读出、反量化、转置、重新量化为 128x1 tile，并存储在 HBM 中。</p>
<p>DeepSeek-V3 的预训练阶段围绕着高质量的数据构建、精心设计的超参数、长上下文扩展以及全面的性能评测展开。</p>
<h1 id="4-pretraining">4. Pretraining</h1>
<h2 id="41-data-construction">4.1 Data Construction</h2>
<ul>
<li><strong>训练语料</strong>:
<ul>
<li>模型在一个包含 <strong>14.8T</strong> 高质量、多样化 token 的语料库上进行预训练。</li>
<li>与 DeepSeek-V2 相比，新语料提升了数学和编程相关样本的比例，并扩展了除中英文之外的多语言覆盖范围。</li>
<li>数据处理流程经过优化，旨在最小化冗余，同时保持语料的多样性。</li>
</ul>
</li>
<li><strong>FIM 策略</strong>:
<ul>
<li>模型训练中采用了 FIM (Fill-in-Middle) 策略，该策略被证明在不损害常规“下一词预测”能力的同时，赋予了模型根据上下文准确预测中间文本的能力。</li>
<li>FIM 策略在文档层面以 10% 的应用率实施，并采用 Prefix-Suffix-Middle (PSM) 框架构建数据格式。</li>
</ul>
</li>
<li><strong>分词器</strong>:
<ul>
<li>分词器采用 Byte-level BPE，词汇表大小扩展至 <strong>128K</strong>.</li>
<li>为了优化多语言压缩效率，对预分词器和训练数据进行了修改。</li>
<li>为了解决因合并标点和换行符可能导致的 token边界偏差，训练中会随机拆分一部分这类组合 token.</li>
</ul>
</li>
</ul>
<h2 id="42-hyper-parameters">4.2 Hyper-Parameters</h2>
<ul>
<li><strong>模型结构超参数</strong>:
<ul>
<li>总共有 61 层 Transformer，隐藏层维度为 7168.</li>
<li>MLA 注意力头数 $n_h$ 为128，每个头的维度为 128. KV 压缩维度 $d_c$ 为 512，Query 压缩维度 $d_c^{'}$ 为1536.</li>
<li>除了前三层，其余所有 FFN 都被替换为 MoE 层。</li>
<li>每个 MoE 层包含 1个共享专家 和 256个路由专家。每个 token 会激活其中的 8个 路由专。</li>
<li>采用 MTP 策略，预测深度为 1，即除了下一个词，还会额外再预测一个词。</li>
<li>最终模型总参数量为 671B，每个 token 的激活参数量为 37B.</li>
</ul>
</li>
<li><strong>训练超参数</strong>:
<ul>
<li>优化器采用 AdamW，其中 $\beta_{1}=0.9, \beta_{2}=0.95$，权重衰减为 0.1.</li>
<li>预训练阶段的最大序列长度为 4K.</li>
<li>学习率调度：先在 2K 步内线性增长至 $2.2\times10^{-4}$，保持该速率直到消耗10T token，然后在 4.3T token 内余弦衰减至 $2.2\times10^{-5}$，最后在 500B token 的训练中进一步调整。</li>
<li>采用了批次大小调度策略，从 3072 逐步增加到15360.</li>
<li>路由机制被限制为每个 token 最多发送到 4 个节点，以平衡负载。</li>
<li>负载均衡策略主要采用 auxiliary-loss-free，偏置更新速率 $\gamma$ 在前 14.3 Token 时为 0.001，后 500B token 时为 0.0.</li>
<li>对于序列级平衡损失 $\alpha=0.00001$，以防止单一样本内的极端不平衡。</li>
<li>MTP loss 权重 $\lambda$ 对于前 10T token 为 0.3，对于后 4.8T token 为 0.1.</li>
</ul>
</li>
</ul>
<h2 id="43-long-context-extension">4.3 Long Context Extension</h2>
<ul>
<li><strong>扩展方法</strong>: 采用与 DeepSeek-V2 类似的方法，在预训练后应用 <strong>YaRN</strong> 技术进行上下文扩展。</li>
<li><strong>扩展阶段</strong>: 分为两个阶段，分别将上下文窗口从 4K 扩展到 32K，再进一步扩展到 128K.</li>
<li><strong>效果验证</strong>: 通过大海捞针 (Needle In A Haystack) 测试表明，模型在高达 128K 的完整上下文长度内均表现出色且稳定。</li>
</ul>
<h2 id="44-evaluations">4.4 Evaluations</h2>
<ul>
<li><strong>评测范围</strong>: 主要在中英文基准测试以及一个多语言基准上进行评测，与当前最先进的开源基础模型进行比较，如 DeepSeek-V2-Base, Qwen2.5 72B Base, 和 LLaMA-3.1 405B Base.</li>
<li><strong>评测结果</strong>:
<ul>
<li>DeepSeek-V3-Base 全面超越了 DeepSeek-V2-Base 和 Qwen2.5 72B Base，并在绝大多数基准上超过了 LLaMA-3.1 405B Base，成为当前最强的开源模型。</li>
<li>与拥有 11 倍激活参数量的 LLaMA-3.1 405B Base 相比，DeepSeek-V3-Base 在多语言、代码和数学基准上表现出好得多的性能。</li>
<li>在英语和中文语言基准上，DeepSeek-V3-Base 也展现出有竞争力或更好的性能。</li>
</ul>
</li>
<li><strong>训练效率</strong>: 得益于高效的架构和工程优化，DeepSeek-V3 的训练效率极高。每训练 1T token 仅需 180K H800 GPU 小时，远比训练 72B 或 405B 的密集模型便宜。</li>
</ul>
<h2 id="45-discussion">4.5 Discussion</h2>
<p>本节通过一系列消融实验，深入探讨了模型采用的两个关键新策略的有效性，并对负载均衡的不同实现方式进行了对比分析。</p>
<h3 id="451-ablation-studies-for-multi-token-prediction">4.5.1 Ablation Studies for Multi-Token Prediction</h3>
<ul>
<li><strong>实验设置</strong>:
<ul>
<li>在两个不同规模 (一个15.7B，一个228.7B) baseline MoE模型上进行验证。</li>
<li>对比模型在 baseline 模型的基础上增加了一个预测深度为 1 的MTP模块，其他设置 (如训练数据、架构) 保持不变。</li>
<li>为了保证公平比较，在推理阶段会丢弃MTP模块，因此对比模型的推理成本完全相同。</li>
</ul>
</li>
<li><strong>实验结论</strong>:
<ul>
<li>实验结果 (Table 4) 表明，MTP策略在绝大多数评测基准上都能稳定地提升模型性能。</li>
<li>例如，在大型模型上，HumanEval (代码生成) 和 GSM8K (数学推理) 等任务的性能得到了显著提升。</li>
</ul>
</li>
</ul>
<h3 id="452-ablation-studies-for-the-auxiliary-loss-free-balancing-strategy">4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy</h3>
<ul>
<li><strong>实验设置</strong>:
<ul>
<li>同样在两个不同规模 (一个小型15.7B，一个大型228.7B) baseline MoE 模型上进行验证。</li>
<li>baseline 模型完全依赖传统的辅助损失函数来促进专家负载均衡。</li>
<li>对比模型则移除了所有辅助损失，并引入了 Auxiliary-Loss-Free 的均衡策略，其他设置保持一致。</li>
</ul>
</li>
<li><strong>实验结论</strong>:
<ul>
<li>实验结果 (Table 5) 显示，Auxiliary-Loss-Free 策略在绝大多数评测基准上都取得了比纯辅助损失方法更好的模型性能。</li>
<li>在代码和数学等任务上，性能提升尤为明显。</li>
</ul>
</li>
</ul>
<h3 id="453-batch-wise-load-balance-vs-sequence-wise-load-balance">4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance</h3>
<ul>
<li><strong>核心区别</strong>: Auxiliary-Loss-Free 策略是在整个训练批次 (batch-wise) 上实现均衡，而传统的辅助损失则是在每个序列 (sequence-wise) 内部强制实现均衡。</li>
<li><strong>理论优势</strong>: 批次级的均衡约束更为灵活，它不强制每个序列内部的专家使用频率都一样，从而允许专家更好地专精于特定领域 (如代码、数学等).</li>
<li><strong>实验验证</strong>:
<ul>
<li>通过分析模型在不同领域数据上的专家负载，观测到 Auxiliary-Loss-Free 模型展现出了更强的专家特化模式。</li>
<li>进一步的实验表明，只要能实现相似水平的批次级负载均衡，无论是使用 Auxiliary-Loss-Free 方法还是新设计的批次级 Auxiliary-Loss-Free 方法，都能达到相似的优异模型性能，且均优于序列级辅助损失方法。</li>
</ul>
</li>
<li><strong>潜在挑战与解决方案</strong>:
<ul>
<li>批次级均衡可能面临两个挑战：单个序列或小批次内的负载不均，以及推理时因领域切换导致的负载不均。</li>
<li>第一个挑战通过使用大规模的专家并行和数据并行 (确保了每个微批次的规模足够大) 得以自然解决。</li>
<li>第二个挑战则通过在推理部署中采用冗余专家策略来克服。</li>
</ul>
</li>
</ul>
<h1 id="5-post-training">5. Post-Training</h1>
<p>后训练阶段旨在将预训练好的基础模型与人类偏好对齐，并进一步解锁其潜力。该阶段主要包括监督微调 (SFT) 和强化学习 (RL)，并涉及从 DeepSeek-R1 系列模型中蒸馏推理能力。</p>
<h2 id="51-supervised-fine-tuning-sft">5.1 Supervised Fine-Tuning, SFT</h2>
<ul>
<li><strong>数据集构建</strong>: SFT 数据集包含 150 万个实例，涵盖多个领域。
<ul>
<li><strong>推理数据</strong>: 对于数学、代码、逻辑等推理任务，利用内部的 DeepSeek-R1 模型生成数据。虽然 R1 生成的数据准确性高，但存在过度思考、格式不佳和长度过长等问题。为了平衡准确性与简洁性，SFT 训练中会混合使用原始应答和经过精心设计的系统提示词引导下的 R1 应答。</li>
<li><strong>非推理数据</strong>: 对于创意写作、角色扮演等任务，使用 DeepSeek-V2.5 生成应答，并由人类标注员进行验证。</li>
</ul>
</li>
<li><strong>SFT 设置</strong>:
<ul>
<li>模型在 SFT 数据集上微调了 2 个 epoch.</li>
<li>学习率采用余弦衰减策略，从 $5\times10^{-6}$ 降至 $1\times10^{-6}$.</li>
<li>训练序列由多个样本打包而成，但采用样本掩码策略确保样本间相互隔离。</li>
</ul>
</li>
</ul>
<h2 id="52-reinforcement-learning">5.2 Reinforcement Learning</h2>
<h3 id="521-reward-model">5.2.1 Reward Model</h3>
<p>RL 过程采用了 Rule-Based 模型和 Model-Based 的奖励模型。</p>
<ul>
<li><strong>Rule-Based RM</strong>: 用于有明确验证规则的问题，如数学题的确定性答案或代码题的单元测试结果。这种方式可靠性高，不易被模型钻空子。</li>
<li><strong>Model-Based RM</strong>: 用于答案更开放、没有确定性对错的问题。该奖励模型由 DeepSeek-V3 的 SFT 版本训练而来，并通过包含思维链的偏好数据进行训练，以降低 reward hacking 的风险。</li>
</ul>
<h3 id="522-grpo">5.2.2 GRPO</h3>
<ul>
<li>采用了 <strong>GRPO (Group Relative Policy Optimization)</strong> 算法进行强化学习。</li>
<li>GRPO 的一个特点是它不需要一个与策略模型同等大小的 critic 模型，而是从一组采样输出的分数中估计 baseline.</li>
<li>RL 过程融合了来自编码、数学、写作、角色扮演等不同领域的提示词，这不仅使模型与人类偏好更对齐，也提升了在 SFT 数据有限场景下的基准测试性能。</li>
</ul>
<h2 id="53-evaluations">5.3 Evaluations</h2>
<ul>
<li><strong>评测设置</strong>:
<ul>
<li>除了基础模型评测用的基准外，进一步在 IFEval, GPQA, LongBench v2, SWE-Bench Verified, Aider, Codeforces, AIME 2024 等更具挑战性的基准上进行评估。</li>
<li>对比的 baseline 模型包括其他强大的开源和闭源模型，如 Qwen2.5-72B-Inst, LLaMA-3.1-405B-Inst, Claude-3.5-Sonnet, 和 GPT-4o-0513。</li>
</ul>
</li>
<li><strong>Standard Evaluation</strong>:
<ul>
<li>评测结果 (Table 6) 显示，DeepSeek-V3 是表现最好的开源聊天模型。</li>
<li>在知识基准 (MMLU, MMLU-Pro, GPQA-Diamond) 上，其性能与顶级的闭源模型相当或相近。</li>
<li>在长上下文理解基准 (DROP, LongBench v2, FRAMES) 上，表现出顶级水平，例如在 DROP 上取得了 91.6 的 F1 分数，超越了所有其他模型。</li>
<li>在代码和数学基准上表现卓越，尤其是在 AIME, MATH-500 等高难度数学竞赛基准上，绝对得分领先第二名约 10%，优势巨大。</li>
<li>在中文基准上，如 C-SimpleQA，其表现也超越了包括 Qwen2.5 在内的其他模型。</li>
</ul>
</li>
<li><strong>Open-Ended Evaluation</strong>:
<ul>
<li>在 Arena-Hard 基准测试中，DeepSeek-V3 取得了超过 85% 的胜率，与顶级的 Claude-3.5-Sonnet-1022 表现持平，成为首个在该基准上突破 85% 的开源模型。</li>
<li>在 AlpacaEval 2.0 上，其表现同样出色，超越了所有对比的开源和闭源模型。</li>
</ul>
</li>
<li><strong>作为奖励模型的能力</strong>:
<ul>
<li>在 RewardBench 基准上评测其作为奖励模型的判断能力，结果显示 DeepSeek-V3 与最新版本的 GPT-4o 和 Claude-3.5-Sonnet 表现相当。</li>
</ul>
</li>
</ul>
<h2 id="54-discussion">5.4 Discussion</h2>
<ul>
<li><strong>从 DeepSeek-R1 蒸馏知识</strong>:
<ul>
<li>消融实验 (Table 9) 证明，从长思维链 (long-CoT) 模型 DeepSeek-R1 中蒸馏知识的策略非常有效，显著提升了模型在 LiveCodeBench 和 MATH-500 上的性能。</li>
<li>实验也揭示了一个权衡：蒸馏带来了性能提升，但也显著增加了回应的平均长度。因此，在 DeepSeek-V3 的开发中对蒸馏设置进行了仔细选择以求平衡。</li>
</ul>
</li>
<li><strong>Self-Rewarding</strong>:
<ul>
<li>在缺乏明确验证规则的通用场景中，模型开发采用了 constitutional AI 的方法，即<strong>使用 DeepSeek-V3 自身的投票评估结果作为反馈源</strong>来进行优化。</li>
<li>这种自奖励的范式产生了显著的对齐效果，并被认为是实现LLM自我改进的重要方向。</li>
</ul>
</li>
<li><strong>MTP 评测</strong>:
<ul>
<li>模型采用的 MTP 技术可以预测第 2 个token.</li>
<li>评测显示，这个额外预测的 token 的接受率在 85%-90%之间。</li>
<li>结合 speculative decoding 框架，这个高接受率使得模型的解码速度 (TPS) 提升了1.8倍.</li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>DeepSeekMoE</title>
      <link>http://localhost:1313/blogs/deepseek/deepseekmoe/</link>
      <pubDate>Thu, 19 Jun 2025 17:04:18 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepseek/deepseekmoe/</guid>
      <description>Paper Reading of DeepSeekMoE</description>
      <content:encoded><![CDATA[<h1 id="preliminary-mixture-of-experts-for-transformers">Preliminary: Mixture-of-Experts for Transformers</h1>
<p>一个标准的 Transformer backbone LLM 由堆叠层标准 Transformer 块构成，每个块可以表示如下:</p>
$$
\mathbf{h}_t^l = \sum_{i=1}^{N} \left( g_{i,t} \text{FFN}_i(\mathbf{u}_t^l) \right) + \mathbf{u}_t^l \tag{3}
$$$$
g_{i,t} = \begin{cases} s_{i,t} & \text{if } s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \le j \le N\}, K) \\ 0 & \text{otherwise} \end{cases} \tag{4}
$$$$
s_{i,t} = \text{Softmax}_i(\mathbf{u}_t^T \mathbf{e}_i^l) \tag{5}
$$<ul>
<li>$N$: 专家总数</li>
<li>$\text{FFN}_i(\cdot)$: 第 $i$ 个专家的 FFN.</li>
<li>$g_{i,t}$: 第 $i$ 个专家的门控值。</li>
<li>$s_{i,t}$: token 对专家的亲和度。</li>
<li>$\text{Topk}(\cdot, K)$: 在为第 $t$ 个 token 和所有 $N$ 个专家计算出的亲和度分数中，包含 $K$ 个最高分数的集合，</li>
<li>$\mathbf{e}_i^l$: 第 $l$ 层中第 $i$ 个专家的中心。</li>
</ul>
<p>注意到 $g_{i,t}$ 是稀疏的，说明在 $N$ 个门控值中只有 $K$ 个非零。这种稀疏性确保了 MoE 层内的计算效率，即每个 token 只会被分配给 $K$ 个专家并由它们计算。此外，在上述公式中，为了简洁起见，我们省略了层归一化操作。</p>
<h1 id="deepseekmoe-architecture">DeepSeekMoE Architecture</h1>
<p>
<figure class="post-figure">
    <a href="https://arxiv.org/html/2401.06066v1/x2.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://arxiv.org/html/2401.06066v1/x2.png" alt="Illustration of DeepSeek MoE">
    </a><figcaption>Illustration of DeepSeek MoE</figcaption></figure></p>
<h2 id="fine-grained-expert-segmentation">Fine-Grained Expert Segmentation</h2>
<p>在上图 (a) 的情况下将每个专家 FFN 的中间隐藏层维度缩小到原先的 1/m，专家数增加 m 倍。这样可以在参数量和计算量保持不变的情况下使得每个 token 被路由到更多的专家。通过细粒度的专家划分，MoE 层的输出可以表示为</p>
$$
\begin{aligned}
\mathbf{h}_t^l &= \sum_{i=1}^{mN} \left( g_{i,t} \text{FFN}_i(\mathbf{u}_t^l) \right) + \mathbf{u}_t^l \quad&(6)\\
g_{i,t} &= \begin{cases} s_{i,t} & \text{if } s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \le j \le mN\}, mK) \\ 0 & \text{otherwise} \end{cases} \quad&(7)\\
s_{i,t} &= \text{Softmax}_i(\mathbf{u}_t^T \mathbf{e}_i^l) \quad&(8)
\end{aligned}
$$<ul>
<li>$mN$: 细粒度专家的总数。</li>
<li>$mK$: 非零门控值的数量也将增加到。</li>
</ul>
<p>专家参数总数等于 $N$ 乘以标准 FFN 中的参数数量。从组合可能性的角度来看，细粒度专家分割策略显著增强了激活专家的组合灵活性。</p>
<h2 id="shared-expert-isolation">Shared Expert Isolation</h2>
<p>如上图 (c) 所示 将 $K_s$ 个专家隔离出来作为共享专家。无论路由模块如何，每个 token 都会被确定性地分配给这些共享专家。为了保持计算量不变，其他路由专家中激活的专家数量将减少 $K_s$.</p>
$$
\begin{aligned}
\mathbf{h}_t^l &= \sum_{i=1}^{K_S} \text{FFN}_i(\mathbf{u}_t^l) + \sum_{i=K_S+1}^{mN} \left( g_{i,t} \text{FFN}_i(\mathbf{u}_t^l) \right) + \mathbf{u}_t^l \quad&(9)\\
g_{i,t} &= \begin{cases} s_{i,t} & \text{if } s_{i,t} \in \text{Topk}(\{s_{j,t} | K_S+1 \le j \le mN\}, mK - K_S) \\ 0 & \text{otherwise} \end{cases} \quad&(10)\\
s_{i,t} &= \text{Softmax}_i(\mathbf{u}_t^T \mathbf{e}_i^l) \quad&(11)
\end{aligned}
$$<p>于是在 DeepSeekMoE 中，共享专家的数量为 $K_S$，路由专家的总数为 $mN - K_S$，非零门控值的数量是 $mK - K_S$</p>
<h2 id="load-balance-consideration">Load Balance Consideration</h2>
<p>自动学习的路由策略可能会遇到负载不平衡的问题</p>
<ol>
<li>存在路由崩溃的风险，即模型总是只选择少数几个专家，导致其他专家无法得到充分训练。</li>
<li>如果专家分布在多个设备上，负载不平衡会加剧计算瓶颈。</li>
</ol>
<hr>
<p><em>Expert-Level Balance Loss.</em>
</p>
$$ 
\begin{aligned}
\mathcal{L}_{\text{ExpBal}} &= \alpha_1 \sum_{i=1}^{N'} f_i P_i \quad&(12)\\
f_i &= \frac{N'}{K'T} \sum_{t=1}^{T} \mathbf{1}(\text{Token } t \text{ selects Expert } i) \quad&(13)\\
P_i &= \frac{1}{T} \sum_{t=1}^{T} s_{i,t} \quad&(14)
\end{aligned}
$$<p>$\mathcal{L}_{\text{ExpBal}}$ 的目的是<strong>促进专家之间的负载均衡</strong>，避免出现某些专家过载 (被选中太多次) 而其他专家闲置 (很少被选中) 的情况。</p>
<ul>
<li>$N'$: 表示可路由的专家总数，即 $mN - K_S$。</li>
<li>$K'$: 表示每个 token 选择的路由专家数量，即 $mK - K_S$。</li>
<li>$T$: 表示总的 token 数量。</li>
</ul>
<p>该损失函数的解释如下</p>
<ul>
<li>$f_i$ (Expert Load/Utilization):
<ul>
<li>公式 (13) 计算的是专家 $i$ 在一个批次/序列中被选中的频率。</li>
<li>$\mathbf{1}(\text{Token } t \text{ selects Expert } i)$ 是一个指示函数，如果 token $t$ 选中了专家 $i$，则为 1，否则为 0。</li>
<li>$\frac{1}{T} \sum_{t=1}^{T} \mathbf{1}(\text{Token } t \text{ selects Expert } i)$ 得到了专家 $i$ 在 $T$ 个 token 中被选中的平均次数 (频率) 。</li>
<li>前面的 $\frac{N'}{K'}$ 是一个归一化因子。当所有专家被均匀选中时，每个专家被选择的平均次数为$TK'/N'$，此时 $f_i$ 的期望值为 1. 如果专家 $i$ 被选中次数多于平均，则 $f_i > 1$；反之 $f_i < 1$.</li>
</ul>
</li>
</ul>
<p>$f_i$ 可以理解为<strong>专家 $i$ 的归一化负载或利用率</strong>。</p>
<ul>
<li>$P_i$ (Expert Routing Probability):
<ul>
<li>公式 (14) 计算的是专家 $i$ 在所有 token 中平均的门控亲和度分数。</li>
<li>$s_{i,t}$ 是 token $t$ 对专家 $i$ 的原始亲和度分数 (Softmax 之前的输出) 。</li>
</ul>
</li>
</ul>
<p>$P_i$ 可以理解为<strong>专家 $i$ 被门控网络选择的平均倾向性</strong>。</p>
<ul>
<li>$\mathcal{L}_{\text{ExpBal}} = \alpha_1 \sum_{i=1}^{N'} f_i P_i$:
<ul>
<li>这个损失项是 $f_i$ 和 $P_i$ 乘积的和。它的目标是<strong>最小化这个和</strong>。</li>
<li>如果某个专家 $i$ 的 $f_i$ (负载高) 和 $P_i$ (被倾向性选择的概率高) 都很大，那么 $f_i P_i$ 就会很大，导致损失增大。</li>
<li>通过最小化这个损失，模型会被<strong>激励将 token 分配给那些负载较低或被选择倾向性较低的专家</strong>。这有助于分散负载，使得所有专家都能得到训练和利用，从而提高模型的整体效率和性能。</li>
<li>$\alpha_1$ 是一个超参数，用于控制这个平衡损失在总损失中的权重。</li>
</ul>
</li>
</ul>
<hr>
<p><em>Device-Level Balance Loss.</em>
</p>
$$
\begin{aligned}
\mathcal{L}_{\text{DevBal}} &= \alpha_2 \sum_{i=1}^{D} \hat{f}_i \hat{P}_i \quad&(15)\\
\hat{f}_i &= \frac{1}{|\mathcal{E}_i|} \sum_{j \in \mathcal{E}_i} f_j \quad&(16)\\
\hat{P}_i &= \sum_{j \in \mathcal{E}_i} P_j \quad&(17)
\end{aligned}
$$<p>$\mathcal{L}_{\text{DevBal}}$ 的目的是<strong>促进专家在不同计算设备之间的负载均衡</strong>。在分布式训练中，通常会将专家分散到不同的设备上，如果某些设备上的专家过于繁忙，而另一些设备上的专家闲置，就会导致计算瓶颈和效率低下。</p>
<ul>
<li>$D$: 表示计算设备的数量。</li>
<li>$\mathcal{E}_i$: 表示分配给第 $i$ 个设备的所有专家的集合。$|\mathcal{E}_i|$ 是这个集合中专家的数量。</li>
</ul>
<p>该损失的解释如下</p>
<ul>
<li>
<p>$\hat{f}_i$ (Device-level Expert Load): 公式 (16) 计算的是第 $i$ 个设备上所有专家的平均负载 ($f_j$). 代表了<strong>设备 $i$ 的总体计算负载</strong>。</p>
</li>
<li>
<p>$\hat{P}_i$ (Device-level Routing Probability): 公式 (17) 计算的是第 $i$ 个设备上所有专家的平均路由倾向性 ($P_j$) 的总和。代表了<strong>设备 $i$ 的专家集合被门控网络选择的总体倾向性</strong>。</p>
</li>
<li>
<p>$\mathcal{L}_{\text{DevBal}} = \alpha_2 \sum_{i=1}^{D} \hat{f}_i \hat{P}_i$:</p>
<ul>
<li>这个损失项是 $\hat{f}_i$ 和 $\hat{P}_i$ 乘积的和，目标也是<strong>最小化这个和</strong>。</li>
<li>如果某个设备 $i$ 的 $\hat{f}_i$ (负载高) 和 $\hat{P}_i$ (被选择倾向性高) 都很大，那么 $\hat{f}_i \hat{P}_i$ 就会很大，导致损失增大。</li>
<li>通过最小化这个损失，模型会被<strong>激励将 token 路由到那些整体负载较低的设备上的专家</strong>。这确保了分布式训练或推理时，所有设备都能得到更均匀的利用，避免了单个设备成为瓶颈。</li>
<li>$\alpha_2$ 是一个超参数，用于控制这个损失在总损失中的权重。</li>
</ul>
</li>
</ul>
<hr>
<p>超参数 $\alpha_1$ 和 $\alpha_2$ 的设置策略:</p>
<ul>
<li>**小的 $\alpha_1$ (专家级平衡因子) : 用于“减轻路由崩溃的风险”。路由崩溃指的是少数专家被过度使用，导致它们“饱和”而无法有效学习，同时其他专家则完全未被使用。较小的 $\alpha_1$ 意味着我们允许一定程度的专家专业化，但仍会进行微调以避免极端的不平衡。</li>
<li>**大的 $\alpha_2$ (设备级平衡因子) : 用于“促进跨设备的平衡计算”。这意味着我们更强烈地要求模型将计算负载均匀地分散到所有可用的计算设备上，以最大限度地提高并行效率。设备级的负载均衡对于分布式系统而言至关重要。</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>DeepSeekMLA</title>
      <link>http://localhost:1313/blogs/deepseek/deepseekmla/</link>
      <pubDate>Thu, 19 Jun 2025 11:24:45 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepseek/deepseekmla/</guid>
      <description>Principle of DeepSeekV3 MLA</description>
      <content:encoded><![CDATA[<h1 id="preliminary-what-is-rope">Preliminary: What is RoPE</h1>
<h2 id="introduction">Introduction</h2>
<p>旋转位置编码 (RoPE) 是一种新颖的、基于相对位置的编码方法，它被设计用于提高 Transformer 模型处理长序列的能力，同时保持计算效率。与传统的绝对位置编码 (如正弦/余弦位置编码) 或直接的相对位置编码 (如 T5 中使用的相对偏置) 不同，RoPE 将位置信息集成到自注意力机制的 Q 和 K 的表示中，使得 Q 和 K 的点积自然地编码了<strong>相对位置信息</strong>。</p>
<p>RoPE 的核心思想是，通过对查询和键向量应用特定的旋转操作，使得两个向量的点积结果只依赖于它们之间的相对距离，而不是它们的绝对位置。这使得模型能够更好地泛化到更长的序列，并且在处理位置信息时更加高效。</p>
<p><strong>RoPE 的主要优点包括：</strong></p>
<ul>
<li><strong>编码相对位置信息：</strong> 自然地将相对位置信息融入到注意力分数中。</li>
<li><strong>长序列外推能力：</strong> 提高了模型在训练时未见过的更长序列上的性能。</li>
<li><strong>与自注意力机制的兼容性：</strong> 无缝集成到 QKV 点积注意力中。</li>
<li><strong>简单且高效：</strong> 实现相对简单，且不会显著增加计算复杂度。</li>
</ul>
<h2 id="formular">Formular</h2>
<p>RoPE 的主要思想是通过对查询 $q$ 和键 $k$ 应用一个旋转矩阵 $R_t$ (取决于其绝对位置 $t$) ，使得点积 $q_m^T k_n$ 能够通过某种方式转化为只依赖于相对位置 $m-n$ 的函数。</p>
<p>对于一个向量 $x \in \mathbb{R}^d$ 在位置 $m$ 处，RoPE 的变换函数 $f(x, m)$ 可以定义如下：</p>
<p>如果向量维度是偶数 $d$，我们可以将其分成 $d/2$ 对，每对执行一个二维旋转。
对于向量 $x = [x_0, x_1, \ldots, x_{d-1}]^T$，RoPE 对其每个维度对 $(x_{2i}, x_{2i+1})$ 应用旋转：</p>
$$
f(x, m)_{2i} = x_{2i} \cos(m\theta_i) - x_{2i+1} \sin(m\theta_i) \\
f(x, m)_{2i+1} = x_{2i} \sin(m\theta_i) + x_{2i+1} \cos(m\theta_i)
$$<p>其中 $\theta_i$ 是预设的频率，通常定义为 $\theta_i = 10000^{-2i/d}$. $i=0, \dots, d/2 - 1$ 是维度对的索引。</p>
<p><strong>用矩阵形式表示：</strong>
我们可以将这种旋转操作表示为一个稀疏的块对角矩阵 $R_m^d$，其形式为：
</p>
$$R_m^d = \begin{pmatrix}
\cos(m\theta_0) & -\sin(m\theta_0) & 0 & 0 & \cdots \\
\sin(m\theta_0) & \cos(m\theta_0) & 0 & 0 & \cdots \\
0 & 0 & \cos(m\theta_1) & -\sin(m\theta_1) & \cdots \\
0 & 0 & \sin(m\theta_1) & \cos(m\theta_1) & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}$$<p>
那么，经过 RoPE 编码的查询和键可以表示为：
</p>
$$\mathbf{q}_m = R_m^d \mathbf{q}$$<p>
</p>
$$\mathbf{k}_n = R_n^d \mathbf{k}$$<p>
其中 $\mathbf{q}$ 和 $\mathbf{k}$ 是原始的查询和键向量 (不含位置信息) ，$\mathbf{q}_m$ 和 $\mathbf{k}_n$ 是经过 RoPE 处理后的查询和键向量。</p>
<p><strong>RoPE 的关键特性：点积与相对位置</strong>
经过 RoPE 变换后，注意力机制中的点积可以分解为：
</p>
$$\mathbf{q}_m^T \mathbf{k}_n = (R_m^d \mathbf{q})^T (R_n^d \mathbf{k})$$<p>
由于 $R_m^d$ 是正交矩阵，其逆矩阵等于其转置，即 $(R_m^d)^T = (R_m^d)^{-1} = R_{-m}^d$. 因此有
</p>
$$\mathbf{q}_m^T \mathbf{k}_n = \mathbf{q}^T (R_m^d)^T R_n^d \mathbf{k} = \mathbf{q}^T R_{-m}^d R_n^d \mathbf{k} = \mathbf{q}^T R_{n-m}^d \mathbf{k}$$<p>
这个最终结果 $\mathbf{q}^T R_{n-m}^d \mathbf{k}$ 表明，两个向量的点积只依赖于它们的<strong>相对位置差 $n-m$</strong>，而与它们的绝对位置 $n$ 和 $m$ 无关。这就是 RoPE 能够编码相对位置信息的数学基础。</p>
<h1 id="workflow">Workflow</h1>
<h2 id="notation">Notation</h2>
<ul>
<li>$d$: embedding 维度</li>
<li>$d_h$: 每个注意力头的维度</li>
<li>$\mathbf{h}_t\in\mathbb{R}^d$: 某个 attention 层第 t 个 token 的输入。</li>
</ul>
<h2 id="kv-compression">KV Compression</h2>
$$
\textcolor{red}{c_t^{KV}} = W^{DKV}h_t  \tag{1}
$$<p>
</p>
$$
[k_{t,1}^{C}, k_{t,2}^{C}, \ldots, k_{t,n_h}^{C}] = k_t^C = W^{UK}c_t^{KV}  \tag{2}
$$<p>
</p>
$$
\textcolor{red}{k_t^R} = \text{RoPE}(W^{KR}h_t)  \tag{3}
$$<p>
</p>
$$
k_{t,i} = [k_{t,i}^C, k_{t}^R] \tag{4}
$$<p>
</p>
$$
[v_{t,1}^C, v_{t,2}^C, \ldots, v_{t,n_h}^C] = v_t^C = W^{UV}c_t^{KV} \tag{5}
$$<ul>
<li>$c_t^{KV} \in \mathbb{R}^{d_c}$: 压缩后的 KV 潜在向量。</li>
<li>$d_c (\ll d_h n_h)$: KV 压缩到的维度。</li>
<li>$W^{DKV} \in \mathbb{R}^{d_c \times d}$: KV 降维投影矩阵。</li>
<li>$W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}$ 分别是 K &amp; V 的升维投影矩阵。</li>
<li>$W^{KR} \in \mathbb{R}^{d_h^R \times d}$: 用于生成携带 RoPE 的解耦键的矩阵 (Su et al., 2024)</li>
</ul>
<p>红色的是需要缓存的向量，后续说明原因。注意到对 K 进行 RoPE 之前是对输入向量乘以了个投影再进行的。而且 K 的每个注意力头被拼接的都是同一个 $k_{t}^R$，有点类似于 MQA.</p>
<h2 id="q-compression">Q Compression</h2>
$$c_t^Q = W^{DQ}h_t \tag{6}$$<p>
</p>
$$[q_{t,1}^C, q_{t,2}^C, \ldots, q_{t,n_h}^C] = q_t^C = W^{UQ}c_t^Q \tag{7}$$<p>
</p>
$$[q_{t,1}^R, q_{t,2}^R, \ldots, q_{t,n_h}^R] = q_t^R = \text{RoPE}(W^{QR}q_t^C) \tag{8}$$<p>
</p>
$$q_{t,i} = [q_{t,i}^C, q_{t,i}^R] \tag{9}$$<ul>
<li>$c_t^Q \in \mathbb{R}^{d_c'}$: Q 压缩后的潜在向量。</li>
<li>$d_c'(\ll d_h n_h)$ 表示 Q 压缩后的维度。</li>
<li>$W^{DQ} \in \mathbb{R}^{d_c' \times d}, W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}$: 分别是 Q 的降维和升维矩阵。</li>
<li>$W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c'}$ 是用于生成携带 RoPE 的解耦 Q 的矩阵。</li>
</ul>
<p>注意到对 Q 的 RoPE 是在压缩后进行的，即为每个注意力头都生成了一个位置编码信息后进行拼接。</p>
<h2 id="attention-computation">Attention Computation</h2>
<p>最终 $q_{t,i}$, $k_{j,i}$, $v_{j,i}^C$ 被组合起来以生成最终的注意力输出 $u_t$</p>
$$\mathbf{o}_{t,i} = \sum_{j=1}^{t} \text{Softmax}\left(\frac{q_{t,i}^T \mathbf{k}_{j,i}}{\sqrt{d_h + d_R}}\right)v_{j,i}^C \tag{10}$$<p>
</p>
$$\mathbf{u}_t = W^O[\mathbf{o}_{t,1}, \mathbf{o}_{t,2}, \ldots, \mathbf{o}_{t,n_h}] \tag{11}$$<ul>
<li>$W^O \in \mathbb{R}^{d \times d_h n_h}$: 输出投影矩阵。</li>
</ul>
<h1 id="why-decoupled-rope">Why Decoupled RoPE</h1>
<p>假设不加 RoPE 的情况下进行 $q_{t,i}$, $k_{j,i}$ 的内积则有</p>
$$
q_{t,i}^{T}\times k_{j,i}=(W_{(i)}^{UQ}c_{t}^{Q})^{T}\times W_{(i)}^{UK}c_{j}^{KV}=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}W_{(i)}^{UK}\times c_{j}^{KV} \tag{12}
$$<p>RoPE 通过对向量应用一个<strong>位置依赖的旋转变换</strong>来注入相对位置信息。对于一个向量 $X$ 在位置 $t$，RoPE 可以被表示为一个旋转矩阵 $R_t$ 乘以 $X$：
</p>
$$\text{RoPE}(X, t) = R_t X$$<p>
这里的 $R_t$ 是一个正交旋转矩阵，它取决于位置 $t$.</p>
<p>如果直接对压缩后 $k_t^C$ 的 使用 RoPE 那么情况会变成</p>
$$
\begin{aligned}
q_{t,i}^{T}\times k_{j,i}&=(\mathcal{R}_{t}W_{(i)}^{UQ}c_{t}^{Q})^{T}\times\mathcal{R}_{j}W_{(i)}^{UK}c_{j}^{KV} \\
&=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}\mathcal{R}_{t}^{T}\mathcal{R}_{j}W_{(i)}^{UK}\times c_{j}^{KV}\\
&=(c_{t}^{Q})^{T}\times(W_{(i)}^{UQ})^{T}\mathcal{R}_{t-j}W_{(i)}^{UK}\times c_{j}^{KV}
\end{aligned} \tag{13}
$$<p>中间的矩阵与相对位置有关，无法提前计算出来。因此文中就是对所有头都使用同一个 k 和计算 RoPE. 拼接后的向量再计算时</p>
$$
q_{t,i}^T\times k_{j,i}=[q_{t,i}^C;q_{t,i}^R]^T\times[k_{j,i}^C;k_t^R]=(q_{t,i}^C,k_{j,i}^C)+(q_{t,i}^R,k_t^R) \tag{14}
$$<p>前一部分按照公式 (12) 进行计算，后一部分按照 MQA 方式计算。因此只用缓存 $c_t^{KV}$ 和 $k_t^R$.</p>
<h1 id="source-code">Source Code</h1>
<p><a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/f6e34dd26772dd4a216be94a8899276c5dca9e43/inference/model.py#L393-L494">DeepSeek-V3 MLA</a> 对应的源码位置</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MLA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    Multi-Head Latent Attention (MLA) Layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">    Attributes:
</span></span></span><span class="line"><span class="cl"><span class="s2">        dim (int): Dimensionality of the input features.
</span></span></span><span class="line"><span class="cl"><span class="s2">        n_heads (int): Number of attention heads.
</span></span></span><span class="line"><span class="cl"><span class="s2">        n_local_heads (int): Number of local attention heads for distributed systems.
</span></span></span><span class="line"><span class="cl"><span class="s2">        q_lora_rank (int): Rank for low-rank query projection.
</span></span></span><span class="line"><span class="cl"><span class="s2">        kv_lora_rank (int): Rank for low-rank key/value projection.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        qk_head_dim (int): Total dimensionality of query/key projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        v_head_dim (int): Dimensionality of value projections.
</span></span></span><span class="line"><span class="cl"><span class="s2">        softmax_scale (float): Scaling factor for softmax in attention computation.
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">ModelArgs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算当前进程（卡）负责的注意力头数量，用于模型并行</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">//</span> <span class="n">world_size</span> 
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">q_lora_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">kv_lora_rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_nope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># QK 头总维度 = 非 RoPE 部分 + RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="n">args</span><span class="o">.</span><span class="n">qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">v_head_dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 查询投影 (wq) 的 LoRA 实现</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果 q_lora_rank 为 0，表示不使用 LoRA，直接进行全秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 dim 维度的输入投影到 n_heads * qk_head_dim 维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果 q_lora_rank &gt; 0，使用 LoRA 结构进行低秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wq_a: dim -&gt; q_lora_rank (低秩投影的第一步)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># q_norm: RMSNorm 应用于低秩维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wq_b: q_lora_rank -&gt; n_heads * qk_head_dim (低秩投影的第二步)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">wq_b</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 键值投影 (wkv) 的 LoRA 实现</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># wkv_a: dim -&gt; kv_lora_rank + qk_rope_head_dim</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应图中的 W^{DKV} 投影到低秩 KV 潜在空间 (kv_lora_rank) 和解耦的 RoPE 键 (qk_rope_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 kv_lora_rank 对应公式中的 d_c</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 qk_rope_head_dim 对应公式中的 d_h</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wkv_a</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># kv_norm: RMSNorm 应用于低秩维度</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># wkv_b: kv_lora_rank -&gt; n_heads * (qk_nope_head_dim + v_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应图中的 W^{UK} 和 W^{UV} 的组合</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 它将压缩后的 KV 潜在向量 (kv_lora_rank) 投影回非 RoPE 键 (qk_nope_head_dim) 和值 (v_head_dim) 的高维度空间</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span> <span class="o">=</span> <span class="n">ColumnParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 输出投影 (wo)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">wo</span> <span class="o">=</span> <span class="n">RowParallelLinear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Softmax 缩放因子，用于注意力分数的缩放，防止内积过大</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 如果序列长度超过原始训练长度，根据 RopeFactor 进行额外缩放，用于处理长序列外推问题</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">original_seq_len</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">mscale</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">mscale</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">rope_factor</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">*</span> <span class="n">mscale</span> <span class="o">*</span> <span class="n">mscale</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 根据注意力实现方式（naive 或 optimized）选择不同的 KV 缓存结构</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># naive 实现直接缓存完整键 K 和值 V</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_cache: (max_batch_size, max_seq_len, n_local_heads, qk_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;k_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># v_cache: (max_batch_size, max_seq_len, n_local_heads, v_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;v_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># optimized 实现缓存压缩后的 KV 潜在向量和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># kv_cache: (max_batch_size, max_seq_len, kv_lora_rank) - 对应论文中的 c_t^{KV}</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;kv_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># pe_cache: (max_batch_size, max_seq_len, qk_rope_head_dim) - 对应论文中的 k_t^R</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&#34;pe_cache&#34;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">max_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Forward pass for the Multi-Head Latent Attention (MLA) Layer.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args:
</span></span></span><span class="line"><span class="cl"><span class="s2">            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).
</span></span></span><span class="line"><span class="cl"><span class="s2">            start_pos (int): Starting position in the sequence for caching.
</span></span></span><span class="line"><span class="cl"><span class="s2">            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.
</span></span></span><span class="line"><span class="cl"><span class="s2">            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">        Returns:
</span></span></span><span class="line"><span class="cl"><span class="s2">            torch.Tensor: Output tensor with the same shape as the input.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_pos</span> <span class="o">=</span> <span class="n">start_pos</span> <span class="o">+</span> <span class="n">seqlen</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. 查询 (Q) 的生成</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_lora_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 全秩投影</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># LoRA 投影：x -&gt; wq_a -&gt; q_norm -&gt; wq_b</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wq_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wq_a</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape Q</span>
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 分离 Q 的非 RoPE 部分和 RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># q_nope 对应论文中的 q_{t,i}^C (非位置信息查询)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># q_pe 对应论文中的 q_{t,i}^R (携带 RoPE 的解耦查询)</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_nope</span><span class="p">,</span> <span class="n">q_pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 对 Q 的 RoPE 部分应用旋转位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应论文中的 q_t^R = RoPE(W^{QR}c_t^Q) 的 RoPE 部分</span>
</span></span><span class="line"><span class="cl">        <span class="n">q_pe</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">q_pe</span><span class="p">,</span> <span class="n">freqs_cis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. 键值 (KV) 的生成</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 将输入 x 投影到低秩 KV 潜在空间和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 对应论文中的 c_t^{KV} 和 k_t^R</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_a</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 分离出 KV 潜在向量和解耦的 RoPE 键</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># kv 对应论文中的 c_t^{KV}</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># k_pe 对应论文中的 k_t^R (RoPE 解耦键)</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span><span class="p">,</span> <span class="n">k_pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 对 K 的 RoPE 部分应用旋转位置编码</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 注意 k_pe.unsqueeze(2) 是因为 apply_rotary_emb 期望 (..., seq_len, head_dim) 结构</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 这里的 k_pe 可能是 (bsz, seqlen, qk_rope_head_dim)，需要添加一个 head 维度</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_pe</span> <span class="o">=</span> <span class="n">apply_rotary_emb</span><span class="p">(</span><span class="n">k_pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">freqs_cis</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. 注意力计算：根据实现方式 (naive 或 optimized)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Naive 实现直接拼接 Q 的 RoPE 和非 RoPE 部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">q_nope</span><span class="p">,</span> <span class="n">q_pe</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Q 恢复为 (bsz, seqlen, n_local_heads, qk_head_dim)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 对 KV 潜在向量应用归一化，并进行第二阶段投影</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中将 c_t^{KV} 投影到非 RoPE 键和值的部分 (k_t^C 和 v_t^C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span><span class="p">(</span><span class="n">kv</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 KV 结果重塑为 (batch_size, seq_len, n_local_heads, qk_nope_head_dim + v_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 分离出非 RoPE 键和值</span>
</span></span><span class="line"><span class="cl">            <span class="n">k_nope</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 拼接非 RoPE 键和 RoPE 键，组成完整的键 K</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_pe 之前是 (bsz, seqlen, 1, qk_rope_head_dim)，需要 expand 到 n_local_heads</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k_nope</span><span class="p">,</span> <span class="n">k_pe</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 K 和 V 缓存 (在推理时用于自回归生成)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">k_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">v_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算注意力分数 (Q @ K^T)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># scores: (batch_size, q_seq_len, n_local_heads, k_seq_len)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 使用整个缓存中的键进行计算</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshd,bthd-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># optimized 实现</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 获取 wkv_b 权重，如果使用了量化则进行反量化</span>
</span></span><span class="line"><span class="cl">            <span class="n">wkv_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">weight</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">scale</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">weight_dequant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">wkv_b</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">block_size</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">            <span class="c1"># 将 wkv_b 重塑为 (n_local_heads, head_dim, kv_lora_rank) 以便进行逐头的操作</span>
</span></span><span class="line"><span class="cl">            <span class="n">wkv_b</span> <span class="o">=</span> <span class="n">wkv_b</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_local_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_lora_rank</span><span class="p">)</span> <span class="c1"># (n_heads, (qk_nope+v), kv_rank)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算 Q_nope 与 K_nope 的点积 (通过 kv 缓存)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># q_nope: (bsz, seqlen, n_local_heads, qk_nope_head_dim)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wkv_b[:, :self.qk_nope_head_dim] 是 W^{UK} 的部分</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 这对应论文中的 Softmax(q_{t,i}^C @ c_{j,i}^{KV}) 的第一项</span>
</span></span><span class="line"><span class="cl">            <span class="n">q_nope</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshd,hdc-&gt;bshc&#34;</span><span class="p">,</span> <span class="n">q_nope</span><span class="p">,</span> <span class="n">wkv_b</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">qk_nope_head_dim</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 KV 缓存 (kv_cache 对应论文中的 c_t^{KV})</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_norm</span><span class="p">(</span><span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 更新 PE 缓存 (pe_cache 对应论文中的 k_t^R)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># k_pe 之前是 (bsz, seqlen, 1, qk_rope_head_dim)，squeeze 掉那个 1 维度</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">pe_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_pe</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 计算注意力分数</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第一项: 非 RoPE 查询 q_nope 与缓存的 kv_cache (压缩键) 的点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(q_{t,i}^C @ c_{j,i}^{KV}) 的第一部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshc,btc-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q_nope</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span> <span class="o">+</span> \
</span></span><span class="line"><span class="cl">                      <span class="c1"># 第二项: RoPE 查询 q_pe 与缓存的 pe_cache (解耦 RoPE 键) 的点积</span>
</span></span><span class="line"><span class="cl">                      <span class="c1"># 对应论文中的 Softmax(q_{t,i}^R @ k_{j,i}^R) 的第二部分</span>
</span></span><span class="line"><span class="cl">                      <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshr,btr-&gt;bsht&#34;</span><span class="p">,</span> <span class="n">q_pe</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="c1"># 应用缩放因子</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 应用注意力掩码 (如因果掩码，防止看到未来信息)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">scores</span> <span class="o">+=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># unsqueeze(1) 广播到 heads 维度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 对分数应用 Softmax 得到注意力权重</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. 值 (V) 的加权求和</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn_impl</span> <span class="o">==</span> <span class="s2">&#34;naive&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Naive 实现直接与 V 缓存进行点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 sum(Softmax(...) * v_{j,i}^C)</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bsht,bthd-&gt;bshd&#34;</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span> <span class="c1"># optimized 实现</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># optimized 实现通过 wkv_b 的值部分将加权后的压缩 KV 还原为 V</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第一步: 将注意力权重与缓存的 kv_cache (压缩值) 进行点积</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(...) * c_{j,i}^{KV} 的第一部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bsht,btc-&gt;bshc&#34;</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache</span><span class="p">[:</span><span class="n">bsz</span><span class="p">,</span> <span class="p">:</span><span class="n">end_pos</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 第二步: 将加权后的压缩值通过 wkv_b 的值投影部分还原为最终的值向量</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># wkv_b[:, -self.v_head_dim:] 是 W^{UV} 的部分</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 对应论文中的 Softmax(...) * v_{j,i}^C 的第二部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&#34;bshc,hdc-&gt;bshd&#34;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">wkv_b</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">v_head_dim</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 将所有头的结果展平并进行最终的输出投影</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wo</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># x.flatten(2) 将 (bsz, seqlen, n_local_heads, v_head_dim) 展平为 (bsz, seqlen, n_local_heads * v_head_dim)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>ServingLLMsOnHuaweiCloudMatrix384</title>
      <link>http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/</link>
      <pubDate>Tue, 17 Jun 2025 21:50:16 +0800</pubDate>
      <guid>http://localhost:1313/blogs/servingllmsonhuaweicloudmatrix384/</guid>
      <description>Paper Reading of Serving Large Language Models on Huawei CloudMatrix384</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<ol>
<li>
<p><strong>CloudMatrix384硬件架构</strong>:</p>
<ul>
<li>论文提出了一种 <strong>peer-to-peer</strong> 的硬件设计，包含 384 个 <strong>Ascend 910C NPU</strong> 和 192 个 <strong>Kunpeng CPU</strong>，通过 <strong>Unified Bus (UB) Network</strong> 互联。UB 网络支持高带宽 (392 GB/s单向带宽)和低延迟 (1.9 µs)的全局通信，解决了传统AI集群中跨节点通信的瓶颈问题。</li>
<li>架构特点包括:
<ul>
<li><strong>资源解耦和池化</strong>: 计算、存储和网络资源可以动态分配，支持灵活的并行策略 (如专家并行EP、数据并行DP).</li>
<li><strong>三层网络平面</strong>: UB平面 (超节点内通信)、RDMA平面 (跨超节点通信)和VPC平面 (数据中心网络接入)。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CloudMatrix-Infer软件优化</strong>:</p>
<ul>
<li><strong>预填充-解码-缓存 (PDC)解耦架构</strong>: 将LLM推理拆分为Prefill, Decode &amp; Caching 三个子系统，通过 UB网络实现高效协同。</li>
<li><strong>large-scale expert parallelism (EP) strategy</strong>: 支持高达<strong>EP320</strong>的专家并行度，每个NPU芯片承载一个专家，减少MoE模型中的通信开销。</li>
<li><strong>UB驱动的分布式缓存</strong>: 利用 <strong>弹性内存服务 (EMS)</strong> 构建全局缓存池，支持KV缓存和模型权重的快速访问。</li>
</ul>
</li>
<li>
<p><strong>性能优化技术</strong>:</p>
<ul>
<li>** micro-batch 流水线 (Microbatch Pipeline)**: 重叠计算和通信，提升资源利用率。</li>
<li><strong>INT8量化</strong>: 在Ascend 910C 上实现高效的8位推理，保持模型精度。</li>
</ul>
</li>
</ol>
<p>论文使用<strong>DeepSeek-R1</strong> (671B参数MoE模型)验证了 CloudMatrix-Infer 的性能:</p>
<ul>
<li><strong>预填充吞吐量</strong>: 6,688 tokens/s/NPU (4.45 tokens/s/TFLOPS)，优于 NVIDIA H100 的 SGLang (3.75 tokens/s/TFLOPS).</li>
<li><strong>解码吞吐量</strong>: 1,943 tokens/s/NPU (1.29 tokens/s/TFLOPS)，在TPOT &lt;50 ms约束下仍能保持高吞吐。</li>
<li><strong>缓存命中率提升</strong>: 上下文缓存 (Context Caching)在 90% 重用率时，预填充时间减少 59%.</li>
</ul>
<h1 id="1-introduction">1. Introduction</h1>
<p>LLM 的发展趋势有以下几点:</p>
<ol>
<li>参数规模的指数级增长。DeepSeek-R1, LLaMA-4 和 Qwen-3 通常扩展到数千亿甚至数万亿参数.</li>
<li>专家混合 (MoE)架构的广泛采用。MoE 通过每个 token 选择性激活少数专家，引入了结构稀疏性，实现了更大模型下的效率提升，但同时在专家路由和同步方面带来了新的系统级挑战。</li>
<li>上下文长度的大幅提高。上下文窗口从数万 token 扩展到超过一百万 token，对注意力计算和 KV cache 存储施加了巨大压力。 KV Cache 大小随着并发用户数量线性增长，这对其的分布、放置和访问方式提出了重大限制。</li>
</ol>
<p>LLM 服务系统必须适应可变长度的用户输入、跨 token 的不平衡专家激活以及高度突发的用户查询，同时保持严格的延迟和吞吐量目标。满足这些需求不仅仅是简单地扩展硬件资源，而是需要全面的软硬件协同设计，包括紧密集成的计算、内存和网络硬件资源，辅以智能任务调度、自适应运行时编排以及弹性资源管理策略，能够动态响应不断演变的模型结构和波动的工作负载。</p>
<p>为了应对这些挑战，论文引入了 <strong>华为CloudMatrix</strong>，一个旨在重塑 AI 基础设施基础的下一代 AI 数据中心架构。其首个生产级实现是 <strong>CloudMatrix384</strong>，它具备以下核心特点:</p>
<ul>
<li><strong>硬件构成</strong>: 它是一个集成了384个<strong>昇腾 (Ascend)910C NPU</strong>、192个 <strong>鲲鹏 (Kunpeng)CPU</strong> 的AI超级节点。</li>
<li><strong>核心网络</strong>: 所有组件通过一个超高带宽、低延迟的 UB 网络互联，实现了硬件上的 **完全点对点 (peer-to-peer)**通信。</li>
<li><strong>架构优势</strong>: 与传统层级式设计不同，UB网络允许计算、内存等资源被动态池化、统一访问和独立扩展，特别适合处理MoE专家并行和分布式KV缓存访问这类通信密集型操作。</li>
</ul>
<p>基于CloudMatrix384硬件，论文提出了一个名为 <strong>CloudMatrix-Infer</strong> 的综合性LLM服务解决方案，其包含了三大核心创新:</p>
<ol>
<li>
<p><strong>点对点服务架构</strong>: 该架构将推理系统解耦为 <strong>预填充 (prefill)</strong>, <strong>解码 (decode)</strong> 和 <strong>缓存 (caching)</strong> 三个可独立扩展的资源池。借助UB网络，所有 NPU 都能统一访问共享的缓存数据，从而摆脱了传统架构中因数据局部性限制而导致的调度复杂性和效率低下问题。</p>
</li>
<li>
<p><strong>大规模专家并行 (LEP)策略</strong>: 该策略专为MoE模型优化，利用UB网络高效地进行 token 分发和专家输出合并。它支持极高的专家并行度 (如 <strong>EP320</strong> )，允许每个 NPU Die 只承载一个专家，从而显著降低解码延迟。</p>
</li>
<li>
<p><strong>硬件感知优化</strong>: 包括为昇腾芯片高度优化的算子、基于 micro-batch 的流水线技术 (以重叠计算和通信)以及INT8量化 (以提升计算效率和减少内存消耗)。</p>
</li>
</ol>
<p>在 DeepSeek-R1 模型上的测试表明，CloudMatrix-Infer 实现了业界领先的性能和效率:</p>
<ul>
<li><strong>预填充性能</strong>: 每个NPU的吞吐量为 <strong>6,688 tokens/s</strong>，计算效率为 <strong>4.45 tokens/s/TFLOPS</strong>.</li>
<li><strong>解码性能</strong>: 在低于50ms的单Token输出延迟 (TPOT)下，每个NPU的吞吐量为<strong>1,943 tokens/s</strong>，计算效率为 <strong>1.29 tokens/s/TFLOPS</strong>.</li>
<li><strong>性能对比</strong>: 预填充和解码的计算效率均<strong>超过了在NVIDIA H100上运行SGLang和在NVIDIA H800上运行DeepSeek的公开数据</strong>.</li>
<li><strong>准确性</strong>: 在昇腾910C上进行的 INT8 量化，其模型准确率与官方 DeepSeek-R1 API 在 16 个基准测试中相当。</li>
</ul>
<h1 id="2-llm-trends-and-their-challenges-for-datacenter-infrastructure">2. LLM Trends and Their Challenges for Datacenter Infrastructure</h1>
<h1 id="21-llm-trends">2.1. LLM Trends</h1>
<p><strong>Ever-Larger Parameter Counts.</strong> scaling law 表明，增加 LLM 的参数数量可以提升其在各种任务上的表现。这一趋势的典型代表包括:</p>
<ul>
<li><strong>Meta 的 Llama 4 Behemoth</strong>: 拥有近 2 万亿参数。</li>
<li><strong>DeepSeek-V3</strong>: 包含 6710 亿参数。</li>
<li><strong>Google 的 PaLM</strong>: 包含 5400 亿参数。</li>
<li><strong>xAI 的 Grok-1</strong>: 拥有 3140 亿参数。</li>
</ul>
<p><strong>Sparsity through MoE.</strong> 为了控制不断攀升的训练和推理成本，现代 LLM 越来越多地采用稀疏激活的 MoE 架构。这种架构将模型的总大小与处理每个 token 所需的计算量解耦:</p>
<ul>
<li><strong>Mixtral 8x7B</strong>: 总参数量为 467 亿，但每个 token 只激活 129 亿参数。</li>
<li><strong>Databricks 的 DBRX</strong>: 总参数量为 1320 亿，每个 token 激活 360 亿参数。</li>
<li><strong>Meta 的 Llama 4 系列</strong>: Llama 4 Maverick 使用 128 个专家，而 Llama 4 Scout 使用 16 个专家。</li>
<li><strong>DeepSeek-V3</strong>: 将每层的专家数量从 160 个增加到 256 个，从而在不显著增加计算负载的情况下提升了模型容量。</li>
<li><strong>阿里巴巴的 Qwen3-235B</strong>: 集成了 128 个专家，每个 token 激活 220 亿参数。</li>
<li><strong>华为的盘古 Ultra MoE</strong>: 总参数量达 7180 亿，每个 token 激活 390 亿参数。</li>
</ul>
<p>这些模型共同凸显了 LLM 扩展策略的范式转变，即更强调通过架构稀疏性而非单纯的参数数量来提升性能和效率。</p>
<p><strong>Extension of Context Windows.</strong> LLM 上下文窗口的扩大使其能够处理更长的序列，这对于需要扩展推理和连贯性的任务至关重要。近期的进展包括:</p>
<ul>
<li><strong>OpenAI 的 GPT-4.5</strong>: 支持 128,000 个 token 的上下文窗口。</li>
<li><strong>Google 的 Gemini 2.5 Pro</strong>: 提供高达 100 万个 token 的上下文窗口。</li>
</ul>
<p>然而处理长文本会显著增加计算成本和推理延迟。为了缓解这一问题，生产系统普遍采用<strong>上下文缓存 (context caching)</strong> 技术。该技术通过存储和复用由先前提示片段生成的 KV block，来消除对提示的冗余注意力计算，从而降低延迟并提高效率。</p>
<h2 id="22-challenges-for-datacenter-infrastructure">2.2. Challenges for Datacenter Infrastructure</h2>
<p>上述 LLM 的发展趋势对底层的数据中心基础设施提出了严峻的新要求。随着模型能力的扩展，它们催生了如强化学习、交互式媒体生成和自主 AI 代理等日益复杂的工作负载。这些应用不仅需要海量的计算和内存资源，还需要对基础设施进行根本性的重新架构，以支持高带宽通信和低延迟存储，从而在动态、异构的真实世界条件下满足严格的服务水平目标。</p>
<p><strong>Scaling Communication-Intensive Parallelism.</strong>. 随着模型规模的增长，单个计算节点已无法容纳最先进的 AI 模型，必须使用多节点并行策略。尽管现有的 AI 集群通过 RDMA 网络支持跨节点通信，但其带宽和拓扑结构通常只为数据并行 (DP) 或流水线并行 (PP) 等通信量较小的场景优化。然而，张量并行 (Tensor Parallelism, TP) 和 专家并行 (Expert Parallelism, EP) 需要频繁、细粒度且低延迟的通信，这种通信模式难以在节点之间高效扩展。这迫使许多部署方案将 TP 和 EP 限制在单个计算节点内，从而限制了可扩展性。</p>
<p><strong>Maintaining High Utilization under Heterogeneous AI Workloads.</strong> 现代 AI 工作负载表现出高度多样化和动态的资源需求:</p>
<ul>
<li>训练任务通常是计算密集型的。</li>
<li>LLM 推理的解码阶段往往受限内存带宽。</li>
<li>自动驾驶模型训练等任务涉及大量的 CPU 端数据预处理。</li>
</ul>
<p>固定的节点配置无法高效地适应这种多样性，常常导致资源过配或利用不足。为了最大化效率和适应性，现代 AI 基础设施必须能够根据每种工作负载的特定需求，动态、细粒度地组合异构资源 (如 NPU, CPU 和内存).</p>
<p><strong>Enabling Converged Execution of AI and Data-Intensive Workloads.</strong> AI 工作流与传统数据密集型操作 (如数据摄取、预处理、检索、分析和模拟) 的交叉越来越频繁。同时，数据库、大数据和高性能计算 (HPC) 等通用工作负载本身也在不断集成 AI 功能。这种融合执行模式要求高吞吐、低延迟的通信和灵活的资源编排。然而，主要为传统通用工作负载优化的老旧数据中心基础设施难以满足这些苛刻的要求。</p>
<p><strong>Delivering Memory-class Storage Performance.</strong> 现代 AI 流水线操作的数据规模已远超传统存储系统的能力。诸如摄取 PB 级数据集、管理 TB 级模型检查点以及支持延迟敏感的推理 (特别是在使用大型 KV cache 和检索增强生成 RAG 模块时) 等任务，都需要存储子系统具备<strong>内存级的带宽、延迟和 IOPS (Input/Output Operations Per Second)</strong>. 围绕磁盘访问模式设计的传统存储层次结构频繁地成为性能瓶颈，因数据供给不足而导致 NPU 利用率低下。</p>
<h1 id="3-huawei-cloudmatrix">3 Huawei CloudMatrix</h1>
<h2 id="31-vision-for-huawei-cloudmatrix">3.1. Vision for Huawei CloudMatrix</h2>
<p>如下图所示，CloudMatrix 超越了传统以 CPU 为中心的分层设计，实现了无需 CPU 介质下包括 NPU、CPU、DRAM、SSD、NIC 及领域专用加速器在内的所有异构系统组件之间的直接高性能通信。该架构的核心是超高带宽、低延迟的 UB 网络。</p>
<p><strong>Scalable Communication for TP/EP.</strong> UB 网络可以为张量并行 (TP) 和专家并行 (EP) 提供强大的通信支持，使其能够轻松扩展到单节点边界之外。</p>
<p><strong>Flexible Resource Composition for Heterogeneous Workloads.</strong> 将 CPU、NPU、内存等资源解耦成独立的资源池，允许根据工作负载按需、细粒度地进行组合。</p>
<p><strong>Unified Infrastructure for Converged Workloads.</strong> 在单一架构内同时支持 AI 和数据密集型工作负载的融合执行。</p>
<p><strong>Memory-class Storage via Disaggregated Memory Pool.</strong> 通过聚合集群中所有 CPU 挂载的 DRAM，形成一个可通过 UB 访问的共享高性能内存池，为 KV cache 复用、参数加载等提供加速。</p>
<h2 id="32-cloudmatrix384-overview-a-fully-peer-to-peer-hardware-architecture">3.2. CloudMatrix384 Overview: A Fully Peer-to-Peer Hardware Architecture</h2>
<p>CloudMatrix384 是一个集成了 <strong>384 个昇腾 (Ascend) 910C NPU</strong> 和 <strong>192 个鲲鹏 (Kunpeng) CPU</strong> 的 AI 超级节点。其最显著的特点是，通过 UB 网络实现了跨节点通信性能与节点内性能的高度一致 (带宽下降 &lt; 3%，延迟增加 &lt; 1µs). CloudMatrix384 包含三个互补的网络平面:</p>
<ol>
<li><strong>UB 平面</strong>: 超级节点内的主要<strong>纵向扩展 (Scale-Up)</strong> 网络，以全互联、无阻塞的方式连接所有 384 个 NPU 和 192 个 CPU. 每个昇腾 910C NPU 为该平面贡献超过 392 GB/s 的单向带宽。</li>
<li><strong>RDMA 平面</strong>: 用于超级节点间的<strong>横向扩展 (Scale-Out)</strong> 通信，采用 RoCE 协议，确保与现有 RDMA 生态的兼容性。每个 NPU 为该平面贡献高达 400 Gbps 的单向带宽。</li>
<li><strong>VPC 平面</strong>: 通过擎天卡 (Qingtian Card) 将超级节点接入更广泛的数据中心网络，用于管理、控制和访问持久化存储等操作。</li>
</ol>
<h2 id="33-hardware-components">3.3. Hardware Components</h2>
<ul>
<li><strong>昇腾 910C 芯片</strong>: 作为系统的核心，它采用双晶粒 (dual-die) 封装。每个芯片包提供 752 TFLOPS 的 BF16/FP16 算力，支持 INT8 数据类型。它集成 128 GB 片上内存，总带宽高达 3.2 TB/s.</li>
<li><strong>昇腾 910C 节点</strong>: 每个计算节点集成 8 个昇腾 910C NPU 和 4 个鲲鹏 CPU.</li>
<li><strong>UB 交换系统</strong>: 采用两级 (L1/L2) 无阻塞交换网络拓扑，将所有节点紧密连接成一个统一的超级节点。</li>
</ul>
<h2 id="34-software-stack">3.4. Software Stack</h2>
<ul>
<li><strong>CANN (神经网络计算架构)</strong>: 华为为昇腾 NPU 开发的完整软件生态系统，类似于 NVIDIA 的 CUDA. 它包含驱动层、运行时层 (Runtime) 和库层 (如用于分布式通信的 HCCL)，并通过图引擎 (Graph Engine, GE) 对上层框架 (如 PyTorch, TensorFlow) 的计算图进行编译和优化。</li>
<li><strong>云部署基础设施软件</strong>: 包括 MatrixResource、MatrixLink、MatrixCompute 和 MatrixContainer 等一系列软件，用于在云环境中对 CloudMatrix 集群进行资源管理、网络配置和容器化部署。上层的 <strong>ModelArts</strong> 平台则提供端到端的 AI 开发和 MLOps 服务。</li>
</ul>
<h2 id="35-suitability-analysis-for-deepseek-models">3.5. Suitability Analysis for DeepSeek Models</h2>
<details class="custom-details">
    <summary class="custom-summary">DeepSeek Models and Their Deployment on NVIDIA H800</summary>
    <div><p>DeepSeek 在由 NVIDIA H800 GPU 组成的集群上部署其 V3 和 R1 模型，每个 GPU 内存 80 GB，节点内通过 NVLink 连接，节点间通过 400 Gbps InfiniBand 连接。该部署采用了分离式预填充-解码架构。在预填充阶段，DeepSeek 将四个 H800 节点（共 32 个 GPU）组织成一个部署单元。在每个单元内，256 个路由专家被策略性地分布在 GPU 上，每个 GPU 负责 9 个路由专家和 1 个共享专家。该配置标为 DP32+EP32，利用 32 个 GPU 之间的 EP，同时共享专家和 MLA 机制通过 DP 在同一组 GPU 上复制。在解码阶段，DeepSeek 进一步扩展并行度至 DP144+EP144，将 18 个节点组合成总计 144 个 GPU。在这一更大规模的部署中，每个 GPU 管理两个路由专家和一个共享专家，保持系统范围内 32 个路由专家副本的冗余。</p>
<p>DeepSeek 采用了 DualPipe 策略用于重叠计算和 All-to-all 通信。当一个 micro-batch 正在进行 MoE 相关的 dispatch 和 combine 时，下一个 micro-batch 则同时进行局部注意力或 MLP 计算。</p>
<p>每个 H800 GPU 在 prefill 阶段达到最高 9,213 token/s (6.3% 的上下文缓存命中率). 剔除缓存命中后有效吞吐量为 4,026 token/s. Decode 阶段，每个 GPU 维持平均 1,850 token/s 的吞吐量。</p>
</div>
</details><br>
<p>本节分析了 CloudMatrix384 的架构特性为何与大规模 MoE 模型 (以 DeepSeek-R1 为例) 的需求高度协同。</p>
<ul>
<li><strong>MoE 通信协同</strong>: 高带宽、低延迟的 UB 网络非常适合 MoE 模型中通信开销巨大的 token dispatch 和专家输出 combine 阶段。</li>
<li><strong>内存容量与管理</strong>: 整个超级节点提供高达 49.2 TB 的 NPU 内存，足以容纳像 DeepSeek-R1 (671B 参数) 这样的巨型模型及其庞大的 KV cache.</li>
<li><strong>上下文缓存复用</strong>: UB 网络使 NPU 能够以内存级的速度直接访问由 CPU DRAM 构成的解耦内存池，极大地加速了历史 KV cache 的读取，从而降低了首 token 生成延迟 (TTFT).</li>
<li><strong>量化支持</strong>: 昇腾 910C 对 INT8 计算的原生支持，为通过量化来降低模型内存占用、减少计算开销和提升推理性能提供了宝贵的机会。</li>
</ul>
<h1 id="4-deepseek-serving-on-huawei-cloudmatrix384">4. DeepSeek Serving on Huawei CloudMatrix384</h1>
<h2 id="41-overview-a-peer-to-peer-serving-architecture-with-pdc-disaggregation">4.1. Overview: A Peer-to-Peer Serving Architecture with PDC Disaggregation</h2>
<p>如下图所示，文中提出了一种独特的点对点服务架构，将系统划分为三个功能子系统，prefill, decode 和 caching (PDC)，每个子系统独立运行，并通过显式的 KV cache 传输接口进行通信。</p>
<p><strong>KVCache-centric vs. Peer-to-Peer Serving Architectures:</strong> 现有的 LLM 服务系统如 NVIDIA Dynamo（NVIDIA Corporation，2025）和 Mooncake（Qin 等，2025）采用 KVCache 为中心的设计，其中请求调度与 KV cache 的局部性紧密耦合。在这些系统中，请求通常被路由到已经持有对应 KV cache 的特定计算节点。此类缓存感知调度对于缓解远程内存访问带来的显著性能损失至关重要，因为节点内存访问（例如通过 PCIe，约 256 GB/s）远远快于节点间带宽（通常约 25 GB/s 或 200 Gbps）。因此，远程 KV cache 加载通常会带来较大延迟。然而，这种设计引入了复杂的调度难题，并且在动态工作负载下可能导致负载均衡恶化。此外，该设计限制了全局资源效率，因为解码节点上的 DRAM 通常处于孤立状态且利用率低，无法有效贡献于共享缓存容量。</p>
<p>CloudMatrix-Infer 中的点对点服务架构充分利用了 CloudMatrix384 的超高带宽 UB 互联。这使得基于解耦内存池构建的分布式缓存集群 (4.4) 能够实现统一访问。无论 NPU 是执行预填充任务还是解码任务，都可以直接访问共享解耦内存池。这种完全的点对点设计有效地扁平化了内存层次结构，弥补了本地访问和远程访问延迟之间的传统差距。这种将请求调度与 KV 缓存放置解耦带来了几个关键优势。</p>
<ol>
<li>使推理请求可以调度到任何可用的 NPU 实例，而不受数据局部性的限制。显著提升了系统范围内的负载均衡和 NPU 利用率。</li>
<li>消除了对复杂的亲和性调度机制的需求，从而降低了架构复杂性，简化了系统维护。</li>
<li>通过在预填充和解码节点之间共享 DRAM 资源，系统形成了一个统一的弹性缓存底层，提高了内存利用率，增加了缓存命中率，并在负载失衡或突发情况下提供了更强的弹性。</li>
</ol>
<p>每个预填充实例在 CloudMatrix384 上配备 16 个 Ascend 910C NPU（32 个芯片），并以 32 路专家并行（EP32）运行。每个 rank 上放置 10 个专家: 1 个共享专家、8个路由专家和 1 个冗余路由专家，以支持专家并行负载均衡（EPLB）。为了进一步提高效率，对 MLA 计算采用混合并行策略，并应用基于 micro-batch 的流水线以重叠通信开销（4.3）。</p>
<p>每个解码实例分配了 160 个 Ascend 910C NPU（320 个芯片）应于 MoE 层的 320 路专家并行（EP320）. 每个 rank 承载一个专家，整体配置包括 32 个共享专家、256 个独立路由专家和 32 个冗余路由专家，以支持 EPLB。为了进一步加速解码，引入了优化的 Ascend 原生算子、流水线解码策略以及 MTP (4.2).</p>
<h2 id="42-tightly-coupled-decode-with-large-scale-expert-parallelism">4.2. Tightly-Coupled Decode with Large-scale Expert Parallelism</h2>
<h3 id="421-fused-communication-operators-for-lep">4.2.1 Fused Communication Operators for LEP</h3>
<p>MoE 计算流程为: 在 gate network 为每个 token 选择 Top-K 专家后，进入 FFN 阶段前需要两次 all-to-all 通信。第一次 all-to-all 操作交换路由信息 (如 token 到专家的分配信息). 第二次 all-to-all 操作交换实际的 token 数据。该数据最初以 BF16 格式存储，为减少通信和计算开销每个 NPU 进行量化为 INT8 格式，然后由分配的 FFN 处理。计算完成后，第三次 all-to-all 通信将专家输出发送回其原先的 rank，每个 NPU 执行最终的 token 合并以重构输出。该流程存在三个低效问题:</p>
<ol>
<li>通信开销大: 三次 all-to-all 通信引加上通信横跨几百个 NPU 导致延迟很大。</li>
<li>动态形状: 因为每次解码迭代中分配给每个专家的 token 数量不同，导致 all2ll 通信中数据形状不固定。需要动态内存分配和频繁的 CPU-NPU 同步，降低了执行效率。</li>
<li>顺序依赖:MoE 计算的顺序执行特性导致步骤之间存在依赖关系，降低了资源利用率和吞吐量。</li>
</ol>
<p>为了解决这些问题，本文开发了 FusedDispatch 和 FusedCombine 两个融合算子，将通信和计算集成在一起，专门设计用于在 CloudMatrix384 上实现最佳的解码性能。</p>
<p><strong>AIV-Direct Communication across NPUs</strong>: AIV-Direct 使 AI vector 核心能够通过 UB 互连直接将数据写入远程 NPU 的内存，完全绕过了易产生延迟的 Serial Direct Memory Access (SDMA) 路径 (下图蓝线).</p>
<p><strong>Early Quantization</strong>: Dispatch 的时候不再发送 BF16 数据，而是传输 INT8 量化后的数据及其缩放因子。INT8 表示每个 token (7,168 维) 需要 7 KB。 缩放因子占用 4 字节（INT32），但为了对齐分配 512 B 给缩放因子。因此，每个 token 的传输消息大小为 7.5 KB.</p>
<p><strong>Static Execution via Shared‑Memory Pre‑allocation</strong>: 在每个 NPU rank 中静态预分配共享内存缓冲区。</p>
$$
buffer_size = rank_num × max_tokens × msg_size
$$<p>
其中 $max_tokens = local_batch × \min(topK, experts_per_die)$. msg_size 是每个 token 的消息长度（INT8 量化后，dispatch 为 7.5 KB，combine 为 14 KB）</p>
<p>由于 FusedDispatch 和 FusedCombine 是连续执行的，共用一个缓冲区会产生竞争，因此采用双缓冲机制来避免写入覆盖。本文设置中每个芯片处理最多 local_batch=96，并最多放置 2 专家，产生 max_tokens=96×min⁡(8,1)=96. 在包含 320 个设备的通信域中，分发缓冲区占用 320×96×7.5⁢KB≈225⁢MB ，合并缓冲区占用 320×96×14⁢KB≈420⁢MB.</p>
<p><strong>Data-Sending Pipeline</strong>: 远程数据写入需要计算目标 NPU 预分配缓冲区的偏移量，但顺序执行此计算和传输会导致执行阻塞。因此文中将执行分成三阶段流水线</p>
<ol>
<li>将下一个 micro-batch 复制到本地 UBuffer.</li>
<li>计算远程缓冲区偏移量，并进行 INT8 量化。</li>
<li>向对应 NPU 的内存发起 AIV-Direct 写入。</li>
</ol>
<p>完整的 FusedDispatch 如下:</p>
<ol>
<li>每个设备会检查自己有哪些 token 要发给其他设备的专家。 AIV 核心从内存里把 token 读到本地的 UBuffer. 把数据量化成 INT8 格式，同时记录缩放因子。给每个 token 加上标签，包括：源 rank ID，token 属于哪个批次（batch-slot ID）以及 token 在数据里的位置（key offset）.通过 AIV-direct 把打包好的数据写到目标节点的预分配内存里。</li>
<li>等所有 token 数据都通过 AIV-direct 发完后，系统会设置一个 barrier，确保每个设备的数据都写完了。设备计算每个专家收到了多少 token 后会互相同步，确保计数没出错。最后设备通过 AIV-direct 再发一个完成标志和设备上每个专家的 token 计数 (每个专家发了几个 token).</li>
<li>每个设备会一直检查别人发来的完成标志，等着所有标志都变成 1. 收到所有标志后，设备会读取每个专家的 token 计数，算出数据在内存里的偏移。然后，设备里的 AIV 核心会并行工作，把收到的数据从共享内存里取出来，整理成一个连续的的输出缓冲区。</li>
</ol>
<p>完整的 FusedCombine 如下:</p>
<ol>
<li>结合 AIV 核心遍历其负责的 peer 设备，根据接收计数，从内存中提取 FFN 结果数据，存入本地 UBuffer。核心利用 token 元数据计算原始设备的接收地址，通过 AIV-direct 通道将数据传输至原始节点的预分配缓冲区。
标志更新</li>
<li>核心根据 token 元数据推算目标设备的标志地址，通过 AIV-direct 发出原子加操作，在对目标节点上的标志增量计数。</li>
<li>每个节点的核心会一直检查自己的标志，等待所有标志都变成 1. 随后从共享内存收集 FFN 输出，提取对应缩放因子，进行逐元素缩放并求和。将合并结果加到共享的 FFN 输出中，生成最终的 token 输出。</li>
</ol>
<h3 id="422-mla-optimization">4.2.2 MLA Optimization</h3>
]]></content:encoded>
    </item>
    <item>
      <title>Fast-dLLM</title>
      <link>http://localhost:1313/blogs/fast-dllm/</link>
      <pubDate>Thu, 12 Jun 2025 23:01:49 +0800</pubDate>
      <guid>http://localhost:1313/blogs/fast-dllm/</guid>
      <description>Paper Reading of Fast-dLLM</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Diffusion LLMs 被视为下一代文本生成技术的有力竞争者，其核心优势在于理论上可以并行生成多个 token，从而有望实现比自回归模型快几个数量级的推理速度。谷歌的 Gemini Diffusion 和 Inception Labs 的Mercury等模型已经展示了其惊人的潜力，宣称能达到每秒上千 token 的生成速度。</p>
<p>当前开源的扩散LLM (LLaDA、Dream) 在实际应用中的速度远远达不到预期，甚至比优化良好的自回归模型还要慢。这篇论文的工作，就是要拆掉阻碍扩散 LLM 起飞的两座大山。</p>
<ol>
<li>无法使用 KV Cache</li>
</ol>
<p>扩散LLM的注意力机制是双向的，即一个 token 的生成不仅依赖于它前面的内容，也依赖于它后面的内容 (尽管后面可能是未知的 MASK token ) 。这种特性使得过去的信息和未来的信息相互纠缠，无法像自回归模型那样简单地缓存和复用过去的信息。导致扩散LLM在每一步推理中都需要进行大量的重复计算，严重拖慢了速度。</p>
<p>Fast-dLLM 的第一个核心贡献，就是提出了一种分块近似 (block-wise approximate) KV Cache 机制。</p>
<blockquote class="quote"><p>While the bidirectional nature of attention in Diffusion LLMs precludes a fully equivalent KV Cache, our approximation closely resembles an ideal cache in practice.</p></blockquote>
<p>它将待生成的文本序列分成若干个块. 在生成某一个块 (比如Block 1) 时，它会提前计算并缓存其他所有块 (比如 Prompt 和 Block 0) 的 KV. 在这个块的内部生成过程中，这些缓存被反复利用。当这个块生成完毕后，再整体更新一次所有块的KV缓存 。</p>
<p>这个方法的近似在于，在一个块的生成过程中，缓存是固定的，而实际上随着块内 token 的不断去噪和清晰化，这些缓存理论上也应该随之微调。但论文通过可视化实验 (图3) 有力地证明，在相邻的推理步骤中，KV 激活值的 余弦相似度非常高，几乎接近于1. 这说明使用固定的近似缓存带来的误差微乎其微，完全可以用极小的精度损失换取巨大的速度提升。</p>
<p>论文还进一步提出了双缓存 (DualCache) 版本，不仅缓存了前面的“前缀” (prefix) ，还缓存了后面的“后缀” (suffix，通常是 MASK  token )  ，从而进一步压榨了计算优化的空间，实现了更快的速度。</p>
<ol start="2">
<li>并行解码带来的质量下降</li>
</ol>
<p>扩散LLM的另一大理论优势是 并行解码 (Parallel Decoding)，即一次性预测和生成多个 token  。然而，实践再次证明，当并行解码的 token 数量增多时，生成文本的质量会急剧下降 。</p>
<p>论文深刻地剖析了其根源：条件独立性假设 (conditional independence assumption) 的破坏 。在并行解码时，模型是独立地为每个待生成的 MASK 位置预测一个概率分布，然后从中采样。但实际上，一句话中的 token 之间存在着强烈的依赖关系。论文举了一个例子:</p>
<blockquote class="quote"><p>Consider an example from [30]: The list of poker hands that consist of two English words are: The subsequent two words could be, for instance, &ldquo;high card,&rdquo; &ldquo;two pair,&rdquo; &ldquo;full house,&rdquo; or &ldquo;straight flush.&rdquo; [&hellip;] However, the multi-token prediction procedure in MDMs first generates a probability distribution for each token and then samples from these distributions independently. This independent sampling can lead to undesirable combinations, such as &ldquo;high house.&rdquo;</p></blockquote>
<p>模型可能会独立地预测出 &ldquo;high&rdquo; 和 &ldquo;house&quot;这两个词，但把它们组合在一起就成了毫无意义的 high house. 这是因为模型在并行预测时忽略了 token 间的联合概率，而错误地直接使用了边缘概率的乘积。</p>
<p>为了解决这个问题，Fast-dLLM提出了第二个核心贡献：置信度感知并行解码 (Confidence-Aware Parallel Decoding) 策略 。这个想法非常直观且有效：我们只对那些模型非常有把握的 token 进行并行解码。</p>
<p>具体来说，在每一步解码时，模型会为每个待生成的 MASK 位置计算一个 置信度分数 (比如softmax概率的最大值). 然后，设定一个全局的置信度阈值 τ，只有那些置信度超过这个阈值的 token 才会被揭开，而置信度不足的 token 则继续保持 MASK 状态，留到下一步再做决策。为了避免无限循环，如果没有任何 token 的置信度达标，模型会强制解码置信度最高的那一个。</p>
<p>这个策略的精妙之处在于，它在理论上是站得住脚的。论文通过定理一从数学上证明了：当模型对一组 token 的预测置信度足够高时 (即 p&gt;1−ϵ，且 ϵ 足够小)，基于独立边缘概率的“贪心并行解码”与基于真实联合概率的“贪心串行解码”会得到完全相同的结果。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&amp;shareKey=c9e48ddb1e1f0600394ce8baa1d84426" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&amp;shareKey=c9e48ddb1e1f0600394ce8baa1d84426" alt="Effectiveness of Components of Fast-dLLM across Different Approaches">
    </a><figcaption>Effectiveness of Components of Fast-dLLM across Different Approaches</figcaption></figure></p>
<p>Fast-dLLM 的创新性体现在它是一种 training-free 的加速框架。它没有修改模型结构，也不需要重新训练，而是通过两项即插即用的推理策略——“分块近似KV缓存”和“置信度感知并行解码”，分别从减少重复计算和提升并行效率两个维度，精准地解决了当前开源扩散 LLM 面临的核心瓶颈。 实验结果在 LLaDA 和 Dream 等模型上，结合两种策略，实现了高达 27.6 倍的端到端吞吐量提升，同时在多个基准测试上几乎没有精度损失。</p>
<h1 id="2-preliminary">2. Preliminary</h1>
<h3 id="21-masked-diffusion-model">2.1. Masked Diffusion Model</h3>
<p>针对离散数据的扩散模型最早在 Argmax Flows and Multinomial Diffusion 和 Deep Unsupervised Learning using
Nonequilibrium Thermodynamics 中被探提出。随后 D3PM 提出了一个更通用的框架，通过特定的转移矩阵 $Q_{t}$ 定义了前向加噪过程的离散状态马尔可夫链，并通过最大化 ELBO 来学习反向过程的参数化模型 $p_{\theta}(x_{0}|x_{t})$. CTMC 进一步将 D3PM 扩展到连续时间，将其形式化为一个连续时间马尔可夫链 (CTMC) 框架。在另一种不同的方法中，SEDD 通过参数化似然比 $\frac{p_{t}(y)}{p_{t}(x)}$ 来学习反向过程，并采用去噪分数熵来训练该比率。</p>
<p>在各种离散扩散的噪声处理方式中，<strong>Masked Diffusion Models, MDMs</strong>，也被称为吸收状态离散扩散模型，获得了相当大的关注。MDMs 采用一种前向加噪过程，其中 token 被逐步替换为一个特殊的 MASK  token  。这个过程由以下转移概率定义：</p>
$$
q_{t|0}(x_{t}|x_{0})=\prod_{i=1}^{n}q_{t|0}(x_{t}^{i}|x_{0}^{i})=\prod_{i=1}^{n}Cat(x_{t}^{i};(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}) \tag{1}
$$<ul>
<li>$q_{t|0}(x_t|x_0)$: 表示给定原始序列 $x_0$，得到噪声序列 $x_t$ 的概率 。</li>
<li>$\prod_{i=1}^{n}$: 连乘符号，表示整个序列的噪声过程是序列中每个 token  (token) 独立进行噪声过程的概率乘积 。</li>
<li>$Cat(\cdot)$: 代表<strong>类别分布 (Categorical Distribution)</strong> 。</li>
<li>$t \in [0,1]$: 表示<strong>扩散时间</strong>或<strong>掩码级别</strong>。当 $t=0$ 时，序列完全是原始的；当 $t=1$ 时，序列被完全替换为 <code>[MASK]</code>  token 。</li>
<li>$(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}$: 在时间 <code>t</code>，第 <code>i</code> 个 token 有 $1-t$ 的概率保持其原始身份 $x_0^i$，有 $t$ 的概率变成 <code>[MASK]</code>  token 。<code>$\delta$</code> 是克罗内克函数，用于指定概率。</li>
</ul>
<p>最近，MDLM 和 RADD 的工作表明，对于 MDMs 不同的参数化是等价的。此外，他们证明了 MDMs 的训练目标可以被简化或直接从数据似然中推导出来 。这导出了以下目标函数，即 $log~p_{\theta}(x)$ 的一个 ELBO:</p>
<details class="custom-details">
    <summary class="custom-summary">Reparameterized Absorbing Discrete Diffusion, RADD</summary>
    <div><p><strong>定理 1（Theorem 1）</strong>
</p>
$$
\frac{p_t(\hat{x}_t)}{p_t(x_t)} = \underbrace{\frac{e^{-\bar{\sigma}(t)}}{1-e^{-\bar{\sigma}(t)}}}_{\text{时间相关的标量}} \cdot \underbrace{p_0(\hat{x}_t^i | x_t^{UM})}_{\text{干净数据的条件概率}}
$$<ul>
<li>$p_t(x_t)$ 是时间步 $t$ 的数据分布</li>
<li>$x_t$ 是带噪声（被掩码）的序列</li>
<li>$x_t^{UM}$ 是其中未被掩码的部分</li>
<li>$\hat{x}_t$ 是在 $x_t$ 的一个掩码位置上填入一个新 token 后的序列</li>
<li>$p_0$ 是原始干净数据的分布，$\bar{\sigma}(t)$ 是一个与噪声水平相关的函数。</li>
</ul>
<p>这个公式表明，模型需要学习的目标可以分解。其中一部分是一个可以精确计算的、只与时间 $t$ 有关的标量，而另一部分则是一个<strong>与时间无关</strong>的、在给定其他可见 token 的条件下，预测被掩码 token 的条件概率。正是LLM 所做的事情。这个看似简单的改动带来了巨大的实际优势：</p>
<ol>
<li><strong>架构简化</strong>：移除了时间编码和相关的自适应归一化层，使得模型参数更少，结构更简洁 。</li>
<li><strong>采样加速</strong>：由于模型输出不再依赖于时间 $t$，当输入序列 $x_t$ 在某个采样区间内没有发生变化时，可以直接缓存上一步的计算结果，而无需再次调用网络。这极大地减少了<strong>函数评估次数（Number of Function Evaluations, NFEs）</strong>。论文给出了在特定采样策略下，期望函数评估次数（E-NFEs）的解析公式 ：</li>
</ol>
$$
E\text{-}NFEs(n) = n \left( 1 - \left( 1 - \frac{1}{n} \right)^l \right)
$$<p><strong>定理 2（Theorem 2）</strong></p>
<p>证明了吸收态扩散模型的训练目标（具体来说是 DSE 损失）在数学上等价于**任意阶自回归模型（Any-Order Autoregressive Models, AO-ARMs）**的训练目标 。</p>
<p>AO-ARMs 是一类特殊的生成模型，它们不像标准自回归模型那样固定从左到右的生成顺序，而是学习在所有可能的 $d!$（$d$ 为序列长度）种生成顺序下对数据进行建模。论文通过一系列精巧的数学推导，建立了四种不同损失函数之间的等价关系链 ：</p>
<p>$\mathcal{L}_{DSE} \iff \mathcal{L}_{t-DCE} \iff \mathcal{L}_{\lambda-DCE} \iff \mathcal{L}_{AO}$</p>
<p>它表明吸收态扩散模型本质上是在学习一个集成了所有可能生成顺序的自回归模型的期望 。这可能解释了为什么它们在某些任务上表现得非常稳健。</p>
</div>
</details><br>
$$
-log~p_{\theta}(x)\le\int_{0}^{1}\frac{1}{t}\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})]dt:=\mathcal{L}_{MDM}. \tag{2}
$$<ul>
<li>$-log~p_{\theta}(x)$: 模型的目标是最大化生成真实数据 $x$ 的对数似然，这等价于最小化它的负对数似然。这个公式给出了负对数似然的一个* ELBO.</li>
<li>$\int_{0}^{1}...dt$: 对所有可能的噪声级别 <code>t</code> (从0到1) 进行积分，意味着模型需要学会在任何噪声水平下都能很好地复原数据 。</li>
<li>$\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[...]$: 表示对所有可能的噪声样本求期望。在训练时，我们根据公式(1)随机生成一个带 <code>[MASK]</code> 的噪声序列 $x_t$.</li>
<li>$\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})$:
<ul>
<li>$\sum_{i:x_{t}^{i}=[MASK]}$: 对所有被 <code>[MASK]</code> 的位置 <code>i</code> 进行求和 。</li>
<li>$-log~p_{\theta}(x_{0}^{i}|x_{t})$: 这是交叉熵损失。它的意思是，给定带有 <code>[MASK]</code> 的序列 $x_t$，模型 $p_{\theta}$ 需要预测在位置 i 上的原始 token  $x_0^i$ 应该是什么。模型预测得越准，这个损失值就越小。</li>
</ul>
</li>
</ul>
<h3 id="22-mdms-的生成过程">2.2. MDMs 的生成过程</h3>
<p>对于公式1中定义的前向过程，其解析上的逆过程在生成时计算效率低下，因为它通常每步只修改一个 token 。一个常见的加速策略是采用 $\tau$-leaping 近似法来处理反向过程。在 MDMs 的背景下，这允许一个迭代式的生成过程，其中多个被掩码的 token 可以从一个噪声水平 t 近似地单步恢复到一个更早的水平 s &lt; t.</p>
$$
q_{s|t}(x_s|x_t)=\prod_{i=0}^{n-1}q_{s|t}(x_{s}^{i}|x_{t})
$$<p>其中</p>
$$
q_{s|t}(x_{s}^{i}|x_{t})=\begin{cases}1, & \text{if } x_{t}^{i}\ne[MASK], x_{s}^{i}=x_{t}^{i} \\ \frac{s}{t}, & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}=[MASK] \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}\ne[MASK]\end{cases} \tag{3}
$$<ul>
<li>$q_{s|t}(x_{s}^{i}|x_{t})$: 表示从 <code>t</code> 时刻的 token  $x_t^i$ 变为 <code>s</code> 时刻的 token  $x_s^i$ 的概率 。</li>
<li><strong>Case 1</strong>: 如果一个 token 在 <code>t</code> 时刻就不是 <code>[MASK]</code>，那么它在更早的 <code>s</code> 时刻也保持不变 。</li>
<li><strong>Case 2</strong>: 一个在 t 时刻是 <code>[MASK]</code> 的 token ，在更早的 s 时刻仍然是 <code>[MASK]</code>.</li>
<li><strong>Case 3</strong>: 这是关键的去噪步骤。如果一个 token 在 <code>t</code> 时刻是 <code>[MASK]</code>，模型会尝试在 s 时刻预测出一个具体的 token.
<ul>
<li>$\frac{t-s}{t}$: 代表一个在 <code>t</code> 时刻被掩码的 token，在 <code>s</code> 时刻被“揭示”出来的概率 。</li>
<li>$q_{0|t}(x_{s}^{i}|x_{t})$: 这是由神经网络模型给出的预测分布。模型会观察整个带有 <code>[MASK]</code> 的上下文 $x_t$，然后为当前位置预测一个最有可能的原始 token ，并给出一个在整个词汇表上的概率分布 。</li>
</ul>
</li>
</ul>
<p>在涉及条件数据的场景中，例如根据一个 propmt p 生成一个回应 $x_{0}$，MDM 的反向过程 (公式3所定义) 需要进行调整。具体来说，模型用于揭示一个 token  $x_{s}^{i}$ 的预测分布 $q_{0|t}(x_{s}^{i}|x_{t})$ 现在也需要以 prompt p 为条件，即 $q_{0|t}(x_{s}^{i}|x_{t},p)$ 。</p>
<p><strong>并行解码的诅咒</strong>
直接逆转公式1的前向过程来进行生成是缓慢的，通常每步只改变一个 token. 一个常见的加速策略是采用 $\tau$-leaping 近似法来处理反向过程。对于 MDMs，这意味着多个被掩码的 token 将在一个步骤中并行生成。然而，由于条件独立性假设，多 token 预测中出现了一个重大挑战。考虑一个例子：由两个英文单词组成的扑克手牌列表是：随后的两个词可能是，例如，high card，two pair，full house，或 straight flush. 值得注意的是，这两个词之间存在着关联。然而，MDMs 中的多 token 预测过程首先为每个 token 生成一个概率分布，然后独立地从这些分布中进行采样。这种独立采样可能导致不希望的组合，例如 high house.</p>
<p>为了将其形式化，考虑揭示两个 token 位置 i 和 j. 由于条件独立性假设，MDMs 从 $p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t})$ 中采样这些 token. 然而，真实的联合概率需要考虑它们之间的依赖关系：</p>
$$
p(x_{s}^{i},x_{s}^{j}|x_{t})=p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t},x_{s}^{i})
$$<p>或者对称地，通过将 i 依赖于条件 j. 这种假设的独立生成与真实的依赖性数据分布之间的差异，会降低生成序列的质量和连贯性。当在单一步骤中同时揭示大量 token 时，这个问题会变得更加严重。</p>
<h1 id="3-methodology">3. Methodology</h1>
<h2 id="31-pipeline-overview">3.1. Pipeline Overview</h2>
<p><strong>Fast-dLLM</strong>，建立在 MDM 架构之上，以实现高效和高质量的序列生成。为了加速推理，整体流水线融合了两大关键策略：通过 KV Cache 实现的高效注意力计算，以及一个由预测置信度引导的 并行解码方案。具体来说，我们采用了分块解码设计的 KV Cache，它允许在不同步骤间复用注意力激活值，并显著减少了冗余计算。在每个块内部，进一步提出了置信度感知的并行解码，它能根据置信度分数选择性地更新 token ，从而在保持输出质量的同时提高效率。通过结合这些策略，Fast-dLLM 在对生成性能影响最小的情况下，显著加快了 MDM 的推理速度。整体流程在算法 1 中进行了总结。</p>
<h2 id="32-key-value-cache-for-block-wise-decoding">3.2. Key-Value Cache for Block-Wise Decoding</h2>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&amp;shareKey=8952caa17d664bd8bcc33b9ebcec321e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&amp;shareKey=8952caa17d664bd8bcc33b9ebcec321e" alt="Illustration of our Key-Value Cache for Block-Wise Decoding">
    </a><figcaption>Illustration of our Key-Value Cache for Block-Wise Decoding</figcaption></figure></p>
<p>如上图所示，我们采用了一种分块解码的策略来支持 KV Cache 的使用。一开始计算并存储 prompt 的 KV 缓存，这个缓存将在整个块 0的解码过程中被复用。在每个块的内部，相同的缓存会被多个解码步骤复用。<strong>在完成一个块的解码之后，更新所有 token (不仅仅是新生成的 token) 的缓存</strong>。这个缓存更新可以与解码步骤联合执行，因此与不使用缓存相比，没有额外的计算开销。由于掩码扩散模型中使用的是完全注意力机制，这种方法导致了一个近似的解码过程。</p>
<p>我们的近似 KV 缓存方法的有效性，源于我们观察到 KV 激活值在相邻的推理步骤中表现出高度的相似性，如下图所示。图 a 中红色方框区域突显了块内的相似性分数，这些分数始终接近于 1. 表明在分块解码期间，前缀 (prefix) 的键和值的差异可以忽略不计，使我们能够安全地复用缓存而不会有显著的准确率损失。 此外，我们实现了一个我们 KV 缓存机制的双向版本，名为 <strong>DualCache</strong>，它不仅缓存前缀 token ，还缓存后缀 (suffix)  token ，在我们的分块解码方案中，后缀完全由掩码 token 组成。如表3所示，DualCache 带来了进一步的加速。图 b 中的红色方框区域进一步证明，在分块解码期间，后缀的键和值的差异也可以忽略不计。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&amp;shareKey=6a5005c556aaa11edb4006a48b755b4a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&amp;shareKey=6a5005c556aaa11edb4006a48b755b4a" alt="Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA">
    </a><figcaption>Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA</figcaption></figure></p>
<h2 id="33-confidence-aware-parallel-decoding">3.3. Confidence-Aware Parallel Decoding</h2>
<p>尽管存在一些方法，例如使用辅助模型来显式地捕捉不同位置 token 之间的依赖关系，但它们通常会增加整个流水线的复杂性。与这些方法相反，我们提出了一个简单而有效的<strong>置信度感知解码算法</strong>，旨在缓解这种条件独立性问题。</p>
<p>在每次迭代中，我们不是冒然地使用它们独立的边缘概率来揭示所有被掩码的 token ，而是为每个 token 计算一个置信度分数 (例如最大的 softmax 概率). 只有那些置信度超过一个阈值的 token 才会在当前步骤被揭示；其余的则保持掩码状态，并在未来的步骤中重新考虑。如果没有 token 的置信度超过阈值，就揭示置信度最高的那一个，以确保过程能够进行并防止无限循环。这个策略在加速生成的同时，减少了由不确定或模糊预测引起的错误。</p>
<p>一个关键问题是</p>
<blockquote class="quote"><p><em>When is it theoretically justifiable to decode tokens in parallel using independent marginals, despite the true joint distribution potentially containing dependencies?</em></p></blockquote>
<p>以下结果来回答了在高置信度情况下，greedy parallel 解码等同于 greedy sequential 解码的条件，并量化了两种分布之间的差异。在给出定理之前，我们将定义其表述中使用的数学符号。</p>
<p>设 $p_{\theta}(\cdot|E)$ 表示一个 MDM 在给定 E (包括 prompt $p_{0}$ 和先前生成的 token) 的条件下给出的 PMF. 假设模型要为不在 E 中的位置 $i_{1},...,i_{n}$ 预测 n 个 token.</p>
<p>令 $X=(X_{i_{1}},...,X_{i_{n}})$ 是 n 个 token 的向量，其中每个 $X_{i_{j}}$ 在词汇表 V 中取值。设 $p(X|E)\equiv p_{\theta}(X_{i_{1}},...,X_{i_{n}}|E)$ 是模型给出的联合条件 PMF。设 $p_{j}(X_{i_{j}}|E)\equiv p_{\theta}(X_{i_{j}}|E)$ 是位置 $i_{j}$ 的边缘条件 PMF。并行解码使用边缘概率的乘积来生成 token ：$q(X|E)=\tilde{\prod}_{j=1}^{n}p_{j}(X_{i_{j}}|E)$。定理1的证明及相关讨论见附录A。</p>
<p><strong>定理 1 (高置信度下的并行解码).</strong> 假设存在一个特定的 token 序列 $x^{*}=(x_{i_{1}},...,x_{i_{n}})$，使得对于每个 $j\in\{1,...,n\}$，模型对 $x_{i_{j}}$ 都有很高的置信度：$p_{j}(X_{i_{j}}=x_{i_{j}}|E)>1-\epsilon$，对于某个很小的 $\epsilon>0$. 那么，以下结论成立：</p>
<ol>
<li><em>Equivalence of Greedy Decoding</em>：如果 $(n+1)\epsilon\le1$ (即 $\epsilon\le\frac{1}{n+1}$) ，那么
$$
\text{argmax}_{z} p(z|E) = \text{argmax}_{z} q(z|E) = x^{*}. \tag{4}
$$</li>
</ol>
<p>这意味着 greedy parallel 解码 (选择 argmax q) 与贪婪序贯解码 (选择 argmax p) 产生相同的结果。  这个界是紧的：如果 $\epsilon > \frac{1}{n+1}$，则存在满足高置信度边缘假设的分布 $p(X|E)$，使得 argmax $p(z|E)$ ≠ argmax $q(z|E)$。</p>
<ol start="2">
<li><em>Distance and Divergence Bounds</em>：为简洁起见，将 $p(\cdot|E)$ 和 $q(\cdot|E)$ 表示为 p 和 q.</li>
</ol>
<p><strong>$L_p$ Distance ($p \ge 1$)</strong>: 对于 $n>1$，$D_{p}(p,q)<((n-1)^{p}+2n)^{1/p}\epsilon$。特别地，对于总变差距离 ($D_{TV}(p,q)=\frac{1}{2}D_{1}(p,q)$)，$D_{TV}(p,q)<\frac{3n-1}{2}\epsilon$.</p>
<p>这个公式说明，<strong>真实分布 p 和近似分布 q 之间的总变差距离有一个上限</strong>。这个上限取决于两个因素：</p>
<ol>
<li>$n$: 生成序列的长度。序列越长，这个上限就越大。这是符合直觉的，因为每增加一个 token，近似所累积的潜在误差就可能增加一点。</li>
<li>$\epsilon$: 模型在每个位置上的“不确定性”。$\epsilon$ 越小 (即模型越自信)，这个上限就越低。</li>
</ol>
<p><strong>Forward KL Divergence</strong>: 对于 $n > 1$，$D_{KL}(p||q)<(n-1)(H_{b}(\epsilon)+\epsilon~ln(|\mathcal{V}|-1))$，其中 $H_{b}(\epsilon)=-\epsilon~ln~\epsilon-(1-\epsilon)ln(1-\epsilon)$ 是二元熵函数，而 $|\mathcal{V}|$ 是词汇表的大小。</p>
<ol>
<li>$n-1$: 同样，损失会随着序列长度线性增长。</li>
<li>$H_{b}(\epsilon)$: 它衡量了一个概率为 $\epsilon$ 的事件带来的“意外程度”或不确定性。当 $\epsilon$ 很小时，$H_b(\epsilon)$ 也非常小。</li>
<li>$\epsilon~ln(|\mathcal{V}|-1)$: 这一项反映了那部分微小的 $\epsilon$ 概率被分配到词汇表 $\mathcal{V}$ 中其他所有 token 上所带来的不确定性。即使 $\epsilon$ 很小，如果词汇表非常巨大 ($|\mathcal{V}|$ 很大)，这一项也可能有影响。</li>
</ol>
<hr>
<ul>
<li>$L_p$ 距离说明在高置信度下，两种方法找到的<strong>最佳答案</strong>是相同的。</li>
<li>KL 散度说明高置信度下，不仅最佳答案相同，两种方法描绘的概率分布都非常相似。近似方法 q 不仅猜对了可能性最大的 token， 对其他可能性的估计，也和精确方法 p 的判断高度一致。</li>
</ul>
<h1 id="4-experiments">4. Experiments</h1>
<h2 id="41-experimental-setup">4.1 Experimental Setup</h2>
<ul>
<li><strong>硬件与环境</strong> 🖥️: 所有实验均在单张 <strong>NVIDIA A100 80GB GPU</strong> 上进行，batch size=1.</li>
<li><strong>评测模型</strong> 🧠: <strong>LLaDA</strong>  和 <strong>Dream</strong>.</li>
<li><strong>评测基准</strong> 📊: 采用了四个广泛使用的基准数据集：<strong>GSM8K</strong>、<strong>MATH</strong>、<strong>HumanEval</strong> 和 <strong>MBPP</strong>.</li>
<li><strong>核心指标</strong> ⏱️:
<ul>
<li><strong>准确率 (Accuracy)</strong>: 衡量模型在具体任务上的表现。</li>
<li><strong>吞吐量 (Throughput)</strong>: 以 tokens/sec 为单位，反映端到端的真实解码速度。</li>
</ul>
</li>
<li><strong>超参数</strong> ⚙️:
<ul>
<li><strong>缓存块大小</strong>: 在 4 到 32 之间进行探索。</li>
<li><strong>置信度阈值</strong>: 在 0.5 到 1.0 之间进行探索。</li>
<li>实验默认使用 <strong>PrefixCache</strong>，块大小为 <strong>32</strong>，置信度阈值为 <strong>0.9</strong>.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="42-main-results-performance-and-speed">4.2 Main Results: Performance and Speed</h2>
<p>实验结果表明，Fast-dLLM 在各种任务和设置上都取得了显著的速度提升，同时对模型准确率的影响微乎其微 。</p>
<ul>
<li>加速效果:
<ul>
<li>单独引入 KV Cache 机制，通常能带来 <strong>2x-3.6x</strong> 的速度提升。</li>
<li>当 KV Cache 和并行解码两种策略结合使用时，性能提升更为显著。在 LLaDA 模型上，最 高可达 <strong>11.0x</strong> 的吞吐量提升；在 Dream 模型上，最高可达 <strong>7.8x</strong> 的提升 。</li>
</ul>
</li>
<li>极小的精度损失: 在所有基准测试中，加速后模型的准确率与原始基线模型的差距基本保持在 <strong>1-2个百分点</strong> 以内，有时甚至略有提高。</li>
<li>对长序列更友好: 实验还发现，在处理更长的文本序列时 (例如 few-shot 场景或长代码生成)，Fast-dLLM 的加速效果更为明显。</li>
</ul>
<p>下表以 GSM8K (5-shot) 任务为例，直观展示了 Fast-dLLM (即 +Cache+Parallel) 相较于 baseline 模型的性能提升。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">模型</th>
          <th style="text-align: left">生成长度</th>
          <th style="text-align: left">配置</th>
          <th style="text-align: left">准确率 (%)</th>
          <th style="text-align: left">吞吐量 (tok/s)</th>
          <th style="text-align: left">相对加速</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>LLaDA</strong></td>
          <td style="text-align: left">256</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">79.3</td>
          <td style="text-align: left">6.7</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>78.5</strong></td>
          <td style="text-align: left"><strong>54.4</strong></td>
          <td style="text-align: left"><strong>8.1x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">512</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">77.5</td>
          <td style="text-align: left">3.2</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>77.2</strong></td>
          <td style="text-align: left"><strong>35.3</strong></td>
          <td style="text-align: left"><strong>11.0x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Dream</strong></td>
          <td style="text-align: left">256</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">75.0</td>
          <td style="text-align: left">9.1</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>74.8</strong></td>
          <td style="text-align: left"><strong>48.2</strong></td>
          <td style="text-align: left"><strong>5.3x</strong></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">512</td>
          <td style="text-align: left">Baseline</td>
          <td style="text-align: left">76.0</td>
          <td style="text-align: left">7.7</td>
          <td style="text-align: left">1x</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left"><strong>Fast-dLLM</strong></td>
          <td style="text-align: left"><strong>74.0</strong></td>
          <td style="text-align: left"><strong>42.9</strong></td>
          <td style="text-align: left"><strong>5.6x</strong></td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="43-ablations-and-analysis">4.3 Ablations and Analysis</h2>
<p>为了深入理解各个组件的贡献，论文进行了一系列详细的消融实验。</p>
<ul>
<li>
<p><strong>输入与生成长度的影响</strong>:</p>
<ul>
<li>实验证明，更长的上下文 (prefill，如从 5-shot 增加到 8-shot) 和更长的生成长度，都能显著放大加速效果。</li>
<li>在 8-shot 和 1024 生成长度的设置下，<strong>DualCache</strong> 实现了 <strong>27.6x</strong> 端到端加速。</li>
</ul>
</li>
<li>
<p><strong>PrefixCache vs. DualCache</strong>:</p>
<ul>
<li><strong>DualCache</strong> 通常比只缓存前缀的 <strong>PrefixCache</strong> 实现更高的加速比，尤其是在长序列生成任务中 。</li>
</ul>
</li>
<li>
<p><strong>缓存块大小的影响</strong>:</p>
<ul>
<li><strong>small block size</strong>：准确率最高，但因频繁更新缓存导致开销较大，速度提升有限 。</li>
<li><strong>small block size</strong>：速度快，但可能因上下文不匹配导致准确率下降 。</li>
<li>实验发现，块大小为 <strong>32</strong> 时在速度和精度之间取得了最佳平衡。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&amp;shareKey=1e3a007e630de1a9cbf8b3d9f318f307" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&amp;shareKey=1e3a007e630de1a9cbf8b3d9f318f307" alt="Impact of Cache Block Size on Accuracy and Throughput">
    </a><figcaption>Impact of Cache Block Size on Accuracy and Throughput</figcaption></figure></p>
<ul>
<li><strong>动态阈值 vs. 固定步数策略</strong>:
<ul>
<li>论文提出的 <strong>置信度感知并行解码</strong> 策略，在性能上持续优于每步固定解码 K 个 token 的 baseline 方法。</li>
<li>在达到相似甚至更高准确率的同时，该动态策略能实现更高的平均每步解码 token 数，从而获得更高的吞吐量。</li>
</ul>
</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&amp;shareKey=88d29eb3e40615a74c4846d278413e5b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&amp;shareKey=88d29eb3e40615a74c4846d278413e5b" alt="Threshold VS Fxied Step">
    </a><figcaption>Threshold VS Fxied Step</figcaption></figure></p>
<h1 id="5-related-work">5. Related Work</h1>
<p>本章节回顾了与 Fast-dLLM 相关的两个核心领域：扩散语言模型的发展，以及大语言模型的通用加速技术。</p>
<hr>
<h2 id="51-diffusion-llm">5.1. Diffusion LLM</h2>
<p>扩散模型作为一种强大的生成范式，最初在图像和音频等连续数据领域取得了巨大成功，随后其影响力扩展到了 NLP. 特别是离散扩散模型的最新进展为大语言模型提供了一种替代自回归 (AR) 范式的可行方案 。</p>
<ul>
<li>
<p><strong>理论基础的发展</strong>:</p>
<ul>
<li>离散数据的扩散模型最早由 [29, 11] 探索 。</li>
<li><strong>D3PM</strong> 提出了一个更通用的框架，将前向加噪过程建模为离散状态马尔可夫链，并通过最大 ELBO 来学习反向过程。</li>
<li><strong>CTMC</strong> 将 D3PM 扩展到连续时间设定 。</li>
<li><strong>SEDD</strong> 采用了不同的方法，通过参数化边际似然比来学习反向过程 。</li>
<li><strong>MDMs</strong> 近期受到了广泛关注，其中 <strong>MDLM</strong> 和 <strong>RADD</strong> 的研究表明，MDMs 的不同参数化方法是等价的，并且其训练目标可以被简化 。</li>
</ul>
</li>
<li>
<p><strong>与预训练语言模型的结合</strong>: 一个关键的突破是将离散扩散与现有的大语言模型架构相结合 。</p>
<ul>
<li><strong>Diffusion-NAT</strong> [40] 将离散扩散的去噪过程与 BART 的非自回归解码相结合，通过迭代式地优化被掩码的 token ，实现了比同类自回归 Transformer 快20倍的生成速度 。</li>
<li><strong>LLaDA</strong> [21]、<strong>DiffuLLaMA</strong> [7] 和 <strong>Dream</strong> [36] 等框架将扩散模型扩展到了 7B 参数的规模，通过在扩散时间步上进行递归式的 token 预测，展现了与 LLaMA3 等主流自回归模型相匹敌的性能 。</li>
</ul>
</li>
</ul>
<h2 id="52-llm-acceleration">5.2. LLM Acceleration</h2>
<ul>
<li>KV Cache</li>
</ul>
<p>由于 LLaDA 等扩散语言模型采用的是 <strong>full attention</strong>，将 KV 缓存直接应用于这类模型并非易事。 一篇相关的研究 <strong>Block diffusion</strong>  通过<strong>分块生成 (block-by-block)</strong> 的方式，克服了先前扩散语言模型的局限，使得缓存和复用先前已解码块的键和值成为可能 。</p>
<ul>
<li>Non-Autoregressive Generation</li>
</ul>
<p>非自回归 (NAR) 生成标志着一种根本性的转变，它通过同时生成多个 token 来显著加速推理过程。NAR 方法最初被用于神经机器翻译，现已扩展到语法纠错、文本摘要和对话系统等多种任务
。
尽管 NAR 在速度上优势巨大，但它通常以牺牲一定的生成质量为代价。扩散语言模型是 NAR 领域一个新兴的范式；然而，先前的工作 (如 LLaDA) 在实践中难以实现预期的加速，因为并行生成会导致输出质量显著下降。</p>
<h1 id="weakness">Weakness</h1>
<p>近似缓存的误差累积效应：论文证明了在相邻步骤中，KV激活值的差异很小 。但随着生成块的增多，这种“近似”带来的微小误差是否会累积，并在生成非常长的文本 (如数万 token 的小说) 时导致语义漂移或一致性下降？论文的最长测试序列为1024 ，对于更长的序列，其鲁棒性有待进一步验证。</p>
<p>对模型能力的依赖：“置信度感知解码”策略的有效性，隐式地依赖于模型本身具有良好的“校准度” (calibration) ，即模型的置信度能够真实反映其预测的正确性。如果模型本身“过于自信”或“不够自信”，可能会导致该策略效果不佳。论文没有对所用模型的校准度进行分析。
定理一的理论与实践差距：论文坦诚地指出了定理一的局限性</p>
<blockquote>
<p>In practice, while MDM may not strictly satisfy this property, its behavior typically offers a close approximation.</p></blockquote>
<p>理论证明假设了一个“理想的”联合概率分布，而真实模型是否以及在多大程度上符合这个理想假设，是一个需要进一步探究的问题。理论和实践之间的差距可能在某些刁钻的 (adversarial) 或分布外 (Out-of-Distribution) 的场景下被放大。
超参数的敏感性与调优成本：尽管论文分析了块大小和阈值的影响，但并未提供一套系统性的方法来为新模型或新任务选择最佳超参数。在实际应用中，这可能意味着需要为每个特定用例进行成本不菲的网格搜索 (grid search) ，增加了方法的应用门槛。
评估维度的局限性：论文主要使用了基于准确率的基准测试。但在开放式生成、对话等任务中，评估指标 (如流畅度、一致性、多样性) 更为复杂。Fast-dLLM是否会在这些“软”指标上引入不易察觉的负面影响，需要更全面的评估。</p>
<h1 id="source-code">Source Code</h1>
<ol>
<li>
<p><strong>初始化</strong>:</p>
<ul>
<li>函数首先创建一个张量 <code>x</code>，其长度为“提示词长度 + 待生成长度”。</li>
<li>提示词 (<code>prompt</code>) 部分被填充到 <code>x</code> 的开头，而所有待生成的位置则被初始化为特殊的掩码标记 <code>[MASK]</code> (<code>mask_id</code>) 。</li>
<li>将总生成任务分解为多个块 (<code>num_blocks</code>) ，并为每个块分配固定的解码步数 (<code>steps</code>)</li>
</ul>
</li>
<li>
<p><strong>分块生成 (外层循环)</strong>:</p>
<ul>
<li>代码以块为单位进行循环，依次生成每个文本块。</li>
</ul>
</li>
<li>
<p><strong>处理单个块 (内层循环与缓存机制)</strong>:</p>
<ul>
<li>
<p><strong>步骤 A: 全局缓存初始化 (第一次模型调用)</strong></p>
<ul>
<li>在处理一个新块的开始，它首先将<strong>整个序列 <code>x</code></strong> (包含提示词、已生成的块和所有未来待生成的<code>[MASK]</code>块) 完整地输入模型。</li>
<li>这次调用的主要目的是计算并存储整个序列的键值对缓存 (<code>past_key_values</code>). 这是一个全局缓存。</li>
<li>然后，模型根据输出的 <code>logits</code>，使用 <code>get_transfer_index</code> 函数决定在<strong>当前块</strong>中，哪些 <code>[MASK]</code> 标记应该被优先替换掉 (例如，基于最高置信度的预测) ，并用预测出的 token  (token) 进行填充。这个过程只发生一次。</li>
</ul>
</li>
<li>
<p><strong>步骤 B: 块内迭代优化 (第二次及后续模型调用)</strong></p>
<ul>
<li>接下来，进入一个 <code>while</code> 循环，对当前块进行迭代式地“精炼”，直到这个块中所有的 <code>[MASK]</code> 标记都被填满。</li>
<li><strong>核心优化点</strong>：在这次及后续的模型调用中，<strong>不再需要输入整个序列</strong>。它只将<strong>当前块的张量</strong> (<code>x[:, current_block_start:current_block_end]</code>) 作为输入，并<strong>重用步骤 A 中生成的全局缓存 <code>past_key_values</code></strong>。</li>
<li>这就是 dual cache: 一个为上下文 (提示词+之前块) 准备的、基本不变的静态缓存，和一个为当前块服务的、动态更新的缓存。这避免了对上下文部分的重复计算，极大地提升了效率。</li>
<li>模型会为当前块中剩余的 <code>[MASK]</code> 位置生成新的预测，并根据策略继续填充。</li>
<li>这个迭代过程会持续进行，直到当前块不再有 <code>[MASK]</code> 标记为止。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>完成与返回</strong>:</p>
<ul>
<li>当所有块都处理完毕后，函数返回最终生成的完整序列 <code>x</code> 和总的模型前向传播次数 <code>nfe</code> (一个衡量计算成本的指标) 。</li>
</ul>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@torch.no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_with_dual_cache</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">gen_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">block_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">remasking</span><span class="o">=</span><span class="s1">&#39;low_confidence&#39;</span><span class="p">,</span> <span class="n">mask_id</span><span class="o">=</span><span class="mi">126336</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Generates text using a non-autoregressive, block-wise decoding strategy with a dual-cache mechanism.
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">        model: The mask predictor model.
</span></span></span><span class="line"><span class="cl"><span class="s1">        prompt: A tensor of shape (1, L) representing the input prompt.
</span></span></span><span class="line"><span class="cl"><span class="s1">        steps: Total number of sampling/refinement steps for the entire generation.
</span></span></span><span class="line"><span class="cl"><span class="s1">        gen_length: The desired length of the generated text.
</span></span></span><span class="line"><span class="cl"><span class="s1">        block_length: The size of each block to be generated in parallel. gen_length must be divisible by this.
</span></span></span><span class="line"><span class="cl"><span class="s1">        temperature: Sampling temperature for token selection. 0 means greedy decoding.
</span></span></span><span class="line"><span class="cl"><span class="s1">        remasking: The strategy for choosing which masks to fill (&#39;low_confidence&#39; or &#39;random&#39;).
</span></span></span><span class="line"><span class="cl"><span class="s1">        mask_id: The token ID for the [MASK] token.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create the full tensor &#39;x&#39; with the prompt and space for generation, initialized with the mask token.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">gen_length</span><span class="p">),</span> <span class="n">mask_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Copy the prompt into the beginning of the tensor &#39;x&#39;.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Ensure that the generation length can be evenly divided into blocks.</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">gen_length</span> <span class="o">%</span> <span class="n">block_length</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">gen_length</span> <span class="o">//</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Distribute the total steps among the blocks.</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">steps</span> <span class="o">%</span> <span class="n">num_blocks</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">steps_per_block</span> <span class="o">=</span> <span class="n">steps</span> <span class="o">//</span> <span class="n">num_blocks</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># nfe: Number of Forward-pass Evaluations. A counter for computational cost.</span>
</span></span><span class="line"><span class="cl">    <span class="n">nfe</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Outer loop: iterate through each block to be generated.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">num_block</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Define the start and end positions of the current block within the full tensor &#39;x&#39;.</span>
</span></span><span class="line"><span class="cl">        <span class="n">current_block_start</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num_block</span> <span class="o">*</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">        <span class="n">current_block_end</span> <span class="o">=</span> <span class="n">current_block_start</span> <span class="o">+</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Find the indices of mask tokens within the current block.</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Determine the number of tokens to fill at each refinement step for this block.</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_transfer_tokens</span> <span class="o">=</span> <span class="n">get_num_transfer_tokens</span><span class="p">(</span><span class="n">block_mask_index</span><span class="p">,</span> <span class="n">steps_per_block</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- First Model Call: Initialize Global Cache ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># A single forward pass on the ENTIRE sequence (prompt + all masked blocks) to pre-calculate</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># the Key-Value cache for all tokens. This is the &#34;global&#34; cache.</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">past_key_values</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Identify all mask tokens up to the end of the current block.</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Ignore masks that are in future blocks for this step&#39;s prediction.</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask_index</span><span class="p">[:,</span> <span class="n">current_block_end</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Select which tokens to predict and fill in this initial step for the current block.</span>
</span></span><span class="line"><span class="cl">        <span class="n">x0</span><span class="p">,</span> <span class="n">transfer_index</span> <span class="o">=</span> <span class="n">get_transfer_index</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">remasking</span><span class="p">,</span> <span class="n">mask_index</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">num_transfer_tokens</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Update the tensor &#39;x&#39; by filling the selected mask positions with the predicted tokens.</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">nfe</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Increment the forward-pass counter.</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Counter for refinement steps within the block.</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># A boolean mask indicating the position of the current block, used to update the cache efficiently.</span>
</span></span><span class="line"><span class="cl">        <span class="n">replace_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">replace_position</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- Inner Loop: Iterative Refinement of the Current Block ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This loop continues until all masks in the current block are filled.</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">nfe</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Increment the forward-pass counter for each refinement step.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Find the remaining masks ONLY within the current block.</span>
</span></span><span class="line"><span class="cl">            <span class="n">mask_index_block</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># --- Efficient Model Call using Dual Cache ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Instead of passing the whole sequence, only pass the CURRENT BLOCK&#39;s tokens.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Reuse the &#39;past_key_values&#39; (global cache) computed earlier. The model internally</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># uses &#39;replace_position&#39; to update the cache only at the current block&#39;s location.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># This is the &#34;dual cache&#34; trick, avoiding re-computation for the prompt and previous blocks.</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">],</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">replace_position</span><span class="o">=</span><span class="n">replace_position</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># Select which of the remaining masks to fill in this refinement step.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0</span><span class="p">,</span> <span class="n">transfer_index</span> <span class="o">=</span> <span class="n">get_transfer_index</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">remasking</span><span class="p">,</span> <span class="n">mask_index_block</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                            <span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">],</span> <span class="n">num_transfer_tokens</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Update the current block with the newly predicted tokens.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">][</span><span class="n">transfer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># If there are no more masks in the current block, exit the refinement loop.</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">current_block_start</span><span class="p">:</span><span class="n">current_block_end</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">break</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Move to the next refinement step.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Return the fully generated sequence and the total number of model evaluations.</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">nfe</span>
</span></span></code></pre></div>]]></content:encoded>
    </item>
    <item>
      <title>LLaDA</title>
      <link>http://localhost:1313/blogs/llada/</link>
      <pubDate>Thu, 12 Jun 2025 13:43:16 +0800</pubDate>
      <guid>http://localhost:1313/blogs/llada/</guid>
      <description>Paper Reading of LLaDA</description>
      <content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>LLM 主要的思想是 <em>generative modeling</em> 的思想是通过最大似然估计来优化模型的分布 $\log p_\theta(\cdot)$ 来逼近数据的分布 $\log p_{\text{data}}(\cdot)$
</p>
$$
\underbrace{\max_\theta\mathbb{E}_{p_{\text{data}}(x)}\log p_\theta(x)\Leftrightarrow\min_\theta\operatorname{KL}(p_{\text{data}}(x)||p_\theta(x)).}_{\text{Generative modeling principles}} \tag{1}
$$<p>当前，几乎所有我们熟知的大语言模型，从GPT系列到LLaMA系列，都基于<em>autoregressice modeling</em> 来实现。这种范式的核心是 <strong>next-token prediction</strong> ，即根据已经生成的文本序列，逐 toekn 地预测下一个最有可能出现的 token.</p>
$$
\underbrace{p_\theta(x)=p_\theta(x^1)\prod_{i=2}^Lp_\theta(x^i\mid x^1,\ldots,x^{i-1})}_{\text{Autoregressive formulation}} \tag{2}
$$<p>这种单向、顺序的生成方式在处理需要双向推理的任务时表现不佳，一个典型的例子就是 <strong>Reversal Curse</strong> ——模型知道 A is B，却往往无法推断出 B is A.</p>
<p>LLM 能力的核心基石是生成式建模原理本身，即通过最大似然估计让模型学习真实世界的数据分布 ，而非自回归这一具体的实现形式。</p>
<blockquote class="quote"><p><strong>It is the generative modeling principles (i.e., Eq. (1)), rather than the autoregressive formulation (i.e., Eq. (2)) itself, that fundamentally underpin the essential properties of LLMs.</strong></p></blockquote>
<ol>
<li>
<p>大语言模型的可扩展性 (scalability) ——即模型越大、数据越多、效果越好的特性——并非自回归模型所独有 。相反，这种可扩展性来源于更底层的生成式建模原理，而这些原理恰好保证了<em>fisher consistency</em><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</li>
<li>
<p><em>instruction-following</em> 和 <em>in-context learning</em><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> 并非自回归模型所独有，而是所有设计得当的条件生成模型 (conditional generative models) 在处理结构化语言任务时都应具备的内在属性 。</p>
</li>
</ol>
<p>因此作者提出了<strong>LLaDA</strong> (<strong>L</strong>arge <strong>L</strong>anguage <strong>D</strong>iffusion with m<strong>A</strong>sking)，一个从零开始训练的、参数量达到 8B 的扩散语言模型。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&amp;shareKey=94170299ede39d5102cf1cf6e397c5c7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&amp;shareKey=94170299ede39d5102cf1cf6e397c5c7" alt="Zero&amp;Few-Shot Benchmarks">
    </a><figcaption>Zero&amp;Few-Shot Benchmarks</figcaption></figure></p>
<p>LLaDA 使用了 Masked Diffusion Model (MDM)，该方法结合了离散随机掩蔽过程，并训练了一个掩码预测器来近似其反向过程。</p>
<h1 id="2-approach">2 Approach</h1>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&amp;shareKey=0293b80db53bfd7b8a9ba03f15a6f802" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&amp;shareKey=0293b80db53bfd7b8a9ba03f15a6f802" alt="A Conceptual Overview of LLaDA">
    </a><figcaption>A Conceptual Overview of LLaDA</figcaption></figure></p>
<h2 id="21-probabilistic-formulation">2.1 Probabilistic Formulation</h2>
<p>与公式(2)中的自回归模型不同，LLaDA通过<strong>前向过程 (forward process)</strong> 和 <strong>反向过程 (reverse process)</strong> 来定义模型分布 $p_{\theta}(x_{0})$。</p>
<h3 id="forward-process">Forward Process</h3>
<p>逐步地、独立地 mask $x_{0}$ 中的 token，直到在 $t=1$ 时序列被完全 mask.</p>
<p>给定 $x_{0}$ 时 $x_{t}$ 的条件分布可以被分解为：</p>
$$
q_{t|0}(x_{t}|x_{0}) = \prod_{i=1}^{L} q_{t|0}(x_{t}^{i}|x_{0}^{i})
$$<p>对于 $t \in (0,1)$，序列 $x_{t}$ 是部分被掩码的，其中每个 token 有 $t$ 的概率被mask，或有 $1-t$ 的概率保持不变。</p>
$$
q_{t|0}(x_{t}^{i}|x_{0}^{i}) = \begin{cases} 1-t, & x_{t}^{i} = x_{0}^{i} \\ t, & x_{t}^{i} = M \end{cases}
$$<p>其中 M 表示掩码 token. 直观上，每个 token 要么保持不变，要么被掩码，<strong>被掩码的概率随着 t 从 0 到 1 线性增加</strong>。在 $t=1$ 时，所有 token 都被 mask. 线性变化的被掩码概率和原先扩散模型的加噪流程不一样，是基于文本信息和 token 长度成正比的假设。</p>
<h2 id="reverse-process">Reverse Process</h2>
<p>反向过程则通过在 $t=1\rightarrow 0$ 从完全被掩码的序列中生成新数据。</p>
<p>对于 $0 \le s < t \le 1$，反向过程的条件分布分解为：</p>
$$
q_{s|t}(x_{s}|x_{t}) = \prod_{i=1}^{L} q_{s|t}(x_{s}^{i}|x_{t})
$$<p>其中每个 token 的条件分布为：</p>
$$
q_{s|t}(x_{s}^{i}|x_{t}^{i}) = \begin{cases} 1, & x_{t}^{i} \ne M, x_{s}^{i} = x_{t}^{i} \\ \frac{s}{t}, & x_{t}^{i} = M, x_{s}^{i} = M \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & x_{t}^{i} = M, x_{s}^{i} \ne M \\ 0, & \text{otherwise} \end{cases}
$$<p>需要估计的关键函数是条件分布 $q_{0|t}(x_{s}^{i}|x_{t})$，它在输入 $x_{t}$ 中对应位置被掩码的情况下，预测出原始的 token. 类似于连续扩散模型中的数据预测形式。如 (Ou et al., 2024) 所证明，可以推导出一个等价但无时间依赖的参数化形式</p>
$$
q_{0|t}(x_s^i|x_t)=p_{\text{data}}(x_0^i|x_t^\text{UM}),\quad\forall i\text{ such that }x_t^i=\mathbf{M}
$$<p>其中 $x_{t}^{\text{UM}}$ 表示 $x_{t}$ 中未被掩码 token 的集合，它与原始数据 $x_{0}$ 中对应的 token 相同，因为未掩码的 token 仅由 $x_{0}$ 决定且与时间 t 无关 。直观上，这意味着估计数据预测函数等同于估计在干净数据上的条件分布，而后者是时不变的。因此，时间 t 不需要作为输入提供给参数化模型 。</p>
<p>尽管 MDM 的推导过程不简单，但其实现是直接的。我们首先引入<strong>掩码预测器</strong>，一个参数化模型 $p_{\theta}(\cdot|x_{t})$ (例如一个没有因果掩码的 Transformer)，它将任意 t 时刻的 $x_{t}$ 作为输入，并同时预测所有被 mask 的 token. 然后，我们如下定义模型分布 $p_{\theta}(x_{0})$：从一个被完全 mask 序列的 $x_{1}$ 开始，从 $t=1$ 到 0 模拟一个由 $p_{\theta}(\cdot|x_{t})$ 参数化的近似反向过程。在 $t=0$ 时刻推导出的边缘分布即代表了模型分布 $p_{\theta}(x_{0})$ 。</p>
<p>掩码预测器将 $x_{t}$ 作为输入并同时预测所有被掩码的 token (表示为 M). 它通过一个仅在被掩码 token 上计算的交叉熵损失进行训练：</p>
$$
\mathcal{L}(\theta)\triangleq-\mathbb{E}_{t,x_{0},x_{t}}[\frac{1}{t}\sum_{i=1}^{L}I[x_{t}^{i}=M]log~p_{\theta}(x_{0}^{i}|x_{t})], \tag{3}
$$<p>其中，$x_{0}$ 从训练数据中采样，$t$ 从<code>[0, 1]</code>中均匀采样<span class="sidenote-number"><small class="sidenote">Notably, LLaDA employs a masking ratio that <em>varies randomly</em> between 0 and 1 while masked language models (Devlin, 2018) use a fixed ratio.</small></span>
，$x_{t}$ 从前向过程中采样。指示函数 $I[\cdot]$ 确保损失仅针对被掩码的 token 计算。一旦训练完成，便可以模拟一个由该掩码预测器参数化的反向过程（详见2.4节），并将模型分布 $p_{\theta}(x_{0})$ 定义为该过程的边缘分布。</p>
<p>公式(3)已被证明是模型分布负对数似然的上界</p>
$$
-\mathbb{E}_{p_{\text{data}}(x_{0})}\left[\log p_{\theta}(x_{0})\right]\leq\mathcal{L}(\theta) \tag{4}
$$<p>该方法通过在正向过程中逐步屏蔽 token 并在反向过程中学习恢复数据分布来训练生成模型，所有这些都在（近似）最大似然估计框架下。</p>
<h2 id="pretraining">Pretraining</h2>
<ul>
<li>
<p>LLaDA 8B 模型在一个包含 2.3T tokens 的高质量、多源数据集上从零开始进行预训练。该数据集覆盖了通用文本、代码、数学和多语言内容 。</p>
</li>
<li>
<p>训练总共消耗了 0.13M H800 GPU hours. 训练序列长度固定为4096. 其核心训练步骤是：对每个序列随机采样一个掩码率 t，并独立地以该概率掩码每个 token，然后让模型去预测被掩码的部分 。</p>
</li>
<li>
<p><strong>架构调整</strong> 相较于LLaMA3 8B，LLaDA 8B在架构上做了一些必要调整，如使用标准的 MHA 而非 GQA，并相应地调整了 FFN 的维度以保持模型总参数量相当 。</p>
</li>
<li>
<p><strong>优化器与学习率</strong> 训练使用了 AdamW 优化器和一个特殊的 Warmup-Stable-Decay 学习率调度策略。整个8B模型的训练实验只执行了一次，没有进行任何超参数调优。</p>
</li>
</ul>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center">Our ARM Baseline 1B</th>
          <th style="text-align: center">LLaDA IB</th>
          <th style="text-align: center">Our ARM Baseline 7B</th>
          <th style="text-align: center">LLaDA 8B</th>
          <th style="text-align: center">LLaMA3 8B</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>Layers</strong></td>
          <td style="text-align: center">22</td>
          <td style="text-align: center">22</td>
          <td style="text-align: center">28</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Model dimension</strong></td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">4096</td>
          <td style="text-align: center">4096</td>
          <td style="text-align: center">4096</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Attention heads</strong></td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">32</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Vocabulary size</strong></td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126,464</td>
          <td style="text-align: center">126.464</td>
          <td style="text-align: center">128,000</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>FFN dimension</strong></td>
          <td style="text-align: center">5634</td>
          <td style="text-align: center">5634</td>
          <td style="text-align: center">13.440</td>
          <td style="text-align: center">12,288</td>
          <td style="text-align: center">14,336</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Key/Value heads</strong></td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">4</td>
          <td style="text-align: center">8</td>
          <td style="text-align: center">32</td>
          <td style="text-align: center">8</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Total parameters</strong></td>
          <td style="text-align: center">1.49 B</td>
          <td style="text-align: center">1.49 B</td>
          <td style="text-align: center">6.83 B</td>
          <td style="text-align: center">8.02 B</td>
          <td style="text-align: center">8.03 B</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Non-embedding parameters</strong></td>
          <td style="text-align: center">0.97 B</td>
          <td style="text-align: center">0.97 B</td>
          <td style="text-align: center">5.80 B</td>
          <td style="text-align: center">6.98 B</td>
          <td style="text-align: center">6.98 B</td>
      </tr>
  </tbody>
</table>
<h2 id="supervised-fine-tuning">Supervised Fine-Tuning</h2>
<p>我们通过使用配对数据 $(p_{0}, r_{0})$ 进行 <strong>监督微调 (SFT)</strong> 来增强LLaDA遵循指令的能力，其中 $p_{0}$ 是 prompt，$r_{0}$ 表示响应(response). 这是针对LLM最简单、最基础的 post-training 方法。从技术上讲，这要求模型对条件分布 $p_{\theta}(r_{0}|p_{0})$，而非预训练中的 $p_{\theta}(x_{0})$ 进行建模。</p>
<p>其实现方式与预训练类似。如图2(b)所示，保持 prompt 部分不变，并像处理 $x_{0}$ 一样，独立地 mask response 中的 token. 然后，将提示和被掩码的响应 $r_{t}$ 一同送入预训练好的掩码预测器，以计算用于 SFT 的损失</p>
$$
-\mathbb{E}_{t,p_{0},r_{0},r_{t}}[\frac{1}{t}\sum_{i=1}^{L^{\prime}}I[r_{t}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{t})] \tag{5}
$$<p>其中，$L^{\prime}$ 表示稍后指定的动态长度。这种方法与预训练是完全兼容的。本质上，将 $p_{0}$ 和 $r_{0}$ 拼接起来可以被视为干净的预训练数据 $x_{0} $，而将 $p_{0}$ 和 $r_{t}$ 拼接起来则可作为其被掩码后的版本 $x_{t}$. 这个过程与预训练完全相同，唯一的区别在于所有被掩码的 token 恰好都出现在 $r_{0}$ 部分。</p>
<p>LLaDA 8B 模型在一个包含 4.5M 对样本的数据集上进行了 SFT. 与预训练过程一致，数据准备和训练都遵循了现有LLM (Chu et al., 2024; Yang et al., 2024) 中使用的 SFT 协议，没有引入任何额外的技术来优化 LLaDA 的性能。该数据集涵盖了多个领域，包括代码、数学、指令遵循和结构化数据理解。我们在每个 mini-batch 中的短样本对末尾附加 EOS token，以确保所有数据长度相等。在训练期间将 EOS视为一个普通 token ，并在采样时将其移除，使得LLaDA能够自动控制响应的长度。</p>
<p>我们在SFT数据上训练了 3 个 epoch，其调度策略与预训练阶段相似。学习率在最初 50 次迭代中从 0 线性增加到 $2.5 \times 10^{-5}$，然后保持不变。在最后 10% 的迭代中，学习率性降低到 $2.5 \times 10^{-6}$. 此外，我们将权重衰减设置为 0.1，全局 batch size 设置为 256，每个 GPU 的本地 batch size 设置为 2. SFT实验只执行了一次，没有进行任何超参数调优。</p>
<h2 id="inference">Inference</h2>
<p>作为一个生成式模型，LLaDA既能 <strong>采样 (sampling)</strong> 新文本，也能 <strong>评估 (evaluating)</strong> 候选文本的似然。</p>
<p>先从采样说起。如图 2(c) 所示，给定一个 prompt $p_{0}$，我们通过离散化反向过程来从模型分布 $p_{\theta}(r_{0}|p_{0})$ 中进行采样，这个过程从一个被完全掩码的 response 开始。</p>
<p><strong>总的采样步数是一个超参数</strong>，为 LLaDA 提供了一个在效率和样本质量之间的权衡（详见3.3节分析）。我们默认使用均匀分布的时间步。
此外，<strong>生成长度也被视为超参数</strong>，它指定了采样过程开始时完全被掩码句子的长度。如附录B.4所述，由于预训练和SFT都是在可变长度的数据集上进行的，最终结果对这个长度超参数不敏感。</p>
<p>在一个从时间 $t \in (0, 1]$ 到 $s \in [0, t)$的中间步骤中，我们将 $p_{0}$ 和 $r_{t}$ 同时送入掩码预测器，并一次性预测所有被掩码的 token. 随后 <em>remask</em> $\frac{s}{t}$ 比例的已预测 token 得到$r_{s}$，从而确保反向过程的转换与前向过程保持一致，以实现准确采样。</p>
<p>受 LLM 采样中退火技巧的启发，我们探索了两种确定性但有效的重掩码策略。</p>
<ul>
<li><strong>low-confidence remasking</strong>: remask 那些基于预测置信度最低的 $\frac{s}{t}$ 比例的 token.</li>
<li><strong>semi-autoregressive remasking</strong>: 对于经过 SFT 的 LLaDA 模型，将序列分成几个块，并从左到右地生成. 在每个块内部，采用反向过程进行采样。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&amp;shareKey=838350c5b31c7e78112324263cdf5621" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&amp;shareKey=838350c5b31c7e78112324263cdf5621" alt="A Conceptual Overview of the Semi-autoregressive Sampling">
    </a><figcaption>A Conceptual Overview of the Semi-autoregressive Sampling</figcaption></figure></p>
<p>对于条件似然评估，我们自然可以利用公式(5)中的上界。然而，我们发现下面这个等价形式（公式6）表现出更低的方差，在评估时更为稳定：</p>
$$
-\mathbb{E}_{l,r_{0},r_{l}}[\frac{L}{l}\sum_{i=1}^{L}I[r_{l}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{l})] \tag{6}
$$<p>其中，$l$ 从 ${1, 2, ..., L}$ 中均匀采样，$r_{l}$ 是通过从 $r_{0}$ 中不放回地均匀采样 $l$ 个没被 mask 的 token 得到的。此外，我们还采用了 unsupervised classifier-free guidance.</p>
<p><strong>虽然这两个形式的期望值相同，但它们的方差不同</strong>。直观上，在公式 (5) 中，我们期望 $x_{t}=[p_0,r_t]$ 有 $t$ 比例的 token 被掩码。然而，前向过程的随机性常常会导致偏差，尤其当 $x_{t}$ 包含的 token 很少时。相比之下，在公式 (6) 中，$r_{l}$ 中被掩码 token 的比例 $\frac{l}{L}$ 是确定的。</p>
<p>虽然理论分析取决于数据分布，但经验结果表明，公式 (5) 需要超过 1000 次蒙特卡洛估计才能得到稳定结果，而公式 (6) 仅需 128 次估计即可达到稳定。</p>
<p>Any-order autoregressive models (AO-ARM)  通过对 L 个变量所有可能的排列顺序进行自回归来描述联合分布。为了学习这样的分布，AO-ARM 利用一个权重共享的神经网络来为所有单变量条件概率建模，并使用掩码 token 来表示缺失的变量。在训练期间，模型会最小化在所有顺序的均匀分布 $U_{\pi}$ 上的期望负对数似然：</p>
$$
-\mathbb{E}_{x_{0},\pi \sim U_{\pi}}[\sum_{i=1}^{L}log~p_{\theta}(x_{0}^{\pi(i)}|x_{0}^{\pi(<i)}; \pi)]
$$<p> (15)</p>
<p>直观上，$x_{0}^{\pi(<i)}$ 可以被理解为一个被掩码的 token 序列 $x_{t}$，其中索引在 $\pi(\ge i)$ 的 token 被掩码 。可以进一步证明，公式 (15) 等价于公式 (12) 。这种联系解释了 LLaDA 的双向推理能力，即使它在推理过程中从未被显式使用 。</p>
<p>Nie et al. (2024) 引入了无监督的无分类器指导，这是一种即插即用的技术，可以平衡与提示的对齐度和文本多样性 。具体来说，无监督的无分类器指导在推理时采用以下修改过的掩码预测器 ：</p>
$$
\tilde{p}_{\theta}(r_{0}|p_{0},r_{t}) \propto \frac{p_{\theta}(r_{0}|p_{0},r_{t})^{1+w}}{p_{\theta}(r_{0}|m,r_{t})^{w}}
$$<p> (16)</p>
<p>其中，$m$ 是一个与 $p_{0}$ 长度相同的掩码序列，$w$ 是一个控制 $p_{0}$ 强度的超参数 。我们在下游任务中采用了无监督的无分类器指导，详见附录 B.5 。</p>
<h1 id="3-experiment">3 Experiment</h1>
<p>实验主要围绕以下三个核心方面展开：</p>
<ol>
<li>
<p>可扩展性 (Scalability)：研究 LLaDA 的性能是否随着计算资源和模型规模的增加而稳定提升。通过与自建的自回归模型 (ARM) 基线在相同数据上进行对比，结果显示 LLaDA 表现出强大的可扩展性，其性能增长趋势与 ARM 相当，甚至在 MMLU 和 GSM8K 等任务上更具优势。</p>
</li>
<li>
<p>基准测试结果 (Benchmark Results)：将 8B 规模的 LLaDA 与 LLaMA3 8B、LLaMA2 7B 等主流模型在涵盖通用，数学，代码和中文四大类的 15 个标准基准上进行对比。</p>
<ul>
<li>
<p>预训练模型：LLaDA 8B Base 模型的性能全面超越 LLaMA2 7B，并与 LLaMA3 8B 整体上具有竞争力，尤其在数学和中文任务上表现突出。</p>
</li>
<li>
<p>微调模型：仅经过 SFT 的 LLaDA 8B Instruct 模型，在未进行强化学习对齐的情况下，其性能在多数任务上得到提升 ，并展现出令人印象深刻的 Instruction Follow 能力。</p>
</li>
</ul>
</li>
<li>
<p>反向推理 (Reversal Reasoning)：为了量化模型克服“反转诅咒”的能力，实验在一个中文古诗补全任务上进行了测试。结果表明，LLaDA 在正向和反向任务上表现均衡，一致性强，而 GPT-4o 等模型则在反向任务上表现出显著的性能下降。</p>
</li>
</ol>
<h1 id="generation-code">Generation code</h1>
<ol>
<li>
<p><strong>初始化 (The Canvas) 🎨</strong></p>
<p>函数首先会创建一个如下所示的序列：
<code>[&lt;start_token&gt;, &lt;prompt_tokens&gt;, [MASK], [MASK], ..., [MASK]]</code>
generate 的目标就是用一个连贯的答案来替换掉所有的 <code>[MASK]</code> 标记。</p>
</li>
<li>
<p><strong>分块 (Semi-Autoregressive) 🧱</strong></p>
<p>算法并不会一次性填充所有 <code>gen_length</code> 个掩码，而是将整个过程分解为 <code>num_blocks</code> 个块。它会先完全填满第一个 <code>block_length</code> 长度的掩码，然后再开始处理下一个块。这种方式在宏观层面引入了从左到右的生成顺序。</p>
</li>
<li>
<p><strong>迭代式精炼 (核心循环) 🔄</strong></p>
<p>对于每一个块，代码都会进入一个内部循环，该循环运行 <code>steps_per_block</code> 次。循环的每一步中：</p>
<ul>
<li>
<p><strong>A. 预测：</strong> 将当前的 <code>x</code> 包含其中剩余的掩码 输入到 <code>LLaDA</code> 模型中。模型会为序列中的<em>每一个</em>位置预测最可能的 token，即使是那些没有被掩码的位置也会进行预测。</p>
</li>
<li>
<p><strong>B. 生成候选 token：</strong> 算法通过对模型的输出 <code>logits</code> 执行 <code>argmax</code> 操作，为每个位置确定一个候选 token. 在这里可以加入 Gumbel 噪声来引入随机性，其作用类似于自回归采样中的 <code>temperature</code>。这样我们就得到了一个完整的候选序列 <code>x0</code>。</p>
</li>
<li>
<p><strong>C. 置信度评分：</strong> 算法需为每个 <code>[MASK]</code> 位置上预测出的 token 计算一个<strong>置信度分数</strong>。<code>low_confidence</code> 策略（尽管其在代码逻辑中的命名可能有点误导）使用预测 token 的 softmax 概率作为其置信度。概率越高，代表模型越自信。</p>
</li>
<li>
<p><strong>D. token 选择：</strong> 基于置信度分数，算法会保留<strong>置信度最高的 K 个</strong>预测结果。每一步要保留的 token 数量 (K) 由 <code>get_num_transfer_tokens</code> 函数预先计算好，以确保线性的 unksk 速率。</p>
</li>
<li>
<p><strong>E. 状态更新：</strong> 在那些被选中的高置信度位置，<code>[MASK]</code>  token 会被替换成 <code>x0</code> 中对应的预测 token. 而其他的 <code>[MASK]</code> 位置则保持不变，留待下一次迭代。</p>
</li>
</ul>
</li>
<li>
<p><strong>重复与推进 ➡️</strong>
内部循环不断重复。在下一次迭代中，模型会看到更新后的 <code>x</code>，其中包含了更多上下文信息和更少的掩码。这使得模型在后续步骤中能做出更好的预测。这个精炼过程会一直持续，直到当前块中所有的 <code>[MASK]</code> 都被去除。</p>
</li>
<li>
<p><strong>下一区块与完成 ✅</strong>
当一个块完成后，外部循环会移动到下一个 <code>[MASK]</code> 块，并重复整个迭代式精炼过程，直到生成了完整的 <code>gen_length</code> 长度。最后，返回最终被完全填充的序列。</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">add_gumbel_noise</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    The Gumbel max is a method for sampling categorical distributions.
</span></span></span><span class="line"><span class="cl"><span class="s1">    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Thus, we use float64.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">logits</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">gumbel_noise</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">noise</span><span class="p">))</span> <span class="o">**</span> <span class="n">temperature</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">logits</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">/</span> <span class="n">gumbel_noise</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_num_transfer_tokens</span><span class="p">(</span><span class="n">mask_index</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.
</span></span></span><span class="line"><span class="cl"><span class="s1">    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),
</span></span></span><span class="line"><span class="cl"><span class="s1">    the expected number of tokens transitioned at each step should be consistent.
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    This function is designed to precompute the number of tokens that need to be transitioned at each step.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">mask_num</span> <span class="o">=</span> <span class="n">mask_index</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">base</span> <span class="o">=</span> <span class="n">mask_num</span> <span class="o">//</span> <span class="n">steps</span>
</span></span><span class="line"><span class="cl">    <span class="n">remainder</span> <span class="o">=</span> <span class="n">mask_num</span> <span class="o">%</span> <span class="n">steps</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">num_transfer_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mask_num</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">steps</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">mask_index</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">+</span> <span class="n">base</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mask_num</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_transfer_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">remainder</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">num_transfer_tokens</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">gen_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">block_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">cfg_scale</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">remasking</span><span class="o">=</span><span class="s1">&#39;low_confidence&#39;</span><span class="p">,</span> <span class="n">mask_id</span><span class="o">=</span><span class="mi">126336</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">    Args:
</span></span></span><span class="line"><span class="cl"><span class="s1">        model: Mask predictor.
</span></span></span><span class="line"><span class="cl"><span class="s1">        prompt: A tensor of shape (1, L).
</span></span></span><span class="line"><span class="cl"><span class="s1">        steps: Sampling steps, less than or equal to gen_length.
</span></span></span><span class="line"><span class="cl"><span class="s1">        gen_length: Generated answer length.
</span></span></span><span class="line"><span class="cl"><span class="s1">        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
</span></span></span><span class="line"><span class="cl"><span class="s1">        temperature: Categorical distribution sampling temperature.
</span></span></span><span class="line"><span class="cl"><span class="s1">        cfg_scale: Unsupervised classifier-free guidance scale.
</span></span></span><span class="line"><span class="cl"><span class="s1">        remasking: Remasking strategy. &#39;low_confidence&#39; or &#39;random&#39;.
</span></span></span><span class="line"><span class="cl"><span class="s1">        mask_id: The toke id of [MASK] is 126336.
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1. Initialization: Create the full sequence tensor &#39;x&#39;.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># It starts with the prompt, followed by `gen_length` [MASK] tokens.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># This is the &#34;canvas&#34; that will be filled in.</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">gen_length</span><span class="p">),</span> <span class="n">mask_id</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span><span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Keep track of where the original prompt is, so we don&#39;t modify it.</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">!=</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 2. Semi-Autoregressive Setup (Blocking)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># The generation is split into &#39;num_blocks&#39; chunks. This handles long generation</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># by generating `block_length` tokens at a time before moving to the next chunk.</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">gen_length</span> <span class="o">%</span> <span class="n">block_length</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">gen_length</span> <span class="o">//</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># The total number of refinement steps is distributed among the blocks.</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">steps</span> <span class="o">%</span> <span class="n">num_blocks</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">steps_per_block</span> <span class="o">=</span> <span class="n">steps</span> <span class="o">//</span> <span class="n">num_blocks</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 3. Outer Loop: Process each block sequentially.</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">num_block</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Define the current working area (the block to be filled).</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Note: The original code has a small typo `...:]`, corrected here for clarity.</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_pos</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num_block</span> <span class="o">*</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_pos</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">num_block</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_length</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">start_pos</span><span class="p">:</span><span class="n">end_pos</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Calculate how many tokens to &#34;unmask&#34; or &#34;confirm&#34; in each refinement step for this block.</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This ensures a steady, linear progression of unmasking.</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_transfer_tokens</span> <span class="o">=</span> <span class="n">get_num_transfer_tokens</span><span class="p">(</span><span class="n">block_mask_index</span><span class="p">,</span> <span class="n">steps_per_block</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. Inner Loop: Iteratively refine the current block.</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps_per_block</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Get the indices of all currently masked tokens in the entire sequence.</span>
</span></span><span class="line"><span class="cl">            <span class="n">mask_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">mask_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4a. Prediction with optional Classifier-Free Guidance (CFG) ---</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">cfg_scale</span> <span class="o">&gt;</span> <span class="mf">0.</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Create an unconditional version of the input by masking the prompt.</span>
</span></span><span class="line"><span class="cl">                <span class="n">un_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">un_x</span><span class="p">[</span><span class="n">prompt_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_id</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="c1"># Run the model on both the conditional (x) and unconditional (un_x) inputs.</span>
</span></span><span class="line"><span class="cl">                <span class="n">x_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">un_x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">logits_cat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">                <span class="n">logits</span><span class="p">,</span> <span class="n">un_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">logits_cat</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                
</span></span><span class="line"><span class="cl">                <span class="c1"># Combine logits to steer the generation towards the prompt.</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># The formula is: unconditional + scale * (conditional - unconditional)</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># An algebraic simplification gives the line below.</span>
</span></span><span class="line"><span class="cl">                <span class="n">logits</span> <span class="o">=</span> <span class="n">un_logits</span> <span class="o">+</span> <span class="p">(</span><span class="n">cfg_scale</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">un_logits</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># If no CFG, just do a single forward pass.</span>
</span></span><span class="line"><span class="cl">                <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4b. Candidate Generation ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Add Gumbel noise for stochastic sampling. If temperature is 0, this is a simple argmax.</span>
</span></span><span class="line"><span class="cl">            <span class="n">logits_with_noise</span> <span class="o">=</span> <span class="n">add_gumbel_noise</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Get the most likely token prediction for every position in the sequence.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits_with_noise</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4c. Confidence Scoring for Remasking ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Determine which of the new predictions to keep for the next step.</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">remasking</span> <span class="o">==</span> <span class="s1">&#39;low_confidence&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Calculate the softmax probabilities of the predicted tokens.</span>
</span></span><span class="line"><span class="cl">                <span class="n">p</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Get the probability of the chosen token `x0` at each position. This is the &#34;confidence&#34;.</span>
</span></span><span class="line"><span class="cl">                <span class="n">x0_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">elif</span> <span class="n">remasking</span> <span class="o">==</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Use random scores as confidence for random unmasking.</span>
</span></span><span class="line"><span class="cl">                <span class="n">x0_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">x0</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">remasking</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4d. Selecting Tokens to Update ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># We are only interested in updating tokens inside the current block.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Set confidence outside the current active generation area to -infinity to ignore them.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0_p</span><span class="p">[:,</span> <span class="n">end_pos</span><span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Only consider predictions for positions that are currently masked.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Original tokens (prompt and previously confirmed tokens) should have -infinity confidence.</span>
</span></span><span class="line"><span class="cl">            <span class="n">confidence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask_index</span><span class="p">,</span> <span class="n">x0_p</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># Replace the content of `x` at masked positions with the new predictions (`x0`).</span>
</span></span><span class="line"><span class="cl">            <span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask_index</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># This will hold the indices of the tokens we decide to &#34;confirm&#34; in this step.</span>
</span></span><span class="line"><span class="cl">            <span class="n">transfer_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x0</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># For each item in the batch (here, just 1)...</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">confidence</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># ...select the `k` tokens with the HIGHEST confidence scores among the masked positions.</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># `k` is determined by `num_transfer_tokens` for the current step `i`.</span>
</span></span><span class="line"><span class="cl">                <span class="n">_</span><span class="p">,</span> <span class="n">select_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">confidence</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="n">num_transfer_tokens</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># Mark these high-confidence positions to be updated.</span>
</span></span><span class="line"><span class="cl">                <span class="n">transfer_index</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">select_index</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># --- 4e. State Update ---</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Update the main tensor &#39;x&#39; by replacing [MASK] tokens with the selected high-confidence predictions.</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># The other [MASK]s remain for the next refinement iteration.</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span><span class="p">[</span><span class="n">transfer_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 5. Return Final Generation</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Once all blocks and steps are complete, return the generated part of the sequence.</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></div><h1 id="reference">Reference</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>简单来说就是拥有无限数据、一个足够大的网络和最优训练的理想条件下，模型有能力恢复出真实的数据分布。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>在不更新其自身参数的情况下，仅通过在 Prompt 中提供少量示例 (few-shot) 或任务描述 (zero-shot)，就能当场学会并执行一个新任务的能力。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Transformer Family</title>
      <link>http://localhost:1313/blogs/transformerfamily/</link>
      <pubDate>Sat, 07 Jun 2025 21:24:13 +0800</pubDate>
      <guid>http://localhost:1313/blogs/transformerfamily/</guid>
      <description>Introduction of Transformer Family</description>
      <content:encoded><![CDATA[<h1 id="origin-of-transformer">Origin of Transformer</h1>
<p>Transformer 由谷歌研于 2017 年在一篇名为 <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a> 的论文中提出。与 RNN 的输入仅为一个 token 不同，Transformer 一次性可以输入一整个完整的序列。总体结构如下图所示，包含一个 Encoder 和一个 Decoder.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBd293bc1a46904e1af31ce993b83c68f1?method=download&amp;shareKey=47cf357e488e7da5483a1b98f3257ab1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBd293bc1a46904e1af31ce993b83c68f1?method=download&amp;shareKey=47cf357e488e7da5483a1b98f3257ab1" alt="Transformers Architecture">
    </a><figcaption>Transformers Architecture</figcaption></figure></p>
<h2 id="embedding">Embedding</h2>
<p>Embedding 是一种将离散的、稀疏的输入 (如词语、字符、类别标签&hellip;) 转换为连续的、密集的向量表示的技术，核心是通过一个映射函数将离散的输入符号 (如单词) 映射到一个低维向量空间中。假设我们有一个包含 V 个单词的 Vocabulary，维度为 d，那么 Embedding Matrix 将是一个大小为 V×d 的矩阵，其中每一行是一个单词的向量表示。通过嵌入层，输入的词索引 (通常是整数) 就会被映射到该矩阵的对应行，从而得到词的向量表示。常见的预训练词嵌入方法包括：</p>
<ul>
<li>Word2Vec：通过上下文预测词语的方式学习词向量。</li>
<li>GloVe：通过统计词共现信息来学习词向量。</li>
<li>FastText：考虑了子词信息的词嵌入方法，能更好地处理词形变化。</li>
</ul>
<p>在 PyTorch 和 TensorFlow 等框架中，通常有专门的 Embedding 层，Hugging Face 也有 tokenizer 将句子划分成单词并转换成对应的索引：</p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Positional Encoding 作用是为输入的序列中的每个元素提供位置信息。由于 Transformer 架构并没有使用递归或卷积结构，本身无法捕捉输入序列中元素的相对位置关系，因此需要通过位置编码来显式地引入这些位置信息。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>Transformer 的主要优势是通过 Self-Attention 并行处理序列中的每个元素，但是这也意味着它没有自带顺序感知能力，它并不会自动知道一个单词是在句子的开头还是结尾，因此需要额外的机制来编码每个元素在序列中的位置。</p>
<p>位置编码 通过将每个单词的位置信息 (即它在序列中的位置) 编码为一个向量，并将该向量添加到单词的嵌入表示中，从而让模型能够感知每个元素的相对或绝对位置。</p></div>

<p>经典的 Transformer 位置编码使用 正弦和余弦函数的组合，为每个位置生成的向量在不同维度上具有不同的周期性，这能够捕捉到不同级别的相对位置关系。假设输入的序列中有 N 个单词，每个单词的嵌入维度为 d，那么 Positional Encodin(PE) 的计算公式如下:</p>
$$
\begin{aligned}
&PE_{(pos,2i)}=\sin\left(\frac{pos}{10000^{\frac{2i}d}}\right)\\
&PE_{(pos,2i+1)}=\cos\left(\frac{pos}{10000^{\frac{2i}d}}\right)
\end{aligned}
$$<p>其中：</p>
<ul>
<li>pos 是单词在序列中的位置索引 (位置从 0 开始).</li>
<li>i 是位置编码的维度索引，表示该位置编码向量中的第 i 个元素。</li>
<li>d 是 Embedding 的维度</li>
</ul>
<p>这些位置编码与单词的词嵌入 (Word Embedding) 相加，最终形成输入模型的向量表示。</p>
<h2 id="masked-multi-head-attention">(Masked) Multi-Head Attention</h2>
<p>Multi-Head Attention (MHA) 的目的是通过并行地计算多个注意力头 (Attention Head)，从多个子空间中学习输入序列的不同表示。经过 Word Embedding 后的输入 X 形状为 Nxd. 计算步骤如下</p>
<ol>
<li>
<p>通过学习的变换矩阵将 X 映射到查询 (Q)、键 (K) 和值 (V) 空间。
</p>
$$
\begin{aligned}&Q=XW^{Q}\\&K=XW^{K}\\&V=XW^{V}\end{aligned}
$$<p>
其中 $W^{Q},W^{K}\in\mathbb{R}d_{model}\times d_{k},W^{Q},W^{V}\in\mathbb{R}d_{model}\times d_{v}$</p>
</li>
<li>
<p>根据 QKV 计算 Attention
每个查询向量会与所有键向量进行相似度计算 (一般采用 scaled inner product)，从而获得权重，然后利用这些权重对所有值向量进行加权求和。</p>
</li>
</ol>
$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$<p><br>在多头注意力中，为了增加模型的表达能力，通常将 Q、K 和 V 通过多个不同的线性变换矩阵进行多次计算，得到多个注意力头 (Attention Heads). 每个头的计算是独立的，但它们的结果会在最后进行拼接并经过线性变换。最终的 Multi-Head Attention 公式为：</p>
$$
\text{MultiHead}(Q,K,V)=\text{Concat}(head_1,head_2,\ldots,head_h)W^O
$$<p><br>每个头 $head_i$ 计算公式为</p>
$$
\text{MultiHead}(Q,K,V)=\text{Concat}(head_1,head_2,\ldots,head_h)W^O
$$<p><br>这里的 $W^{Q}_{i},W^{K}_{i},W^{V}_{i}$ 是为每个头学习到的不同权重矩阵，$W^O$ 是输出的线性变换矩阵。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB85e0bf86b5d9f2c649bbc3f08c03d203?method=download&amp;shareKey=b5e662d324237709f786beb08c27b774" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB85e0bf86b5d9f2c649bbc3f08c03d203?method=download&amp;shareKey=b5e662d324237709f786beb08c27b774" alt="Multi-Head Attention">
    </a><figcaption>Multi-Head Attention</figcaption></figure></p>
<p>Decoder 中的 Masked MHA 确保模型只能在解码序列的当前位置及其之前的位置上操作，而不能 “看到” 将要生成的未来信息。与标准的 MHA 相同，注意力分数 $\mathrm{Attention Scores}=\frac{QK^T}{\sqrt{d_k}}$ 是通过 Q 和 K 的点积计算得到的。计算完成后我们给其加上一个下三角元素 (包含主对角线) 为 0，上三角元素为 —∞ 的 mask，这样未来的信息经过 Softmax 后的权重为 0，被完全屏蔽。</p>
<h2 id="grouped-query-attentiongqa-multi-query-attention-mqa">Grouped Query Attention（GQA）&amp; Multi-query Attention (MQA)</h2>
<p><a href="https://arxiv.org/pdf/2305.13245">GQA</a> 将多个 Q 分成若干组，每一组共享相同的权重矩阵。这使得每组查询可以共同处理同一个 K 和 V，降低了计算量和内存需求。在 MHA 中，所有的头共享相同的输入 X，但使用不同的投影矩阵来生成 K 和 V. GQA 中 K 和 V 通常是对输入 X 进行一次性线性变换，并在所有同一分组中的 Q 共享。MQA 更为极端，所有的 Q 共享一个 K 和 V.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3c7dc003db55abf4b8a1ebeb4aabd667?method=download&amp;shareKey=f1570d975432b38d6f74742e9bb4cf6e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3c7dc003db55abf4b8a1ebeb4aabd667?method=download&amp;shareKey=f1570d975432b38d6f74742e9bb4cf6e" alt="Overview of MHA, GQA &amp; MQA">
    </a><figcaption>Overview of MHA, GQA &amp; MQA</figcaption></figure></p>
<h2 id="multi-head-cross-attention">Multi-Head Cross Attention</h2>
<p>Multi-Head Cross Attention 是 Transformer Decoder 中的一个核心组件。与 Self-Attention 不同，Cross Attention 负责将解码器的隐藏状态与编码器的输出上下文信息进行交互，允许解码器的每一个解码时间步的状态 <strong>查看整个编码器的输出</strong>。每个解码的时间步 t，Decoder 的隐藏状态作为 Q，Encoder 的输出作为 K 和 V，计算过程与 标准的 Self-Attention 相同。</p>
<h1 id="evolution-tree-of-transformer">Evolution Tree of Transformer</h1>
<p>后续的研究逐渐把 Encoder 和 Decoder 分离开来，形成 Encoder-Only 和 Decoder-Only 的模型。如下图所示</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa2db49ee75b563db2d846dab14947060?method=download&amp;shareKey=12514a3314f3bb4c5e30936c2d634650" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa2db49ee75b563db2d846dab14947060?method=download&amp;shareKey=12514a3314f3bb4c5e30936c2d634650" alt="Transformer Evolution Tree">
    </a><figcaption>Transformer Evolution Tree</figcaption></figure></p>
<h2 id="feed-forward-network">Feed Forward Network</h2>
<p>FFN 是一个两层的前馈全连接网络，中间有一个非线性激活函数。第一层全连接将 $d_model$ 映射到 $4d_model$ ，经过非线性激活函数后，第二层全连接再重新映射回 $d_model$.</p>
<h1 id="decoder-only-transformer">Decoder-Only Transformer</h1>
<p>Decoder-Only 删除了原先 Transformer Encoder 的部分以及 Encoder 和 Decoder 进行 Cross Attention 的部分。它具有三个必要的特征:</p>
<ol>
<li>在给定编码器输入作为上下文的情况下基于迄今为止生成的 token 自动回归预测下一个。</li>
<li>在评估对输入序列的 Q 时看不到未来值。这就是为什么仅解码器的模型通常被称为 Casual Language Model (CLM).</li>
<li>训练模型以在给定当前输入序列的情况下预测下一个 token. 这种训练方法与回归相结合，允许模型自回归生成任意长 (最高达输入序列的最大长度) 的序列。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa6c37075488053053efa01808163d0ba?method=download&amp;shareKey=5542015805dbda24ff7ab5dbf44a368b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa6c37075488053053efa01808163d0ba?method=download&amp;shareKey=5542015805dbda24ff7ab5dbf44a368b" alt="Decoder-only (left) and Encoder-only (right) Transformer Architectures">
    </a><figcaption>Decoder-only (left) and Encoder-only (right) Transformer Architectures</figcaption></figure></p>
<h1 id="llama-transformer-architecture">LLaMA Transformer Architecture</h1>
<p>LLaMA Transformer 结构如下，主要有以下变化</p>
<ol>
<li>使用 RoPE (Rotary Position Embedding) 替代传统的位置编码。</li>
<li>RMSNorm 替代 LayerNorm</li>
<li>引入 Gated Linear Unit (GLU)</li>
</ol>
<h2 id="rotary-position-embedding">Rotary Position Embedding</h2>
<p>传统的 Transformer 模型使用可学习的绝对位置编码 (如 sinusoidal position embedding)，但 RoPE 采用了旋转矩阵的思想，将位置编码与输入的 token 表示直接结合，而不依赖于额外的可学习参数。</p>
<p>输入向量的旋转角度为 $\theta(p,i)=p\cdot10000^{-2i/d}$. p 表示位置索引，i 表示维度索引，d 为向量的总维度。对于输入的 token 向量 x 中的每一对偶数和奇数维度 $(x_{2i},x_{2i+1})$，旋转操作可以用 2D 旋转矩阵表示为</p>
$$\begin{bmatrix}x_{2i}^{\prime}\\x_{2i+1}^{\prime}\end{bmatrix}=\begin{bmatrix}\cos(\theta)&-\sin(\theta)\\\sin(\theta)&\cos(\theta)\end{bmatrix}\cdot\begin{bmatrix}x_{2i}\\x_{2i+1}\end{bmatrix}$$<p><br>对于输入的 token 向量 $\mathbf{x}\left[x_{0},x_{1},x_{2},x_{3},\cdots,x_{d-1}\right]$, RoPE 将其两两一组配对，每一组都会与位置相关的旋转角度 θ 对应地应用旋转操作。这个过程的本质是对输入 token 的表示做了旋转变换，使得这些特征不仅依赖于输入的特征，还隐含了该 token 在序列中的位置。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBf24aca24d7ff8bc2901ca4983cbf6c47?method=download&amp;shareKey=9ac054d415fe2e172bb8a719935d4793" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBf24aca24d7ff8bc2901ca4983cbf6c47?method=download&amp;shareKey=9ac054d415fe2e172bb8a719935d4793" alt="RoPE">
    </a><figcaption>RoPE</figcaption></figure></p>
<h2 id="rmsnorm">RMSNorm</h2>
<p>RMSNorm 相对于 LayerNorm 去掉了均值计算，仅基于输入的均方根进行归一化 $\mathrm{RMSNorm}(\mathbf{x})=\frac{\mathbf{x}}{\mathrm{RMS}(\mathbf{x})+\epsilon}\cdot\gamma$</p>
<p>其中</p>
<ul>
<li>$\mathrm{RMS}(\mathbf{x})=\sqrt{\frac1d\sum_{i=1}^dx_i^2}$ 为输入的均方根。</li>
<li>$\gamma{:}$ 为可学习的缩放参数。</li>
<li>$\epsilon{:}$ 为防止除以 0 的小数。</li>
</ul>
<h2 id="silu">SiLU</h2>
<p>SiLU (Sigmoid Linear Unit) 是一种激活函数，也称为 Swish，其定义为输入 x 和 Sigmoid 函数输出的乘积。其定义为
</p>
$$\mathrm{SiLU}(x)=x\cdot\sigma(x)$$<p>
其中 $\sigma(x)=\frac1{1+e^{-x}}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB552a846c520bf2b5194c621e7b8e224e?method=download&amp;shareKey=519f3a1e4cce59da1895fa7bc2bcc842" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB552a846c520bf2b5194c621e7b8e224e?method=download&amp;shareKey=519f3a1e4cce59da1895fa7bc2bcc842" alt="SiLU">
    </a><figcaption>SiLU</figcaption></figure></p>
]]></content:encoded>
    </item>
    <item>
      <title>ZeRO, ZeRO-Offload, ZeRO-Infinity</title>
      <link>http://localhost:1313/blogs/zero/</link>
      <pubDate>Sat, 07 Jun 2025 21:11:32 +0800</pubDate>
      <guid>http://localhost:1313/blogs/zero/</guid>
      <description>Paper reading of ZeRO.</description>
      <content:encoded><![CDATA[<h1 id="zero">ZeRO</h1>
<p>Zero 用于优化内存，极大地提高了训练速度，同时增加了可以训练的模型大小。ZeRO 消除了数据和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度，能够以持续的高效率按设备数量等比例扩展可训练模型的大小。</p>
<h2 id="introduction">Introduction</h2>
<p>ZeRO 首先总结了下当前并行方法存在的问题</p>
<ul>
<li>Basic DP: 没有减少每个设备的内存，在 32GB 内存的 GPU 上训练超过 1.4B 参数的模型便会 OOM.</li>
<li>Model Parallelsim (MP): 切分了每一层的计算和激活到每个设备上，但引入了大量的通信 (前向和反向都需要 2xAll-Reduce)，因此扩展性差，通常只在一个节点内的高带宽连接的 GPU 中进行。在 DGX-2 节点训练 40B 参数的模型每个 V100 GPU 仅能达到硬件峰值的 5% 算力 (5T flops).</li>
</ul>
<p>模型状态通常占据了训练时的大部分内存，但 DP 在所有数据并行进程中保存一份模型状态，导致冗余内存消耗；而 MP 对这些状态进行切分以获得高内存效率，但通常会导致过于细粒度的计算和昂贵的通信，扩展效率较低。此外，这些方法都静态地维护整个训练过程所需的<strong>整个模型状态</strong>。</p>
<p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">ZeRO-DP</a> 通过在数据并行过程中划分模型状态 (参数、梯度和优化器状态) 消除了数据并行过程中的内存冗余。</p>
<p>结论：如下图所示 ZeRO-DP 有三个主要的优化阶段，它们对应于优化器状态、梯度和参数的划分。
对于使用 FP16 的模型，内存占用包括参数 (FP16)、梯度 (FP16)、Adam 优化器状态 (动量 (FP32)，方差 (FP32) 以及更新后的参数 (FP32), 因此 K=12).</p>
<ol>
<li>优化器状态划分 (Pos) —— 内存减少 4 倍，需要对梯度进行 reduce-scatter，用各自的优化器状态更新梯度后进行 All-gather 使所有设备都有最新的梯度，通信量与数据并行性相同 (对 Loss 进行一次 All-reduce).</li>
<li>添加梯度划分  (Pos+g) &ndash; 内存减少 8 倍，每个设备需要将自己的梯度 scatter 到负责更新那部分参数的设备上，然后使用 Gather 将其他设备更新后的模型参数同步到自己上面，通信量与数据并行性相同。</li>
<li>添加参数划分 (Pos+g+p) &ndash; 内存减少与数据并行度 Nd 呈线性关系。通信量增加了50%，因为在前向/反向传播中需要每个设备需要额外广播自己存储的模型参数 <code>2*(N-1)/N*P</code>，反向传播时需要对发送梯度到对应的设备上 <code>(N-1)/N*P</code>.</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBcfab82173b0f76eb5b3c8396e81e238a?method=download&amp;shareKey=1b8bb86256be5b15bec039beecee062b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBcfab82173b0f76eb5b3c8396e81e238a?method=download&amp;shareKey=1b8bb86256be5b15bec039beecee062b" alt="Memory Savings and Communication Volume for the 3-stage of ZeRO">
    </a><figcaption>Memory Savings and Communication Volume for the 3-stage of ZeRO</figcaption></figure></p>
<p>激活、临时缓冲区和不可用的内存片段会成为次要内存瓶颈。作者开发了 ZeRO-R 优化了这三个因素分别消耗的剩余内存。</p>
<ol>
<li>对于激活 (在前向传播中存储，反向传播中使用)，仅仅使用激活检查点是不够的。ZeRO-R 通过激活划分识别和删除现有 MP 方法中重复存储的激活，并且在适当时候将激活存储在 CPU 中。</li>
<li>ZeRO-R 定义了适当大小的临时缓冲区，以实现内存和计算效率的平衡。</li>
<li>由于不同张量的寿命存在差异，ZeRO-R 根据张量的不同生命周期主动管理内存，防止内存碎片。</li>
</ol>
<p>在某些情况下，MP 仍可以和 ZeRO 一起使用：i）当与 ZeRO-R 一起使用时，MP 可以减少超大模型的激活内存占用。ii）对于较小模型，当单独使用 DP 的 batchsize 太大而无法实现良好的收敛时，MP 也可以带来好处。</p>
<h2 id="where-did-all-the-memory-go">Where Did All the Memory Go?</h2>
<p>在模型训练期间，大部分内存被模型状态消耗 (优化器状态、梯度和参数). 除了这些模型状态之外，剩余的内存被激活、临时缓冲区和碎片内存所消耗，称之为剩余状态。</p>
<h3 id="model-states-optimizer-states-gradients-and-parameters">Model States: Optimizer States, Gradients and Parameters</h3>
<p>Adam 优化器需要存储两个优化器状态：时间平均动量和梯度方差来计算更新后的参数。此外，还需要有足够的内存来存储梯度和权重本身。</p>
<p><strong>混合精度训练 (Mixed-Precision Training)</strong> 中参数和激活以 fp16 格式存储并且在前向和反向传播中也使用 fp16 格式的权重和激活。Adam 优化器存储 fp32 格式的参数副本、动量和方差以保证更新的精度。</p>
<p>假设模型参数量为 ψ，模型参数需要占用 2ψ 字节的内存，反向传播中产生的 fp16 梯度需要占用 2ψ 字节的内存。Adam 优化器存储 fp32 格式的参数副本、动量和方差每个都需要占用 4ψ 字节的内存。因此训练时总共需要 16ψ 字节的内存，为存储模型参数的 8x.</p>
<h3 id="residual-memory-consumption">Residual Memory Consumption</h3>
<p>在训练过程中，<strong>激活</strong>会占用大量的内存。基于 transformer 的模型的激活内存占用与层数×隐藏维度×序列长度×批大小成正比。对于类似 GPT-2的结构，总激活约为 12×隐藏亮度×批大小×序列长度×变层数 (<code>QKV(h*3h) + O(h*h) + MLP(h*4h+4h*h)=12h*h</code>，没有考虑 mask). 激活重计算可以以 33% 的额外计算开销 (之前是一次前向，一次反向，反向因为需要对输入和参数都进行求导所以计算量是前向的两倍，现在多了一次前向) 换取接近原先激活大小平方级别的内存占用。</p>
<p>对于大型模型，用于存储中间结果的<strong>临时缓冲区</strong>会消耗大量内存。对梯度进行 All-Reduce 或梯度归一化计算等操作倾向于在操作之前将所有梯度融合到单个扁平缓冲区中，以提高吞吐量。</p>
<p><strong>碎片化内存</strong>会导致即使有足够的内存但没有足够大的连续块进行分配时的 OOM，作者观察到极端情况下在有 30% 剩余内存时也会产生 OOM.</p>
<h2 id="zero-insight-and-overview">ZeRO: Insight and Overview</h2>
<p>ZeRO有两组优化：ZeRO-DP 旨在减少模型状态的内存占用；ZeRO-R 旨在减少剩余内存消耗。</p>
<p>ZeRO-DP 基于三个关键见解：</p>
<ol>
<li>DP 比 MP 具有更好的扩展效率，因为 MP 减少了计算的粒度，同时也增加了通信开销。</li>
<li>DP 内存效率低下，因为模型状态被在所有数据并行进程中都存有一份。</li>
<li>DP 和 MP 都保留了整个训练过程中所需的所有模型状态，但并非所有状态在整个训练期间都需要。</li>
</ol>
<p>ZeRO-DP 划分模型状态，并使用动态通信调度利用模型状态的内在的暂时性，同时最小化通信量。</p>
<p>ZeRO-R 基于两个关键见解：</p>
<ol>
<li>MP 对模型状态进行切分，但通常需要重复存储激活。</li>
<li>对于GPT-2或更大的模型，算术强度 (每次迭代计算量与激活检查点数量的比值) 非常大 (≥10K)，并且随着隐藏维数的增加而线性增加，即使在带宽较低的情况下，也可以隐藏激活检查点的数据移动成本。</li>
</ol>
<p>ZeRO 通过跨 GPU 划分激活检查点来消除 MP 中的内存冗余，并根据需要使用 All-Gather 来重建；使用恒定大小的缓冲区来避免临时缓冲区随着模型大小的增加而爆炸；通过将激活检查点和梯度移动到预分配的连续内存缓冲区来执行动态内存碎片整理。</p>
<h2 id="deep-dive-into-zero-dp">Deep Dive into ZeRO-DP</h2>
<p>下表显示了逐渐切分 (1) 优化器状态、(2) 梯度和 (3) 参数冗余后的内存占用。称为ZeRO-DP的三个优化阶段：Pos， Pg和Pp，将在下面详细说明。</p>
<table border="1" cellspacing="0" cellpadding="5">
  <thead>
    <tr>
      <th rowspan="2">DP</th>
      <th colspan="3">7.5B Model (GB)</th>
      <th colspan="3">128B Model (GB)</th>
      <th colspan="3">1T Model (GB)</th>
    </tr>
    <tr>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
      <th>Pos</th>
      <th>Pos+g</th>
      <th>Pos+g+p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>120</td>
      <td>120</td>
      <td>120</td>
      <td>2048</td>
      <td>2048</td>
      <td>2048</td>
      <td>16000</td>
      <td>16000</td>
      <td>16000</td>
    </tr>
    <tr>
      <td>4</td>
      <td>52.5</td>
      <td>41.3</td>
      <td><b>30</b></td>
      <td>896</td>
      <td>704</td>
      <td>512</td>
      <td>7000</td>
      <td>5500</td>
      <td>4000</td>
    </tr>
    <tr>
      <td>16</td>
      <td>35.6</td>
      <td><b>21.6</b></td>
      <td>7.5</td>
      <td>608</td>
      <td>368</td>
      <td>128</td>
      <td>4750</td>
      <td>2875</td>
      <td>1000</td>
    </tr>
    <tr>
      <td>64</td>
      <td><b>31.4</b></td>
      <td>16.6</td>
      <td>1.88</td>
      <td>536</td>
      <td>284</td>
      <td><b>32</b></td>
      <td>4187</td>
      <td>2218</td>
      <td>250</td>
    </tr>
    <tr>
      <td>256</td>
      <td>30.4</td>
      <td>15.4</td>
      <td>0.47</td>
      <td>518</td>
      <td>263</td>
      <td>8</td>
      <td>4046</td>
      <td>2054</td>
      <td>62.5</td>
    </tr>
    <tr>
      <td>1024</td>
      <td>30.1</td>
      <td>15.1</td>
      <td>0.12</td>
      <td>513</td>
      <td>257</td>
      <td>2</td>
      <td>4011</td>
      <td>2013</td>
      <td><b>15.6</b></td>
    </tr>
  </tbody>
</table>
<h3 id="pos-optimizer-state-partitioning">Pos: Optimizer State Partitioning</h3>
<p>设 DP 并行度为 Nd, 每个数据并行进程只需要存储和更新总优化器状态的 1/Nd，然后只更新参数的 1/Nd. 在每个训练步骤结束时，在数据并行进程中执行一次 All-Gather，以获得所有数据并行过程中完全更新的参数。这使得每个设备上保存模型状态需要的内存从 4ψ+Kψ 变成 4ψ+Kψ/Nd，当使用 Adam 优化器 (K=12) 并且 Nd 很大时，内存需求可以降低接近 4x.</p>
<h3 id="pg-gradient-partitioning">Pg: Gradient Partitioning</h3>
<p>由于每个数据并行进程只用更新自己被分配的参数，因此他也只需要那部分参数 reduce 后的梯度。只在负责更新相应参数的数据并行过程中进行 reduce. 完成后它们的内存可以被释放。这使得了梯度所需的内存占用从 2Ψ 字节减少到 2Ψ/Nd. 更新后的参数再被 scatter 到其他进程。</p>
<p>通常为了效率，将需要 reduce 的梯度按照参数的分区划分成多个 buckets，每个 bucket 对应特定的一组参数，对每个 bucket 进行整体 reduce 操作，而不是对单个梯度进行操作。进一步划分梯度后，每个设备上保存模型状态需要的内存进一步减少到 2ψ+(K+2)ψ/Nd</p>
<p>蓝色箭头串起来的白色长方形代表的是 Transformer Block，蓝色的第一行代表 FP16 参数；橙色的第二行代表 FP16 梯度，反向传播时将用于更新参数；绿色的行代表优化器状态 (FP32 的梯度，动量，方差，以及更新后的参数)，其中在计算完 FP16 梯度以后不再需要保存 FP32 参数。同时也需要 buffer 来保存部分 transformer block 的输出激活。</p>
<h3 id="pp-parameter-partitioning">Pp: Parameter Partitioning</h3>
<p>更进一步我们可以将模型参数也进行划分，当设备所没有的参数需要进行向前和向后传播时，通过广播从其他的的数据并行进程接收。通过前面的分析可知这使得通信量变为原来的 1.5x， 但使得所有的模型参数都被划分到每个设备上，只需要 (4+K)/Nd 字节的内存。</p>
<h3 id="execution-steps-of-zero3">Execution Steps of ZeRO3</h3>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fd41e8b92bcd256a910ce757d4eea21?method=download&amp;shareKey=d82f49f0d59309d987c164a100966895" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fd41e8b92bcd256a910ce757d4eea21?method=download&amp;shareKey=d82f49f0d59309d987c164a100966895" alt="Overview of Memory Consumption">
    </a><figcaption>Overview of Memory Consumption</figcaption></figure></p>
<p>每个 GPU 只需要保存自己部分的 Pos+g+p. 前向传播时保存对应模型参数的 GPU 需要把参数广播到其他 GPU 中，其他 GPU 用自己部分的数据完成前向传播后就可以删除这部分参数 (最后一部分除外). <code>(N-1)/N*P</code></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB82b66a8a7b40a2fdf9512545189cc37a?method=download&amp;shareKey=37a4614d85a77d2ca00e6d5a4769e2f9" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB82b66a8a7b40a2fdf9512545189cc37a?method=download&amp;shareKey=37a4614d85a77d2ca00e6d5a4769e2f9" alt="Broadcast of Model Parameters">
    </a><figcaption>Broadcast of Model Parameters</figcaption></figure></p>
<p>前向传播完成后，第一次反向传播可以利用最后一次正向传播已经广播了的模型参数，每个 GPU 计算自己部分的梯度，然后 Reduce 到存储对应模型参数的 GPU 中。之后和前向传播一样，每个 GPU 都需要广播自己的参数，然后其他 GPU 用自己的数据完成梯度计算以后 Reduce 到自己的梯度。<code>(N-1)/N*P + 1/N*G*(N-1)</code></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB575b3869e59814ae1449351cf1b18d01?method=download&amp;shareKey=10f29390f47227ca8eefbcc00f4fca6e" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB575b3869e59814ae1449351cf1b18d01?method=download&amp;shareKey=10f29390f47227ca8eefbcc00f4fca6e" alt="Gradient Accumulation">
    </a><figcaption>Gradient Accumulation</figcaption></figure></p>
<p>反向传播结束以后，每个 GPU 使用优化器更新自己的 FP32 模型参数后转换成 FP16 格式。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB19cb21dfa63ab76437a2246ff52b00aa?method=download&amp;shareKey=ddb63d57f3bf976b1dce4596e77a2009" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB19cb21dfa63ab76437a2246ff52b00aa?method=download&amp;shareKey=ddb63d57f3bf976b1dce4596e77a2009" alt="Update Parameters Locally">
    </a><figcaption>Update Parameters Locally</figcaption></figure></p>
<h2 id="deep-dive-into-zero-r">Deep Dive into ZeRO-R</h2>
<h3 id="pa-partitioned-activation-checkpointing">Pa: Partitioned Activation Checkpointing</h3>
<p>一旦计算了模型的一层的前向传播，输入激活将在所有模型并行过程中进行划分，直到在反向传播期间再次需要输入激活。此时，ZeRO 使用一个 All-Gather 操作来重新实现激活的复制副本。称这个优化为 Pa. 将 Pa 与激活检查点结合，只存储分区的激活检查点，这样使得激活占用空间的减少与 MP 并行度成正比。</p>
<h3 id="cb-constant-size-buffers">CB: Constant Size Buffers</h3>
<p>通信的效率不仅仅与数据量相关，还受到固定启动开销和带宽利用率的影响。较大的输入更容易充分利用硬件的带宽和优化机制，因而能显著提高 All-Reduce 操作的效率。因此经常将需要进行通信的数据合并到一个缓冲器。然而，合并缓冲区的内存开销与模型大小成正比，模型过大时容易 OOM. 为了解决这个问题，<strong>当模型很大时，简单地使用一个性能高效的固定大小的合并缓冲区</strong>。</p>
<h3 id="md-memory-defragmentation">MD: Memory Defragmentation</h3>
<p>前向传播中只需要保存检查点的激活而丢弃其他激活会产生碎片化内存。同样的反向传播中只需要保存参数的梯度而丢弃激活的梯度也会产生碎片化内存。内存碎片导致两个问题: (1) 即使有足够的可用内存，由于缺乏连续内存导致 OOM. (2) 由于内存分配器花费大量时间搜索连续内存块以满足内存请求而导致效率低下。ZeRO 通过<strong>为激活检查点和梯度预分配连续内存块，并在它们产生时将它们复制到预分配的内存中</strong>，从而实时地进行内存碎片整理。</p>
<h2 id="communication-analysis-of-zero-dp">Communication Analysis of ZeRO-DP</h2>
<p>使用 Pos 和 Pg 时，ZeRO-DP 不会产生额外的通信，同时可以减少高达 8 倍的内存。使用 Pos+g+p 时，ZeRO-DP 最多会产生 1.5 倍的通信，同时减少内存占用为原来的 1/Nd.</p>
<p>在数据并行训练过程中，在计算下一步的更新之前，在反向传播结束时对所有数据并行进程的梯度使用 All-Reduce 进行平均，因此通信量为 2ψ. 使用 Pos+g 时每个设备需要将自己的梯度 scatter 到负责更新那部分参数的设备上，然后使用 Gather 将其他设备更新后的模型参数同步到自己上面，总通信量仍为 2ψ，与数据并行相同。使用 Pos+g+p 时负责该分区的数据并行进程将权重 brocast 给所有数据并行进程 (前向反向各一次)，最后仍需要 Gather 其他进程上更新好的参数，因此总通信量为 3ψ.</p>
<h2 id="communication-analysis-of-zero-r">Communication Analysis of ZeRO-R</h2>
<p>在使用激活检查点的 Megatron-LM 中，每个 transformer block 在前向传播中执行 2 次大小为 批大小×序列长度×隐藏维度的 All-Reduce 操作，反向传播中执行 2 次同样大小的 All-Reduce 操作，同时激活重计算也需要 2 次同样大小的 All-Reduce 操作。因此每个块的总通信量为 12×序列长度×隐藏维度。</p>
<p>当使用 ZeRO-R 划分激活检查点时，在对每个激活检查点上的反向传播进行前向重新计算之前，需要进行额外的一次 All-Gather 操作。因此，Pa的总通信开销相对于原先 MP 通信量增加了 1/12，但是使得激活内存占用减小到原来的 1/MP_degree.</p>
<p>如果使用了 Pa+cpu，则分区激活检查点将被存储到 CPU，对激活内存需求减少到几乎为零，而代价是与 Pa 相比，需要从 CPU 和内存之间的数据移动增加了 2 倍。</p>
<h1 id="zero-offload">ZeRO-Offload</h1>
<p>ZeRO-Offload 通过将数据和计算下放到 CPU 来实现大型模型训练。为了保持计算效率，它尽可能减少数据在 GPU 和 CPU 之间的移动，同时最大限度地减少 CPU 的计算时间，并最大限度地节省 GPU 上的内存。</p>
<h2 id="introduction-1">Introduction</h2>
<p>PP, MP 和 ZeRO 等并行技术都需要有足够的 GPU 设备，使得它们的内存之和能够容纳训练所需的模型状态的存储。目前基于注意力的大模型训练的主要内存瓶颈是模型状态，而不是激活。现有的异构训练在两个主要方面受到限制：(i) 几乎所有的训练都利用 CPU 内存，而不是 CPU算力。(ii) 它们主要是为单个 GPU 设计和评估的。</p>
<p>ZeRO-Offload 为了提高计算效率采取的设计原则有三条：(i) 它需要的 CPU 计算量与 GPU 相比减少了几个数量级。(ii) 它最小化了 CPU 和 GPU 之间的通信量，防止了通信成为瓶颈。(iii) 可以证明在实现最小通信量的同时最大限度地节省了 GPU 的内存。</p>
<p>ZeRO-Offload 将梯度，优化器状态和优化器计算卸载到 CPU，而将参数和前向和反向计算保留在 GPU上。这样 CPU 上的计算量为 O(M)，而 GPU 上的计算量则为 O(MB)，其中 M 和 B 分别为模型大小和 batchsize. 因为 CPU 只处理模型参数的更新，而不参与与 batch size 相关的梯度求平均的操作。在大多数情况下，batchsize 较大，因此 CPU 计算不是瓶颈。但是对于较小的 batchsize，CPU 计算可能会成为瓶颈。</p>
<h2 id="unique-optimal-offload-strategy">Unique Optimal Offload Strategy</h2>
<p>为了确定最佳的卸载策略，ZeRO-Offload 将 DL 训练建模为如下图所示的数据流，并有效地在 CPU 和 GPU 设备之间进行划分。GPU 和 CPU 之间的卸载策略可以使用该图的二分图来表示，这样一个分区中的计算节点将在拥有该分区的设备上执行，分区中的数据节点也存储在拥有该分区的设备上。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc55ea0bd058b2e603052658a6cb25aa6?method=download&amp;shareKey=e2474b8904eff6869bcdce3b09e545f6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc55ea0bd058b2e603052658a6cb25aa6?method=download&amp;shareKey=e2474b8904eff6869bcdce3b09e545f6" alt="The Dataflow of Fully Connected Neural Networks">
    </a><figcaption>The Dataflow of Fully Connected Neural Networks</figcaption></figure></p>
<p>由于 CPU 的算力远远低于 GPU，所以前向传播和反向传播 (它们的计算复杂度都是 O(MB)) 必须在 GPU上完成，而其余复杂度为 O(M) 的计算 (如归一化计算、权重更新等) 会被卸载到 CPU 上。</p>
<p>CPU 内存带宽 (100xGB) 至少比 CPU 和 GPU 之间的 PCIe 带宽 (10xGB) 快一个数量级，而 GPU 内存带宽比 CPU 内存带宽 (TB) 快另一个数量级。数据流中的每个节点都是环的一部分。因此，对该图进行任何划分都需要切割至少两条边，每条边的权值至少为 2M，从而总通信量至少 4M (通过仅卸载部分模型状态，可以进一步减少通信量). 因此，为了实现最小的通信量，所有卸载策略必须使得关于 fp32 模型状态操作的生产者和消费者相同。fp16 参数节点必须和 FWD-BWD 节点在一个子图中，因为这两个节点之间的边权值是 4M.</p>
<p>下表显示了最小化通信量情况下的所有有效分区策略所节省的内存。通过将 fp16 梯度和 Update Super 节点放到 CPU 可以实现 8x 的最大内存节省。</p>
<table>
  <thead>
      <tr>
          <th>FWD-BWD</th>
          <th>p16</th>
          <th>g16</th>
          <th>Update</th>
          <th>Memory</th>
          <th>Reduction</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>gpu</td>
          <td>gpu</td>
          <td>16M</td>
          <td>1x (baseline)</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>cpu</td>
          <td>gpu</td>
          <td>14M</td>
          <td>1.14x</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>gpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>4M</td>
          <td>4x</td>
      </tr>
      <tr>
          <td>gpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>cpu</td>
          <td>2M</td>
          <td>8x</td>
      </tr>
  </tbody>
</table>
<p>综上所述 ZeRO-Offload 在 CPU 上存储所有 fp32 模型状态以及 fp16 梯度，并且还在 CPU 上计算更新后的参数。fp16 的参数保存在 GPU 上，前向和反向计算也在GPU上完成。</p>
<h2 id="zero-offload-schedule">ZeRO-Offload Schedule</h2>
<p>在训练过程中，首先通过前向传播计算损失。由于 fp16 参数已经存放在GPU上，因此这部分计算不需要与 CPU 通信。在损失的反向传播过程中，不同设备计算不同参数的梯度。ZeRO-Offload 可以在计算完每个参数后，将这些梯度单独或分组传输到 CPU 内存中。由于梯度是逐层传输的，因此 GPU 上只需要很小的缓冲区来存放每一层的梯度。在反向传播之后，ZeRO-Offload 直接在 CPU 上更新 fp32 参数和优化器状态），并将更新后的 fp32 参数从 CPU 内存复制到 GPU 内存上的 fp16 参数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB810cb8d722c2c9e8e140b80084c47cbe?method=download&amp;shareKey=6ab97f8c087ad948e03121afde1c266a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB810cb8d722c2c9e8e140b80084c47cbe?method=download&amp;shareKey=6ab97f8c087ad948e03121afde1c266a" alt="ZeRO-Offload Training Process on a Single GPU">
    </a><figcaption>ZeRO-Offload Training Process on a Single GPU</figcaption></figure></p>
<p>在卸载之前进行如上一节所述的划分的主要好处是，对于具有超过 1 个 GPU 的系统，每个数据并行进程只负责更新参数的一个子集。所有数据并行进程的 GPU 到 CPU 的通信量总和保持不变，CPU 资源可以并行使用，共同计算单个权重更新。ZeRO-Offload 在不同 GPU 之间划分梯度和优化器状态，每个 GPU 将其拥有的部分卸载到 CPU 内存中，并在整个训练过程中将其一直保存在那里。在反向传播过程中，在 GPU上使用 reduce-scatter 计算普遍复核一遍梯度，每个 GPU 只将属于其那一部分的平均梯度卸载到 CPU 内存中。然后优化器状态将由每个数据并行进程直接在 CPU 上并行更新。更新后，参数被移回 GPU，然后在 GPU 上执行类似于 ZeRO-2 的 All-Gather 操作来获取所有更新后的参数。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6e4ed2b9f7f37bf8a5f8f9326bb98971?method=download&amp;shareKey=0bf3ca60c8edb9a602a3ae93262c0967" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6e4ed2b9f7f37bf8a5f8f9326bb98971?method=download&amp;shareKey=0bf3ca60c8edb9a602a3ae93262c0967" alt="ZeRO-Offload Data Placement with Multiple GPUs">
    </a><figcaption>ZeRO-Offload Data Placement with Multiple GPUs</figcaption></figure></p>
<h2 id="optimized-cpu-execution">Optimized CPU Execution</h2>
<ol>
<li>作者使用高性能计算技术实现了一个加速版的 CPU Adam 优化器</li>
<li>开发了一个一步延迟参数更新计划，将 CPU 参数更新计算与 GPU 上的前向和反向计算重叠，隐藏了 CPU 执行时间。</li>
</ol>
<h3 id="implementing-the-cpu-optimizer">Implementing the CPU Optimizer</h3>
<p>作者使用三级并行性来提高 CPU 优化器的性能。</p>
<ol>
<li>SIMD 矢量指令，充分利用 CPU 架构的硬件并行性。</li>
<li>循环展开，一种提高指令级并行性的有效技术，能更好地利用内存带宽。</li>
<li>OMP 多线程，可以有效地并行利用 CPU 上的多个内核和线程。</li>
</ol>
<p>算法的输入为 β₁(动量系数), β₂(RMSProp 的平方梯度衰减系数), α(学习率)，以及梯度，动量，方差和 fp32 参数作为输入。我们还使用了一些特定于实现的参数，如 simd_width 和 unroll_width. Adam 优化器分别发送更新的方差、动量和参数的 fp16 和 fp32 格式到 GPU 和 CPU. 首先将数据读入矢量寄存器。然后，主循环中使用 Fused Multiplication Add 矢量操作。其他操作，如乘法、除法和平方根，也在矢量模式下运行。为了获得最佳性能，使用 AVX512 simd 指令集和基于自动调优结果的 unroll_width=8. 除了 CPU-Adam 优化器之外，还以分块的方式实现了 CPU 到 GPU 的 fp16 参数复制。通过并行化 Adam 计算并将参数复制到 GPU 来重叠 CPU 和 GPU 的执行。<strong>当在 CPU 上处理当前数据块的 Adam 计算时，将先前处理过的数据块的参数写回 GPU.</strong></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7e25392bc19670dfd7b80cc9a84a5d73?method=download&amp;shareKey=ac89809b9f51717d9a5429d1cd5d9865" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7e25392bc19670dfd7b80cc9a84a5d73?method=download&amp;shareKey=ac89809b9f51717d9a5429d1cd5d9865" alt="CPU-ADAM Optimizer">
    </a><figcaption>CPU-ADAM Optimizer</figcaption></figure></p>
<h3 id="one-step-delayed-parameter-update">One-Step Delayed Parameter Update</h3>
<p>下图展示了 Delayed Parameter Update(DPU) 的 ZeRO-Offload 训练的工作流程。</p>
<ol>
<li>前 N−1 步不使用 DPU 进行训练，避免在梯度变化迅速的早期阶段破坏训练的稳定性。</li>
<li>在第 N 步中，从 GPU 获取梯度，但跳过 CPU 优化步骤，也不更新 GPU 上的 fp16 参数。</li>
<li>在第 N+1 步中，我们使用第 N 步的梯度计算 CPU 上的参数更新，同时使用第 N-1 步更新的参数并行计算 GPU 上的前向和反向。</li>
</ol>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc56f9c814071e1766b9f33833024d829?method=download&amp;shareKey=d5be7d41c0247d8243bddb3452eba75b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc56f9c814071e1766b9f33833024d829?method=download&amp;shareKey=d5be7d41c0247d8243bddb3452eba75b" alt="Delayed Parameter Update During the Training Process">
    </a><figcaption>Delayed Parameter Update During the Training Process</figcaption></figure></p>
<h1 id="zero-infinity">ZeRO-Infinity</h1>
<p>ZeRO-Infinity 是一种新的异构系统技术，它利用 GPU, CPU 和 NVMe 内存，在有限的资源上实现前所未有的模型扩展，并且不需要模型代码重构。</p>
<p>目前大型模型训练技术中最先进的是三维并行 (3D parallelism)，它将模型（张量切片）和流水线并行与数据并行相结合。但是 GPU 内存跟不上模型大小的增长。</p>
<p>ZeRO-Infinity 的优势如下</p>
<ol>
<li>通过同时利用 CPU 和 NVMe 内存，在有限的 GPU 资源上支持大模型训练。</li>
<li>引入了一种称为 <em>memory-centric tiling</em> 的 GPU 内存优化技术，以应对 GPU 内存无法一次放下的超大 block 情况。</li>
<li>引入了一种称作 <em>bandwidth-centric partitioning</em> 的数据分区策略，用于利用所有设备上的内存带宽，并将其与重叠通信与计算的技术结合。</li>
</ol>
<h2 id="memory-requirements">MEMORY REQUIREMENTS</h2>
<p><strong>Memory for Model States:</strong> 基于 Transformer 的模型中的参数总数主要取决于隐藏维度 (hd) 和 Transformer 层数 (nl). Transformer block 中的几乎所有参数都来自四个线性层，大小分别为：QKV_Linear(nd,3nd), O_Linear(hd, hd),MLP(hd, 4hd)+(4hd, hd). 因此一个 Transformer block 的参数量约为 <strong>12 x nl x (hd)²</strong>，因此占用的内存大小为 192 x nl x (hd)² 字节。</p>
<p><strong>Memory for Residual States:</strong> 剩余状态主要由激活内存组成，它取决于模型架构、批处理大小 (bsz) 和序列长度 (seq). 存储激活检查点所需的内存估计为 <strong>2×bsz×seq×hd×nl/ci</strong>，其中 ci(checkpoint interval) 是两个激活检查点之间的 Transformer block 的数量。</p>
<p><strong>Model State Working Memory (MSWM)</strong> 是在所有模型状态被卸载到 CPU 或 NVMe 之后，在模型中最大的单个算子上执行前向或反向传播所需的最小 GPU 内存。对于基于 Transformer 的模型，最大的算子是将隐藏维度从 hd 转换为 4hd 的线性层，因此 fp32 格式下需要 <strong>4xhdx4hd</strong> 字节的内存。</p>
<p><strong>Activation Working Memory (AWM):</strong> 是在执行实际的反向传播之前重新计算激活所需的内存，即两个连续激活检查点之间的激活大小 bsz × seq × ci × (16 × hd + 2 × attn_heads × seq) 字节。</p>
<h2 id="bandwidth-requirements">BANDWIDTH REQUIREMENTS</h2>
<p>假设没有任何计算和通信重叠的工作负载执行，我们可以使用峰值计算吞吐量 (peaktp)，数据移动带宽 (bw) 及其算术强度 (ait) 来估计训练效率。需要注意 peaktp 不是理论上的硬件峰值，而是在没有任何通信瓶颈的情况下可以达到的峰值。</p>
<p>算术强度 (AIT) 是总计算量与计算所需数据量之比。它描述了每次数据移动的计算量。</p>
<ol>
<li>compute_time = total_computation / peaktp</li>
<li>ait = total_computation / total_data_movement</li>
<li>communication_time = total_data_movement / bw = total_computation / (ait × bw)</li>
<li>efficiency = compute_time / (compute_time + communication_time) = ait x bw / (ait x bw + peaktp)</li>
</ol>
<h3 id="quantifying-ait-in-dl-training">Quantifying AIT in DL training</h3>
<p>Transformer block 中一次前向传播中的计算量可以近似为输入乘以参数大小 2 × bsz × seq × params. 反向传播则为其 2 倍。如果使用激活检查点则还需要一次额外的前向传播，因此每次迭代的总计算量为 computation_per_iter = 2 × 4 × bsz × seq × parameters = 2 × 4 × 12 × bsz × seq × nl × (hd)²</p>
<p><strong>AIT w.r.t. Parameters and Gradients:</strong> 前向和反向过程中模型参数必须从存储位置位置加载到 GPU 寄存器各次。在使用激活检查点的情况下，还需要加载一次，以便在反向传播期间重新计算。此外，梯度必须从 GPU 寄存器存储到其最终位置至少一次。因此总共要移动模型参数 4 次，总计 2 x 4 x parameters 字节。因此关于参数和梯度的计算强度为 seq x bsz.</p>
<p><strong>AIT w.r.t. Optimizer States:</strong> 优化器状态必须至少读取和写入一次。所以总的数据移动是 2 × optimizer_states，总计 2 × 16 × parameters 字节。因此关于优化器状态的计算强度为 seq x bsz / 4.</p>
<p><strong>AIT w.r.t. Activation Checkpoints:</strong> 前向传播时必须将激活检查点保存到它们的最终位置，然后在反向传播期间加载激活检查点。因此总数据移动量为 4 × nl/ci × hd × seq × bsz 字节。因此关于激活检查点的计算强度为 24 × hd × ci.</p>
<h3 id="bandwidth-requirements-1">Bandwidth Requirements</h3>
<p>通过前面的分析可知模型状态的计算强度仅取决于批大小和序列长度，激活检查点的计算强度仅取决于存储间隔和模型的隐藏维度大小。下图 a 说明当传输参数和梯度的带宽超过 70 GB/s 时，即使是最小的批处理大小，也可以实现超过 50% 的效率。图 b 说明，传输优化器状态需要近 4 倍的带宽才能达到 50% 的效率。并且<strong>优化器状态更新需要等待所有前向和反向传播结束，不能与计算重叠</strong>。图 c 说明，启用激活检查点后，即使隐藏大小为2K，2 GB/s 的带宽也能够保持 50% 以上的效率。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfc095fefa5837b449dcafbd0b9441d63?method=download&amp;shareKey=42ab0064e0986bfde564af6e879f5cd6" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfc095fefa5837b449dcafbd0b9441d63?method=download&amp;shareKey=42ab0064e0986bfde564af6e879f5cd6" alt="Impact of Bandwidth on Efficiency with 70 TFlops of single GPU Peak Throughput">
    </a><figcaption>Impact of Bandwidth on Efficiency with 70 TFlops of single GPU Peak Throughput</figcaption></figure></p>
<h2 id="zero-infinity-design-overview">ZERO-INFINITY DESIGN OVERVIEW</h2>
<p>GPU 集群采用异构内存存储，除了 GPU 内存还拥有 CPU 内存以及比 GPU 内存大 50x, 比 CPU 内存大近 20x 的大规模 NVMe 存储。下图为 ZeRO-Infinity 架构，描述了第一层的反向传递的通信。将划分后的参数从慢速内存移动到 GPU，然后 All-Gather 以形成完整的层。在计算梯度之后，参数被聚合和重新划分，然后卸载到慢速内存中。层用下标表示，DP rank 用上标表示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB07d94538fa97fe4b478f44d44eb19e79?method=download&amp;shareKey=82f65d6660819491e1bec3688d2715ae" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB07d94538fa97fe4b478f44d44eb19e79?method=download&amp;shareKey=82f65d6660819491e1bec3688d2715ae" alt="A Snapshot of ZeRO-Infinity Training a Model with 2 Layers on 4 DP Ranks">
    </a><figcaption>A Snapshot of ZeRO-Infinity Training a Model with 2 Layers on 4 DP Ranks</figcaption></figure></p>
<p><strong>Efficiency w.r.t Parameter and Gradients:</strong> 现有的异构解决方案 (例如 ZeRO-Offload) 要求先将参数从 CPU 移动到拥有这些参数的 GPU，然后再进行广播。这种方式需要在每个 GPU 上使用足够大的 batchsize，以确保通信能被计算掩盖。但这带来了两个问题：</p>
<ol>
<li>对于超大规模模型，激活的内存占用会过大，甚至超过 CPU 的内存容量。</li>
<li>当扩展到数百甚至上千个 GPU 时，为了实现有效的收敛，实际的 batchsize 会变得过大。</li>
</ol>
<p><strong>Efficiency w.r.t Optimizer States:</strong> 与在前向和反向传播期间参数和梯度的产生有先后顺序不同，优化器状态可以同时更新。ZeRO-Infinity 建立在 ZeRO-3 之上，因此在将优化器状态卸载到 CPU 内存时，它还可以利用所有的 GPU 和 CPU 内存带宽以及所有 CPU 算力用于优化器状态更新。然而，使用 NVMe 卸载，需要将数据从 NVMe 传入到 CPU 内存中，再从 CPU 内存返回。由于 CPU 内存有限，必须将数据分块从 NVMe 加载到 CPU 内存，进行计算后再写回 NVMe.</p>
<p><strong>Efficiency w.r.t Activations:</strong> 在一台 DGX-2 节点上，每个 GPU 可以通过 PCIe 接口以大约 3 GB/s 的速度并行读写数据到 CPU 内存。这使得在隐藏层大小为 8K 或更大时，可以将激活检查点卸载到 CPU 内存的同时保持超过 80% 的效率。</p>
<h2 id="efficiency-optimizations">EFFICIENCY OPTIMIZATIONS</h2>
<h3 id="bandwidth-centric-partitioning">Bandwidth-Centric Partitioning</h3>
<p>在 ZeRO-3 和 ZeRO-Offload 中每层的参数为单个数据并行进程拥有，在需要时将它们广播给其他进程，ZeRO-Infinity 在所有数据并行进程中划分单个参数，并在需要参数时使用 All-Gather. 相较于广播只用到了单个 PCIe 链路将参数从存储位置加载到 GPU，All-Gather 同时使用所有的 PCIe 链路，每条链路传输 1/dp 的参数。</p>
<h3 id="overlap-centric-design">Overlap Centric Design</h3>
<p>访问 NVMe 内存需要三个步骤：(i) 从 NVMe 读取数据到CPU内存 (nc-transfer). (ii) 将数据从 CPU 内存复制到 GPU 内存 (cg-transfer). (iii) 执行 All-Gather 以在所有 GPU 上获得完整参数 (gg-transfer).</p>
<p>ZeRO-Infinity 的通信重叠有两个组件</p>
<ol>
<li>一个 dynamic prefetcher，在每次迭代期间，跟踪其在算子序列中的位置，并预取未来算子所需的参数。在执行第 i 个操作符之前，prefetcher 可以分别对第 i+3，第 i+2 和第 i+1 个算子所需的参数调用 nc, cg 和 gg-transfer.</li>
<li>一个通信和卸载重叠机制，用于并行执行梯度所需的数据移动和反向计算。将第 i+1 个算子中参数梯度的 Reduce-Scatter 与第 i 个算子的计算重叠，同时将第 i+2 个算子 Reduce-Scatter 划分的梯度传输给 CPU 或 NVMe.</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>All2All Communication Cost</title>
      <link>http://localhost:1313/blogs/all2allcommcost/</link>
      <pubDate>Sun, 12 Jan 2025 16:05:23 +0800</pubDate>
      <guid>http://localhost:1313/blogs/all2allcommcost/</guid>
      <description>Introduction of Transformer Family</description>
      <content:encoded><![CDATA[<p>在 All2All 通信中，每个设备给其他设备发送大小为 m 的不同的消息。此操作相当于使用一维数组分区对分布在 p 个进程中的二维数据数组进行转置，因此也被称作全交换 (<strong>total exchange</strong>)</p>
<h2 id="ring--bidirectional-linear-array">Ring / Bidirectional Linear Array</h2>
<p>线性数组拓扑结构的 All2All 通信中，每个设备需要发送 p-1 份大小为 m 的数据。用 {i,j} 表示消息需要从设备 i 发送到设备 j. 首先，每个节点将所有要发送的数据作为一个大小为 m(p-1) 的合并消息发送给它邻居 (假设所有设备通信方向相同)。当邻居收到这个消息后提取他所需要的那一部分，发送剩下的大小为 m(p-2). 每个设备一共发送 p-1 次，每次要发送的消息大小减少 m.</p>
<p>由此可以得出在 p 个设备组成的线性数组拓扑上进行 All2All 每个设备需要向相邻设备通信 p-1 次，第 i 次通信的消息大小为 m(p-i). 如果向两个方向都进行发送，那么每个方向都只用发送原先一半的数据。</p>
$$
\begin{aligned}T_{ring}&=\quad\sum_{i=1}^{p-1}(t_{s}+t_{w}m(p-i))\\&=\quad t_{s}(p-1)+\sum_{i=1}^{p-1}it_{w}m\\&=\quad(t_{s}+t_{w}mp/2)(p-1).\end{aligned}
$$<p>环状网络中每份消息的平均传输跳数是 $\frac{\sum_{d=1}^{p-1}i}{p-1} = p/2$，因此 p 个节点总共的通信量之和为  $p\times m(p-1)\times\frac p2$  环状网络中总的链路数目为 p. 因此负载平均的情况下，最少需要的时间为 $\frac{m(p-1)\times\frac p2\times p}p = m(p-1)\frac p2$ ，因此算法时间为最优的。</p>
<p>跳数为 d 的消息数量对应于相距 d 的节点对 (i, j)，其中 |i-j|=d</p>
<ul>
<li>(0, d),(1, d+1), \ldots,(p-1-d, p-1)，即 i 从 0 到 p-1-d, j=i+d ，共有 p-d 对。</li>
<li>(d, 0),(d+1,1), \ldots,(p-1, p-1-d)，即  i  从  d  到  p-1, ~ j=i-d  ，也有 p-d 对。
总共有 2(p-d) 条消息的跳数为 d</li>
</ul>
<p>总跳数</p>
$$
\begin{aligned}
\text { 总跳数 } & =\sum_{d=1}^{p-1} d \times 2(p-d) \\
& =2 \sum_{d=1}^{p-1} d(p-d)=2\left(p \sum_{d=1}^{p-1} d-\sum_{d=1}^{p-1} d^{2}\right) \\
& = p \cdot \frac{(p-1) p}{2}-\frac{(p-1) p(2 p-1)}{6} \\
& = =\frac{(p-1) p(p+1)}{6}
\end{aligned}
$$<p>因此平均跳数 =$\frac{\text { 总跳数 }}{\text { 总消息数 }}=\frac{\frac{(p-1) p(p+1)}{3}}{p(p-1)}=\frac{p+1}{3}$</p>
<h2 id="mesh">Mesh</h2>
<p>若 p 个设备组成大小为 $\sqrt{p} \times \sqrt{p}$ 的 mesh 进行 All2All 通信，每个设备首先将其 p 个数据按照目的设备的列进行分组，即分成 $\sqrt{p}$ 组，每组包含大小为 $m\sqrt{p}$ 的消息。假设 3x3 的 mesh，则第一组消息的目的节点为 {0,3,6}，第二组消息的目的节点为 {1,4,7}，第三组消息的目的节点为 {2,5,8}</p>
<p>首先同时分别在每一行中进行 All2All 通信，每一份数据大小为 $m\sqrt{p}$. 通信结束后每个设备拥有的是该行目的设备为所在列的所有数据。然后将数据按照目的设备所在的行进行分组。即设备 {0,3,6} 第一组消息的目的节点为 0，第二组消息的目的节点为 3，第三组消息的目的节点为 6. 然后同时分别在每一列中进行 All2All 通信。</p>
<p>我们只需要将 Linear Array 拓扑结构中的公式的 p 换成 $\sqrt{p}$ ，m 换成 $m\sqrt{p}$，再乘以 2 就得到在 mesh 上进行 All2All 的时间</p>
$$
T_{mesh}=(2t_{s}+t_{w}mp)(\sqrt{p}-1).
$$<h2 id="hypercube">Hypercube</h2>
<p>超立方体拓扑在每个维度上都有两个节点，一共有 $\log{p}$ 个维度。在一共有 p 个节点超立方体中，在某个维度 $d$ 上，超立方体可以被划分为两个 (n−1) 维的子立方体，这两个子立方体通过维度 d 上的 p/2 条链路相连。</p>
<p>在 All2All 通信的任何阶段，每个节点都持有 $p$ 个大小为 $m$ 的数据包。当在特定维度上通信时，每个节点发送 $p/2$ 个数据包 (合并为一条消息)。这些数据包的目的地是由当前维度的链路连接的另一个子立方体包含的节点。在上述过程中，节点必须在每个 $\log{p}$ 通信步骤之前在本地重新排列消息。</p>
<p>$\log{p}$ 步中的每一步，每个设备沿当前维度的双向链路交换大小为 mp/2 的数据。因此在 hypercube 上进行 All2All 的时间为</p>
$$
T_{hcube}=(t_{s}+t_{w}mp/2)\log p.
$$<p>值得注意的是与 ring 和 mesh 算法不同，超立方体算法不是最优的。每个设备发送和接收大小为 m(p- 1) 的数据，超立方体上任意两个节点之间的平均距离为 $\log{p}/2$ . 因此，网络上的总数据流量为 $p\times m(p - 1)\times(\log{p})/2$. 每个超立方体一共有 $p\log{p}/2$  条双向链路，如果流量能够被平分，则通信用时下界应该为</p>
$$
\begin{aligned}T_{min}&=\frac{t_{w}pm(p-1)(\log p)/2}{(p\log p)/2}\\&=t_{w}m(p-1).\end{aligned}
$$<h2 id="optimal-algorithm-in-hypercube">Optimal Algorithm in Hypercube</h2>
<p>在超立方体上，执行 All2All 的最佳方法是让每一对节点彼此直接通信。因此，每个节点只需执行 p-1 次通信，每次与不同设备交换大小为 m 的数据。设备必须在每次通信中选择不会出现拥塞的通信对象。在第 j 次通信中，节点 i 与节点 $i \oplus j$ 交换数据。在超立方体上，从节点 i 到节点 j 的消息必须经过至少 l 条链路，其中 l 是 i 和 j 之间的汉明距离 (即 $i \oplus j$ 的二进制表示中的非零比特数). 我们通过 E-cube 路由来选择路径：</p>
<ol>
<li>将当前节点地址 C 与目标节点地址 D 进行 XOR 操作，得到 $R=C\oplus D$.</li>
<li>找到 R 的最低有效非零位，决定下一步跳转的维度。</li>
<li>沿选定维度跳转到下一个节点，更新当前节点地址。</li>
<li>重复上述步骤，直到 R=0， 即到达目标节点。
对于节点i和节点j之间的消息传输，该算法保证每一步的通信时间为 t_s + t_wm，因为在节点 i 和节点 j 之间的链路上沿着同一方向传播的任何其他消息都不存在竞争，切每一步只切换一个维度，通信距离为 1. 整个 All2All 的总通信时间为</li>
</ol>
$$T_{xor}=(t_{s}+t_{w}m)(p-1).$$<h1 id="bruck-algorithm-in-full-connected-network">Bruck Algorithm in Full-connected Network</h1>
<p>Bruck是一种存储-转发 (store-and-forward) 算法，需要 log(P) 次通信步骤。这意味着发送缓冲区 S 和接收缓冲区 R 都用于在中间通信轮次中发送、接收和存储数据。因为某些接收到的数据块必须在后续通信步骤中使用。这种存储-转发的特性对通信轮次的顺序提出了约束。与线性步骤实现不同，Bruck 必须保持明确的通信顺序，其中第 i+1 次迭代必须在第 i 次迭代之后物理时间上发生。

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1b5aaffb71ec91ead2f725d9249728f1?method=download&amp;shareKey=e4eea75f3b72f77982d47b17590c24b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1b5aaffb71ec91ead2f725d9249728f1?method=download&amp;shareKey=e4eea75f3b72f77982d47b17590c24b3" alt="Bruck">
    </a><figcaption>Bruck</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-plaintext" data-lang="plaintext"><span class="line"><span class="cl">Algorithm 2 NCCL Bruck algorithm
</span></span><span class="line"><span class="cl">P ← total number of processes.
</span></span><span class="line"><span class="cl">for i ∈ [0, P] do
</span></span><span class="line"><span class="cl">   R[i] = S[(p+i) % P] // S and R are send and receive buffers, and p is rank id of each process;
</span></span><span class="line"><span class="cl">end for
</span></span><span class="line"><span class="cl">allocate temporary buffer T with SC × (P+1) / 2 elements; // SC is number of elements per data-block.
</span></span><span class="line"><span class="cl">for k = 1; k &lt; P; k &lt;&lt;= 1 do
</span></span><span class="line"><span class="cl">   allocate send indexes array SB with (P+1) / 2 integers;
</span></span><span class="line"><span class="cl">   number of send data-blocks NB ← 0;
</span></span><span class="line"><span class="cl">   for i ∈ [k, P] do
</span></span><span class="line"><span class="cl">      if i &amp; k then
</span></span><span class="line"><span class="cl">            SB[NB] ← i;
</span></span><span class="line"><span class="cl">            copy R[i] into T[NB];
</span></span><span class="line"><span class="cl">            NB ← NB + 1;
</span></span><span class="line"><span class="cl">      end if
</span></span><span class="line"><span class="cl">      sendproc ← (p + k) % P;
</span></span><span class="line"><span class="cl">      recvproc ← (p - k + P) % P;
</span></span><span class="line"><span class="cl">      ncclGroupStart()
</span></span><span class="line"><span class="cl">      send data in T to sendproc;
</span></span><span class="line"><span class="cl">      receive data from recvproc into S;
</span></span><span class="line"><span class="cl">      ncclGroupEnd()
</span></span><span class="line"><span class="cl">      for i ∈ [0, SB] do
</span></span><span class="line"><span class="cl">            copy T[i] into R[SB[i]];
</span></span><span class="line"><span class="cl">      end for
</span></span><span class="line"><span class="cl">   end for
</span></span><span class="line"><span class="cl">   for i ∈ [0, P] do
</span></span><span class="line"><span class="cl">      R[i] = R[(p - i + P) % P] // final rotation;
</span></span><span class="line"><span class="cl">   end for
</span></span><span class="line"><span class="cl">end for
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>line(2-4): 将每个设备发送缓冲区 S 中的数据按照 rank 偏移重新排列拷贝到接收缓冲区 R 中。</li>
<li>line(5): 为通信阶段准备一个临时缓冲区 T</li>
<li>line(6): 通信步开始 k 以指数方式增长 (1, 2, 4, &hellip;)，总共执行 logP 次迭代
<ul>
<li>line(7-14): 用索引数组 SB，记录需要发送的数据块位置。遍历 k~P-1 同通过对 i&amp;k 判断哪些数据块需要在此轮发送. (若 P 是 2 的指数幂，因为 k 是 2 的指数幂，因此只有一位为 1，那么就是每轮发送 p/2 个数据块) 将接收缓冲区 R 中满足条件的数据拷贝到临时缓冲区 T，并记录索引。</li>
<li>line(15-16): 确定要接收和发送的目标。</li>
<li>line(17-20): 进行通信操作，将数据发送到目标的发送缓冲区。</li>
<li>line(21-23): 更新接收缓冲区。</li>
<li>line(25-27): 反向调整接收缓冲区数据的位置。</li>
</ul>
</li>
</ul>
<p>总共 log(p) 步骤每步发送 m 消息。</p>
<h1 id="tree-based">Tree-based</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6ed5f5f2681e4f2c3a57bfb7b901515a?method=download&amp;shareKey=7aafd92596dbc981100138525e0f6d09" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6ed5f5f2681e4f2c3a57bfb7b901515a?method=download&amp;shareKey=7aafd92596dbc981100138525e0f6d09" alt="Tree">
    </a><figcaption>Tree</figcaption></figure></p>
<p>采用先在行上进行 All-gather, 再在列上进行 Scatter. 也需要 log(p) 步，其中 gather 阶段第一步通信量为 m(p-1)，一共进行 0.5log(p) 步每一步通信量翻倍，跳数也翻倍；scatter阶段则是相反，因此两步的通信时间相同总共 t_s*log(p) + m(p-1)^2/3</p>
]]></content:encoded>
    </item>
    <item>
      <title>DistriFusion</title>
      <link>http://localhost:1313/blogs/distrifusion/</link>
      <pubDate>Wed, 23 Oct 2024 14:28:37 +0800</pubDate>
      <guid>http://localhost:1313/blogs/distrifusion/</guid>
      <description>Paper reading about DistriFusion.</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>DistriFusion 将模型输入分割成多个 patch 后分配给 GPU。但是直接实现这样的算法会破坏 patch 之间的交互并失去保真度，而同步 GPU 之间的激活将产生巨大的通信开销。为了克服这一困境，根据观察到的相邻扩散步输入之间的高度相似性提出了 <strong>displaced patch parallelism</strong>，该方法通过重用前一个时间步骤中预先计算的 feature map 来利用扩散过程的顺序性，为当前步提供 context. 该方法支持异步通信，可以通过计算实现流水线化。</p>
<h1 id="introduction">Introduction</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBdce9158a9908f3ebe7782f7bf5b29f61?method=download&amp;shareKey=a571f8710a6ac4e8b859402edd5c069b" alt="Original, Navie Patch &amp; DistriFusion">
    </a><figcaption>Original, Navie Patch &amp; DistriFusion</figcaption></figure></p>
<p>加速扩散模型推理主要集中在两种方法上：减少采样步骤和优化网络推理。随着计算资源的快速增长，利用多个 GPU 来加速推理是很有吸引力的。例如在 NLP 中， LLM 已经成功地利用了 GPU 之间的张量并行性，从而显著降低了延迟。然而，对于扩散模型，由于激活尺寸大，张量并行这样的技术不太适合扩散模型。多个 GPU 通常只用于 batch 推理，当生成单个图像时，通常只涉及一个GPU.</p>
<blockquote>
<p>Techniques like tensor parallelism are less suitable for diffusion models due to the large activation size, as communication costs outweigh savings from distributed computation.</p></blockquote>
<p>自然而然的一种方法是将图像分成几个 patch 后分配给不同的设备进行生成。由于各个 patch 之间缺乏相互作用，它在每个 patch 的边界处都有一个清晰可见的分界线。</p>
<p>DistriFusion 也是基于 patch parallelism. 关键在于扩散模型中相邻去噪步骤的输入是相似的，因此，只在第一步采用同步通信。后续步骤重用前一步中预先计算的激活，为当前步骤提供全局上下文和 patch 交互。通过异步通信有效地隐藏了计算中的通信开销。并且还稀疏地在指定的区域上进行卷积和注意力计算，从而按比例减少每个设备的计算量。</p>
<h1 id="method">Method</h1>
<h2 id="displaced-patch-parallelism">Displaced Patch Parallelism.</h2>
<p>在预测 $\epsilon_{\theta}(\mathbf{x}_{t})$ 时 (忽略条件 c 和时间步 t 的输入) ，首先将 $\mathbf{x}_{t}$ 分割成多个 patch $\mathbf{x}_t^{(1)},\mathbf{x}_t^{(2)},\ldots,\mathbf{x}_t^{(N)}$ ，对于每一层 l 和设备 i，在获得输入激活 patch $\mathbf{A}_{t}^{l,(i)}$ 后异步处理两个操作：首先，对于设备i， 激活 $\mathbf{A}_{t}^{l,(i)}$ 首先 scatter 到上一步旧的激活 $\mathbf{A}_{t+1}^{l}$ 中。然后将此分散操作的输出送入稀疏算子 Fl (线性、卷积或注意层)，该算子专门对新区域执行计算并产生相应的输出。同时，对 $\mathbf{A}_{t}^{l,(i)}$ 执行 AllGather 操作，为下一步的全尺寸激活 $\mathbf{A}_{t}^{l}$ 做准备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfee0ed5c1a6065b8adb21371ea3cbc31?method=download&amp;shareKey=66860ad5956c2a8afb949b3fd821015d" alt="Overview of DistriFusion">
    </a><figcaption>Overview of DistriFusion</figcaption></figure></p>
<p>我们对除第一层 (采用同步通信获得其他设备上的输入) 外的每一层重复这个过程。然后将最终输出 Gather 在一起以近似 $\epsilon_{\theta}(\mathbf{x}_{t})$，用于计算 $\mathbf{x}_{t-1}$</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB41fa5a52bf206399a49358ada4f5c07b?method=download&amp;shareKey=686b36b99eb8ced2b48594a380d17d62" alt="Timeline Visualization on Each Device">
    </a><figcaption>Timeline Visualization on Each Device</figcaption></figure></p>
<h2 id="sparse-operations">Sparse Operations</h2>
<p>对于每一层 l，如果原始算子 Fl 是一个卷积层、线性层或交叉注意层，调整使其专门作用于新激活的区域。这可以通过从 scatter 输出中提取最新部分并将其输入到 Fl 中来实现。对于 self-attention，将其转换为 cross-attention，仅在设备上保留来自新激活的 Q，而 KV 仍然包含整个特征图。</p>
<h2 id="corrected-asynchronous-groupnorm">Corrected Asynchronous GroupNorm</h2>
<p>仅对新 patch 进行归一化或重用旧特征都会降低图像质量。同步 AllGather 所有均值和方差将产生相当大的开销。为了解决这一困境，DistriFusion 在陈旧的统计数据中引入了一个校正项。计算公式如下</p>
$$
\mathbb{E}[\mathbf{A}_t]\approx\underbrace{\mathbb{E}[\mathbf{A}_{t+1}]}_{\text{stale global mean}}+\underbrace{\mathbb{E}[\mathbf{A}_t^{(i)}]-\mathbb{E}[\mathbf{A}_{t+1}^{(i)}]}_{\text{correction}}
$$<p>同样对二阶矩 $\mathbb{E}[\mathbf{A}^2_t]$ 也采用这种计算方式，然后通过 $\mathbb{E}[\mathbf{A}^2_t] - \mathbb{E}[\mathbf{A}_t]^2$ 来计算方差。对于方差结果为负的部分，将使用新鲜 patch 的局部方差代替。</p>
<h1 id="code-implementation">Code Implementation</h1>
<p>Distrifusion 中主要就是将 <a href="https://github.com/huggingface/diffusers/blob/9366c8f84bfe47099ff047272661786ebb54721d/src/diffusers/models/unets/unet_2d_condition.py#L71">UNet2DConditionModel</a> 中的 Conv2d, Attention 和 GroupNorm 替换成对应的 patch 实现的网络结构 <a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L15">DistriUNetPP</a>. 这里继承的 BaseModel 类为集成了 PatchParallelismCommManager 类 (介绍见后文) 的网络。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6bd750d9e4b5d582be9d1f41cc267bc5?method=download&amp;shareKey=39d825554b65a9c57f59a1dd9a23fb28" alt="UNet2DConditionModel">
    </a><figcaption>UNet2DConditionModel</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriUNetPP</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>  <span class="c1"># for Patch Parallelism</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">UNet2DConditionModel</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">UNet2DConditionModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39; 
</span></span></span><span class="line"><span class="cl"><span class="s1">                Substitute Conv2d, Attention, GroupNorm with DistriConv2dPP, DistriSelfAttentionPP, DistriCrossAttentionPP, DistriGroupNorm 
</span></span></span><span class="line"><span class="cl"><span class="s1">                &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">subname</span><span class="p">,</span> <span class="n">submodule</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>  
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">submodule</span><span class="o">.</span><span class="n">kernel_size</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                            <span class="k">continue</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriConv2dPP</span><span class="p">(</span>  
</span></span><span class="line"><span class="cl">                            <span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="o">=</span><span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;conv_in&#34;</span>
</span></span><span class="line"><span class="cl">                        <span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">Attention</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="k">if</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn1&#34;</span><span class="p">:</span>  <span class="c1"># self attention</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="k">else</span><span class="p">:</span>  <span class="c1"># cross attention</span>
</span></span><span class="line"><span class="cl">                            <span class="k">assert</span> <span class="n">subname</span> <span class="o">==</span> <span class="s2">&#34;attn2&#34;</span>
</span></span><span class="line"><span class="cl">                            <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriCrossAttentionPP</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                        <span class="n">wrapped_submodule</span> <span class="o">=</span> <span class="n">DistriGroupNorm</span><span class="p">(</span><span class="n">submodule</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                        <span class="nb">setattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">subname</span><span class="p">,</span> <span class="n">wrapped_submodule</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriUNetPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchparallelismcommmanager">PatchParallelismCommManager</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/utils.py#L112">PatchParallelismCommManager</a> 类主要处理异步通信的部分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchParallelismCommManager</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span> <span class="o">=</span> <span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">torch_dtype</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 已经注册的张量的累计总元素数量</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">numel_dict</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># 记录每个 layer_type 所注册的张量的累计元素数量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 在每个设备上存储所有注册张量的数据，通信所用的 buffer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">starts</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的起始位置 (在 buffer_list 中的起始索引)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">ends</span> <span class="o">=</span> <span class="p">[]</span>    <span class="c1">#                 结束                       结束</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shapes</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 记录每个注册张量的 shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">idx_queue</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 需要进行通信的张量索引的队列</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># 存储每个设备通信操作的句柄的 list, 用于检查通信是否完成</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>成员函数功能介绍如下</p>
<ol>
<li>
<p><code>register_tensor(self, shape: tuple[int, ...] or list[int], torch_dtype: torch.dtype, layer_type: str = None) -&gt; int</code>: 用于注册张量的形状和数据类型，同时计算并记录张量在缓冲区中的起始位置和结束位置。</p>
<ul>
<li>如果尚未指定 <code>torch_dtype</code>，则将传入的 <code>torch_dtype</code> 设为类成员的默认数据类型。</li>
<li>计算传入张量形状的总元素数 <code>numel</code>，并更新 <code>starts</code>、<code>ends</code> 和 <code>shapes</code> 列表。</li>
<li>如果指定了 <code>layer_type</code>，更新 <code>numel_dict</code> 中该层类型对应的元素数目。</li>
</ul>
</li>
<li>
<p><code>create_buffer(self)</code> : 每个设备上为所有注册的张量创建一个统一的缓冲区。</p>
<ul>
<li>为每个设备创建一个形状为 <code>(numel,)</code> 的张量，并将其放入 <code>buffer_list</code> 中。</li>
<li>输出在各设备上创建的缓冲区总参数量。</li>
</ul>
</li>
<li>
<p><code>get_buffer_list(self, idx: int) -&gt; list[torch.Tensor]</code>: 返回每个设备上对应于指定索引 <code>idx</code> 的缓冲区张量。</p>
<ul>
<li>根据 <code>starts</code> 和 <code>ends</code> 信息，从 <code>buffer_list</code> 中提取指定索引 <code>idx</code> 的张量片段并调整其形状。</li>
</ul>
</li>
<li>
<p><code>communicate(self)</code>: 调用 <code>dist.all_gather</code> 将缓冲区中的张量在不同设备间进行广播。</p>
<ul>
<li>确定当前需要通信的张量范围 (根据 <code>idx_queue</code> 中的索引).</li>
<li>调用 <code>dist.all_gather</code> 在设备组内进行异步广播通信，并将句柄存储在 <code>handles</code> 中。</li>
</ul>
</li>
<li>
<p><code>enqueue(self, idx: int, tensor: torch.Tensor)</code>: 将指定索引 <code>idx</code> 处的张量数据复制到 <code>buffer_list</code> 中，并将索引添加到通信队列 <code>idx_queue</code>。</p>
<ul>
<li>如果通信队列不为空且索引为 0，则先执行一次通信操作。</li>
<li>将张量数据复制到 <code>buffer_list</code> 中的对应位置。</li>
<li>当通信队列长度达到 <code>distri_config</code> 中设定的通信检查点值时，进行通信。</li>
</ul>
</li>
<li>
<p><code>clear(self)</code>: 执行一次所有待通信张量的通信，并等待所有异步操作完成。</p>
<ul>
<li>如果通信队列不为空，则进行通信操作。</li>
<li>遍历所有句柄，等待所有异步操作完成后，将句柄设为 <code>None</code>.</li>
</ul>
</li>
</ol>
<h2 id="districonv2dpp">DistriConv2dPP</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/models/distri_sdxl_unet_pp.py#L10">DistriConv2dPP</a> 计算自己负责 patch 部分的卷积，需要通信其他设备需要自己负责 patch 的上下 padding 部分。</p>
<ul>
<li><code>__init__</code>：构造函数，初始化成员变量，设置是否为第一层卷积。</li>
<li><code>naive_forward</code>：执行标准的前向传播，不进行任何切片操作。这是单个设备处理时的普通卷积操作。</li>
<li><code>sliced_forward</code>：处理输入张量的切片操作。根据当前设备索引 (<code>split_idx</code>) 计算输入张量在高度方向的起始和结束位置，并在必要时为切片后的张量添加 padding 后进行卷积操作。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriConv2dPP</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">,</span> <span class="n">is_first_layer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriConv2dPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_layer</span> <span class="o">=</span> <span class="n">is_first_layer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">naive_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#  x: [B, C, H, W]</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">sliced_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;&#39;&#39;...&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 等待上一步通信完成</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">boundary_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># buffer_list 存储的是每个 devive 进行卷积所需要的其他 devive 的数据</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">                    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;conv2d&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    
</span></span><span class="line"><span class="cl">            <span class="k">def</span> <span class="nf">create_padded_x</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">                <span class="s1">&#39;&#39;&#39;拼接接收到的数据&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># rank 0</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">elif</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">==</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># rank n-1</span>
</span></span><span class="line"><span class="cl">                    <span class="n">concat_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">boundary_size</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>  <span class="c1"># other ranks</span>
</span></span><span class="line"><span class="cl">                    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                        <span class="p">[</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                            <span class="n">x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="p">],</span>
</span></span><span class="line"><span class="cl">                        <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">padded_x</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">            <span class="c1"># 提取当前输入张量需要发送给其他设备的部分</span>
</span></span><span class="line"><span class="cl">            <span class="n">boundary</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">boundary_size</span><span class="p">,</span> <span class="p">:],</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">boundary_size</span><span class="p">:,</span> <span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># 直接用上一步的 buffer 拼接</span>
</span></span><span class="line"><span class="cl">            <span class="n">padded_x</span> <span class="o">=</span> <span class="n">create_padded_x</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">padded_x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">boundary</span><span class="p">)</span>  <span class="c1"># 插入自己要发送的数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distriselfattentionpp">DistriSelfAttentionPP</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/attn.py#L107">DistriSelfAttentionPP</a> 只负责计算自己 patch 的输出，需要完整的 KV，将 self attention 运算变成 cross-attention 计算。需要通信自己的 KV.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriSelfAttentionPP</span><span class="p">(</span><span class="n">DistriAttentionPP</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriSelfAttentionPP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>  <span class="c1"># 获取 Attention 模块</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>  <span class="c1"># 残差连接</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">args</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">USE_PEFT_BACKEND</span> <span class="k">else</span> <span class="p">(</span><span class="n">scale</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Q Projection</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_kv</span><span class="p">(</span><span class="n">encoder_hidden_states</span><span class="p">)</span>  <span class="c1"># KV Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># 如果缓冲区未创建</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">kv</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">buffer</span> <span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span> <span class="o">=</span> <span class="n">kv</span>
</span></span><span class="line"><span class="cl">        <span class="n">full_kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">new_buffer_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&#34;no_sync&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">kv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将 full_kv 分割为 key 和 value</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">full_kv</span><span class="p">,</span> <span class="n">full_kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="o">//</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># multi-head attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">attn</span><span class="o">.</span><span class="n">heads</span> <span class="o">*</span> <span class="n">head_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># O Projection</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">to_out</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>  <span class="c1"># Dropout</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">attn</span><span class="o">.</span><span class="n">residual_connection</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">residual</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span> <span class="o">/</span> <span class="n">attn</span><span class="o">.</span><span class="n">rescale_output_factor</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">hidden_states</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="distrigroupnorm">DistriGroupNorm</h2>
<p><a href="https://github.com/mit-han-lab/distrifuser/blob/cfb9ea624ef95020aafcda929a69ba4100f99e9d/distrifuser/modules/pp/groupnorm.py#L9">DistriGroupNorm</a> 根据上一步全特征图的以及当前步 patch 的均值和二阶矩近似当前步的全特征图均值和方差。需要通信 patch 均值和二阶矩。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistriGroupNorm</span><span class="p">(</span><span class="n">BaseModule</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">:</span> <span class="n">DistriConfig</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">DistriGroupNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">distri_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span>
</span></span><span class="line"><span class="cl">        <span class="n">distri_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distri_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">handles</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">4</span>
</span></span><span class="line"><span class="cl">        <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_groups</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">        <span class="n">group_size</span> <span class="o">=</span> <span class="n">c</span> <span class="o">//</span> <span class="n">num_groups</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">register_tensor</span><span class="p">(</span>  <span class="c1"># register for E[x], E[x^2]</span>
</span></span><span class="line"><span class="cl">                    <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layer_type</span><span class="o">=</span><span class="s2">&#34;gn&#34;</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">get_buffer_list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算 patch 均值和二阶矩</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x2_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1, num_groups, 1, 1, 1]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x_mean</span><span class="p">,</span> <span class="n">x2_mean</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="n">slice_mean</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Equation 2 in the paper E[A_t] = E[A_(t+1)] + (E[A^i_t] - E[A^i_(t+1)]), same for E[A^2_t]</span>
</span></span><span class="line"><span class="cl">            <span class="n">correction</span> <span class="o">=</span> <span class="n">slice_mean</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">[</span><span class="n">distri_config</span><span class="o">.</span><span class="n">split_idx</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">            <span class="n">full_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer_list</span><span class="p">)</span> <span class="o">/</span> <span class="n">distri_config</span><span class="o">.</span><span class="n">n_device_per_batch</span> <span class="o">+</span> <span class="n">correction</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">comm_manager</span><span class="o">.</span><span class="n">enqueue</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">idx</span><span class="p">,</span> <span class="n">slice_mean</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">full_x_mean</span><span class="p">,</span> <span class="n">full_x2_mean</span> <span class="o">=</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">full_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">full_x2_mean</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算方差</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_x_mean</span><span class="p">,</span> <span class="n">slice_x2_mean</span> <span class="o">=</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">slice_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">slice_var</span> <span class="o">=</span> <span class="n">slice_x2_mean</span> <span class="o">-</span> <span class="n">slice_x_mean</span><span class="o">**</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">var</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">slice_var</span><span class="p">,</span> <span class="n">var</span><span class="p">)</span>  <span class="c1"># Correct negative variance</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">num_elements</span> <span class="o">=</span> <span class="n">group_size</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl">        <span class="n">var</span> <span class="o">=</span> <span class="n">var</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_elements</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">std</span> <span class="o">=</span> <span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">full_x_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scale and shift</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">module</span><span class="o">.</span><span class="n">affine</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">view</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>DeepSpeedUlysses</title>
      <link>http://localhost:1313/blogs/deepspeedulysses/</link>
      <pubDate>Mon, 21 Oct 2024 11:09:12 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepspeedulysses/</guid>
      <description>Paper reading of Deepseed Ulysses.</description>
      <content:encoded><![CDATA[<h1 id="deepspeed-ulysses-core-design">DeepSpeed-Ulysses Core Design</h1>
<h2 id="system-design">System Design</h2>
<p>原理如下图所示，假设设备数 P 等于多头注意力的头数 hc. 输入 <code>x[N,d]</code> 被切分到每个设备上 <code>[N/p, d]</code>，之后进行 QKV Projection，随后将 K 进行转置后进行一次 all-to-all 通信，这样每个设备上就有 <code>Q[N, d/P], K[d/P, N], V[N, d/P]</code>, 再执行标准的 attention 计算 $Outputcontext=Softmax((QK^T)/\sqrt{d})V$. 再进行一次 all-to-all 通信使得每个设备上有 <code>[N, d/P]</code> 结果再进行后续操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB06300727bd2f239239db47091e81223c?method=download&amp;shareKey=6abbc645b3fa3a039b464dd405f96d4a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB06300727bd2f239239db47091e81223c?method=download&amp;shareKey=6abbc645b3fa3a039b464dd405f96d4a" alt="DeepSpeed Sequence Parallelism (DeepSpeed-Ulysses) Design">
    </a><figcaption>DeepSpeed Sequence Parallelism (DeepSpeed-Ulysses) Design</figcaption></figure></p>
<h2 id="communication-analysis">Communication Analysis</h2>
<p>在采用节点内 NVSwitch 互连和节点间 fat tree IB 拓扑的集群中，对于总消息大小为 M 的 all-to-all 通信，每条链路通过 P 个 gpu 传输的通信量为 M/P。对于隐藏层大小为 h、序列长度为 N、并行度为 P 的 transform 模型，DS-Sequence 对注意力计算前总消息大小为 3Nh 的 QKV Projection 执行 all-to-all 通信，对每个 transformer block 的输出执行 all-to-all 通信，大小为 Nh. 因此，DeepSpeed 序列下每条链路的总通信量为 4Nh/P (或复杂度为 O(N/P)). 也就是说当 N 和 P 按比例增加时，该通信量是恒定的。</p>
<h2 id="comparison-of-other-works">Comparison of Other Works</h2>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBff8d584feabe45900c3a57eea94a78a0?method=download&amp;shareKey=7bae2e87b18707dabcd5e5ae7976e644" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBff8d584feabe45900c3a57eea94a78a0?method=download&amp;shareKey=7bae2e87b18707dabcd5e5ae7976e644" alt="Comparison of DS-Ulysses to Other Sequence Parallelism Methods">
    </a><figcaption>Comparison of DS-Ulysses to Other Sequence Parallelism Methods</figcaption></figure></p>
<ul>
<li>ColAI-SP 发明了 Ring-Attention，Q 存储在本地 而 KV 以环形方式传输以计算全局注意力，导致通信复杂度与消息大小 M 呈线性关系。</li>
<li>Megatron-LM 序列并行方法与 Megatron 张量并行紧密集成。Megatron-LM 沿着序列维度划分序列，并应用 all gather 和 reduce scatter 来聚合 QKV 注意力计算的投影。并行通信量随消息大小 M 线性增加。</li>
<li>DeepSpeed-Ulysses 通过增加与序列长度成比例的设备数来保持通信量恒定。同时将 Zero3 扩展到数据并行和序列并行的组合。ZeRO 跨序列和数据并行组划分模型状态，并在需要时使用 allgather 收集每个 rank 的部分。</li>
</ul>
<h2 id="general-and-attention-agnostic-solution">General and Attention Agnostic Solution</h2>
<p>DeepSpeed-Ulysses 的优势在于一种以注意力为中心的序列并行设计。在注意力计算是 N/P 划分的序列并行之前，注意力计算是头并行，每个头的注意力都是完整的，但只有较少的头，因此注意力计算可以被任何类型的注意机制所取代，例如 dense attention 和各种形式的 sparse attention.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Efficient Large-Scale Language Model Training on GPU</title>
      <link>http://localhost:1313/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/</link>
      <pubDate>Sat, 05 Oct 2024 10:09:35 +0800</pubDate>
      <guid>http://localhost:1313/blogs/efficient-large-scale-language-model-training-on-gpu-clusters/</guid>
      <description>Paper reading about Efficient Large-Scale Language Model Training on GPU Clusters.</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>本文展示了如何将张量、流水线和数据并行性组合起来以扩展到数千个gpu。我们提出了一种新的交错流水线调度，可以在内存占用与现有方法相当的同时将吞吐量提高 10%.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb0e290b7248b1bb11233e55661bbb736?method=download&amp;shareKey=c1cd9d017d6f99b06b1fc2635f48fc19" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb0e290b7248b1bb11233e55661bbb736?method=download&amp;shareKey=c1cd9d017d6f99b06b1fc2635f48fc19" alt="Trend of Sizes of SOTA NLP Models">
    </a><figcaption>Trend of Sizes of SOTA NLP Models</figcaption></figure></p>
<h1 id="introduction">Introduction</h1>
<p>张量（层内）模型并行对于较大的模型会崩溃。较大的模型在多个多 GPU 服务器上进行切分会导致两个问题：</p>
<ol>
<li>张量并行所需的 all-reduce 通信需要通过服务器间链路进行，这比多 GPU 服务器内可用的高带宽 NVLink 要慢</li>
<li>高度模型并行会产生小规模的矩阵乘法（GEMM），从而可能降低 GPU 利用率。</li>
</ol>
<p>流水线模型并行化是指模型的各层在多个 GPU 上进行条带化处理。batch 被拆分成更小的 microbatch ，并在这些 microbatch 之间流水线执行。无论进度如何，为了保持严格的优化器语义，优化器步骤需要跨设备同步，从而在每个 batch 结束时进行流水线刷新 (<em>pipeline flush</em>)，允许 microbatch 完成执行 (不再注入新的 microbatch). microbatch 数量与流水线级数的比例越大，流水线刷新所花费的时间就越少。</p>
<p>我们展示了如何结合流水线、张量和数据并行性，我们称之为PTD-P. 配置分布式训练的指导原则如下:</p>
<ul>
<li>不同形式的并行性以不同的方式相互作用: 并行策略对通信量、执行内核的计算效率以及由于流水线冲洗 (流水线气泡) 而花费的等待计算的空闲时间有影响。</li>
<li>用于流水线并行性的调度对通信量、流水线气泡大小和用于存储激活的内存有影响。</li>
<li>超参数 (如 microbatch 大小) 的值会影响内存占用、在工作线程上执行的内核的算术效率和流水线气泡大小。</li>
<li>随着规模扩展分布式训练是通信密集型的。使用较慢的节点间互连或更密集的通信分区会影响扩展性能。</li>
</ul>
<h1 id="model-parallelism">Model Parallelism</h1>
<p>本节中将讨论有助于不适合单个 GPU 内存的大模型的并行训练方法。我们将流水线模型并行和张量模型并行 (如图 2 所示的组合) 与数据并行结合起来，简称为PTD-P.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB09f440d2c0b5bd3b923d2d09dbf47eb5?method=download&amp;shareKey=59701400f477d8dd03c4ea43138c933f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB09f440d2c0b5bd3b923d2d09dbf47eb5?method=download&amp;shareKey=59701400f477d8dd03c4ea43138c933f" alt="Combination of Tensor and Pipeline Model Parallelism (MP)">
    </a><figcaption>Combination of Tensor and Pipeline Model Parallelism (MP)</figcaption></figure></p>
<h2 id="data-parallelism">Data Parallelism</h2>
<p>使用数据并行时，每个 worker 都有一个完整模型的副本，输入数据集被分片， worker 定期汇总他们的梯度，以确保所有 worker 看到一个一致的权重版本。</p>
<h2 id="pipeline-parallelism">Pipeline Parallelism</h2>
<p>通过流水线并行，模型的层被分散到多个设备上。一个 batch 被分成更小的 microbatch. 在 microbatch 之间进行流水线执行。为了准确地保持优化器语义，我们引入了定期的流水线刷新，以便在设备之间同步优化器步骤。在每个 batch 处理的开始和结束时，设备都是空闲的。我们称这段空闲时间为流水线气泡 (<em>pipeline bubble</em>).</p>
<h3 id="default-schedule">Default Schedule</h3>
<p>GPipe 提出了一个调度方案，如图 3 所示 (假设<strong>反向传播的时间是前向传播的两倍</strong>，管道调度的效率并不取决于这个因素)，首先执行一个 batch 中所有 microbatch 的前向传播，然后执行所有 microbatch 的反向传播。设 GPipe 流水线气泡的大小为 t_pb，microbatch 的数量为 m，流水线阶段数量 (用于流水线并行的设备数量) 表示为 p，每次迭代的理想时间表示为 t_id (假设理想缩放)，执行单个 microbatch 的向前和反向传播的时间表示为 t_f 和 t_b. 在该调度中，流水线气泡由批处理开始时的 p−1 个前向传播和 p−1 个反向传播组成。则流水线气泡总时间为 t_pb=(p−1)·(t_f+t_b). batch 的理想执行时间为 t_id=m·(t_f+t_b)。因此，在流水线气泡中花费与理想计算时间的比例为:</p>
<p>流水线气泡占比 = t_pb / t_id = (p−1) / m.</p>
<p>为了使流水线气泡占比小，我们需要 m 远大于 p. 然而 m 非常大时这种方法的内存占用很高，因为它需要在训练一次迭代时间内为所有 m 个 microbatch 保存中间激活.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB04e4031b886573061f614e73854d1f43?method=download&amp;shareKey=97391411ed3d06fc2c6de5de5f20d1d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB04e4031b886573061f614e73854d1f43?method=download&amp;shareKey=97391411ed3d06fc2c6de5de5f20d1d0" alt="GPipe Pipeline Schedule">
    </a><figcaption>GPipe Pipeline Schedule</figcaption></figure></p>
<h3 id="schedule-with-interleaved-stages">Schedule with Interleaved Stages</h3>
<p>为了缩小流水线气泡的大小，每个设备都可以对多个层的子集（称为模型块）进行计算，流水线中的每个设备都被分配了多个流水线阶段（与之前相比，每个流水线阶段的计算量更少），而不是单个连续的层。</p>
<details class="custom-details">
    <summary class="custom-summary">An Example</summary>
    <div>例如，如果每个设备之前被分配 4 层 (即设备 1 有 1 - 4 层，设备 2 有 5 - 8层&hellip;)，我们可以让每个设备为两个模型块执行计算 (每个模型块被分配 2 层)，即设备 1 有 1、2、9、10 层; 设备 2 具有第3、4、11、12层&hellip;</div>
</details><br>
<p>和上一小节一样，我们可以执行完所有 microbatch 的前向传播然后执行所有反向传播 (all-forward, all-backward)，但这将占用大量内存 (与 m 成正比). 因此如图 4 所示，我们设计了一个适配于之前的内存高效 1F1B 的交错调度。它要求 <strong>microbatch 数量是流水线并行度 (流水线中的设备数量) 的整数倍</strong>。</p>
<p>如果每个设备都有 v 个阶段 (模型块)，那么每个阶段 microbatch 的前向和反向传播的时间分别为 t_f/v 和 t_b/v. 流水线气泡时间因此减少到 𝑡^int_pb=(p−1)·(tf+tb)/v，</p>
<p>流水线气泡占比为 𝑡^int_pb / t_id = (p−1) / (m·v).</p>
<p>这意味着该调度减少气泡时间到原先的 1/v，但该计划需要额外的通信，因此通信量也为原来的 v 倍。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb74cfaaf752e14cf2e44f4abd7e3e7bf?method=download&amp;shareKey=4558481494c1d4b22574e739743b123d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb74cfaaf752e14cf2e44f4abd7e3e7bf?method=download&amp;shareKey=4558481494c1d4b22574e739743b123d" alt="Default and Interleaved 1F1B Pipeline Schedules">
    </a><figcaption>Default and Interleaved 1F1B Pipeline Schedules</figcaption></figure></p>
<h2 id="tensor-model-parallelism">Tensor Model Parallelism</h2>
<p>详情见 Megatron-LM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB02b1b15f4d736bfee41738f3c3ee72b3?method=download&amp;shareKey=b1a7e4a6171585f4f0a8a39fb4b2d8b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB02b1b15f4d736bfee41738f3c3ee72b3?method=download&amp;shareKey=b1a7e4a6171585f4f0a8a39fb4b2d8b3" alt="Blocks of Transformer Model Partitioned with Tensor Model Parallelsim">
    </a><figcaption>Blocks of Transformer Model Partitioned with Tensor Model Parallelsim</figcaption></figure></p>
<h1 id="performance-analysis-of-parallelization-configurations">Performance Analysis of Parallelization Configurations</h1>
<p>首先定义下符号含义</p>
<ul>
<li>(p,t,d): 并行化维度。p 表示流水线模型并行大小，t 表示张量模型并行大小，d 表示数据并行大小。</li>
<li>n: GPU 数量，要求 ptd=n.</li>
<li>B: 全局批大小 (作为输入提供)</li>
<li>b: microbatch 大小。</li>
<li>m = B/(db): 一个 batch 中每个流水线中的 microbatch 的数量。</li>
</ul>
<h2 id="tensor-and-pipeline-model-parallelism">Tensor and Pipeline Model Parallelism</h2>
<p>如前所述，使用带有周期性冲洗的流水线并行会产生大小为 (p−1)/m 的流水线气泡. 固定 d=1，则 tp=n，气泡大小可以用 t 表示为</p>
<p>(p−1)/m=(n/t-1)/m.</p>
<p>GPU 之间的通信量也受 p 和 t 大小的影响。流水线模型并行的特点是更便宜的点对点通信，每个 microbatch 的每对连续设备之间 (前向或后向传递) 需要执行的通信总量为 bsh. 张量模型并行则使用 all-reduce 通信，总大小为 bsh 的张量需要在每层的前向和后向传递中，在 t 个模型副本之间进行两次 all-reduce，因此每个 microbatch 每层每个设备的总通信量为 4bsh(t-1)/t. 每个设备通常有多个层，则每个设备上每个 microbatch 的张量并行通信总量为 l^stage4bsh(t-1)/t, 其中 l^stage 为流水线阶段的层数。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 1: 当 t 大于单个节点中的 GPU 数量时，在较慢的节点间链路上执行张量模型并行的开销非常大。在考虑不同形式的模型并行时，使用 g-GPU 服务器时张量模型并行度一般为 g (all-reduce 通信量大，NVLink 带宽高)，然后可以使用流水线模型并行来扩展到跨服务器的更大模型 (P2P 通信量小，PCIe 带宽低).</p></div>

<h2 id="data-and-model-parallelism">Data and Model Parallelism</h2>
<p>管道模型并行性。设 t=1，每个管道的 microbatches 数量 m=𝐵/(db)=b&rsquo;/d, b&rsquo;:=B/b. 设 GPU 总数为 n ，流水线阶段数为 p=n/d，气泡大小为</p>
<p>(p−1)/m=(n/d-1)/(b&rsquo;/d)=(n-d)/b'</p>
<p>管道气泡随着 d 变大而变小。如果数据并行所需的 all-reduce 通信不会随着 d 的变大而急剧增加，那么总体吞吐量将会增加，因为基于环的实现的通信时间随着 d 的变化为 (d−1)/d=1−1/d.同样对于给定的并行配置，随着批量大小的增加，b&rsquo; = B/b 增加，因此吞吐量上升。同时数据并行所需的 all-reduce 通信频率也下降，进一步提高了吞吐量。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB27b82e198a75ad564ac917d6d560dec1?method=download&amp;shareKey=cfef60f8f216007803bdafe8b5a5e64c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB27b82e198a75ad564ac917d6d560dec1?method=download&amp;shareKey=cfef60f8f216007803bdafe8b5a5e64c" alt="Fraction of Time Spent Idling due to Pipeline Flush">
    </a><figcaption>Fraction of Time Spent Idling due to Pipeline Flush</figcaption></figure></p>
<p>在张量模型并行下，每个 microbatch 都需要进行 all-reduce 通信，这在多 GPU 服务器上开销很大；而数据并行只需要在每个 batch 中执行一次的 all-reduce通信。此外，使用张量模型并行，每个设备计算每层的一部分，因此对于不够大的层， GPU 可能无法以峰值效率执行这些子矩阵计算。</p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 2：在使用数据和模型并行时，应使用 M=tp 的总模型并行大小，以便模型参数和中间数据满足 GPU 内存限制；数据并行可用于将训练扩展到更多 GPU.</p></div>

<h2 id="microbatch-size">Microbatch Size</h2>
<p>给定函数 t_f(b) 和 t_b(b)，将 microbatch 大小映射为单个 microbatch 的前向和反向计算时间，计算一个 batch 所花费的总时间 (忽略通信成本) 为</p>
<p>(b&rsquo;/b+p-1)·(t_f(b)+t_b(b)).</p>
<p>microbatch 大小因此既影响运算的算术强度，也影响管道气泡大小。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe3b16b74c3ca1aedb0f81939501da9de?method=download&amp;shareKey=cd1c360e2597b37f705bd5f4906d64b3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe3b16b74c3ca1aedb0f81939501da9de?method=download&amp;shareKey=cd1c360e2597b37f705bd5f4906d64b3" alt="Per-GPU Throughput versus Microbatch Size for a GPT Model">
    </a><figcaption>Per-GPU Throughput versus Microbatch Size for a GPT Model</figcaption></figure></p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB3de398053a7040a405722f3b8c929bf1?method=download&amp;shareKey=7014b2696bffc97d5646b4b2614bd3fb" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB3de398053a7040a405722f3b8c929bf1?method=download&amp;shareKey=7014b2696bffc97d5646b4b2614bd3fb" alt="">
    </a><figcaption>Behavior of Throughput for the same GPT Model</figcaption></figure></p>
<div class="notice tip" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="300.5 134 300 300">
  <path d="M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Tip</p><p>启示 3：最佳 microbatch 大小 b 取决于模型的吞吐量和内存占用特征，以及流水线深度 p、数据并行大小 d 和批量大小 B.</p></div>

<h2 id="activation-recomputation">Activation Recomputation</h2>
<p>激活重计算通过在向后传递之前运行第二次正向传播 (并且仅存储给定流水线阶段的输入激活)，来权衡所执行的计算操作数量的增加对额外内存占用的影响。设 A^input 为一层的输入激活的大小，A^intermediate 为每层的中间激活的大小，一个模型阶段有 l 层， 激活保存点的数量为 c，那么总内存占用为 c·A^input + l/c·A^intermediate. 因此取 c = \sqrt(l·A^input·A^intermediate) 时内存占用最小。</p>
<h1 id="implementation">Implementation</h1>
<h2 id="communication-optimizations">Communication Optimizations</h2>
<p>使用流水线并行时，我们希望在正向和反向并行发送和接收张量。每台 DGX A100 都配备了 8 个 InfiniBand（IB）网卡。然而发送和接收都是点对点的，只发生在两台服务器上的一对 GPU 之间，因此很难充分利用所有网卡。对于流水线内的单次通信，每个 transformer 层的输出都会在张量并行的设备中复制。为了减少这种冗余，我们可以在发送端将张量分割成大小相等的块，然后使用每个 rank 自己的 InfiniBand 发送. 在接收端通过比 InfiniBand 互连快得多的 NVLink 执行 all-gather，重新组装整个张量。通过 scatter-gather 通信优化，将每对连续流水线阶段之间需要执行的通信总量减少为 bsh/t.</p>
<h2 id="computation-optimizations">Computation Optimizations</h2>
<p>将数据布局从 (b,s,a,h) 更改为 (s,b,a,h). 其次，使用 PyTorch JIT 为一系列元素操作 (bias+GeLU 和 bias+dropout+add) 生成融合算子。</p>
<h1 id="evaluation">Evaluation</h1>
<p>在 Selene 超级计算机上以混合精度运行。每个集群节点有</p>
<ul>
<li>8 个 NVIDIA 80GB A100 GPU，通过 NVLink 和 NVSwitch 互连。</li>
<li>8 个 NVIDIA Mellanox 200Gbps HDR Infiniband HCA 用于应用程序通信</li>
<li>额外有 2 个 HCA 用于专用存储。
节点以三级 (leaf, spine, core) 胖树拓扑结构连接，一共有 850个交换机。集群使用 all-NVME 共享并行文件系统进行高性能数据访问和存储。16 位精度的 A100 GPU 的峰值设备吞吐量为 312 teraFLOP/s.</li>
</ul>
<p>QKV 变换的线性层权重参数量均为 h^2, attention 后的线性层权重参数量为 h^2, 两层前馈网络每个线性层的权重参数量为 4h^2，因此每一个 transformer block 的所有线性层的参数量为 12h^2. 词嵌入的参数量为 Vh，位置编码的参数量为 sh.</p>
<p>一个 $A_{m\times k}\times X_{k\times n}$ 矩阵乘法需要 2mkn FLOPs( 2 是因为乘法和加法). transformer block 包含一个注意力块和一个两层前馈网络组成。对于注意力块，主要的 FLOP 来源于 QKV 转换 (6Bsh^2 次操作)、注意力矩阵计算 (2Bhs^2 次操作)、注意力乘 Value (2Bhs^2 次操作) 和 attention 后的线性层 (2Bsh^2 次操作). 前馈网络将隐藏维度放大到 4h，然后再减小到 1h，需要 16Bsh^2 次操作。将这些加在一起，每个 transformer block 一共有 24Bsh^2+4Bhs^2 FLOPs. 反向传播需要两倍的计算量，因为需要计算关于输入张量和权重张量的梯度。此外，使用激活重计算需要在反向传播之前进行额外的正向传播。因此，每一层的总计算量为 FLOPs 为 4*(24Bsh^2+4Bhs^2).</p>
<p>计算量另一方面来源于 head 的 logit 层，它将维度的特征 h 转换为词汇表维度的特征 V. 该操作所需的计算量为正向传播的 2BshV 和反向传播的 4BshV，总共 6BshV FLOPs.</p>
<h2 id="result">Result</h2>
<p>Pipeline-parallel 并行度增加降低 GPU 的计算效率，因为 bubble 变多了。
Batchsize 的增大可以减少 pipeline-parallel 并行度大小带来的影响。</p>
<p>Batch size增加有助于提高GPU的计算效率。
Interleaved schedules 能显著提高GPU的计算效率。</p>
<p>不使用激活重计算的话单位时间内的训练的吞吐是要高于使用重计算的，因为重计算在反向传播中引入额外的计算量。
由于重计算可以节省显存，batchsize 可以相应提高不少。由于 batchsize 的提高，训练吞吐量也得到了提高，从而达到了优化的效果。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Megatron-LM</title>
      <link>http://localhost:1313/blogs/megatronlm/</link>
      <pubDate>Wed, 02 Oct 2024 15:51:50 +0800</pubDate>
      <guid>http://localhost:1313/blogs/megatronlm/</guid>
      <description>Paper reading about Megatron-LM</description>
      <content:encoded><![CDATA[<h1 id="abstract">Abstract</h1>
<p>我们的方法不需要新的编译器或更改库，与流水线模型并行 (<em>pipeline model parallelism</em>) 正交互补，并且可以通过在原生 PyTorch 中插入一些通信操作来实现。为了阐述我们的方法，使用 512 个 GPU 将基于 transformer 的模型扩展到 83 亿个参数。与可保持 39 TeraFLOPs (峰值 FLOPs 的 30%) 的强大单 GPU 基准相比，我们在整个应用中保持了 15.1 PetaFLOPs，扩展效率高达 76%.</p>
<h1 id="introduction">Introduction</h1>
<p>随着 LLM 变得越来越大，它们会超出现代处理器的内存限制，并需要如激活检查点 (activation checkpoint) 等额外的内存管理技术。广泛使用的优化算法 (如ADAM) 需要每个参数额外的内存来存储动量和其他优化器状态。这减少了可以有效训练的模型的大小。模型并行性的几种方法克服了这一限制，它们对模型进行分区，使权重及其相关的优化器状态不需要并发地驻留在处理器上。</p>
<details class="custom-details">
    <summary class="custom-summary">Activation Checkpoint</summary>
    <div>在深度学习模型的训练过程中，前向传播会计算并存储每一层的激活值，这些激活值在后向传播时被用来计算梯度。然而，对于深度很大的模型因为需要存储大量的激活值，可能会导致内存溢出。激活检查点技术通过在前向传播过程中只存储一部分的激活值来解决内存占用问题，如果在后向传播过程中需要没有存储的激活值就进行重新计算。</div>
</details><br>
<p>为了证明方法的可扩展性，通过在单个英伟达 V100 32GB GPU 上训练一个包含 12 亿个参数的模型来建立基准。训练该模型可维持 39 TeraFLOPs 的算力，是在 DGX-2H 服务器中配置的单个 GPU 理论峰值 FLOPS 的 30%. 在 512 个 GPU 上将模型扩展到 83 亿个参数，并采用 8 路模型并行，在整个应用中实现了高达 15.1 PetaFLOPs 的持续运行速度。与单 GPU 情况相比，扩展效率提高了 76%. 下图展示了更详细的扩展结果。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB64c800c2db5cda251cb35df9208d8f94?method=download&amp;shareKey=3c2e4f94cd1ca9520e5d9f49a7dfb620" alt="Model (blue) and model&#43;data (green) parallel FLOPS">
    </a><figcaption>Model (blue) and model&#43;data (green) parallel FLOPS</figcaption></figure></p>
<h1 id="background--chanllenges">Background &amp; Chanllenges</h1>
<h2 id="neural-language-model-pretraining">Neural Language Model Pretraining</h2>
<p>早期的预训练和传递语言神经表示的例子表明，与从头开始学习的词嵌入表相比，预训练的词嵌入表改善了下游任务的结果。目前的技术水平已经从传输单词嵌入表发展到传输整个数十亿参数的语言模型。这种方法的进步要求硬件、系统技术和框架能够高效地大规模运行。</p>
<h2 id="transformer-language-models-and-multi-head-attention">Transformer Language Models and Multi-Head Attention</h2>
<p>下图展示了使用的 transformer 模型的示意图。最近利用 transformer 进行语言建模的工作，如 BERT 和 GPT-2 根据需要分别只使用编码器和解码器。</p>
<blockquote>
<p>GPT-2 和 BERT 都对多头注意和 FFN 的输入使用 GeLU 非线性和层归一化，而原始 transformer 使用 ReLU 非线性并对输出进行层归一化。</p></blockquote>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB1fcc47c83c934bf20c33fa9a88bfc34e?method=download&amp;shareKey=2132c1442224cafae4ca86d6fd01720d" alt="Transformer Architecture">
    </a><figcaption>Transformer Architecture</figcaption></figure></p>
<h2 id="data-and-model-parallelism-in-deep-learning">Data and Model Parallelism in Deep Learning</h2>
<p>将深度神经网络训练扩展到多硬件加速器有两种范式:</p>
<ul>
<li>Data Parallelism (DP): 将 batch 拆分到多个 worker</li>
<li>Model Parallelism (MP): 将模型的内存使用和计算分布在多个 worker 中。
<ul>
<li>Pipeline Parallelism (PP): 一组操作在一个设备上执行，然后将输出传递到流水线中的下一个设备执行另一组操作。</li>
<li>Distributed Tensor Computation: 将张量运算分割到多个设备上，以加速计算或增加模型大小。</li>
</ul>
</li>
</ul>
<p>然而，这些技术有一个基本的限制: 模型权重必须能加载进 worker. 我们的方法是利用模型并行性在多个加速器之间分割模型。</p>
<h1 id="model-parallel-transformers">Model Parallel Transformers</h1>
<p>我们利用 transformer 网络的结构 (self-attention 和 FFN (2*MLP) 组成)，通过添加一些同步原语，创建了一个简单的并行计算模型。下面分别阐述对 FFN 和 self-attention 的并行化。</p>
<p>FFN 第一个 MLP 由一个 GEMM，后跟一个 GeLU 非线性组成:</p>
$$
Y=\text{GeLU}(XA)
$$<p>并行化 GEMM 的一种选择是将权重矩阵 A 沿着行切分，并将 X 沿着其列切分:</p>
$$
X=[X_1,X_2], A=\begin{bmatrix}A_1\\A_2\end{bmatrix}
$$<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBb1a0688321b545061bd3652261e6bf71?method=download&amp;shareKey=2cc0ee6c275925d756b0b877c961e682" alt="Row Split of Weight">
    </a><figcaption>Row Split of Weight</figcaption></figure></p>
<p>可以得出 $Y = X_1A_1+X_2A_2$. 由于 GeLU 是非线性函数，因此这种方法需要在 GeLU 函数之前进行同步。</p>
<p>另一个选择是沿着列切分 $A=\begin{bmatrix}A_1,A_2\end{bmatrix}$. 这样可以让 GeLU 独立地应用于每个 GEMM 的输出</p>
<p>$[Y_1, Y_2]=\begin{bmatrix}\text{GeLU}(XA_1),\text{GeLU}(XA_2)\end{bmatrix}$.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5732c68a72330d13b288ab3d1828a6d2?method=download&amp;shareKey=01e733100161309cbb283548474f22f7" alt="Column Split of Weight">
    </a><figcaption>Column Split of Weight</figcaption></figure></p>
<p>这种切分方式的优点是不需要进行同步操作。</p>
<p>如下图所示，以列并行方式切分第一个 GEMM，并沿着行切分第二个GEMM。然后，在将输出传递给 dropout 层之前，第二个GEMM 的输出在 GPU 之间进行 all-reduce 操作。这种方法将 FFN 中的两个 GEMM 拆分到多个 GPU 上执行，并且只需要在正向传播 (g 操作符) 和反向传播 (f 操作符) 中分别执行一次 all-reduce 操作。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBc1b8e9f509879d5984e2b85db312760f?method=download&amp;shareKey=39a5f19a9a477a8d9c3a80b4c5c3bd0f" alt="Parallelism of MLP">
    </a><figcaption>Parallelism of MLP</figcaption></figure></p>
<p>如下图所示，利用多头注意力操作中本身存在的并行性，以列并行的方式划分与 QKV 相关的 GEMM，以便每个注意力头对应的矩阵乘法在一个 GPU 上独立完成。输出线性层的 GEMM 沿着其行并行化，并直接获取并行 attention 的输出。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbd9aaf24f7a7a275f026d617e5d49da7?method=download&amp;shareKey=f927c189a40038f613ca4f917effa454" alt="Parallelism of Self-Attention">
    </a><figcaption>Parallelism of Self-Attention</figcaption></figure></p>
<p>如下图所示，这使能够仅在正向传播和反向传播中分别中使用两个 all-reduce 操作执行 transformer 中所有的 GEMM.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB471b6d468483ddff2c68c46e71ec70ee?method=download&amp;shareKey=7e29126eacd44c304a8f9b2b25b4bbc3" alt="Parallelism of Transformer Layer">
    </a><figcaption>Parallelism of Transformer Layer</figcaption></figure></p>
<p>基于 transformer 的语言模型的输出嵌入维度为隐藏层大小 (H) 乘以词汇表大小 (v). 我们沿着词汇表维度 $E = \begin{bmatrix}E_1,E_2\end{bmatrix}$ 并行化权重矩阵。每一块现在只包含嵌入表的一部分，输入嵌入后需要一个 all-reduce (g 算子).</p>
<p>对于输出嵌入，一种方法是通过并行 $\mathrm{GEMM} [Y_{1},Y_{2}]=[XE_{1},XE_{2}]$ 来获得 logits，并对结果 all-gather 后送入交叉熵损失函数。在这种情况下，all-gather 通信量为 bsv 个元素 (b 是批处理大小，s 是序列长度). 为了减小通信规模，我们将输出与交叉熵损失融合，这样通信量降为 bs.</p>
<p>我们在每个 GPU 上维护层归一化参数的副本，并在将这些张量作为输入送到下一个模型并行区域之前，在本地输出上进行 dropout 和残差连接。为了优化模型，我们允许每个模型并行 worker 优化自己的一组参数。因为所有的值要么是本地的，要么是在 GPU上 重复的，所以在这个公式中不需要通信更新的参数值。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Ring Attention Principle</title>
      <link>http://localhost:1313/blogs/ringattention/</link>
      <pubDate>Thu, 26 Sep 2024 22:59:35 +0800</pubDate>
      <guid>http://localhost:1313/blogs/ringattention/</guid>
      <description>This is a brief introduction to the Ring Attention Principle.</description>
      <content:encoded><![CDATA[<h1 id="background">Background</h1>
<p>如今 LLM 的 token 长度显著增加，从 GPT-3.5 的 16k 到 Claude 2 的 200k，现在 Gemini 1.5 Pro 甚至有 1M 的 token 长度。如此长的 token 在计算 attention 时对显存的需求非常大。<a href="https://arxiv.org/abs/2310.01889">Ring Attention</a> 便是为了并行计算 attention 而提出的一种方法<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<blockquote>
<p>Ring Attention 和 Flash Attention 可以同时使用。</p></blockquote>
<h1 id="attention-and-memory">Attention and Memory</h1>
<p>要计算 attention， 我们需要三个大小为 (s, d) 的矩阵：Q (query)、K (key)、V (value)，其中 s 为序列长度，d 为模型维度。attention 的计算公式为</p>
$$
Attention(Q, K, V) = softmax(QK^T / \sqrt{d})V
$$<p>忽略 sqrt(d) 项，我们记 Score Matrix 为 S = QK^T / \sqrt{d}，然后对 S 进行 softmax 归一化，得到 Attention Matrix. 可以发现它们占用显存大小是 O(s*s) 数量级。即使使用 <a href="https://arxiv.org/abs/2205.14135">Flash Attention</a>，显存占用量也是 O(s) 数量级。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe66e94e161b89a4ba25d05b67a47e393?method=download&amp;shareKey=742185dd412edbdb3266fa16ab91d787" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe66e94e161b89a4ba25d05b67a47e393?method=download&amp;shareKey=742185dd412edbdb3266fa16ab91d787" alt="Attention Compute Process">
    </a><figcaption>Attention Compute Process</figcaption></figure></p>
<p>我们希望如果在 N 个设备上并行计算 attention，每个设备的显存占用量为整个的 1/N, 因此就需要对 Q、K、V 的 sequence 长度进行切分。但是如果得到的最终 attention 矩阵需要在设备间进行集合通信组装每个的计算结果，通信量也和 sequence 长度成正比。Ring Attention 提出了一个巧妙的解决方案：在设备之间进行轮转，并行化所有计算而且完全隐藏通信的开销。</p>
<blockquote>
<p>We will rotate between devices to parallelize all computation and hide the communication overhead completely.</p></blockquote>
<h1 id="splitting-the-query">Splitting the Query</h1>
<p>假设我们有 N 个设备，我们将 Q 沿着 sequence 维度切分为 N 份，每份大小为 (s/N, d). 由于计算 Score 和 Attention 需要完整的 K 和 V，这样它们也被切分成 N 份，每份大小为 (s/N, d). 计算示意图如下。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB170087e68345309f813b8edee9487b92?method=download&amp;shareKey=f848ff8adb5676443347921c65a3b104" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB170087e68345309f813b8edee9487b92?method=download&amp;shareKey=f848ff8adb5676443347921c65a3b104" alt="Split Q">
    </a><figcaption>Split Q</figcaption></figure></p>
<h1 id="splitting-the-key-and-value">Splitting the Key and Value</h1>
<p>对 K 和 V 的切分并不能像 Q 那样直接。因为 softmax 的计算公式如下，要得到分母的值意味着我们需要对每一行进行计算。</p>
$$
softmax(s_i) = \frac{\exp(s_i)}{\sum_{j=i}^d{\exp(s_j)}}
$$<p>如果我们能对 K 和 V 进行切分并正确计算 softmax，那么计算过程可以由下图所示的那样完成 (忽略 softmax). 外循环遍历 Q 的所有分块，内循环遍历 K 和 V 的所有分块，一次计算一部分的 attention. Ring Attention 示意图如下所示，顾名思义所有设备组成一个环状，每个设备存储 Q 的一部分，每次迭代过程会传递 K 和 V 到下一个设备，最终每个设备将得到计算自己 Q 部分的 attention 矩阵所需要的 K 和 V. 每个设备被分配 Q 的一部分 (即一个外层循环索引)，并迭代计算每个 K 和 V 的分块 (内循环)。每个设备只需要跟踪形状为 (s/N, s/N) 的累积和 A_j。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBbc9ef7d01431fe639ecf44842bce0e1a?method=download&amp;shareKey=03d587a38ca574ed1547f2594a45ab4c" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBbc9ef7d01431fe639ecf44842bce0e1a?method=download&amp;shareKey=03d587a38ca574ed1547f2594a45ab4c" alt="Attention Parallel Computation">
    </a><figcaption>Attention Parallel Computation</figcaption></figure></p>
<h1 id="online-softmax">Online Softmax</h1>
<p>在内循环的每次迭代中我们可以更新部分和为 $l^j = l^{j-1} + \sum_{k_t\in K_j}{\exp(Q_ik_t^T)}$. 在内循环结束后我们就可以获得每一行的指数和。归一化和与 V 的相乘顺序不会影响结果，我们可以先累加总和，并在所有其他计算完成后再执行实际的归一化操作。</p>
<p>因此，设备 i 除了计算当前的累计和 $A^j = A^{j-1} + \exp(Q_i K_j^T) V_j$ 外，还需要在内循环每次迭代中更新部分和 $l^j \in \mathbb{R}^{B_q}$ ，其中 $B_q$ 为 Q 的分块大小。</p>
<h1 id="safe-softmax">Safe softmax</h1>
<p>由于指数运算经常容易出现溢出，我们通常减去 max(s_i) 后进行指数运算，公式如下，这样并不会影响结果。</p>
$$
\mathrm{softmax}(s_{1:N})=\frac{\exp(s_{1:N})}{\sum_i\exp(s_i)}\cdot\frac{\exp(-s_{max})}{\exp(-s_{max})}=\frac{\exp(s_{1:N}-s_{max})}{\sum_i\exp(s_i-s_{max})}
$$<p>所以我们在内循环每次迭代中需要先更新当前的最大值 $m^{j+1}=\max(m^j,\max(Q_iK_{j+1}^T))$，然后更新之前迭代的计算结果 A_j 和 部分和 l_j. 最后再计算本次迭代的结果。</p>
$$
A^{j+1}=A^j\cdot\exp(m^j-m^{j+1})+\exp(Q_iK_{j+1}^T-m^{j+1})\cdot V_j
$$<p>更新部分和</p>
$$
l^{j+1}=l^j\cdot\exp(m^j-m^{j+1})+\exp(Q_iK_{j+1}^T-m^{j+1})
$$<h1 id="putting-it-together">Putting it Together</h1>
<p>Ring Attention 计算步骤如下：</p>
<ol>
<li>沿着 Q 的 sequence 长度拆分为一个独立的外循环。</li>
<li>应用 Online Safe Softmax，以便沿着 K 和 V 的sequence 长度拆分，从而在内层循环中累积计算注意力。</li>
</ol>
<p>这种并行化的方式是通过将每个设备分配一个 Q_i 块来实现的。因此，我们需要将 Q 拆分为 N 个相等的部分 (B_Q=N). 每个设备将分别计算它的输出块 $\text{Output}(Qi,K,V)= \text{softmax}(Q_i K^T)V ，通过在 K 和 V 块上执行内循环来迭代计算。难点挑战在于设备无法一次存储完整的 K 和 V 矩阵。</p>
<p>如果我们有 4 个 GPU，那么我们将把每个设备的 Q 按序列维度分成 4 个块，K 和 V 被分割成 B_K=B_Q=N 个块，并对设备进行初始化，使每个设备都持有一个 Qi 块、 一个 Kj 块和 一个 Vj 块。为简单起见，我们可以假设设备 i 在开始时持有 Qi, Ki 和 Vj 块。在设备计算完与其当前 vj kj 相对应的一个内循环步骤后，每个设备都需要接收下一个 Key 和 Value 块，以继续内循环。 我们将 N 个设备围成一个环，其中设备 i 可以向设备 i+1 以此类推，如图所示：</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB5d0930a41cedf1d4e46af9baa5071f78?method=download&amp;shareKey=0e898be310f92f54f0b065a38771eb5f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB5d0930a41cedf1d4e46af9baa5071f78?method=download&amp;shareKey=0e898be310f92f54f0b065a38771eb5f" alt="KV-overlap">
    </a><figcaption>KV-overlap</figcaption></figure></p>
<p>如果在设备 i 上计算内循环的一个步骤 Qi,Vj,Kj 的这段时间内，设备 i 还能向设备 i+1 发送其当前 Kj Vj，并同时从设备 i-1 接收 V_j-1,K_j-1，那么只要发送和接收密钥和值块的时间低于计算时间，那么发送和接收 Key 和 Value 块的延迟就会隐藏在执行实际计算时间之内。一个例子如下图所示。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB935e1d2c0eb43c35c5c828abe8a44612?method=download&amp;shareKey=9ec41a1c178534620d9f7274ff2ce9d0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB935e1d2c0eb43c35c5c828abe8a44612?method=download&amp;shareKey=9ec41a1c178534620d9f7274ff2ce9d0" alt="KV-rotate">
    </a><figcaption>KV-rotate</figcaption></figure></p>
<h1 id="memory-and-arithmetic-complexity">Memory and Arithmetic Complexity</h1>
<p>以深度学习中常用的 bfloat16 数据类型为例。GPU 或 TPU 等并行处理加速器通常以 FLOP:=F 来衡量，即设备理论上每秒可执行的浮点运算次数。我们假设硬件被完全利用。此外，我们设不同设备之间的连接带宽为:=B (Bytes/sec).</p>
<p>内存复杂度: 为了同时进行接收发送和计算，我们需要有用于接收新 KV 块的寄存器器。存储当前 KV  值块需要 2dc 浮点数或 4dc 字节。用于接收新的 KV 块的内存大小也是 2dc 浮点数或 4dc 字节。假设计算本身不需要更多内存 (利用 Flash Attention 或 Blockwise Attention)，计算当前步骤的输出需要 dc 个浮点数或 2dc 字节。此外，每个设备还需要存储其 Qi 块，这也需要 dc 个浮点数或 2dc 字节。总共需要 6dc 个浮点或 12dc 字节。</p>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>Ring Attention 与 Flash Attention 是正交的，可以一起使用 (Flash Attention 实际上用于 Ring Attention 的内循环). Flash Attention 目标是不将整个 Score Matrix 加载到全局内存中，从而在序列长度上获得线性内存复杂度。Ring Attention 将 原始注意力方法和 Flash Attention 的内存复杂度至少降低了 N 倍，使用 N 个设备的内存复杂度至少降低 N 倍，因为它将所有矩阵都拆分为至少 N 个或更多部分 (将 QKV 分别分成 N 份，并将 Score Matrix 分成 N^2 分). 无论内存复杂度是由 QKV，还是由 Score Matrix 主导，Ring Attention 都能将内存成本降低至少 N 倍。</p></div>

<p>通信开销: 在内循环每一步中，每个设备需要通过带宽为 B 的信道向下一个设备发送 2⋅c_Q⋅d 浮点数。每个 bf16 大小为 2字节，因此，所需的时间约为 4⋅c⋅d/B.</p>
<p>运算强度： 一个内循环步骤，计算局部注意力需要 2⋅d⋅c^2 次浮点计算，计算 softmax，归一化向量和最大值向量需要 2⋅c⋅d 次浮点计算，计算局部注意力与 Vj 块的乘积需 2⋅d⋅c^2 次浮点计算。因此，总计算所需时间≈4⋅d⋅c^2/F.</p>
<p>为了重叠通信和计算 (隐藏通信开销)，我们需要 KV 块的传输时间小于等于计算本地 QKV 所需的时间：</p>
$$
4\cdot c\cdot d/B\leq4\cdot d\cdot c^2/F\iff B\geq F/c\iff s/N\geq F/B 
$$<h1 id="futher-optimization">Futher Optimization</h1>
<p>Ring Attention 的一个应用是用于因果 Transformal 模型时，加上三角形掩码用于注意力计算。这意味着有些 GPU 不需要对整个序列进行计算，导致它们大部分时间处于闲置状态。作为 Ring Attention 的扩展，<a href="https://arxiv.org/pdf/2311.09431.pdf">Stripe Attention</a> 解决了这一问题，并提供了一种分配计算更均匀的方案，从而使 Ring Attention 的计算速度更快。</p>
<p>除了 Ring Attention 和 Flash Attention 等使标准 Transformer 架构能有更长的上下文长度的技术外，人们还尝试使用 <a href="https://arxiv.org/abs/2312.00752">Mamba</a> 等具有线性注意力的状态空间模型（SSM）等模型架构。</p>
<h1 id="references">References</h1>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://coconut-mode.com/posts/ring-attention/">https://coconut-mode.com/posts/ring-attention/</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Comparsion of Parallelsim Metods in ViT</title>
      <link>http://localhost:1313/blogs/comparsion-of-parallelsim-metods-in-vit/</link>
      <pubDate>Mon, 13 Nov 2023 16:05:23 +0800</pubDate>
      <guid>http://localhost:1313/blogs/comparsion-of-parallelsim-metods-in-vit/</guid>
      <description>Paper reading of .</description>
      <content:encoded><![CDATA[<h1 id="basic-transformer-block">Basic Transformer Block</h1>
<p>符号含义表示如下</p>
<table>
  <thead>
      <tr>
          <th>Symbol</th>
          <th>Description</th>
          <th>Symbol</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>a</td>
          <td>注意力头数</td>
          <td>n</td>
          <td>并行度大小</td>
      </tr>
      <tr>
          <td>b</td>
          <td>batchsize</td>
          <td>s</td>
          <td>序列长度</td>
      </tr>
      <tr>
          <td>h</td>
          <td>隐藏层维度</td>
          <td>v</td>
          <td>词汇表大小</td>
      </tr>
      <tr>
          <td>L</td>
          <td>tranformer layer 层数数</td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>基本 transformer block 结构如下，输入是形状为 (b, s, h) 的三维张量，其中 b 为 batchsize. 每个变压器层由一个具有注意头的自注意块组成，随后是一个具有两层的 MLP，第一层将隐藏维度增加到 4h，然第二层将其减少到 h. 每个变压器层的输入和输出具有相同的形状.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6446c9e0a905932db1f9e39fa91c01ba?method=download&amp;shareKey=f26e075bcfc51b8c093388f69d39b40d" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6446c9e0a905932db1f9e39fa91c01ba?method=download&amp;shareKey=f26e075bcfc51b8c093388f69d39b40d" alt="Basic Transformer Architecture">
    </a><figcaption>Basic Transformer Architecture</figcaption></figure>

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBfbdc229aca70349939d6e3306e78c434?method=download&amp;shareKey=cff3f2903a8e16c5c46d607749a4b3c1" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBfbdc229aca70349939d6e3306e78c434?method=download&amp;shareKey=cff3f2903a8e16c5c46d607749a4b3c1" alt="Self-attention Block">
    </a><figcaption>Self-attention Block</figcaption></figure></p>
<h2 id="model-parameters">Model Parameters</h2>
<p>QKVO Linear 的权重形状均为 <code>h*h</code>, 偏置形状均为 <code>h*1</code>；MLP 两个 Linear 的权重形分别为 <code>h*4h</code> 和 <code>4h*h</code>，偏置形状分别为 <code>4h*1</code> 和 <code>h*1</code>. 因此每个模型的参数量为 <code>(12hh+13h)L</code>，占用大小还要 <code>x2</code>.</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在传统的 LLM 中最后还需要经过 logits layer，将隐藏层维度 <code>h</code> 转换成词汇表大小 <code>v</code>，参数量还要加上 <code>hv</code>.</p></div>

<h2 id="flops-calculation">FLOPs Calculation</h2>
<p>对于浮点数计算量 (FLOPs)，只考虑占主要部分的通用矩阵乘法 (GEMMs). 对于 Attention 部分，QKV Linear 的计算量为 <code>6bshh</code>，attention matrix (<a href="mailto:Q@K.T">Q@K.T</a>) 的计算量为 <code>2bssh</code>, attention@V 的计算量为 <code>2bssh</code>, O Linear 的计算量为 <code>2bshh</code>. MLP 的两个线性层的每一个计算量都为 <code>8shh</code>. 相加后得到正向传播中总计算量为 <code>(24bshh + 4bssh)L</code> bytes.</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在传统的 LLM 中最后还需要经过 logits layer，将隐藏层维度 <code>h</code> 转换成词汇表大小 <code>v</code>，其计算量为 <code>2bshv</code>.</p></div>

<p>反向传播因为要计算输入和权重的梯度，其计算量为正向传播的两倍，因此整个模型的计算量为 <code>72BLshh(1+s/(6h))</code>.</p>
<h1 id="activation-memory">Activation Memory</h1>
<p>激活的定义为在前向传播中产生并且需要在反向传播中进行梯度计算的张量，即不包括模型参数和优化器状态。并且不考虑相对非常小的激活。例如 LayerNorm 层的输入还需要张量每个通道的均值和方差 (大小均为 bs)，由于 h 大小通常超过 1k，因此只考虑输入张量所占激活的大小 bsh，忽略掉 2bs. 假设数据格式为 fp16/bf16，即每个数据占用 2 bytes 的存储空间，需要特殊处理的是 dropout 层的 mak，每个元素均为 unsigned int，只占用 1 byte.</p>
<p>Attention 部分激活占用如下 (共计 11bsh + 5bssa)</p>
<ul>
<li>QKV Linear: 三个线性层需要的输入相同，占用 2bsh bytes.</li>
<li><a href="mailto:Q@K.T">Q@K.T</a>: 需要存储 Q 和 K，占用 4bsh bytes.</li>
<li>Softmax: 需要存储大小为 2bssa bytes 的输入</li>
<li>Softmax droppot: 需要存储一个大小为 bssa bytes 的 mask.</li>
<li>attention@V: 需要存储 dropout 的输出和 V，分别占用 2bssa 和 2bsh bytes.</li>
<li>O Linear: 需要存储注意力的输出，占用 2bsh bytes.</li>
<li>O dropout 需要存储一个大小为 bsh bytes 的 mask;</li>
</ul>
<p>MLP (共计 18bsh): 第一层和第二层的输入分别占用 2bsh 和 8bsh bytes. GeLU 层需要第二层的输入用于反向传播，占用大小为 8bsh bytes. dropout 需要一个大小为 bsh bytes 的 mask.</p>
<p>LayerNorm (共计 4bsh): 需要存储该层的输入，占用 2bsh bytes. 一共有两个 LayerNorm.</p>
<p>加起来就可以得到每个 transformer block 需要激活大小为 bsh(34+5sa/h) bytes.</p>
<h1 id="tensor-parallelsim">Tensor Parallelsim</h1>
<p><a href="https://darkenstar.github.io/2024/10/02/MegatronLM/#Model-Parallel-Transformers">Megatron 张量并行</a> 的思想是将输入进行连续的两个矩阵乘法的第一个按列切分成 t 份，第二个按行切分成 t 份. 在 Transformer block 中体现为利用多头注意力本身的并行性将 Attention 计算中的 QKV 按列进行切分，O Linear 的权重按行进行切分；MLP 中第一个线性层的权重按列进行切分，第二个权重按行进行切分。</p>
<p>在这种并行方式下，前向传播和反向传播均需要进行 2 次 All-Reduce 通信，由于每次 All-Reduce 通信可以看作 Reduce-Scatter + All-Gather, 因此每次每个设备的通信量为 8αbsh bytes，其中 α=(n-1)/n.</p>
<p>对于激活，2*LayerNorm, QKV Linear 的输入, O dropout mask，MLP 第一层的输入和 MLP dropout 不会被切分，因此每个设备每个 block 要占用的激活为 bsh(10+24/n+5as/(hn))</p>
<p>2D Tensor Parallelsim</p>
<p>2D张量并行将激活第一个矩阵的列切分成 m*n 份，第二个权重 (权重形状为 he) 的行被切分成 m 份，列被切分成 n 份。以下图为例，Rank0-Rank2为通信组 x，Rank0-Rank1为 通信组 y. 第一个矩阵经过一次通信组 y 的 AllGather 后与本设备第二个矩阵进行矩阵乘积，得到的部分和经过一次通信组 x 间的ReduceScatter，计算出正确结果。第一次 AllGather 通信每个设备通信的大小为 bsh(n-1)/(mn). 第二次 ReduceScatter 通信每个设备通信的大小为 bse(m-1)/n.</p>
<h1 id="megatron-sequence-parallelsim">Megatron Sequence Parallelsim</h1>
<p>Megatron 张量并行中 LayerNorm 以及 O Linear 和 MLP 之后的 dropouts 在每个设备中都有一个副本。这些模块不需要大量的计算，但需要占用 10bsh bytes 大小的激活内存。<a href="">Megatron-SP</a> 沿着序列维度划分这些模块来减少激活内存，但需要配合 TP 一起使用，本质上是将 TP 中的 All-Reduce 拆成了在 TP 前进行 All-Gather 和在 TP 后进行 Reduce-Scatter. 但除去第一个 LayerNorm 外的每一个模块的激活都得到了切分。Megatron-SP 这里选择每个设备存储自己的部分并在反向传播中插入一次额外的 All-Gather 通信。因此通信量为 10bsh, 每个设备每个 block 需要占用的激活为 bsh/n*(34+5as/h)</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6800d68e35ee4215289de6aa75f01884?method=download&amp;shareKey=e67ffd54e4d1fe7cf3a10e81108af366" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6800d68e35ee4215289de6aa75f01884?method=download&amp;shareKey=e67ffd54e4d1fe7cf3a10e81108af366" alt="Transformer layer with Megatron-SP">
    </a><figcaption>Transformer layer with Megatron-SP</figcaption></figure></p>
<h1 id="pipeline-parallelsim">Pipeline Parallelsim</h1>
<p>流水线张量并行仅仅将 L 个 Transformer block 平均分到 p 个设备上，并没有划分激活所要占用的内存。在考虑 1F1B 策略下 batchsize 进一步被划分成 p 个 micro batch. 第一个 stage 必须存储 p 个 micro batch 的激活。每个 stage 包含 L/p 层，所以无论流水线并行大小 p 如何，第一个 stage 必须存储 p × L/p = L 层的激活值。在 Megatron-LM 中的 interleaving schedule 需要存储 L(1 + (p−1)/(pm)) 层的激活，其中 m 是 interleaving 的数量。</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>在使用 output-tensor-deallocation 优化 (输出传到下一个 stage 后就释放) 的情况下，可以为为每个设备节省 bshr 内存，其中 r 是每个设备正在运行的 micro batch 的数量，在第一个 stage r=p 时达到峰值。</p></div>

<h1 id="deepseed-ulysses-sequence-parallel">Deepseed-Ulysses Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/10/21/Deepseed%20Ulysses/">DS-SP</a> 也是利用多头注意力的并行性，首先将输入按序列维度切分到每个设备上，每个设备占有的输入形状为 b*(s/n)*h. 在计算 Attention 之前对 QKV 进行 All-to-All 通信变成按隐藏层维度切分 ((a 要能整除 n))，通信量为 6αbsh/n bytes. 计算完 score@v 之后再进行一次 All-to-All 通信，通信量为 2αbsh/n bytes，总计通信量为 8αbsh/n bytes. 激活占用上 Attention 中 Softmax 及其 dropout mask 和 attention 没有被切分，激活占用量为 bsh(34/n+5sa/h). 因此，它不适合 GQA 和 MQA 情况, GQA 的并行度被限制在了组数，MQA 则完全没法使用。而且由于张量并行也需要在 a 维度上进行划分，SP-Ulysses 和 TP 是冲突的。</p>
<h1 id="ring-attention-sequence-parallel">Ring-Attention Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/09/26/Ring_Attention/#Putting-it-Together">Ring-SP</a> 实际上为环状的 FlashAttention，将输入沿着序列维度切分到每个设备上，在 Attention 计算过程中每个设备向相邻设备通信 KV 并更新自己的 Softmax 矩阵，通信量为 4bsh bytes. 激活占用和 DS-SP 一样为 bsh(34/n+5sa/h).</p>
<h1 id="unified-sequence-parallel">Unified Sequence Parallel</h1>
<p><a href="https://darkenstar.github.io/2024/11/14/USP-A%20Unified%20Sequence%20Parallelism%20Approach%20for%20Long%20Context%20Generative%20AI/#Unified-Ulysses-Ring-Sequence-Parallelism">USP</a> 将 SP 进程组分割成两个正交的进程组：SP-Ring 进程组和 SP-Ulysses 进程组。可以将其视为一个 2D mesh ，每一列上运行 SP-Ring，每一行上运行 SP-Ulysses. 具体方法为 QKV 的切分 和 All-to-All 和 DS-Ulysses 相同，然后采用 Ring-Attention 的方式进行计算。如果遇到使用 casual mask 的情况需要加上 balance load 策略，把序列长度分为 2*(ring_degree) 大小，按照 0-&gt;1-&gt;&hellip;-&gt;(ring_degree-1)-&gt;(ring_degree-1)-&gt;&hellip;-&gt;0 的顺序进行分配。USP 消除了 SP-ulysses的头数限制。并且 USP可以通过调整 SP-Ulysses 进程组数目来更好的适应不同带宽的网络结构，可以让 All-to-All 操作在高带宽中运行，而异步 P2P 通信在低带宽部分运行。</p>
<h1 id="comparsion-of-different-parallelsim-in-training">Comparsion of Different Parallelsim in Training</h1>
<table border="1">
  <tr>
    <th rowspan="2"></th>
    <th colspan="4" style="text-align: center;">Communication (FWD+BWD)</th>
    <th rowspan="2">Split Dim</th>
    <th colspan="3" style="text-align: center;">Memory</th>
  </tr>
  <tr>
    <th>Param</th>
    <th>Cost</th>
    <th>Act</th>
    <th>Cost</th>
    <th>P/G</th>
    <th>OS</th>
    <th>Act</th>
  </tr>
  <tr>
    <td>DS-SP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>8*All2All</td>
    <td>(8/N)O(bsh)</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>Ring-SP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>P2P</td>
    <td>4O(bsh)</td>
    <td>L/L</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>DP</td>
    <td>AllReduce</td>
    <td>12O(h²)</td>
    <td>0</td>
    <td>0</td>
    <td>b/b</td>
    <td>P+G</td>
    <td>6P</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>ZeRO1</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>0</td>
    <td>0</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P/N </td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO1</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>P+G</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO2</td>
    <td>AllGather + ReduceScatter</td>
    <td>12O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>P+(G/N)</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>USP + ZeRO3</td>
    <td>2*AllGather + ReduceScatter</td>
    <td>18O(h²)</td>
    <td>P2P + 8*All2All</td>
    <td>≤ 4O(bsh)</td>
    <td>a/s</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
  <tr>
    <td>TP</td>
    <td>0</td>
    <td>0</td>
    <td>4*AllReduce</td>
    <td>8O(bsh)</td>
    <td>a/h</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>αA</td>
  </tr>
  <tr>
    <td>Megatron-SP</td>
    <td>0</td>
    <td>0</td>
    <td>6*AllGather + 4*ReduceScatter</td>
    <td>10O(bsh)</td>
    <td>a/h</td>
    <td>(P+G)/N</td>
    <td>6P/N</td>
    <td>A/N</td>
  </tr>
</table>
<h1 id="analysis">Analysis</h1>
<ol>
<li>All2All 通信使得 DS-SP 的通信开销大于 DP. 使用 Ring-SP 时，尽管异步的 P2P 通信是可以重叠的，理想的性能也是只与 DP 相同。因此只有当批 batchsize 不足以进行切分时才考虑使用 SP.</li>
<li>Megatron-SP 通信量高于 DS-SP 和 Ring-SP. SP-Ring 对于 KV 的通信可以与计算重叠。Megatron-SP 的通信量不会随着并行度的增加而减少，而 DS-SP 可以做到。 DS-SP 和 Ring-SP 具有较低的激活通信成本，但需要同步梯度和参数。不过参数通信量相对于激活通信量较小，可以通过计算进行重叠。GQA/MQA 也可以降低它俩的通信成本，而 Megatron-SP 不受影响。</li>
<li>相同配置下使用 USP+Zero3 来代替 Megatron-SP 并不会增加可训练序列的长度。但与 Megatron-SP 相比，USP 能在通过提高并行度来增加可以训练的序列长度。</li>
<li>Megatron-SP 并行维度受限于注意力头数目。USP 可以通过提高 Ring-SP 的并行度来扩展，以在大规模配置下训练更大模型。</li>
</ol>
<h1 id="sora-inference-modeling-analysis-process">Sora Inference Modeling Analysis Process</h1>
<p>我们需要准备模型的输入：</p>
<ol>
<li>隐空间采样的噪声 z，形状与想生成的视频时常和分辨率相关。生成 1s 的视频为 25.5 frames，经过 VAE Encoder 后输出的通道数为 4，帧数会被压缩到 <code>num_frame*5//17</code>，分辨率的长宽分别被压缩到原来的 1/8. 因此 z 的形状应该为 <code>(B, 4, num_frame*5//17, img_size[0]//8, img_size[1]//8)</code>.</li>
<li>输入的 prompt 会经过 DeepFloyd/t5-v1_1-xxl 编码，该编码器最大的 token 数为 300，编码维度为 4096，文本长度不足时会填充到 300. 因此编码后的 prompt 的形状为 <code>(B, 1, 300, 4096)</code>.</li>
<li>当前去噪的时间步 t，形状为 <code>(B, )</code></li>
<li>生成视频的 fps，形状为 <code>(1, )</code></li>
</ol>
<p>还需要准备相关的模型配置，包括 mesh 形状，sub_mesh 的形状，并行策略以及 stage_ids. 如果需要将模型的 transformer block 切分成多段，则需要配置 sub_mesh 和 stage_ids.</p>
<ul>
<li>mesh_shape: (num_x, num_y)</li>
<li>submesh_shape: <code>[(num_x, num_y, loc_x, loc_y), ]</code></li>
<li>stage_ids: <code>[(submesh0_start, submesh0_end), ]</code></li>
<li>strategy: 并行策略</li>
</ul>
<p>然后初始化模型，Sora 的整体结构如下 我们初始化一个 Pipeline(包含整个流程的函数)，它会有一个或多个 Stage 用于保存模型的不同层，与 stage_ids 中对应。我们将模型分解成 Embedding_blocks(PatchEmbed3D, TimestepEmbedder, SizeEmbedder, Captionembedder, t_block), STDiT3_blocks 和 T2IFinalLayer. 将这个分解函数作为 Pipeline 的 sharding_func.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB9688e46ada2523df1ec522a7649be19a?method=download&amp;shareKey=991eba9aad6eca9f41599d2ad4f75c34" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB9688e46ada2523df1ec522a7649be19a?method=download&amp;shareKey=991eba9aad6eca9f41599d2ad4f75c34" alt="Open-Sora">
    </a><figcaption>Open-Sora</figcaption></figure></p>
<h2 id="init-pipeline">Init Pipeline</h2>
<p>我们需要根据配置以及 PipePatch 并行度和 SP 并行度初始化 Pipeline. 这其中会根据 stage_ids 分配每个 Stage 保存模型的哪些层以及对应的 submesh 大小。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">construct_stages</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">submeshes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">],</span> <span class="n">stages_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># construct layers for each stage</span>
</span></span><span class="line"><span class="cl">    <span class="n">first_part</span><span class="p">,</span> <span class="n">module_list</span><span class="p">,</span> <span class="n">last_part</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stages_ids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">submesh</span> <span class="o">=</span> <span class="n">submeshes</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage_id</span> <span class="o">=</span> <span class="n">stages_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get stage layers from user config stage ids in module list</span>
</span></span><span class="line"><span class="cl">        <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">module_list</span><span class="p">[</span><span class="n">stage_id</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">stage_id</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">first_part</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># concat module first part(if exists) bef module list to stage_0</span>
</span></span><span class="line"><span class="cl">            <span class="n">layers</span> <span class="o">=</span> <span class="n">first_part</span> <span class="o">+</span> <span class="n">layers</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">num</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">last_part</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># concat module last part(if exists) aft module list to last stage</span>
</span></span><span class="line"><span class="cl">            <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">last_part</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># deepcopy module for xla device tracing use</span>
</span></span><span class="line"><span class="cl">        <span class="n">stage_module</span> <span class="o">=</span> <span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">stages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">Stage</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stage_module</span><span class="p">,</span> <span class="n">submesh</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">modules</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="write-sharding-function">Write Sharding Function</h2>
<p>要根据选择的不同的并行策略对每个 Stage 的模型权重，输入，输出进行切分。这里同样我们单独处理 Embedding_blocks, STDiT3_blocks 和 T2IFinalLayer. 让 stage0 包括对 Embedding_blocks 的处理，stage(N-1) 包括对 T2IFinalLayer 的处理。需要注意的是 DS-ulysses 我们需要对 <a href="mailto:Q@K.T">Q@K.T</a> 的结果 和 S@V 的结果也进行切分 SPMD 才会插入正确的 All2All，因此这部分只能放在网络的 forward 里面进行。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_one_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># first 5 modules are embedding layers</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_embedding</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">total_len</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">    <span class="n">shard_sora_final</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_first_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_embedding</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">shard_sora_last_stage</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">total_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_len</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard spatial</span>
</span></span><span class="line"><span class="cl">        <span class="n">shard_sora_block</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>  <span class="c1"># shard temporal</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># skip norm layer mark sharding</span>
</span></span><span class="line"><span class="cl">    <span class="n">shard_sora_final</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="n">total_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">shard_strategy</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="construct-pipeline">Construct Pipeline</h2>
<p>然后为了处理多 stage 的情况，我们需要保存每个 stage 的输入和输出的形状。这一步相当于放到 cuda 上重走一遍整个模型的 forward，记录下每一层输入和输出的形状，保存为 json 一遍。实际上对于每个固定生成大小的视频进行一次就行，下次直接读取这个文件。因为现在都采用 <a href="https://facebookresearch.github.io/xformers/components/ops.html">xformers.ops.memory_efficient_attention</a>，需要输入张量在 cuda 上，我们需要手动在模型的 forward 函数中写一个 navie 的 attention 计算流程好让 torch_xla 能对张量进行跟踪。</p>
<h2 id="trace-mhlo-graph">Trace mhlo Graph</h2>
<p>根据上一步得到的每个 Stage 的输入形状，创建输入张量，放入 xla_device 上，执行 forward. 最后导出输出的 mhlo 计算图。这里需要注意第一个 stage 包含多个非连续的模块，因此需要单独处理，最后一个 stage 最后一层的输入与其他 block 不同，因此也要单独处理。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">trace_stage_mhlo_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">check_res</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    trace stage nn modules to mhlo graph
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># (NOTE): construct xla mesh before trace tensors generate,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># i.e., before any xla device call to avoid xla computation client construct</span>
</span></span><span class="line"><span class="cl">    <span class="n">xla_mesh</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">xla_mesh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_construct_stage_xla_mesh</span><span class="p">()</span>  <span class="c1"># create mesh from submesh info</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Create xla device trace tensors, move module to xla device</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_trace_tensors</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outputs</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">y_embedded</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">t_mlp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">stages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">t_mlp</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t_mlp</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">mod</span> <span class="ow">in</span> <span class="n">module</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">mod</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>  <span class="c1"># first load to cpu</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">xm</span><span class="o">.</span><span class="n">xla_device</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># get pipeline exec mode</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">exec_mode</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">exec_mode</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># load lora cofifg</span>
</span></span><span class="line"><span class="cl">    <span class="n">lora_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">lora_config</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Enter trace mhlo graph for stage: &#34;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Trigger shard func to mark sharding the model</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">shard_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modules</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shard_strategy</span><span class="p">,</span> <span class="n">xla_mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">exec_mode</span> <span class="o">==</span> <span class="n">EXEC_MODE</span><span class="o">.</span><span class="n">INFER</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># set stage name &amp; dump file path</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_set_stage_name_dump_file</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">exec_mode</span><span class="p">,</span> <span class="s2">&#34;fw&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_sampling_steps</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">1000</span>
</span></span><span class="line"><span class="cl">        <span class="n">timesteps</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">i</span> <span class="o">/</span> <span class="n">num_sampling_steps</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_timesteps</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_sampling_steps</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># FIXME: 原先是为每个stage单独生成trace_tensor, 现在要把上一个的结果传给下一个 stage</span>
</span></span><span class="line"><span class="cl">        <span class="c1">#for i in range(30):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">pipeline_patches_height_list</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent_pipeline</span><span class="o">.</span><span class="n">pipeline_patches_height_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stage_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">...</span><span class="p">,</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,:]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">xla_mesh</span><span class="p">)</span>  <span class="c1"># outputs is a list</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_tensors</span><span class="p">,</span> <span class="n">xla_mesh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">check_res</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># check xla results compared to gpu results</span>
</span></span><span class="line"><span class="cl">            <span class="n">check_result_error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># use torch xla _get_xla_tensors_hlo interface</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># to eliminate redundant live tensors as ret values</span>
</span></span><span class="line"><span class="cl">            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;XLA_DUMP_POST_OPTIMIZATIONS&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;true&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="n">torch_xla</span><span class="o">.</span><span class="n">_XLAC</span><span class="o">.</span><span class="n">_get_xla_tensors_hlo</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="analyze-mhlo-graph">Analyze mhlo Graph</h2>
<p>接下来我们要遍历上一步得出的 mhlo 图。</p>
<h3 id="opview">OpView</h3>
<p>从根节点的 ir 开始遍历上一步导出的整个计算图。根据传入 ir 的类型定义调用对应的 visit 函数读取其属性进行操作。主要通过 rsqrt 的位置来划分一个 Transformer block 中第几个 dot 和 dot_general 对应的是什么操作。对于 Sora 来说划分情况如下。这里需要注意的是 mhlo 图记录的是拓扑排序的顺序，不是程序顺序执行的顺序，因此第一个 block 会掺杂着 Embedding_blocks 的一些 dot 操作。因此我们从第二个 block 的第一个 rsqrt 位置开始统计。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">collect_rms_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">rms_collector</span> <span class="o">=</span> <span class="n">RMSCollector</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  <span class="n">rms_collector</span><span class="o">.</span><span class="n">visit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">root_op</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span> <span class="o">=</span> <span class="n">rms_collector</span><span class="o">.</span><span class="n">rms_locs</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># construct attention block &amp; ffn block ranges</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># exclude the rsqrt in T2IFinalLayer</span>
</span></span><span class="line"><span class="cl">  <span class="n">att_rm_locs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">att_rm_locs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># a block has 4 rsqrt, start from 2nd block to avoid embedding</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">0</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cro_attn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">att_rm_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">att_rm_locs</span><span class="p">),</span> <span class="mi">4</span><span class="p">):</span>  <span class="c1"># ORG: range(8, len(att_rm_locs), 4): </span>
</span></span><span class="line"><span class="cl">      <span class="n">start</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="n">i</span><span class="o">+</span><span class="mi">4</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">          <span class="n">end</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">      <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="n">end</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rms_locs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">4</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ffn_ranges</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><table>
  <thead>
      <tr>
          <th>module</th>
          <th>operator</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td></td>
          <td><code>RMSNorm(x)</code></td>
      </tr>
      <tr>
          <td><strong>Self Attention</strong></td>
          <td><code>dot(x, qkvLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(q)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(q, k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(s, v)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(attn, oLinear.weight)</code></td>
      </tr>
      <tr>
          <td><strong>Cross Attention</strong></td>
          <td><code>dot(x, qLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(y, kvLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(q, k)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot_general(s, v)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(attn, oLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>RMSNorm(x) </code></td>
      </tr>
      <tr>
          <td><strong>Feed Forward Network</strong></td>
          <td><code>dot(x, upLinear.weight)</code></td>
      </tr>
      <tr>
          <td></td>
          <td><code>dot(x, downLinear.weight)</code></td>
      </tr>
  </tbody>
</table>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">visit_dot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">dot_lineno</span> <span class="o">=</span> <span class="n">_parse_loc_lineno</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">spt_att_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_attn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">cro_att_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cro_attn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">spt_qkv_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">ffn_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_ranges</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># lie in RMS ops closed attention block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">spt_att_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">spt_att_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="c1">#import pdb;pdb.set_trace()</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">cro_att_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">cro_att_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">cro_att_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># lie ffn block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">spt_qkv_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">spt_qkv_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># pixart pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">dot_lineno</span> <span class="o">&gt;</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">dot_lineno</span> <span class="o">&lt;</span> <span class="n">ffn_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>                 
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Traversal of one block</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">spt_att_dot_cnt</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> \
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">attention_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">block_cnt</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># reset each block level counters</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_qkv_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_att_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">spt_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_dot_cnt</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">att_block_dots</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block_dots</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">generic_visit</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>保存好一个 Transformer block 中每个 dot 或 dotgeneral 对应的是什么操作后，我们便可以访问这个 ir. 这里需要注意只要两个相乘的矩阵有一个是二维张量 (比如线性层的权重)，mhlo 都会将另一个 reshape 成二维张量。dot 算子 (<code>jaxlib.mlir.dialects._mhlo_ops_gen.DotOp</code>) 两个操作数都是二维的张量，qkvLinear 对应的是第一个 dot 操作。左操作数的 shape 为 <code>(BST,3C)</code>. 当两个相乘的矩阵都是 3 维及以上张量的时候就会生成 dot_general 该算子的两个相乘的矩阵都会被 reshape 成三维张量。Self-Attention 的第一个 dot_general 左操作数的 shape 为 <code>(BTN_A,S,C)</code>. 这样我们就可以得到 <code>BT=(BST)/S, N_A=(BTN_A)/(BT)</code>. 同样我们可以得到 OLinear, FFN 中 upLinear 和 downLinear 权重的形状. 以及 Cross-Attention 模块的对应信息。由于之前遍历是从第二个 block 开始的，因此总层数要 ＋1. 最后将得到的参数打包成一个字典返回。</p>
<h3 id="communication-view">Communication View</h3>
<p>我们以同样的方式定义各种集合通信算子的 visit 函数用于评估该算子的通信量，遍历到对应的 ir 后调用它。</p>
<p>AllReduce 将所有的数据通过规约操作集成到各个设备中。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB6e4d9c026bc0632af5040321998fb3ab?method=download&amp;shareKey=f901430ac6bfa781d0b462f0170981d3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB6e4d9c026bc0632af5040321998fb3ab?method=download&amp;shareKey=f901430ac6bfa781d0b462f0170981d3" alt="AllReduce">
    </a><figcaption>AllReduce</figcaption></figure></p>
<p>在 Ring-AllReduce 的 ReduceScatter 步骤中，每个进程发送 M 个元素 N-1 次，总共为 M(N-1). 在 AllGather 步骤中，每个进程发送它计算的块的结果。这是额外的 M 个元素发送了 N-1 次。总的通信量加起来是 2M(N-1).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB69d2b3957cd1863481bff0e785dc9a82?method=download&amp;shareKey=32e60903bafe5dbf240af91c67486e1b" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB69d2b3957cd1863481bff0e785dc9a82?method=download&amp;shareKey=32e60903bafe5dbf240af91c67486e1b" alt="Ring-AllReduce">
    </a><figcaption>Ring-AllReduce</figcaption></figure></p>
<p>All-Gather 示意图如下，每个设备开始拥有初始的一部分数据，通信后每个设备都有一份完整的数据。总的通信量为 M(N-1).</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBe7e6e7a1230ed9ba7ba037556e489d51?method=download&amp;shareKey=5afdf2b669a500a6844aa9e281fe1ac3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBe7e6e7a1230ed9ba7ba037556e489d51?method=download&amp;shareKey=5afdf2b669a500a6844aa9e281fe1ac3" alt="AllGather">
    </a><figcaption>AllGather</figcaption></figure></p>
<p>All2All 示意图如下，每个设备把自己的第 i 块数据发送给第 i 个设备。</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBddc785dcc80dd741fc1f469a85823cd4?method=download&amp;shareKey=085c0a5681116b4e1683d4d6ae5d080f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBddc785dcc80dd741fc1f469a85823cd4?method=download&amp;shareKey=085c0a5681116b4e1683d4d6ae5d080f" alt="All2All">
    </a><figcaption>All2All</figcaption></figure></p>
<p>基于 Bruck 算法的 All2All 流程如下</p>
<ol>
<li>局部循环移位 (Local Shift of Data-Blocks)
每个进程将其本地的数据块重新排列，进行初始的循环移位。对于进程 p 和数据块索引 i: R[i]=S[(p+i)%P]. 其中 S[i] 是进程本地初始的数据，R[i] 是移位后的数据。</li>
<li>全局通信 (Global Communication)
一共进行 log(P) 次通信。
每一步中每个进程将一部分数据发送给相邻的进程，并接收相邻进程发送的数据。若数据块索引 i 用 radix-2 表示的第 k 位为 1，则数据块会被发送到目标进程。
对于进程 p: 发送数据到进程 ((p + 2^k) % P)，接收来自进程 ((p - 2^k) % P) 的数据。
每次发送后，进程将接收到的数据更新到其本地数据中。</li>
<li>局部逆向移位 (Local Inverse Shift of Data-Blocks)
在完成所有全局通信之后，每个进程执行逆向移位，以恢复数据块的正确顺序。对于每个数据块索引 i: R[i]=R[(p−i+P)%P]</li>
</ol>
<p>在进程是 2 次幂的情况下每个设备每次要通信 M*P/2大小的数据，总共为 MPlog(P)/2.</p>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEBa083a4c6002019e62b23c0b24b59a812?method=download&amp;shareKey=58f1b8055307d53e43ce86b9e1762989" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEBa083a4c6002019e62b23c0b24b59a812?method=download&amp;shareKey=58f1b8055307d53e43ce86b9e1762989" alt="Example of the Bruck Algorithm with 4 Processes">
    </a><figcaption>Example of the Bruck Algorithm with 4 Processes</figcaption></figure></p>
<h3 id="tflops-view">TFLOPS View</h3>
<p>计算量主要分成两种，element-wise 的操作计算量为元素个数。两个形状分别为 mxk 和 kxn 的矩阵相乘计算量为 2mkn. 被计入 element-wise 操作的算子有 add, subtract, multiply, divide, rsqrt, negate, exponential. 被计入矩阵乘法的算子有 dot, dot_general.</p>
<h2 id="performance-analysis">Performance Analysis</h2>
<p>我们根据提取出的 Transformer block 的信息送入性能分析器进行分析. tx8 的配置如下</p>
<table>
  <thead>
      <tr>
          <th>Parameter</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>TILE_NUM</td>
          <td>16</td>
      </tr>
      <tr>
          <td>SRAM (MB)</td>
          <td>3</td>
      </tr>
      <tr>
          <td>NOC BW (GB/s)</td>
          <td>128</td>
      </tr>
      <tr>
          <td>DRAM BW (GB/s)</td>
          <td>100</td>
      </tr>
      <tr>
          <td>DRAM LATENCY (us)</td>
          <td>0.1</td>
      </tr>
      <tr>
          <td>GEMM (TFLOPS)</td>
          <td>8</td>
      </tr>
      <tr>
          <td>VECTOR (TOPS)</td>
          <td>0.0625</td>
      </tr>
      <tr>
          <td>HOP LATENCY (us)</td>
          <td>0.01</td>
      </tr>
  </tbody>
</table>
<p>根据提取出的信息构建的 STDiT 的 spt_blk, tmp_blk, cross_blk 的参数字典如下.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                  <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_spt&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                  <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_tmp&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span><span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_cro&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;D_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;D_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FD&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FD&#34;</span><span class="p">]}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>根据这些参数再构建每个层的输入输出形状，计算类型和计算量，以 <code>Gate_ResAdd</code> 为例:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">GB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Gate_ResAdd</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">  Construct each op after MHSA on the config file
</span></span></span><span class="line"><span class="cl"><span class="s1">  &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># {name:{type:&#34;&#34;, size:&#34;&#34;, ishape:[], wshape:[]/None, oshape:[]}}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">construct_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">construct_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">GB</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">30</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;B&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;D_O&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_weight_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;D_O&#39;</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_output_shape</span> <span class="o">=</span> <span class="n">ResAdd_input_shape</span>
</span></span><span class="line"><span class="cl">      <span class="n">ResAdd_compute</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">ResAdd_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="n">GB</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="o">+</span><span class="s2">&#34;_&#34;</span><span class="o">+</span><span class="s2">&#34;ResAdd&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;name&#34;</span><span class="p">:</span><span class="s2">&#34;ResAdd&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;Vector&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;ishape&#34;</span><span class="p">:</span> <span class="n">ResAdd_input_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;wshape&#34;</span><span class="p">:</span> <span class="n">ResAdd_weight_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;oshape&#34;</span><span class="p">:</span> <span class="n">ResAdd_output_shape</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                                          <span class="s2">&#34;compute&#34;</span><span class="p">:</span> <span class="n">ResAdd_compute</span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>就像这样构建整个 Transformer block 的所有操作</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">STDIT2_block</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
</span></span><span class="line"><span class="cl">      <span class="c1"># {name:{type:&#34;&#34;, size:&#34;&#34;, ishape:[], wshape:[]/None, oshape:[]}}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ops</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">construct_model</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="nf">construct_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">      <span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_spt&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_spt&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span> <span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_tmp&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_tmp&#34;</span><span class="p">]</span> <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;B&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;B_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_Q&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_Q_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;S_KV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;S_KV_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;D_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_QKV&#34;</span><span class="p">],</span> 
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;H_QKV&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_QKV&#34;</span><span class="p">],</span><span class="s2">&#34;N_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;N_A&#34;</span><span class="p">],</span> <span class="s2">&#34;H_A&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_A&#34;</span><span class="p">],</span> <span class="s2">&#34;D_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_O_cro&#34;</span><span class="p">],</span> <span class="s2">&#34;H_O&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_O_cro&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                      <span class="s2">&#34;D_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FU&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FU&#34;</span><span class="p">],</span> <span class="s2">&#34;D_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;D_FD&#34;</span><span class="p">],</span> <span class="s2">&#34;H_FD&#34;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&#34;H_FD&#34;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">spatial_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;spatial&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">temporal_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;temporal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cross_block</span> <span class="o">=</span> <span class="n">MHSA_block</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">cross_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;cross&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">mlp_modulate</span> <span class="o">=</span> <span class="n">Modulate</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;mlp&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block</span> <span class="o">=</span> <span class="n">FFN_block</span><span class="p">(</span><span class="n">cross_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="o">.</span><span class="n">mlp_gate_resadd</span> <span class="o">=</span> <span class="n">Gate_ResAdd</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&#34;mlp&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="n">op_list</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">spatial_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="bp">self</span><span class="o">.</span><span class="n">temporal_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="bp">self</span><span class="o">.</span><span class="n">cross_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_modulate</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_block</span><span class="o">.</span><span class="n">ops</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp_gate_resadd</span><span class="o">.</span><span class="n">ops</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">for</span> <span class="n">op_dict</span> <span class="ow">in</span> <span class="n">op_list</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">          <span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">op_dict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后就可以将构建好的 ops 放入 mapper 进行分析。刚才那些操作会被分成 3 类 <code>vector_mapper</code>, <code>gemm_auto_opt_mapper</code> 和 <code>flashatten_mapper</code>. 我们根据操作的类型送入对应的 mapper 进行分析，具体如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">STDIT2_mapper</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">QKV_fusion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">preset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  <span class="n">config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</span></span><span class="line"><span class="cl">  <span class="n">Layers</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;L&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">spatial_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_spt&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">temporal_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_tmp&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">cross_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;B&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;B_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;S_Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_Q_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;S_KV&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;S_KV_cro&#39;</span><span class="p">],</span> <span class="s1">&#39;H_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;H_A&#39;</span><span class="p">],</span> <span class="s1">&#39;N_A&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;N_A&#39;</span><span class="p">],</span> <span class="s1">&#39;Q&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;Q&#39;</span><span class="p">]}</span>
</span></span><span class="line"><span class="cl">  <span class="n">ops</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">ops</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;=========================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Spatial Branch Mapping ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  =========================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">TmTn</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_RMSNorm&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_RMSNorm&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">spatial_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;spatial_ResAdd&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;spatial_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;==========================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Temporal Branch Mapping ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ==========================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>  <span class="c1"># 切分 30 份也无法满足SRAM要求</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_RMSNorm&#39;</span><span class="p">]</span><span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_RMSNorm&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">temporal_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;temporal_ResAdd&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;temporal_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;====================================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Cross Branch Mapping 2x per block ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ====================================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#mapping_result[&#39;spatial_RMSNorm&#39;]= vector_mapper(ops[&#39;spatial_RMSNorm&#39;],arch,splits=None,details=details)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Q_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_K_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_V_proj&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">Tx_Ty</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">flashatten_mapper</span><span class="p">(</span><span class="n">cross_config</span><span class="p">,</span> <span class="n">arch</span><span class="p">,</span> <span class="n">Tx_Ty</span><span class="o">=</span><span class="n">Tx_Ty</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">,</span> <span class="n">Head_fused</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># FIXME</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_Flashatten&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;cross_ResAdd&#39;</span><span class="p">]</span> <span class="o">=</span>  <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;cross_ResAdd&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">  <span class="c1"># HACK: Gate_ResAdd *2 了, cross 无gate 这里无 _2</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="s1">&#39;&#39;&#39;====================================
</span></span></span><span class="line"><span class="cl"><span class="s1">  == Feed Forward Network 2x per block ==
</span></span></span><span class="line"><span class="cl"><span class="s1">  ====================================&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_Modulate&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;FFNup&#39;</span><span class="p">],</span><span class="n">arch</span><span class="p">,</span><span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span><span class="n">fusion_op2</span><span class="o">=</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;SiLU&#39;</span><span class="p">],</span><span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNup&amp;SiLU&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># mapping_result[&#39;FFNgate&#39;] = gemm_auto_opt_mapper(ops[&#39;FFNgate&#39;], arch, TmTn=TmTn, details=details)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># mapping_result[&#39;Hadamard&#39;] = vector_mapper(ops[&#39;Hadamard&#39;], arch, splits=None)</span>
</span></span><span class="line"><span class="cl">  <span class="n">TmTn</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span> <span class="k">if</span> <span class="n">preset</span> <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gemm_auto_opt_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">TmTn</span><span class="o">=</span><span class="n">TmTn</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;FFNdown&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector_mapper</span><span class="p">(</span><span class="n">ops</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">],</span> <span class="n">arch</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="n">details</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapping_result</span><span class="p">[</span><span class="s1">&#39;mlp_ResAdd&#39;</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>mapper 会遍历所有可能的切分策略放入 tx8 执行并选择最好的那一个。对于 vector 类型的算子只会沿着 sequence 维度切分；对于 GEMM 算子则会沿着 m, k, n 维度都进行切分；对于 flash-attention 的切分则与原算法相同，外循环遍历 K, V 的每一块，内循环遍历 Q 的每一块。这样就可以得到每个 tx8 上最优的切分方式对应的通信用时，计算用时和利用率。再用之前统计出的每个 die 上通信量除以 die2die 带宽得到通信用时，由此得到总的推理用时。</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
