<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Source Code Reading on WITHER</title>
    <link>http://localhost:1313/categories/source-code-reading/</link>
    <description>Recent content in Source Code Reading on WITHER</description>
    <generator>Hugo -- 0.147.7</generator>
    <language>en</language>
    <copyright>2024-2025 WITHER</copyright>
    <lastBuildDate>Sat, 21 Jun 2025 22:00:13 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/source-code-reading/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DualPipe</title>
      <link>http://localhost:1313/blogs/deepseek/dualpipe/</link>
      <pubDate>Sat, 21 Jun 2025 22:00:13 +0800</pubDate>
      <guid>http://localhost:1313/blogs/deepseek/dualpipe/</guid>
      <description>Source code reading of DualPipe</description>
      <content:encoded><![CDATA[<h1 id="preliminary">Preliminary</h1>
<p>本节先回顾流水线并行以及 DeepSeek-V3 中作为 baseline 的 <a href="https://arxiv.org/pdf/1806.03377">PipeDream</a> 论文中的 1F1B 和 <a href="https://openreview.net/pdf?id=tuzTN0eIO5">ZeroBubble</a> 论文中的 ZB1P (ZB-H1 的自动搜索结果).</p>
<h2 id="pipedream-1f1b">PipeDream 1F1B</h2>
<p>1F1B (One-Forward-One-Backward) 的工作流程如图所示，想象一条工厂流水线，用于组装一个复杂的设备。这个设备需要经过多个工位（GPU），每个工位负责一部分装配任务（模型的不同层）。当第一个产品的第一个部件在工位1上加工时，其他所有工位都在闲置等待。当它被传递到工位2时，工位1开始加工第二个产品，但工位3、4…依然在等待。这种在流水线启动和结束阶段产生的设备空闲时间，就是流水线气泡 (Pipeline Bubble). 在大模型训练中，这意味着 GPU 算力被浪费，直接导致训练时间延长和成本增加。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB787301b994e9c90258f8ad84fd1f8b67?method=download&amp;shareKey=db772d656fe8be439988e887fd6910a3" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB787301b994e9c90258f8ad84fd1f8b67?method=download&amp;shareKey=db772d656fe8be439988e887fd6910a3" alt="1F1B pipeline Schedule">
    </a><figcaption>1F1B pipeline Schedule</figcaption></figure></p>
<p>后续批次的后向传播永远在前一批次的后向传播全部启动后才开始，为了防止激活占用内存过多，图中 1F1B 的 bs=8，流水线并行过程中最多保存 4 个 batch 的激活，当 batch1 反向传播结束后再进行 batch5 的正向传播。为了减少激活占用，1F1B 中进行反向传播的优先级高于正向传播。</p>
<h2 id="zerobubble-zb1p">ZeroBubble ZB1P</h2>
<p>ZeroBubble 减少气泡的关键是将反向传播中对于权重和输入的梯度计算分开进行。传统上，一个层的反向传播包含两个核心任务:</p>
<ul>
<li>B Pass: 计算关于输入梯度并将其传递给前一层，这是误差反向传播链的一部分。</li>
<li>W Pass: 计算该层自身权重的梯度，用于后续的参数更新。</li>
</ul>
<p>如图所示第 i-1 层的 B Pass 依赖于第 i 层的 B Pass. 但第 i 层的 W Pass，只要在其 B Pass 完成之后，可以被灵活地安排在任何时间点执行。</p>
<h2 id="computation-graph-for-mlp">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBeef090dd44e296a4a4e985200c62e4c7?method=download&amp;shareKey=2d9e68a50dd5b0b3c46f079499ed2bec" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBeef090dd44e296a4a4e985200c62e4c7?method=download&amp;shareKey=2d9e68a50dd5b0b3c46f079499ed2bec" alt="Computation Graph for MLP">
    </a><figcaption>Computation Graph for MLP</figcaption></figure></h2>
<p><strong>Handcrafted Pipeline Schedule</strong></p>
<p>基于这个思想，文中提出了两个手工设计的调度方案作为概念验证:</p>
<ul>
<li>ZB-H1 (Memory Efficient Schedule): 在维持与 1F1B 相似峰值内存消耗的情况下，通过将 W Pass 推迟执行，填充了流水线末尾的 cooldown 气泡，成功将气泡大小减少到 1F1B 的三分之一。</li>
<li>ZB-H2 (Zero Bubble Schedule): 当内存预算更宽松时，在流水线 warm-up 安排更多的 F Pass，并巧妙地重排 W Pass，将整个流水线的执行过程从一个梯形变成了一个平行四边形，从而在理论上完全消除了气泡。</li>
</ul>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB9f702f88f4b48c048d602ddfe7b69ffb?method=download&amp;shareKey=010eea5b8230b6175d7777444e4dcc64" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB9f702f88f4b48c048d602ddfe7b69ffb?method=download&amp;shareKey=010eea5b8230b6175d7777444e4dcc64" alt="Handcrafted pipeline schedules ZB-H1 (top)  &amp; ZB-H2 (bottom)">
    </a><figcaption>Handcrafted pipeline schedules ZB-H1 (top)  &amp; ZB-H2 (bottom)</figcaption></figure></p>
<p>文中基于一个标准的 Transformer架构，其中 FFN 的中间层维度是模型隐藏维度 <code>h</code> 的4倍。给出了 F, B, W 各自的计算量和激活占用。其中计算量只统计占据主要部分的矩阵乘法的浮点运算次数。</p>
<ul>
<li><code>b</code>: microbatch size</li>
<li><code>s</code>: sequence length</li>
<li><code>h</code>: hidden dimension size</li>
<li><code>a</code>: number of attention heads</li>
</ul>
<h2 id="transformer-architecture">
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEBd9f810c1f506d383eb59a0c1186e602b?method=download&amp;shareKey=ee9e8a521ed880701b05e9b25b1ae001" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEBd9f810c1f506d383eb59a0c1186e602b?method=download&amp;shareKey=ee9e8a521ed880701b05e9b25b1ae001" alt="Transformer Architecture">
    </a><figcaption>Transformer Architecture</figcaption></figure></h2>
<p><em>Table1: FLOPs and activations memory required per transformer layer for each pass</em></p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Pass</th>
          <th style="text-align: center">FLOPs</th>
          <th style="text-align: center">Activations Memory Required</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">F</td>
          <td style="text-align: center">$sbh(24h+4s)$</td>
          <td style="text-align: center">0</td>
      </tr>
      <tr>
          <td style="text-align: center">B</td>
          <td style="text-align: center">$sbh(24h+8s)$</td>
          <td style="text-align: center">$sb(34h+5as)$</td>
      </tr>
      <tr>
          <td style="text-align: center">W</td>
          <td style="text-align: center">$sbh(24h)$</td>
          <td style="text-align: center">$32sbh$</td>
      </tr>
  </tbody>
</table>
<hr>
<p>前向传播 $T_F \approx (8bsh^2 + 4bs^2h) + 16bsh^2 = 24bsh^2 + 4bs^2h = sbh(24h + 4s)$. 反向传播关于权重的计算量等于 Linear 层的 GEMM.</p>
<ul>
<li>
<p><strong>Self-Attention</strong>: $6bsh^2 + 2bs^2h + 2bs^2h + 2bsh^2 = 8bsh^2 + 4bs^2h$</p>
<ul>
<li><strong>Q, K, V Projection</strong>：输入 <code>(b, s, h)</code> 通过与权重矩阵 <code>(h, h)</code> 相乘，生成Q, K, V。这涉及到3次矩阵乘法。$\text{FLOPs} \approx 2 \times b \times s \times h \times 3h = 6bsh^2$</li>
<li><strong>Attention Score</strong>:<code>Q</code> <code>(b, a, s, h/a)</code> 与 <code>K^T</code> <code>(b, a, h/a, s)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times a \times s \times (h/a) \times s = 2bshs$.</li>
<li><strong>Score@V</strong>：注意力分数 <code>(b, a, s, s)</code> 与 <code>V</code> <code>(b, a, s, h/a)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times a \times s \times s \times (h/a) = 2bshs$.</li>
<li><strong>O Projecyion</strong>：结果与输出权重矩阵 <code>(h, h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times h \times h = 2bsh^2$.</li>
</ul>
</li>
<li>
<p><strong>FFN FLOPs</strong>: $8bsh^2 + 8bsh^2 = 16bsh^2$</p>
<ul>
<li><strong>Up Projection</strong>：输入 <code>(b, s, h)</code> 与权重矩阵 <code>(h, 4h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times h \times 4h = 8bsh^2$.</li>
<li><strong>Down Projection</strong>：中间结果 <code>(b, s, 4h)</code> 与权重矩阵 <code>(4h, h)</code> 相乘。$\text{FLOPs} \approx 2 \times b \times s \times 4h \times h = 8bsh^2$.</li>
</ul>
</li>
</ul>
<hr>
<p>激活占用方面除了 Dropout Mask 是 INT8 类型以外，假设 activations 均以 16-bit float 类型保存。表中的 activation memory 均以字节为单位进行统计。和权重梯度无关的部分只有 dropout 相关的以及 Softmax output.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Category</th>
          <th style="text-align: center">Item</th>
          <th style="text-align: center">Original</th>
          <th style="text-align: center">TP</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><strong>Attention</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$11sbh + 5as^2b$</strong></td>
          <td style="text-align: center"><strong>$3sbh + \frac{8sbh}{t} + \frac{5as^2b}{t}$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">QKV input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">QK^T</td>
          <td style="text-align: center">$4sbh$</td>
          <td style="text-align: center">$\frac{4sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Softmax output</td>
          <td style="text-align: center">$2as^2b$</td>
          <td style="text-align: center">$\frac{2as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout mask</td>
          <td style="text-align: center">$as^2b$</td>
          <td style="text-align: center">$\frac{as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout output</td>
          <td style="text-align: center">$2as^2b$</td>
          <td style="text-align: center">$\frac{2as^2b}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">V</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$\frac{2sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear projection input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$\frac{2sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Attention dropout mask</td>
          <td style="text-align: center">$sbh$</td>
          <td style="text-align: center">$sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>MLP</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$19sbh$</strong></td>
          <td style="text-align: center"><strong>$3sbh + \frac{16sbh}{t}$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear1 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">GeLU input</td>
          <td style="text-align: center">$8sbh$</td>
          <td style="text-align: center">$\frac{8sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Linear2 input</td>
          <td style="text-align: center">$8sbh$</td>
          <td style="text-align: center">$\frac{8sbh}{t}$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">Dropout mask</td>
          <td style="text-align: center">$sbh$</td>
          <td style="text-align: center">$sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>LayerNorm</strong></td>
          <td style="text-align: center"><strong>Total</strong></td>
          <td style="text-align: center"><strong>$4sbh$</strong></td>
          <td style="text-align: center"><strong>$4sbh$</strong></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">LayerNorm1 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">LayerNorm2 input</td>
          <td style="text-align: center">$2sbh$</td>
          <td style="text-align: center">$2sbh$</td>
      </tr>
  </tbody>
</table>
<hr>
<p>在没有 $T_F = T_B = T_W$ 假设的情况下，ZB-H1 和 ZB-H2 的峰值激活内存和气泡大小如 Table 2 所示。值得注意的是，对于设备 <em>i</em>，其在 ZB-H1 方案下的激活内存为 $(p-i+1)M_B + (i-1)M_W$，在 ZB-H2 方案下的激活内存为 $(2p - 2i + 1)M_B + (2i - 2)M_W$。如 Table 1 所示，<em>W</em> 所需的激活内存小于 <em>B</em> 所需的激活内存。因此，ZB-H1 和 ZB-H2 的峰值激活内存分别为 $pM_B$ 和 $(2p-1)M_B$。</p>
<p><em>Table 2: Comparison between 1F1B and our handcrafted schedules.</em></p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Schedule</th>
          <th style="text-align: center">Bubble size</th>
          <th style="text-align: center">Peak activations memory</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">1F1B</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}+T_{W})$</td>
          <td style="text-align: center">$pM_{B}$</td>
      </tr>
      <tr>
          <td style="text-align: center">ZB-H1</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}-T_{W})$</td>
          <td style="text-align: center">$pM_{B}$</td>
      </tr>
      <tr>
          <td style="text-align: center">ZB-H2</td>
          <td style="text-align: center">$(p-1)(T_{F}+T_{B}-2T_{W})$</td>
          <td style="text-align: center">$(2p-1)M_{B}$</td>
      </tr>
  </tbody>
</table>
<p><strong>Automatic Pipeline Scheduling</strong></p>
<p>手工调度依赖于 F、B、W 的执行时间相等的理想情况。为了应对真实世界中复杂的执行时间和通信延迟，该文开发了一个自动化流水线调度算法。该算法通过一系列启发式策略，在一个给定的内存限制下，自动地为流水线生成一个高效的调度方案。核</p>
<ol>
<li>
<p><strong>Warm-up</strong>：</p>
<ul>
<li>在内存限制的范围内，算法会尽可能多地调度 F pass ，以最小化在第一个 B pass 开始前产生的气泡。</li>
<li>此阶段使用一个超参数来控制是否要调度一个可能会延迟后续B Pass的额外F Pass。</li>
</ul>
</li>
<li>
<p><strong>Steady State</strong>：</p>
<ul>
<li>热身阶段结束后，调度进入一个迭代模式，轮流调度一个F Pass和一个B Pass。</li>
<li>为了填充气泡，算法会伺机插入 W pass. 插入策略是：
<ul>
<li>当出现一个大于 $T_W$ (W Pass 执行时间) 的气泡时，直接插入一个W Pass.</li>
<li>当出现一个小于 $T_W$ 的气泡时，如果这个气泡会导致当前阶段的累计气泡时间成为所有阶段中最长的，那么仍然会插入一个W Pass.</li>
<li>当内存达到上限时，也会插入 W Pass 以回收和释放部分内存。</li>
</ul>
</li>
<li>通常这个启发式策略在稳态阶段会形成一个 1F-1B-1W 的调度模式。</li>
</ul>
</li>
<li>
<p><strong>Global Schedule</strong>：</p>
<ul>
<li>在整个调度过程中，算法始终保证在 F Pass 用完之前，第 i 阶段调度的 F Pass 数量至少比第 i+1 阶段多一个。</li>
<li>当这个数量差超过一时，会使用另一个超参数来决定在不产生额外气泡的前提下，是否要跳过第 i 阶段的一次F Pass调度。</li>
<li>算法会通过 grid search 来寻找这些超参数的最佳组合。</li>
</ul>
</li>
<li>
<p><strong>Final</strong>：当某个阶段的 F Pass 和 B Pass 都执行完毕后，算法会一次性逐个调度完所有剩余的 W Pass.</p>
</li>
</ol>
<hr>
<p><strong>Bypassing Optimizer Synchronization</strong></p>
<p>要实现完美的平行四边形调度，还需要解决优化器同步（Optimizer Synchronization）. 在分布式训练中，通常需要在更新模型参数前，在所有 GPU 间进行一次 All-Reduce，以进行梯度裁剪（Gradient Clipping）或检查数值稳定性 (NaN/INF). 这个同步点会强制所有设备等待，从而破坏平行四边形，重新引入气泡。</p>
<p>论文提出了 Bypassing Optimizer Synchronization，每个 GPU 在执行优化器更新步骤时，不再等待全局同步，而是基于从前一个 GPU 传来的部分 reduce 的信息进行推测性更新。该 micro-batch 完整的全局状态会在下一个迭代的 warp 阶段异步地传回。每个 GPU 在收到最终的全局状态后，会验证自己上一步的更新是否合法。如果发现不一致（例如，全局梯度范数超出了裁剪阈值），它会执行一次原地回滚（In-place Rollback），然后使用正确的全局状态重新执行优化器步骤。</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB69834a68b841e1af30873b5a95a2fc90?method=download&amp;shareKey=0544362bccaefd3cad59bb0be406a145" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB69834a68b841e1af30873b5a95a2fc90?method=download&amp;shareKey=0544362bccaefd3cad59bb0be406a145" alt="The Post-validation Strategy to Replace Optimizer Synchronization">
    </a><figcaption>The Post-validation Strategy to Replace Optimizer Synchronization</figcaption></figure></p>
<h1 id="dualpipe">DualPipe</h1>
<p>DualPipe 是一种创新的双向流水线并行算法。它的核心思想是在一组设备上同时处理两个方向的数据流：一个前向流水线和一个反向流水线。使得计算和通信能够更充分地重叠，从而减少流水线气泡（即 GPU 空闲时间）.</p>
<p>与传统的 GPipe（1F1B）只有一个数据流方向不同，DualPipe 将设备对折，形成两条对称的流水线。例如，在一个有 8 个 PP ranks (GPU) 的设置中：</p>
<ul>
<li>前向流水线 (Forward Pipeline): 数据从 rank 0 -&gt; 1 -&gt; 2 -&gt; 3.</li>
<li>反向流水线 (Backward Pipeline): 同时有另一组数据从 rank 7 -&gt; 6 -&gt; 5 -&gt; 4.
Rank 3 和 Rank 4 成为两条流水线的中间节点，它们之间会交换数据。每个设备实际上会处理两个流水线阶段的模型块，一个用于前向流水线，另一个用于反向流水线。</li>
</ul>
<h2 id="initialization">Initialization</h2>
<ul>
<li>modules: 每个 DualPipe 实例接收一个元组，包含两个 nn.Module. <code>modules[0]</code> 用于处理前向-&gt;反向的计算，<code>modules[1]</code> 用于处理反向-&gt;前向的计算。</li>
<li>Rank 角色判断: 代码会根据当前 rank 的 ID 判断其在整个流水线中的位置（是否是第一个、最后一个、是否在后半部分、是否是中间节点）. 这个角色判断对于后续的通信和计算调度至关重要。例如 <code>is_in_second_half</code> 决定了该 rank 的 phase 0 和 phase 1 究竟对应前向流水线还是反向流水线。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DualPipe</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">modules</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 每个 rank 持有两个模型模块</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">process_group</span> <span class="ow">or</span> <span class="n">dist</span><span class="o">.</span><span class="n">distributed_c10d</span><span class="o">.</span><span class="n">_get_default_group</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算当前 rank 在流水线中的角色</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank_mapping</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="o">.</span><span class="n">rank</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_last_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 判断 rank 是否在对折后的后半部分</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_in_second_half</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 判断是否为中间的 rank</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_ranks</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="core-function-step">Core Function: step</h2>
<p><a href="https://github.com/deepseek-ai/DualPipe/blob/main/dualpipe/dualpipe.py#L294">step</a> 方法是 <code>DualPipe</code> 的核心，它协调了所有 micro-batches 的计算和通信。整个过程被划分为 8 个阶段，以实现最大程度的计算-通信重叠。</p>
<p>输入处理: 只有 rank 0 和 rank N-1 会接收外部输入数据 <code>inputs</code> 和 <code>labels</code>. 这些数据被 <code>scatter</code> (<code>dualpipe/utils.py</code>) 切分成 <code>half_num_chunk</code> 个 micro-batch 。Rank 0 的输入用于前向流水线，Rank N-1 的输入用于反向流水线。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_chunks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 重置状态</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_reset_states</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 将输入数据切分成 micro-batch</span>
</span></span><span class="line"><span class="cl">        <span class="n">inputs</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">half_num_chunks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">labels</span> <span class="o">=</span> <span class="n">scatter</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">half_num_chunks</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">input_chunks</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[])</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">([],</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">input_chunks</span> <span class="o">=</span> <span class="p">([],</span> <span class="n">inputs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">[])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span></code></pre></div><p>接下来是 8 个核心调度阶段的，在此之前会进行一些准备工作：</p>
<ul>
<li>状态重置: <code>_reset_states()</code> 清空上一轮迭代的缓存，如输入/输出块、梯度、损失等。</li>
<li>rank 确定: 计算 <code>num_half_ranks</code>（流水线对折后的一半设备数）和 <code>half_rank</code>（当前秩在对折流水线中的位置. 这些变量将决定每个阶段的循环次数。</li>
<li>数据分发: <code>scatter</code> 函数将输入数据 inputs 和 labels 切分成 half_num_chunks 个 micro-batch 。根据 is_first_rank 或 is_last_rank，将这些 micro-batch 存放到 self.input_chunks 和 self.labels 中。</li>
</ul>
<p>调度示意图如下图所示，红色线分隔了每个步骤</p>
<p>
<figure class="post-figure">
    <a href="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" target="_blank" rel="noopener">
        <img loading="lazy" src="https://share.note.youdao.com/yws/api/personal/file/WEB6ab2cfcee4bad0937e6f0c4c0d225598?method=download&amp;shareKey=754ad9e0092a41faf2c692912283f5ff" alt="DualPipe Schedule">
    </a><figcaption>DualPipe Schedule</figcaption></figure></p>
<p><strong>Step 1: Warm-up Forward nF0</strong></p>
<p>这是一个纯前向计算阶段，用于填满流水线。距离流水线中点越远的 rank（half_rank 越小）执行的预热步骤越多。 <code>_forward_chunk(0)</code> 被调用，在此函数内部:</p>
<ol>
<li><code>_recv_forward(0)</code>: 尝试接收前一个 rank 的数据。对于 rank 0 来说，它直接使用 self.input_chunks 的数据，不接收。</li>
<li><code>_commit_and_wait_comm()</code>: 等待数据接收完成。</li>
<li><code>_forward_compute_chunk(0)</code>: 执行 <code>self.module[0]</code> 的前向计算。</li>
<li><code>_send_forward(0)</code>: 将计算结果异步地发送给下一个 rank.</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">step_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<p><strong>Step 2: Dual Forward nF0F1</strong></p>
<p>两条流水线都开始执行前向计算。两条流水线都开始工作。当前 rank 不仅继续处理 phase 0 的前向计算，也开始处理从另一端（phase 1）传来的数据的前向计算。</p>
<ul>
<li><code>_forward_chunk(0, recv=False, ...)</code> 处理一个 phase 0 的 micro-batch ，但不立即接收下一个，因为前面已经调用了 <code>_recv_forward(0).</code></li>
<li><code>_forward_chunk(1, ...)</code>: 处理一个 phase 1 的 micro-batch 。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 2: nF0F1</span>
</span></span><span class="line"><span class="cl"><span class="n">step_2</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">recv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">send</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">send</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">step_2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_middle_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_send_forward</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<p><strong>Step 3: 前向-后向-权重混合阶段 (Zero Bubble) nB1W1F1</strong></p>
<p>这是 DualPipe 提高效率的关键。当一条流水线开始进行反向计算时，另一条流水线仍在进行前向计算。</p>
<ul>
<li><code>_backward_chunk(1, enable_zb=True)</code>: 执行反向计算，并启用 Zero Bubble (ZB) 优化。ZB 通过 <code>WeightGradStore</code> 将权重梯度（weight gradients）的计算（通常在反向传播中阻塞）缓存起来，推迟执行，从而让路给其他计算或通信。</li>
<li><code>_weight_chunk()</code>: 执行被推迟的权重梯度计算。</li>
<li><code>_forward_chunk(1)</code>: 同时执行另一个方向的前向计算。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 3: nB1W1F1 (Use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_3</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_recv_forward</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">recv</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 4: Main Steady State nF0B1F1B0</strong></p>
<p>这是流水线完全填满后的主循环。在一个循环迭代中，一个 rank 会执行两次计算和通信的重叠操作：一次是（前向计算 + 反向计算），另一次也是（前向计算 + 反向计算）. 这里调用 <code>_forward_backward_chunk(0, 1)</code> 和 <code>_forward_backward_chunk(1, 0)</code>. 这个函数尝试将一个方向的前向计算（F）与另一个方向的反向计算（B）打包在一起执行，实现 F&amp;B 重叠。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 4 (Main step): nF0B1F1B0</span>
</span></span><span class="line"><span class="cl"><span class="n">step_4</span> <span class="o">=</span> <span class="n">half_num_chunks</span> <span class="o">-</span> <span class="n">num_ranks</span> <span class="o">+</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_4</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># i != 0</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 5 &amp; 6: 后向-后向混合阶段 (Cooldown Backward) nB1F1B0 和 nB1B0</strong></p>
<p>当前向数据流耗尽后，流水线进入收尾阶段。这个阶段主要执行剩余的反向计算。同样，ZB 优化在后半段被启用，以减少气泡。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 5: nB1F1B0</span>
</span></span><span class="line"><span class="cl"><span class="n">step_5</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_5</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_forward_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 6: nB1B0 (The second half of the chunks use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_6</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_6</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">step_6</span> <span class="o">//</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">half_rank</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="n">enable_zb</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">step_6</span> <span class="o">//</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">half_rank</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">enable_zb</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="n">enable_zb</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<p><strong>Step 7 &amp; 8: 权重更新收尾阶段 nWB0 和 nW</strong></p>
<ul>
<li>Step 7 将最后的后向计算与权重计算重叠。</li>
<li>Step 8 是纯粹的权重计算阶段，循环调用 _weight_chunk() 直到 WeightGradStore.funcs_queue 队列为空，确保所有梯度都已计算完毕。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 7: nWB0 (Use zero bubble)</span>
</span></span><span class="line"><span class="cl"><span class="n">step_7</span> <span class="o">=</span> <span class="n">num_half_ranks</span> <span class="o">-</span> <span class="n">half_rank</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_7</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_backward_chunk</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">enable_zb</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 8: nW</span>
</span></span><span class="line"><span class="cl"><span class="n">step_8</span> <span class="o">=</span> <span class="n">half_rank</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step_8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="o">.</span><span class="n">_weight_chunk</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">assert</span> <span class="n">WeightGradStore</span><span class="o">.</span><span class="n">funcs_queue</span><span class="o">.</span><span class="n">empty</span><span class="p">()</span>
</span></span></code></pre></div><h2 id="computation-communication-overlap">Computation-Communication Overlap</h2>
<p><code>_forward_backward_compute_chunk</code> 函数是实现计算重叠的关键。在理想情况下（如果模型结构支持），它可以将一个 micro-batch 的前向计算和另一个 micro-batch 的反向计算在同一个函数调用中完成。该函数在 step4 使用的<code>_forward_backward_chunk</code> 函数中被调用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_forward_backward_compute_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">phase0</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">phase1</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">overlapped_forward_backward</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_forward_compute_chunk</span><span class="p">(</span><span class="n">phase0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_compute_chunk</span><span class="p">(</span><span class="n">phase1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># forward &amp; backward</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs0</span><span class="p">,</span> <span class="n">loss0</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">module0</span><span class="p">)</span><span class="o">.</span><span class="n">overlapped_forward_backward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">module0</span><span class="p">,</span> <span class="n">inputs0</span><span class="p">,</span> <span class="n">criterion0</span><span class="p">,</span> <span class="n">labels0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">module1</span><span class="p">,</span> <span class="n">loss1</span><span class="p">,</span> <span class="n">outputs1</span><span class="p">,</span> <span class="n">output_grads1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果模型定义了一个 <code>overlapped_forward_backward</code> (@classmethod)，DualPipe 就会调用它。在这个方法里，开发者可以自定义前向和后向计算的交错执行顺序，以达到最佳的重叠效果。DeepSeek-v3 的重叠方法在技术报告里已经讲解。</p>
<h1 id="real-case">Real Case</h1>
<p>通过 <code>examples/example_dualpipe.py </code>中的 main 函数来详细讲解一个完整的 DualPipe 流程。</p>
<ol>
<li>环境初始化和配置</li>
</ol>
<ul>
<li>分布式设置: main 函数首先初始化 PyTorch 的分布式通信组（init_process_group），并为每个进程（rank）分配一个 GPU.</li>
<li>参数配置: 定义了 micro-batch 数量 (num_chunks)、每个 micro-batch 的大小 (micro_batch_size) 等超参数。</li>
<li>P2P通信设置: 在执行 DualPipe 的 step 方法前，必须调用 <code>set_p2p_tensor_shapes</code> 和 <code>set_p2p_tensor_dtype</code> 来告知 DualPipe 在流水线中传递的张量的形状和数据类型。这是因为 DualPipe 需要预先分配内存来接收来自其他 rank 的数据。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">pp_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 判断当前进程的角色</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_first_rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="n">is_last_rank</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">==</span> <span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 初始化分布式环境</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s2">&#34;env://&#34;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">pp_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">set_default_device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">233</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&#34;CUBLAS_WORKSPACE_CONFIG&#34;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&#34;:4096:8&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 定义流水线参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks</span> <span class="o">=</span> <span class="mi">20</span>
</span></span><span class="line"><span class="cl">    <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">256</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">512</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">pp_size</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">num_chunks</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">seq_len</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">hidden_size</span><span class="si">=}</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 设置P2P通信的Tensor形状和类型</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_p2p_tensor_shapes</span><span class="p">([(</span><span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">    <span class="n">set_p2p_tensor_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>模型和参考基准的创建</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 创建一个完整的、未分割的模型</span>
</span></span><span class="line"><span class="cl"><span class="n">full_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">pp_size</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建完整的输入数据</span>
</span></span><span class="line"><span class="cl"><span class="n">full_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_chunks</span> <span class="o">*</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">full_l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_chunks</span> <span class="o">*</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 参考步骤：在一个GPU上，用标准的数据并行方式运行完整模型，得到基准结果</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_ref</span><span class="p">,</span> <span class="n">output_ref</span> <span class="o">=</span> <span class="n">ref_step</span><span class="p">(</span><span class="n">full_x</span><span class="p">,</span> <span class="n">full_l</span><span class="p">,</span> <span class="n">full_modules</span><span class="p">,</span> <span class="n">num_chunks</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>创建模型: 代码首先创建了一个完整的 <code>nn.Sequential</code> 模型 (full_modules)，它包含了流水线所有的阶段。</li>
<li>参考步骤 (ref_step): 为了验证 DualPipe 的正确性，<code>ref_step</code> 函数模拟了标准的、非流水线并行的训练过程。它将数据分块，依次通过完整模型计算损失和输出。<code>loss_ref</code> 和 <code>output_ref</code> 将作为后续比较的正确答案。</li>
</ul>
<ol start="3">
<li>DualPipe模型的创建和输入准备</li>
</ol>
<ul>
<li>模型分割: 每个 rank r 会持有两个 PipelineStage: 一个是 <code>full_modules[r]</code>，另一个是 <code>full_modules[pp_size - 1 - r]</code>. 这就是 Dual (双向) 的体现。例如，在一个 4-GPU 的设置中：
<ul>
<li>Rank 0 持有 stage 0 和 stage 3 的模型。</li>
<li>Rank 1 持有 stage 1 和 stage 2 的模型。</li>
<li>Rank 2 持有 stage 2 和 stage 1 的模型。</li>
<li>Rank 3 持有 stage 3 和 stage 0 的模型。</li>
</ul>
</li>
<li>输入数据分割: DualPipe 有两个数据入口点：rank 0 和最后一个 rank.
<ul>
<li>rank 0 接收前半部分的输入 (<code>full_x.chunk(2)[0]</code>) 和 后半部分 的标签 (<code>full_l.chunk(2)[1]</code>).</li>
<li>last rank 接收后半部分的输入 (<code>full_x.chunk(2)[1]</code>) 和 前半部分 的标签 (<code>full_l.chunk(2)[0]</code>).</li>
</ul>
</li>
</ul>
<p>一共有两个数据流: 一个从 rank 0 开始，其对应的标签在最后一个 rank；另一个从最后一个 rank 开始，其对应的标签在 rank 0.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># DualPipe 模型创建</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 每个 rank 获取两个处于对称位置的模型块</span>
</span></span><span class="line"><span class="cl"><span class="n">local_full_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">full_modules</span><span class="p">[</span><span class="n">rank</span><span class="p">],</span> <span class="n">full_modules</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">local_modules</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">),</span> <span class="n">PipelineStage</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ... 加载权重 ...</span>
</span></span><span class="line"><span class="cl"><span class="n">dualpipe_model</span> <span class="o">=</span> <span class="n">DualPipe</span><span class="p">(</span><span class="n">local_modules</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># DualPipe输入数据准备</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">full_x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="n">full_l</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">elif</span> <span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">full_x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="n">full_l</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="kc">None</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>执行训练步骤</li>
</ol>
<p>调用 <code>dualpipe_model.step</code>，触发了前面讲解中提到的复杂的8阶段调度流程。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">dualpipe_model</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_chunks</span><span class="o">=</span><span class="n">num_chunks</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">(</span><span class="n">l</span><span class="p">,),</span> <span class="n">return_outputs</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><ol start="5">
<li>结果验证</li>
</ol>
<p>检查损失</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_first_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_ref</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">elif</span> <span class="n">is_last_rank</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">loss_ref</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span>
</span></span></code></pre></div><p>训练步骤完成后，step 方法会返回计算出的损失。</p>
<ul>
<li>rank0 计算出的 loss 对应的是从 last rank 输入的数据流，等于参考损失的后半部分 (<code>loss_ref.chunk(2)[1]</code>).</li>
<li>同理，last rank 计算出的 loss 对应的是从 rank0 输入的数据流，等于参考损失的前半部分 (<code>loss_ref.chunk(2)[0]</code>).</li>
<li>中间的 ranks 不计算最终损失，返回 None.</li>
</ul>
<p>检查梯度</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">local_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">local_modules</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">p0all</span><span class="p">,</span> <span class="n">p0</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_into_tensor</span><span class="p">(</span><span class="n">p1all</span><span class="p">,</span> <span class="n">p1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 手动聚合对称rank的梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">p0</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">p1all</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">p1</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">p0all</span><span class="p">[</span><span class="n">pp_size</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">rank</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">),</span> <span class="n">p_ref</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">local_modules</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">(),</span> <span class="n">local_full_modules</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">cal_diff</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">p_ref</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-13</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>由于每个 rank r 持有 r 和 <code>pp_size - 1 - r</code> 两个阶段的模型，如果这两个阶段在逻辑上是同一个权重（例如，在Encoder-Decoder结构中共享权重），那么它们的梯度需要手动聚合。示例通过 <code>dist.all_gather_into_tensor</code> 收集所有 rank 上对称模块的梯度，然后手动将它们相加。最后，将聚合后的梯度与 ref_step 中计算出的参考梯度进行比较，验证反向传播的正确性。</p>
]]></content:encoded>
    </item>
    <item>
      <title>astra-Sim</title>
      <link>http://localhost:1313/blogs/astra-sim/</link>
      <pubDate>Mon, 09 Jun 2025 13:34:39 +0800</pubDate>
      <guid>http://localhost:1313/blogs/astra-sim/</guid>
      <description>source code reading of astra-sim</description>
      <content:encoded><![CDATA[<h1 id="build-analytical-backend">Build Analytical Backend</h1>
<p><code>build.sh</code> 脚本是构建过程的高级控制器。其核心职责是解析用户意图，执行预构建步骤，并以正确的参数调用底层的 CMake 工具链。</p>
<ol>
<li>
<p><strong>选项解析</strong>: 脚本通过 <code>getopts</code> 处理以下命令行标志：</p>
<ul>
<li><code>-t &lt;target&gt;</code>: 指定编译目标。有效值为 <code>all</code>, <code>congestion_unaware</code>, <code>congestion_aware</code>。此值将作为变量传递给 CMake。</li>
<li><code>-l</code>: 触发清理 (<code>cleanup</code>) 流程，删除所有构建产物并终止脚本。</li>
<li><code>-d</code>: 启用调试 (<code>Debug</code>) 模式进行编译。</li>
</ul>
</li>
<li>
<p><strong>环境准备 (<code>setup</code>, <code>compile_chakra_et</code>)</strong>:</p>
<ul>
<li><code>setup</code> 函数负责创建用于存放中间文件和最终产物的 <code>build</code> 目录，确保源码树的清洁。同时，它会根据系统核心数设置一个上限为 16 的并发编译线程数，以优化编译效率。</li>
<li><code>compile_chakra_et</code> 函数负责处理 <code>et_def.proto</code> 这一 Protobuf 依赖。它检查目标文件是否存在，若不存在，则调用 <code>protoc</code> 编译器生成相应的 C++ 和 Python 源码。</li>
</ul>
</li>
<li>
<p><strong>构建执行 (<code>compile_astrasim_analytical</code>, <code>compile_astrasim_analytical_as_debug</code>)</strong>:</p>
<ul>
<li>这两个函数是脚本与 CMake 交互的核心。它们根据用户是否指定 <code>-d</code> 标志，决定是执行标准 <code>Release</code> 构建还是 <code>Debug</code> 构建。关键在于它们会将用户指定的 <code>build_target</code> 作为 <code>-DBUILDTARGET</code> 参数传递给 CMake。</li>
</ul>
</li>
<li>
<p><strong>后处理 (<code>create_symlink_*</code>)</strong>:</p>
<ul>
<li>编译完成后，<code>create_symlink_congestion_unaware</code> 和 <code>create_symlink_congestion_aware</code> 等函数会为生成的二进制文件创建符号链接。此举旨在维持对旧文件路径的向后兼容性。</li>
</ul>
</li>
</ol>
<hr>
<p><code>CMakeLists.txt</code> 文件是项目的构建蓝图，它向 CMake 阐述了项目的结构、依赖关系以及编译规则。</p>
<ol>
<li>
<p><strong>编译环境设定</strong>:</p>
<ul>
<li><code>cmake_minimum_required(VERSION 3.15)</code>: 规定了运行此配置所需的最低 CMake 版本。</li>
<li><code>set(CMAKE_CXX_STANDARD 17)</code> 和 <code>set(CMAKE_CXX_STANDARD_REQUIRED ON)</code>: 强制项目必须在支持 C++17 标准的编译环境中构建。</li>
</ul>
</li>
<li>
<p><strong>编译标志 (Compiler Flags)</strong>:</p>
<ul>
<li>此文件为不同的构建类型（<code>CMAKE_BUILD_TYPE</code>）定义了不同的编译器标志。</li>
<li><strong><code>Release</code></strong> (默认模式): <code>set(CMAKE_CXX_FLAGS_RELEASE &quot;-O3&quot;)</code> 指示编译器进行高等级优化，以追求最大化程序性能。</li>
<li><strong><code>Debug</code></strong>: <code>set(CMAKE_CXX_FLAGS_DEBUG &quot;...&quot;)</code> 包含一系列用于调试的标志：
<ul>
<li><code>-O0</code>: 关闭所有优化，确保编译后的代码与源码行为一致。</li>
<li><code>-g</code>: 在可执行文件中包含调试符号，这是 GDB 等调试器工作的前提。</li>
<li><code>-fsanitize=address,undefined,leak</code>: 启用 AddressSanitizer、UndefinedBehaviorSanitizer 和 LeakSanitizer。这些是强大的运行时诊断工具，用于捕获内存访问错误、未定义行为及内存泄漏。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>项目结构与依赖</strong>:</p>
<ul>
<li><code>project(AstraSim_Analytical)</code>: 声明项目名称。</li>
<li><code>add_subdirectory(...)</code>: 此指令是组织项目的关键。它将 <code>AstraSim</code> 核心库、<code>Analytical</code> 网络后端和 <code>AstraSim_Analytical</code> 前端等多个子模块纳入构建过程。</li>
</ul>
</li>
<li>
<p><strong>用户自定义选项</strong>:</p>
<ul>
<li><code>set(BUILDTARGET &quot;all&quot; CACHE STRING ...)</code>: 此行定义了一个名为 <code>BUILDTARGET</code> 的可缓存变量。这使得用户可以通过 <code>cmake -D</code> 命令从外部注入该变量的值。此变量随后会被子目录中的 <code>CMakeLists.txt</code> 文件用来实现条件编译。</li>
</ul>
</li>
</ol>
<h1 id="build-ns-3-backend">Build ns-3 Backend</h1>
<p>构建命令为 <code>./build/astra_ns3/build.sh -c</code>，他会执行该脚本里的 compile 函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> compile <span class="o">{</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">NS3_DIR</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl">./ns3 configure --enable-mpi
</span></span><span class="line"><span class="cl">./ns3 build AstraSimNetwork -j <span class="m">12</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="ns3-configure---enable-mpi"><code>./ns3 configure --enable-mpi</code></h2>
<ol>
<li>参数解析 (<code>parse_args</code>): 脚本的 <code>argparse</code> 模块会识别出 <code>configure</code> 子命令和 <code>--enable-mpi</code> 选项。<code>--enable-mpi</code> 是一个预定义的&quot;On-Off&quot;选项，用于控制 MPI (Message Passing Interface) 分布式仿真功能的支持。</li>
<li>进入配置步骤 (<code>configuration_step</code>): 由于检测到 configure 命令，脚本会调用 <code>configuration_step</code> 函数。</li>
<li>调用 CMake (<code>configure_cmake</code>): <code>configuration_step</code> 函数内部会调用 <code>configure_cmake</code>. 这个函数是会动态地构建一个 cmake 命令。
<ul>
<li>它会检测到 <code>--enable-mpi</code> 选项，并通过 <code>on_off_condition</code> 函数将其转换为 CMake 变量 <code>-DNS3_MPI=ON</code>.</li>
<li>最终组装出的命令为为 <code>cmake -S . -B cmake-cache -G &quot;Unix Makefiles&quot; -DCMAKE_BUILD_TYPE=default -DNS3_ASSERT=ON -DNS3_LOG=ON -DNS3_WARNINGS_AS_ERRORS=OFF -DNS3_MPI=ON --warn-uninitialized</code></li>
</ul>
</li>
<li>执行配置: 脚本通过 <code>subprocess.run()</code> 执行这条 cmake 命令</li>
</ol>
<h2 id="ns3-build-astrasimnetwork--j-12"><code>./ns3 build AstraSimNetwork -j 12</code></h2>
<ol>
<li>参数解析 (<code>parse_args</code>): 脚本识别出 <code>build</code> 子命令，目标 <code>AstraSimNetwork</code>，以及并行任务数 <code>-j 12</code>. 前者会被存入 <code>args.build</code> 列表，后者会被存入 <code>args.jobs</code>.</li>
<li>进入构建步骤 (<code>build_step</code>): 脚本检测到 <code>build</code> 命令，并调用 <code>build_step</code> 函数。</li>
<li>调用 CMake 构建 (<code>cmake_build</code>): <code>build_step</code> 函数会遍历 <code>args.build</code> 列表中的所有目标。在这里，它会为 <code>AstraSimNetwork</code> 这个目标调用 <code>cmake_build</code> 函数。
<ul>
<li>cmake_build 函数会组装出一条 <code>cmake --build</code> 命令。</li>
<li>将目标 AstraSimNetwork 转换为 <code>--target AstraSimNetwork</code>.</li>
<li>将并行任务数 12 转换为 <code>-j 12</code>.</li>
<li>最终组装出的命令为 <code>cmake --build cmake-cache --target AstraSimNetwork -j 12</code>.</li>
</ul>
</li>
</ol>
<h1 id="error-when-building-ns-3">Error When Building ns-3</h1>
<h2 id="call-of-overloaded-format-is-ambiguous-">call of overloaded ‘format(&hellip;)’ is ambiguous ❌</h2>
<h3 id="问题诊断-">问题诊断 🩺</h3>
<p>错误信息 <code>call of overloaded ‘format(...)’ is ambiguous</code> 的意思是，编译器在你的代码中遇到了一个名为 <code>format</code> 的函数调用，但它找到了多个同名的、并且参数类型都能匹配的 <code>format</code> 函数定义，导致编译器不知道该选择哪一个，因此产生了“歧义”（ambiguous）。</p>
<p><strong>这个歧义的来源是：</strong></p>
<ol>
<li><strong><code>std::format</code> (来自 C++20 标准库)</strong>: 你的项目很可能正在使用支持 C++20 或更高版本的现代编译器（如 GCC 11+）。C++20 标准库引入了一个新的格式化函数 <code>std::format</code>。</li>
<li><strong><code>fmt::format</code> (来自 {fmt} 库)</strong>: <code>spdlog</code> 这个日志库是基于一个非常流行的第三方格式化库 <code>{fmt}</code> 构建的。这个库也提供了一个功能几乎完全相同的 <code>fmt::format</code> 函数。在 <code>spdlog</code> 的上下文中，它通常可以直接以 <code>format</code> 的形式被调用。</li>
</ol>
<p>当你的代码（这里是 <code>spdlog_setup</code> 的一部分）简单地调用 <code>format(...)</code> 时，如果 C++20 的 <code>&lt;format&gt;</code> 头文件被包含，编译器就会同时看到 <code>std::format</code> 和 <code>spdlog</code> 内部的 <code>fmt::format</code>。由于两者都能处理字符串字面量 (<code>const char[]</code>) 和 <code>std::string</code>，编译器无法决定用哪个，从而报错。</p>
<hr>
<h3 id="关于-using-fmtformat-为何仍然无效的解释">关于 <code>using fmt::format;</code> 为何仍然无效的解释</h3>
<p>原因是，除了常规的命名空间查找规则，C++ 还有一个更强大的规则叫做<strong>参数依赖查找（Argument-Dependent Lookup, ADL）</strong>，有时也被称为 Koenig 查找。</p>
<hr>
<p>我们来梳理一下编译器在看到 <code>format(...)</code> 这行代码时的“思考过程”：</p>
<ol>
<li>
<p><strong>在当前作用域查找</strong></p>
<p>编译器看到了你的 <code>using fmt::format;</code> 声明。很好，它在当前作用域里找到了一个叫做 <code>format</code> 的函数（也就是 <code>fmt::format</code>）。这成为了<strong>候选者 A</strong>。</p>
</li>
<li>
<p><strong>参数依赖查找 (ADL) —— 问题的根源</strong></p>
<p>接下来，编译器会检查 <code>format(...)</code> 函数的所有参数类型。在你的错误日志里，我们看到了 <code>const std::string&amp;</code> 这样的参数。</p>
<ul>
<li>ADL 规则规定：如果一个函数的参数是某个命名空间 <code>N</code> 下的类型（比如 <code>std::string</code> 是 <code>std</code> 命名空间下的），那么编译器<strong>也必须</strong>去那个命名空间 <code>N</code> (这里是 <code>std</code>) 里面去查找同名的函数。</li>
<li>由于 <code>std::string</code> 是 <code>std</code> 命名空间的成员，ADL 规则被触发，编译器自动地去 <code>std</code> 命名空间里寻找名为 <code>format</code> 的函数。</li>
<li>因为你使用了 C++20 编译器，它在 <code>std</code> 命名空间里成功找到了 <code>std::format</code>。这成为了<strong>候选者 B</strong>。</li>
</ul>
</li>
<li>
<p><strong>产生歧义</strong></p>
<p>现在编译器陷入了困境。它手头有两个同样匹配的候选函数：</p>
<ul>
<li><strong>候选者 A</strong>: <code>fmt::format</code> (通过 <code>using</code> 声明找到)</li>
<li><strong>候选者 B</strong>: <code>std::format</code> (通过 ADL 在参数的命名空间里找到)</li>
</ul>
<p><code>using</code> 声明只是将一个名字引入当前作用域，它并**没有足够的“特权”**去压制一个通过 ADL 找到的同样优秀的候选者。因为两个函数都能完美处理你传入的参数，编译器无法做出选择，所以它只能放弃并报告“调用是模糊的 (ambiguous)”。</p>
</li>
</ol>
<h3 id="结论与最终解决方案-">结论与最终解决方案 ✅</h3>
<p>这个 C++ 的特性意味着，只要你的函数参数中包含了 <code>std</code> 命名空间里的类型（如 <code>std::string</code>, <code>std::vector</code> 等），ADL 就有可能被触发，从而把 <code>std</code> 里的函数（如 <code>std::format</code>, <code>std::to_string</code> 等）也拉入候选列表，造成意想不到的冲突。</p>
<p>因此，唯一能 100% 消除歧义、让编译器别无选择的方法，就是使用<strong>显式的命名空间限定</strong>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="c1">// 这样做，是在直接告诉编译器：“别去猜了，我就是要调用 fmt 命名空间里的这个 format！”
</span></span></span><span class="line"><span class="cl"><span class="c1">// 这会完全绕过 ADL 和其他查找规则，直达目标。
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">fmt</span><span class="o">::</span><span class="n">format</span><span class="p">(...);</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="runing-arguments">Runing Arguments</h1>
<p>执行仿真需要传递一些参数，命令模板如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">{</span>ASTRA_SIM_BIN<span class="o">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --workload-configuration<span class="o">=</span><span class="si">${</span><span class="nv">WORKLOAD_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --system-configuration<span class="o">=</span><span class="si">${</span><span class="nv">SYSTEM_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --network-configuration<span class="o">=</span><span class="si">${</span><span class="nv">NETWORK_CONFIG</span><span class="si">}</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --remote-memory-configuration<span class="o">=</span><span class="si">${</span><span class="nv">REMOTE_MEMORY_CONFIG</span><span class="si">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="workload_config">WORKLOAD_CONFIG</h2>
<p>astra-sim 使用的是 Chakra (Execution Trace) 作为 workload 层的输入。将 chakra 作为 python package 安装后有几个命令通过 pyproject.toml 对应到 python函数。</p>
<details class="custom-details">
    <summary class="custom-summary">Explanation of toml file</summary>
    <div><p><code>pyproject.toml</code> 是一个标准化的配置文件，用于定义 Python 项目的元数据、依赖关系以及构建和开发工具的配置。</p>
<hr>
<ol>
<li><code>[build-system]</code> 构建系统配置，这部分定义了如何构建你的 Python 包。</li>
</ol>
<ul>
<li><code>**requires**</code>: 列出了构建项目本身所必需的包。这些是构建环境的依赖，而不是你代码运行时的依赖。
<ul>
<li><code>setuptools</code>, <code>setuptools-grpc</code>: 表明此项目使用 <code>setuptools</code> 作为其构建工具，并需要 <code>setuptools-grpc</code> 插件。</li>
</ul>
</li>
<li><code>**build-backend**</code>: 指定了构建工具中实际执行构建过程的 Python 对象（入口点）。
<ul>
<li><code>setuptools.build_meta</code>: 这是 <code>setuptools</code> 提供的标准构建后端。</li>
</ul>
</li>
</ul>
<hr>
<ol start="2">
<li><code>[project]</code>：这部分包含了项目的基本信息，这些信息会展示在 PyPI (Python Package Index) 上。</li>
</ol>
<ul>
<li><code>**name**</code>: 包的名称，即 <code>pip install chakra</code> 中的 <code>chakra</code>。</li>
<li><code>**requires-python**</code>: 运行此包所需的最低 Python 版本，这里是 <code>3.7</code> 或更高。</li>
<li><code>**version**</code>: 当前包的版本号。</li>
<li><code>**readme**</code>: 指向一个文件，该文件的内容将作为项目在 PyPI 上的详细描述。</li>
<li><code>**license**</code>: 指向包含许可证信息的文件。</li>
<li><code>**authors**</code>：项目的作者信息。</li>
<li><code>**dependencies**</code>: <strong>项目运行时的依赖项</strong>。当用户 <code>pip install chakra</code> 时，这些包也会被一并安装。
<ul>
<li><code>protobuf==5.*</code>: 需要版本为 5.x 的 <code>protobuf</code> 库。</li>
<li><code>graphviz</code>, <code>networkx</code>, <code>pydot</code>: 其他标准的第三方库依赖。</li>
<li><code>HolisticTraceAnalysis @ git+...</code>: 这是一个特殊的依赖。它直接从 GitHub 仓库的一个<strong>特定 commit</strong> (<code>d731cc...</code>) 来安装。这确保了项目依赖于一个稳定且不会意外变动的版本。</li>
</ul>
</li>
</ul>
<hr>
<ol start="3">
<li><code>[project.urls]</code>：项目相关链接，这些链接会显示在 PyPI 页面的侧边栏，为用户提供更多信息的入口。</li>
</ol>
<ul>
<li><code>**Homepage**</code>, <code>**Documentation**</code>, <code>**Repository**</code>: 分别指向项目主页、文档和代码仓库的 URL。</li>
</ul>
<hr>
<ol start="4">
<li><code>[tool.setuptools]</code>：这部分是针对构建工具 <code>setuptools</code> 的详细配置。</li>
</ol>
<ul>
<li><code>**package-dir**</code>: 定义了 Python 包名与实际源代码目录之间的映射关系。
<ul>
<li>例如，<code>&quot;chakra.src.converter&quot; = &quot;src/converter&quot;</code> 表示当用户 <code>import chakra.src.converter</code> 时，Python 会从 <code>src/converter/</code> 目录下寻找代码。这使得项目可以使用 <code>src</code> 布局。</li>
</ul>
</li>
<li><code>**package-data**</code>: 指定需要包含在最终发布包中的非 Python 文件。
<ul>
<li><code>&quot;chakra.schema.protobuf&quot; = [&quot;et_def.proto&quot;]</code>: 表示需要将 <code>et_def.proto</code> 这个文件打包到 <code>chakra.schema.protobuf</code> 这个包里。</li>
</ul>
</li>
</ul>
<hr>
<ol start="5">
<li><code>[project.scripts]</code>：这部分定义了在安装包时应创建的命令行工具。</li>
</ol>
<ul>
<li><code>**chakra_converter = &quot;chakra.src.converter.converter:main&quot;**</code>: 这行配置意味着，当用户安装此包后，他们可以在终端中直接运行 <code>chakra_converter</code> 命令。执行此命令时，系统会调用 <code>chakra.src.converter.converter</code> 模块中的 <code>main</code> 函数。</li>
</ul>
<hr>
<ol start="6">
<li><code>[tool.ruff]</code>：这部分是用于配置 <code>Ruff</code> 高性能代码检查（Linter）和格式化（Formatter）工具。</li>
</ol>
<ul>
<li><code>**target-version**</code>, <code>**line-length**</code>, <code>**exclude**</code>: 基本配置，如目标 Python 版本、每行最大长度和要排除检查的文件。</li>
<li><code>**[tool.ruff.lint]**</code>: Linter 的具体配置。
<ul>
<li><code>**select**</code>: 启用一系列代码规则集（例如 <code>D</code> 代表文档字符串 <code>pydocstyle</code>，<code>I</code> 代表导入排序 <code>isort</code>）。</li>
<li><code>**ignore**</code>: 全局禁用的特定规则。注释中解释了忽略它们的原因（例如，规则冲突或待办事项）。</li>
<li><code>**per-file-ignores**</code>: 针对特定文件或目录禁用规则。例如，<code>&quot;**/tests/*&quot; = [&quot;D&quot;]</code> 表示在所有测试文件中都禁用文档字符串检查。</li>
</ul>
</li>
<li><code>**[tool.ruff.format]**</code>: 格式化器的配置，如使用空格作为缩进风格。</li>
</ul>
<hr>
<ol start="7">
<li><code>[tool.pyright]</code>：这部分配置了 <code>Pyright</code>，一个由微软开发的静态类型检查工具。</li>
</ol>
<ul>
<li><code>**typeCheckingMode**</code>: 类型检查的严格程度，这里是 <code>basic</code>（基础模式）。</li>
<li><code>**exclude**</code>：在进行类型检查时要忽略的文件和目录。</li>
<li><code>**report...**</code>：关闭特定的错误或警告报告。</li>
</ul>
<hr>
<ol start="8">
<li><code>[tool.vulture]</code>：这部分配置了 <code>Vulture</code>，一个用于发现项目中未使用（&ldquo;死&rdquo;）代码的工具。</li>
</ol>
<ul>
<li><code>**ignore_names**</code>: 让 Vulture 忽略某些特定的变量名或函数名，即使它们看起来未使用。</li>
<li><code>**min_confidence**</code>: 设置报告问题的最低置信度阈值。<code>100</code> 表示只有在 Vulture 100% 确定代码是无用的时候才会报告，这可以有效减少误报。</li>
</ul></div>
</details><br>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-toml" data-lang="toml"><span class="line"><span class="cl"><span class="p">[</span><span class="nx">project</span><span class="p">.</span><span class="nx">scripts</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_converter</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.converter.converter:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_generator</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.generator.generator:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_jsonizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.jsonizer.jsonizer:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_timeline_visualizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.timeline_visualizer.timeline_visualizer:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_trace_link</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.trace_link.trace_link:main&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nx">chakra_visualizer</span> <span class="p">=</span> <span class="s2">&#34;chakra.src.visualizer.visualizer:main&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="generate-execution-trace">Generate Execution Trace</h3>
<p>ASTRA-sim 的 ET 命名格式为 <code>{path prefix/trace name}.{npu_id}.et</code>. Chakra ET 的获取流程如下图所示<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<ol>
<li>Collect ET from PyTorch
<ul>
<li>PyTorch ET 负责 CPU 算子，并明确表示它们之间的依赖关系。</li>
<li>Kineto Trace 编码 GPU 算子及其开始和结束时间。</li>
</ul>
</li>
<li>Merge Trace by <code>chkra_trace_link</code>：将它们合并为一个 PyTorch ET+. 该格式本质上遵循 PyTorch ET 的模式，但同时也编码了 GPU 操作符及其依赖关系。</li>
<li>Convert to Chakra ET by <code>chakra_converter</code>

<figure class="post-figure">
    <a href="https://private-user-images.githubusercontent.com/7621438/294028976-67228699-cec5-4a4d-b03e-e76647a80ce8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1NDQxNDUsIm5iZiI6MTc0OTU0Mzg0NSwicGF0aCI6Ii83NjIxNDM4LzI5NDAyODk3Ni02NzIyODY5OS1jZWM1LTRhNGQtYjAzZS1lNzY2NDdhODBjZTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MTBUMDgyNDA1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWE4NzAyMGQ0NWQ0MDA2MzIzMmY1MmNhYWU4YWUzNTJiNjI3OTAzZDk2ZDU3NDIwMWJhZTFlMjNjZDhjN2JmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.-DDH2mackHVASqoCbmyvN2xl8vZemaa73OiLmBER1o0" target="_blank" rel="noopener">
        <img loading="lazy" src="https://private-user-images.githubusercontent.com/7621438/294028976-67228699-cec5-4a4d-b03e-e76647a80ce8.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDk1NDQxNDUsIm5iZiI6MTc0OTU0Mzg0NSwicGF0aCI6Ii83NjIxNDM4LzI5NDAyODk3Ni02NzIyODY5OS1jZWM1LTRhNGQtYjAzZS1lNzY2NDdhODBjZTgucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MTBUMDgyNDA1WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9OWE4NzAyMGQ0NWQ0MDA2MzIzMmY1MmNhYWU4YWUzNTJiNjI3OTAzZDk2ZDU3NDIwMWJhZTFlMjNjZDhjN2JmMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.-DDH2mackHVASqoCbmyvN2xl8vZemaa73OiLmBER1o0" alt="Overview of Trace Collection">
    </a><figcaption>Overview of Trace Collection</figcaption></figure></li>
</ol>
<p>具体的教程和例子可以在 <a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#3-from-raw-traces-to-chakra-a-step-by-step-conversion-guide">Conversion Guide</a> 和 <a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#3-from-raw-traces-to-chakra-a-step-by-step-conversion-guide">Practical Example</a> 找到。</p>
<h3 id="using-et-converter">Using ET Converter</h3>
<p>可以将 astra-sim 1.0 的文本输入转换成 Chakra ET.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">cd</span> ./extern/graph_frontend/chakra/
</span></span><span class="line"><span class="cl">pip3 install .
</span></span><span class="line"><span class="cl">chakra_converter Text <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --input ../../../examples/text_converter/text_workloads/Resnet50_DataParallel.txt <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --output ../../../examples/text_converter/text_workloads/Resnet50_DataParallel <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --num-npus <span class="m">8</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    --num-passes <span class="m">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>workload 文本格式要求如下，其中通信大小单位是字节，计算时间以周期数表示。</p>
<ul>
<li>第一行：(DATA/HYBRID_TRANSFORMER/HYBRID_DLRM)
<ul>
<li>该行指定训练循环的并行化类型。DATA 表示纯数据并行方法，HYBRID_TRANSFORMER 表示专为 Transformer DNN 网络设计的混合并行方法，而 HYBRID_DLRM 表示专为 DLRM DNN 网络优化的混合并行方法。</li>
</ul>
</li>
<li>第二行：(int)
<ul>
<li>该行表示 DNN 的层数。</li>
</ul>
</li>
<li>后续行：每行描述一层。层的描述格式如下：
<ul>
<li>{(string: 层名称)</li>
<li>(int: 保留变量)</li>
<li>(int: 前向计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 前向通信类型)</li>
<li>(int: 前向通信大小)</li>
<li>(int: 输入梯度计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 输入梯度通信类型)</li>
<li>(int: 输入梯度通信大小)</li>
<li>(int: 权重梯度计算时间)</li>
<li>(ALLREDUCE/ALLGATHER/ALLTOALL: 权重梯度通信类型)</li>
<li>(int: 权重梯度通信大小)</li>
<li>(集合通信完成后，权重/输入/输出更新的延迟)}`</li>
</ul>
</li>
</ul>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>每一层的参数写要在同一行！！！</p></div>

<h3 id="enable-communicator-groups">Enable Communicator Groups</h3>
<p>astra-sim 2.0 支持<a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html">通信组</a>。可以通过指定 <code>--comm-group-configuration</code> JSON 文件来指定，默认只有一个通信组。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"><span class="c1">// The first/second communicator group, with ID 0/1, includes GPU IDs from 0-3/4-7. 
</span></span></span><span class="line"><span class="cl"><span class="c1">//   &#34;0&#34;: [0, 1, 2, 3],
</span></span></span><span class="line"><span class="cl"><span class="c1">//   &#34;1&#34;: [4, 5, 6, 7]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="nt">&#34;&lt;communicator_group_id&gt;&#34;</span> <span class="p">:</span> <span class="p">[</span><span class="err">gpu_ids</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h2 id="system_config">SYSTEM_CONFIG</h2>
<h1 id="system-layer">System Layer</h1>
<p>Workload 层会遍历 Chakra ET 中的节点，并为每个节点所指代的操作发出相应的命令。System 层接收这些命令，并将其转换为适合网络、计算或内存后端的格式，从而正确模拟操作。根据操作的类型，系统层的行为会有所不同，具体如下：</p>
<ul>
<li>计算操作：向计算后端发出调用，以模拟操作的持续时间。</li>
<li>内存操作：  内存</li>
<li>通信操作：将集合通信分解为点对点的发送和接收消息，并向网络后端发出“发送”或“接收”调用，以模拟消息的传输过程。</li>
</ul>
<h2 id="collective-scheduler">Collective Scheduler</h2>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-sim-docs/_images/system_overview_queue.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-sim-docs/_images/system_overview_queue.svg" alt="Collective Scheduler">
    </a><figcaption>Collective Scheduler</figcaption></figure></p>
<p>每个队列都有许多 <code>StreamBaseline</code> 对象 (图中右上角)，代表了整个集合通信的流程，<code>phase_to_go</code> 是一个用于表示这些阶段的队列，<code>my_current_phase</code> 是指向当前执行阶段的指针。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">StreamBaseline</span> <span class="o">:</span> <span class="k">public</span> <span class="n">BaseStream</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">public</span><span class="o">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">StreamBaseline</span><span class="p">(</span><span class="n">Sys</span><span class="o">*</span> <span class="n">owner</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">DataSet</span><span class="o">*</span> <span class="n">dataset</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="kt">int</span> <span class="n">stream_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">std</span><span class="o">::</span><span class="n">list</span><span class="o">&lt;</span><span class="n">CollectivePhase</span><span class="o">&gt;</span> <span class="n">phases_to_go</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="kt">int</span> <span class="n">priority</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// my_current_phase[CollectivePhase] is defined in BaseStream
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">void</span> <span class="nf">init</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">call</span><span class="p">(</span><span class="n">EventType</span> <span class="n">event</span><span class="p">,</span> <span class="n">CallData</span><span class="o">*</span> <span class="n">data</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">void</span> <span class="nf">consume</span><span class="p">(</span><span class="n">RecvPacketEventHandlerData</span><span class="o">*</span> <span class="n">message</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于每个 stream <code>proceed_to_next_vnet_baseline</code> (astra-sim/system/Sys.cc) 用于推进通信阶段并且负责在队列之间移动 stream 对象。以下几种情况会调用该函数：</p>
<ol>
<li>stream 第一次被移动出 ready_list 并且将被插入到 <code>active_streams</code>.</li>
<li>stream 完成了一个通信阶段并且等待下一个阶段。</li>
<li>stream 完成了所有的通信阶段。</li>
</ol>
<p>(2-1) 到 (2-5) 描述了该函数的行为</p>
<ol>
<li>
<p>查看当前持有 stream 的队列: 从队列中删除 <code>StreamBaseline</code> 对象 (流的完成顺序可能与它们开始执行的顺序不同)。</p>
</li>
<li>
<p>修改 <code>StreamBaseline</code> 对象: 已完成的集合通信阶段从 <code>phases_to_go</code> 中弹出，<code>my_current_phase</code> 现在指向下一个待执行的阶段。</p>
</li>
<li>
<p>使用 <code>insert_stream</code> 将 <code>StreamBaseline</code> 对象插入到下一个队列中。</p>
</li>
<li>
<p>调用函数 <code>notify_stream_removed</code> 函数查看前一个队列的头部。 <code>stream_pointer</code> 指向队列中第一个未运行的 stream (标记为蓝色)。该函数通过调用 <code>StreamBaseline::init()</code> 来启动 stream 的下一个阶段的执行。</p>
</li>
<li>
<p>使用 <code>notify_stream_added</code> 触发新队列头部 stream 的通信阶段执行。</p>
</li>
</ol>
<p>在其他情况下，<code>proceed_to_next_vnet_baseline</code> 会执行上述步骤的一部分。具体如下：</p>
<ol>
<li>
<p>刚从 <code>ready_list</code> 中移除：<br>
<code>proceed_to_next..</code> 会初始化 stream (1-2)，将其插入到第一个队列中 (1-3)，并触发该队列头部的流执行。</p>
</li>
<li>
<p>stream 完成：<br>
该函数会从之前的队列中删除 stream (3-1)，并触发之前队列头部的 stream 执行。此外，<code>StreamBaseline</code> 对象会被删除，并调用 <code>notify_stream_finished</code>，以通知 <code>Sys</code> 对象 stream 已经结束 (3-6)</p>
</li>
</ol>
<h2 id="collective-implementation">Collective Implementation</h2>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-sim-docs/_images/coll_implementation.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-sim-docs/_images/coll_implementation.svg" alt="Overview of Collective Implementation">
    </a><figcaption>Overview of Collective Implementation</figcaption></figure>
模拟器将集体通信分解为发送和接收消息的方式有两种。目前最常用的方法是模拟器实现一组预定义的常见算法 (例如 Ring、DoubleBinary、HalvingDoubling 等)。这种“原生”实现逻辑位于模拟器的代码库中，允许用户快速探索一组预定义的算法。</p>
<p>自 2024 年 8 月以来，ASTRA-sim 支持了一种新的集合通信算法表示方式。System 层通过暴露一个集体 API，可以接收任意集体算法的定义。</p>
<p>这两种方法都是对 <code>CollectivePhase::Algorithm</code> 对象的实现，该对象是 System 层中的调度单元. <a href="https://github.com/astra-sim/astra-sim/blob/15a4334ade00fe1040fd00495cd13fd1ea5177e4/astra-sim/system/Sys.cc#L1037">generate_collective_phase</a> 会根据不同的算法在创建 <a href="https://github.com/astra-sim/astra-sim/blob/15a4334ade00fe1040fd00495cd13fd1ea5177e4/astra-sim/system/CollectivePhase.hh#L17">CollectivePhase</a> 的时候传入对应的 Algorithm.</p>
<h3 id="astra-sim-native-implementation">ASTRA-Sim Native Implementation</h3>
<p>相关的实现都位于<a href="https://github.com/astra-sim/astra-sim/tree/master/astra-sim/system/collective">该文件夹</a>下, naive 实现的限制是当需要模拟一个新的集合通信算法时算法，必须实现整个集合？随着不规则集合通信 (如 TACOS(Topology Aware CollectiveS), MSCCLang(基于 DSL)) 中工作的增加，快速模拟和迭代各种算法的需求变得越来越多。</p>
<h3 id="chakra-based-arbitrary-definition-through-collective-api">Chakra Based Arbitrary Definition Through Collective API</h3>
<p>因此一个新的 AP来接受任何集合通信算法的定义，而不局限于预定义的规则通信模式。对于通信表示，使用 Chakra ET 模式作为单独的图。将集合通信算法表示为Chakra ET 模式中 COMM_SEND，COMM_RECV 节点的图。也就是说，System 层不是将集合通信分解为发送和接收消息，而是简单地遵循 Chakra 图中已经表示的分解。由于已经使用 Chakra ET 来表示 workload，使用 Chakra ET 来额外定义集合通信算法提供了一种轻松简单的方式来遍历整个图。</p>
<p>如上图所示当 workload 层发出 AllReduce 集体操作时，System 层不会运行模拟器代码库中已有的原生实现逻辑，而是会遍历通过 API 提供的 Chakra ET，该 ET 表示集合通信算法。需要注意 workload Chakra 图和集合通信算法的 Chakra 图是解耦的，并通过不同的输入点提供。最终，asytra-sim 模拟器会将通信节点替换为集体实现。</p>
<h2 id="input-files-for-collective-api">Input Files for Collective API</h2>
<h3 id="astra-sim-native">ASTRA-sim Native</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="s2">&#34;active-chunks-per-dimension&#34;</span><span class="err">:</span> <span class="mi">1</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-reduce-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-gather-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">,</span> <span class="s2">&#34;doubleBinaryTree&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;all-to-all-implementation&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;ring&#34;</span><span class="p">,</span> <span class="s2">&#34;doubleBinaryTree&#34;</span><span class="p">,</span> <span class="s2">&#34;halvingDoubling&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span></code></pre></div><p><code>all-*-implementation</code> 指定了模拟器将如何将给定的集合通信分解为发送和接收消息。All-Gather 操作列表中的两个条目表示模拟器将按两个维度分解 ——第一个维度使用 Ring 算法，第二个维度使用 doubleBinaryTree 算法。</p>
<blockquote class="quote"><p>Native Implementation Requires That the Dimensions for Collective Algorithms Are Same Across All Collectives.</p></blockquote>
<div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p><strong>Native 实现要求所有集体操作的维度必须相同</strong>。换句话说，如果一个集合通信算法被定义为二维的，那么其他集合通信算法也必须是二维操作。上述只是一个例子。</p></div>

<h3 id="collective-api">Collective API</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="s2">&#34;active-chunks-per-dimension&#34;</span><span class="err">:</span> <span class="mi">1</span><span class="err">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;all-reduce-implementation-chakra&#34;</span><span class="err">:</span> <span class="p">[</span><span class="s2">&#34;/app/hoti2024/demo5/inputs/custom_ring&#34;</span><span class="p">]</span><span class="err">,</span>
</span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span></code></pre></div><p>需要注意这里要使用 <code>all-*-implementation-chakra</code>，而不是 <code>all-*-implementation</code>. 另外  Chakra ET 文件与传递给 workload 层的文件是不同的，每一项的值是 Chakra ET 文件的绝对路径，不包括最后的 <code>{rank}.et</code> 字符串 (类似于 Workload 层输入)。此外，即使有许多维度，列表也只接受一个值。这是因为跨维度通信的概念已经包含在 ET 中。</p>
<div class="github">
    <div class="github_bar">
        <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" viewBox="0 0 50 50"><path d="M17.791,46.836C18.502,46.53,19,45.823,19,45v-5.4c0-0.197,0.016-0.402,0.041-0.61C19.027,38.994,19.014,38.997,19,39 c0,0-3,0-3.6,0c-1.5,0-2.8-0.6-3.4-1.8c-0.7-1.3-1-3.5-2.8-4.7C8.9,32.3,9.1,32,9.7,32c0.6,0.1,1.9,0.9,2.7,2c0.9,1.1,1.8,2,3.4,2 c2.487,0,3.82-0.125,4.622-0.555C21.356,34.056,22.649,33,24,33v-0.025c-5.668-0.182-9.289-2.066-10.975-4.975 c-3.665,0.042-6.856,0.405-8.677,0.707c-0.058-0.327-0.108-0.656-0.151-0.987c1.797-0.296,4.843-0.647,8.345-0.714 c-0.112-0.276-0.209-0.559-0.291-0.849c-3.511-0.178-6.541-0.039-8.187,0.097c-0.02-0.332-0.047-0.663-0.051-0.999 c1.649-0.135,4.597-0.27,8.018-0.111c-0.079-0.5-0.13-1.011-0.13-1.543c0-1.7,0.6-3.5,1.7-5c-0.5-1.7-1.2-5.3,0.2-6.6 c2.7,0,4.6,1.3,5.5,2.1C21,13.4,22.9,13,25,13s4,0.4,5.6,1.1c0.9-0.8,2.8-2.1,5.5-2.1c1.5,1.4,0.7,5,0.2,6.6c1.1,1.5,1.7,3.2,1.6,5 c0,0.484-0.045,0.951-0.11,1.409c3.499-0.172,6.527-0.034,8.204,0.102c-0.002,0.337-0.033,0.666-0.051,0.999 c-1.671-0.138-4.775-0.28-8.359-0.089c-0.089,0.336-0.197,0.663-0.325,0.98c3.546,0.046,6.665,0.389,8.548,0.689 c-0.043,0.332-0.093,0.661-0.151,0.987c-1.912-0.306-5.171-0.664-8.879-0.682C35.112,30.873,31.557,32.75,26,32.969V33 c2.6,0,5,3.9,5,6.6V45c0,0.823,0.498,1.53,1.209,1.836C41.37,43.804,48,35.164,48,25C48,12.318,37.683,2,25,2S2,12.318,2,25 C2,35.164,8.63,43.804,17.791,46.836z"></path></svg>
        <a class="github_name" href="https://github.com/astra-sim/collectiveapi" target="_blank">Collective API</a>
    </div>
    <div class="github_description">参考该仓库实现</div>
    <div class="github_language">
        
    </div>
</div>

<h1 id="network-backend">Network Backend</h1>
<h2 id="analytical-network-backend">Analytical Network Backend</h2>
<p>Analytical Network 模拟器通过数学方程模拟所有网络行为。因此，该后端最适合于大规模分布式平台的建模和仿真。目前支持两种分析模式</p>
<ul>
<li>congestion_<strong>unaware</strong> analytical network simulator</li>
<li>congestion_<strong>aware</strong> analytical network simulator</li>
</ul>
<hr>
<ul>
<li>T<strong>Topology</strong></li>
</ul>
<p>Analytical Network 支持三种拓扑结构: Ring, FullConnected, Switch. 并且可以堆叠来表示多维网络。</p>
<p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/network-building-blocks.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/network-building-blocks.svg" alt="Basic Network Building Block">
    </a><figcaption>Basic Network Building Block</figcaption></figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">topology</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="l">Ring, Switch ] </span><span class="w"> </span><span class="c"># 2D topology</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">topology</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="l">Ring, Ring, Ring ] </span><span class="w"> </span><span class="c"># 3D topology</span><span class="w">
</span></span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/multidim-network-example.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/multidim-network-example.svg" alt="Example of 2D &amp; 3D Topologies">
    </a><figcaption>Example of 2D &amp; 3D Topologies</figcaption></figure></p>
<hr>
<ul>
<li><strong>NPUs Count</strong></li>
</ul>
<p>指定了每个维度上的设备数目</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 5 NPUs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 4 × 2 = 8 NPUs</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">npus_count</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="p">]</span><span class="w">  </span><span class="c"># 4 × 2 × 2 = 16 NPUs</span><span class="w">
</span></span></span></code></pre></div><p>
<figure class="post-figure">
    <a href="https://astra-sim.github.io/astra-network-analytical-docs/_images/npus-count-example.svg" target="_blank" rel="noopener">
        <img loading="lazy" src="https://astra-sim.github.io/astra-network-analytical-docs/_images/npus-count-example.svg" alt="NPUs Count Example">
    </a><figcaption>NPUs Count Example</figcaption></figure></p>
<hr>
<ul>
<li><strong>Bandwidth</strong> &amp; <strong>Latency</strong></li>
</ul>
<p><code>latency</code> 定义了每条单向链路的延迟 (ns).
<code>bandwidth</code> 定义了每条单向链路的带宽 (GB/s).</p>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>$1 GB = 2^{30} B$ and $1 s = 10^9 ns$</p></div>

<h2 id="ns3-backend">ns3 backend</h2>
<p>下面是用 ns3 后端进行方针的一个执行命令。这里使用了 <code>--network-backend</code> 和 <code>--logical-topology</code> 这两个参数。需要说明的是，Analytical Backend 中仅使用了-<code>-network-backend</code> 参数，这是因为分析型后端的逻辑拓扑与物理拓扑是相同的，而 ns3 则允许我们将逻辑拓扑与物理拓扑分离。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">   <span class="c1"># {NS3_DIR} is the directory of the ns-3 backend. That is, &#39;{ASTRA_SIM_ROOT_DIRECTORY}/extern/network_backend/ns-3&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="nb">cd</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">NS3_DIR</span><span class="si">}</span><span class="s2">/build/scratch&#34;</span>
</span></span><span class="line"><span class="cl">    ./ns3.42-AstraSimNetwork-default <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --workload-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../extern/graph_frontend/chakra/one_comm_coll_node_allgather  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --system-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/system/Switch.json  <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --network-configuration<span class="o">=</span><span class="s2">&#34;../../../ns-3/scratch/config/config.txt&#34;</span>   <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --remote-memory-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/remote_memory/analytical/no_memory_expansion.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --logical-topology-configuration<span class="o">=</span><span class="s2">&#34;</span><span class="si">${</span><span class="nv">SCRIPT_DIR</span><span class="p">:?</span><span class="si">}</span><span class="s2">&#34;</span>/../../inputs/network/ns3/sample_8nodes_1D.json   <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --comm-group-configuration<span class="o">=</span><span class="se">\&#34;</span>empty<span class="se">\&#34;</span>
</span></span></code></pre></div><hr>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://github.com/mlcommons/chakra/wiki/Chakra-Execution-Trace-Collection-%E2%80%90-A-Comprehensive-Guide-on-Merging-PyTorch-and-Kineto-Traces#2-overview-of-trace-collection-and-simulation-methodology">Overview of Trace Collection</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>xDiT Principle</title>
      <link>http://localhost:1313/blogs/xdit/</link>
      <pubDate>Sat, 07 Jun 2025 20:44:50 +0800</pubDate>
      <guid>http://localhost:1313/blogs/xdit/</guid>
      <description>This is a brief introduction to the xDiT Principle.</description>
      <content:encoded><![CDATA[<h1 id="parse-config-arguments">Parse Config Arguments</h1>
<p>会从命令行参数中获取有关 Model, Runtime, Parallel Processing &amp; Input 有关的信息。前三者被包含在 <code>engine_config</code> 中，而最后者则被包含在 <code>input_config</code> 中。在 <code>create_config()</code> 函数中，会初始化 <code>_WORLD</code> 全局变量，它是一个 <code>GroupCoordinator</code> 实例。很明显它只有一个包含所有的设备进程组。
<details class="custom-details">
    <summary class="custom-summary">GroupCoordinator</summary>
    <div><p><code>GroupCoordinator</code> 类是一个 PyTorch 的进程组封装器，主要用于管理一组进程之间的通信。它可以根据不同的通信后端（如 NCCL、Gloo、MPI 等）来协调进程之间的操作。包含以下信息</p>
<ul>
<li><code>rank</code>: 当前进程的全局索引（全局唯一）。</li>
<li><code>ranks</code>: 组内所有进程的全局索引列表。</li>
<li><code>world_size</code>: 组的大小，即进程的数量 <code>len(ranks)</code></li>
<li><code>local_rank</code>: 当前进程在本地节点中的索引。</li>
<li><code>rank_in_group</code>: 当前进程在组内的索引。</li>
<li><code>cpu_group</code>: 用于 CPU 通信的进程组。</li>
<li><code>device_group</code>: 用于设备（如 GPU）通信的进程组。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">we</span> <span class="n">have</span> <span class="n">a</span> <span class="n">group</span> <span class="n">of</span> <span class="n">size</span> <span class="mi">4</span> <span class="n">across</span> <span class="n">two</span> <span class="n">nodes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="n">Process</span> <span class="o">|</span> <span class="n">Node</span> <span class="o">|</span> <span class="n">Rank</span> <span class="o">|</span> <span class="n">Local</span> <span class="n">Rank</span> <span class="o">|</span> <span class="n">Rank</span> <span class="ow">in</span> <span class="n">Group</span>
</span></span><span class="line"><span class="cl">  <span class="mi">0</span>     <span class="o">|</span>   <span class="mi">0</span>  <span class="o">|</span>  <span class="mi">0</span>   <span class="o">|</span>     <span class="mi">0</span>      <span class="o">|</span>       <span class="mi">0</span>
</span></span><span class="line"><span class="cl">  <span class="mi">1</span>     <span class="o">|</span>   <span class="mi">0</span>  <span class="o">|</span>  <span class="mi">1</span>   <span class="o">|</span>     <span class="mi">1</span>      <span class="o">|</span>       <span class="mi">1</span>
</span></span><span class="line"><span class="cl">  <span class="mi">2</span>     <span class="o">|</span>   <span class="mi">1</span>  <span class="o">|</span>  <span class="mi">2</span>   <span class="o">|</span>     <span class="mi">0</span>      <span class="o">|</span>       <span class="mi">2</span>
</span></span><span class="line"><span class="cl">  <span class="mi">3</span>     <span class="o">|</span>   <span class="mi">1</span>  <span class="o">|</span>  <span class="mi">3</span>   <span class="o">|</span>     <span class="mi">1</span>      <span class="o">|</span>       <span class="mi">3</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>__init__</code> 方法接收以下参数：</p>
<ul>
<li><code>group_ranks</code>: 一个包含多个进程索引列表的列表，每个子列表表示一个进程组。</li>
<li><code>local_rank</code>: 当前进程的本地索引。</li>
<li><code>torch_distributed_backend</code>: 指定用于通信的后端类型 (如 &ldquo;gloo&rdquo; 或 &ldquo;nccl&rdquo;).</li>
</ul>
<p>初始化过程：</p>
<ol>
<li>使用 <code>torch.distributed.get_rank()</code> 获取当前进程的全局索引。</li>
<li>遍历传入的 <code>group_ranks</code> 列表，为每个子列表创建一个新的设备组和 CPU 组。</li>
<li>如果当前进程的索引在当前子列表中，则设置该进程的组内信息 (包括 <code>ranks</code>、<code>world_size</code> 和 <code>rank_in_group</code>).</li>
<li>确保 CPU 组和设备组都已成功创建。</li>
<li>根据是否可用 CUDA 设置当前设备为 GPU 或 CPU.</li>
</ol>
</div>
</details><br></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">parser</span> <span class="o">=</span> <span class="n">FlexibleArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&#34;xFuser Arguments&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">xFuserArgs</span><span class="o">.</span><span class="n">add_cli_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>  <span class="c1"># Add Command Line Interface (CLI) arguments</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_args</span> <span class="o">=</span> <span class="n">xFuserArgs</span><span class="o">.</span><span class="n">from_cli_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>  <span class="c1"># Extract CLI args and pass them to xFuserArgs Constructor</span>
</span></span><span class="line"><span class="cl">    <span class="n">engine_config</span><span class="p">,</span> <span class="n">input_config</span> <span class="o">=</span> <span class="n">engine_args</span><span class="o">.</span><span class="n">create_config</span><span class="p">()</span>  <span class="c1"># Init _WORLD. engine_config: model, run_time &amp; parallel infos, input_config: input shape, prompt &amp; sampler infos</span>
</span></span><span class="line"><span class="cl">    <span class="n">local_rank</span> <span class="o">=</span> <span class="n">get_world_group</span><span class="p">()</span><span class="o">.</span><span class="n">local_rank</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>关于可以支持的并行策略如下，包括 Data Parallel, Sequence Parallel, Pipefusion Parallel &amp; Tensor Parallel.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Parallel Processing Options:
</span></span><span class="line"><span class="cl">  --use_cfg_parallel    Use split batch in classifier_free_guidance. cfg_degree will be <span class="m">2</span> <span class="k">if</span> <span class="nb">set</span>
</span></span><span class="line"><span class="cl">  --data_parallel_degree DATA_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Data parallel degree.
</span></span><span class="line"><span class="cl">  --ulysses_degree ULYSSES_DEGREE
</span></span><span class="line"><span class="cl">                        Ulysses sequence parallel degree. Used in attention layer.
</span></span><span class="line"><span class="cl">  --ring_degree RING_DEGREE
</span></span><span class="line"><span class="cl">                        Ring sequence parallel degree. Used in attention layer.
</span></span><span class="line"><span class="cl">  --pipefusion_parallel_degree PIPEFUSION_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Pipefusion parallel degree. Indicates the number of pipeline stages.
</span></span><span class="line"><span class="cl">  --num_pipeline_patch NUM_PIPELINE_PATCH
</span></span><span class="line"><span class="cl">                        Number of patches the feature map should be segmented in pipefusion parallel.
</span></span><span class="line"><span class="cl">  --attn_layer_num_for_pp <span class="o">[</span>ATTN_LAYER_NUM_FOR_PP ...<span class="o">]</span>
</span></span><span class="line"><span class="cl">                        List representing the number of layers per stage of the pipeline in pipefusion parallel
</span></span><span class="line"><span class="cl">  --tensor_parallel_degree TENSOR_PARALLEL_DEGREE
</span></span><span class="line"><span class="cl">                        Tensor parallel degree.
</span></span><span class="line"><span class="cl">  --split_scheme SPLIT_SCHEME
</span></span><span class="line"><span class="cl">                        Split scheme <span class="k">for</span> tensor parallel.
</span></span></code></pre></td></tr></table>
</div>
</div><p>从 CLI 解析的参数后会在 <code>create_config()</code> 中组成如下的 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/config/config.py#L185">ParallelConfig</a>.</p>
<ul>
<li><code>DataParallelConfig</code>: 总的并行度为 <code>dp_degree * cfg_degree</code>.
<ul>
<li><code>dp_degree</code>: 相当于对 batch 维度进行切分，</li>
<li><code>cfg_degree</code>: Class-free Guidance(cfg) 用于控制无条件的图片生成 (若使用相当于 <code>batchsize *= 2</code>).</li>
</ul>
</li>
<li><code>SequenceParallelConfig</code>: 总的并行度为 <code>sp_degree = ulysses_degree * ring_degree</code>
<ul>
<li><code>ulysses_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2309.14509">DeepSeed-Ulesses</a> 的序列并行度。</li>
<li><code>ring_degree</code>: 用于控制计算 Ring Attention 时对 Q K V 沿着 Sequence 维度的切分块数。</li>
</ul>
</li>
<li><code>TensorParallelConfig</code>: 总的并行度为 <code>tp_degree</code>.
<ul>
<li><code>tp_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2104.05343">2D Tensor Parallel</a> 的并行度。</li>
<li><code>split_scheme</code>: 用于控制张量切分方式.</li>
</ul>
</li>
<li><code>PipeFusionParallelConfig</code>: 总的并行度为 <code>pp_degree=num_pipeline_patch</code>.
<ul>
<li><code>pp_degree</code>: 用于控制 <a href="https://arxiv.org/abs/2112.11446">PipeFusion</a> 中模型 Transoformer Blocks 的切分个数。</li>
<li><code>num_pipeline_patch</code>: 用于控制对 latent feature map 的切分块数.</li>
<li><code>attn_layer_num_for_pp</code>: 是一个 list，表示 <code>pp_degree</code> 里每个 stage 的 Transformer 层数。</li>
</ul>
</li>
</ul>
<style type="text/css">
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     
    @media (prefers-color-scheme:dark) {
        .notice {
            --title-color: #fff;
            --title-background-color: #069;
            --content-color: #ddd;
            --content-background-color: #023;
        }

        .notice.info {
            --title-background-color: #a50;
            --content-background-color: #420;
        }

        .notice.tip {
            --title-background-color: #363;
            --content-background-color: #121;
        }

        .notice.warning {
            --title-background-color: #800;
            --content-background-color: #400;
        }
    }

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
</style><div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>关于 PipeFusion，原文说切分的 patch 数和 pipeline 大小可以不同，但这里要求 <code>len(attn_layer_num_for_pp)=pp_degree</code></p></div>

<div class="notice info" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="92 59.5 300 300">
  <path d="M292 303.25V272c0-3.516-2.734-6.25-6.25-6.25H267v-100c0-3.516-2.734-6.25-6.25-6.25h-62.5c-3.516 0-6.25 2.734-6.25 6.25V197c0 3.516 2.734 6.25 6.25 6.25H217v62.5h-18.75c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h87.5c3.516 0 6.25-2.734 6.25-6.25Zm-25-175V97c0-3.516-2.734-6.25-6.25-6.25h-37.5c-3.516 0-6.25 2.734-6.25 6.25v31.25c0 3.516 2.734 6.25 6.25 6.25h37.5c3.516 0 6.25-2.734 6.25-6.25Zm125 81.25c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z"/>
</svg>

        </span>Info</p><p>设备数必须等于 <code>dp_degree * cfg_degree * sp_degree * tp_degree * num_pipeline_patch</code>，并且 <code>pp_degree</code> 必须小于等于设备数。
<code>ulysses_degree</code> 必须要大于且能被 attention 的头数整除。</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">ParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">dp_config</span><span class="o">=</span><span class="n">DataParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">dp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">use_cfg_parallel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_cfg_parallel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">sp_config</span><span class="o">=</span><span class="n">SequenceParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">ulysses_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ulysses_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">ring_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ring_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">tp_config</span><span class="o">=</span><span class="n">TensorParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">tp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">split_scheme</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">split_scheme</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_config</span><span class="o">=</span><span class="n">PipeFusionParallelConfig</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">pp_degree</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pipefusion_parallel_degree</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_pipeline_patch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_layer_num_for_pp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_layer_num_for_pp</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">),</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="construct-pipeline">Construct Pipeline</h1>
<p>解析完配置参数并构建了 <code>engine_config</code> 后，下一步是构建模型的 pipeline.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">pipe</span> <span class="o">=</span> <span class="n">xFuserPixArtAlphaPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>  <span class="c1"># First construct a PixArtAlphaPipeline, then pass it and engine_config to xFuserPipelineBaseWrapper</span>
</span></span><span class="line"><span class="cl">        <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">engine_config</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">engine_config</span><span class="o">=</span><span class="n">engine_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pipe</span><span class="o">.</span><span class="n">prepare_run</span><span class="p">(</span><span class="n">input_config</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>xFuserPixArtAlphaPipeline 继承自 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/model_executor/pipelines/base_pipeline.py#L61">xFuserPipelineBaseWrapper</a>，_init_runtime_state 函数经过一番调用后会使用 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/core/distributed/parallel_state.py#L265">initialize_model_parallel</a> 初始化 <code>_RUNTIME</code> 有关模型参数的部分和模型并行的全局变量 <code>_DP, _CFG, _PP, _SP, _TP</code>，它是一个 DiTRuntimeState (继承 RuntimeState) 实例，记录了每个 Group 包含的设备索引，除此之外还包括 PipeFusionParallel 中有关 patch 索引的参数 (在稍后 pipeline 执行的时候计算).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xFuserPipelineBaseWrapper</span><span class="p">(</span><span class="n">xFuserBaseWrapper</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">pipeline</span><span class="p">:</span> <span class="n">DiffusionPipeline</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">engine_config</span><span class="p">:</span> <span class="n">EngineConfig</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">:</span> <span class="n">DiffusionPipeline</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_init_runtime_state</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">engine_config</span><span class="o">=</span><span class="n">engine_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># backbone</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;transformer&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">unet</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;unet&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># vae</span>
</span></span><span class="line"><span class="cl">        <span class="n">vae</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;vae&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># scheduler</span>
</span></span><span class="line"><span class="cl">        <span class="n">scheduler</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="s2">&#34;scheduler&#34;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">transformer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_transformer_backbone</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">unet</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">unet</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_unet_backbone</span><span class="p">(</span><span class="n">unet</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pipeline</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_scheduler</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">pipeline</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">   
</span></span><span class="line"><span class="cl">   
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_convert_transformer_backbone</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">				<span class="c1">#...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&#34;Transformer backbone found, paralleling transformer...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">wrapper</span> <span class="o">=</span> <span class="o">**</span><span class="n">xFuserTransformerWrappersRegister</span><span class="o">.</span><span class="n">get_wrapper</span><span class="p">(</span><span class="n">transformer</span><span class="p">)</span><span class="o">**</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span> <span class="o">=</span> <span class="n">wrapper</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">transformer</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="initialize_model_parallel">initialize_model_parallel</h2>
<p>该函数中会初始化一个 <code>RankGenerator</code>，它接收每个并行方法的设备组大小和并行度大小顺序。其主要的方法是通过 <a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/core/distributed/utils.py#L4">generate_masked_orthogonal_rank_groups</a> 函数确定每个并行组由包含哪些设备，先把并行方法按照并行度从小到大排列成 <code>tp-sp-pp-cfg-dp</code>. 再根据要生成的并行组产生对应的 <code>mask</code>. 即如果要生成 <code>pp</code> 组对应的 rank，那么 <code>mask = [0, 0, 1, 0, 0]</code></p>
<p>该函数首先会生成需要生成的并行组的大小组成的 masked_shape 和不需要生成的 unmasked_shape. 首先要用 prefix_product 计算 <code>global_stride</code>，即每个并行度的设备组包含几个设备。再根据 <code>mask</code> 取出对应的 <code>mask_stride</code> 和 <code>unmaskd_stride</code>. <code>group_size = mask_stride[-1]</code> 即为最大并行度的组包含的设备数。<code>num_of_group = num_of_device / mask_stride[-1]</code> 即为要生成几个并行度最大的组。先遍历要生成的每个设备组，并用 decompose 函数确定该设备组在不需要并行维度上的索引；再遍历该组中的每个设备的 lock rank，确定该设备在需要并行维度上的索引，最后用 inner_product 确定该设备的 global rank.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_masked_orthogonal_rank_groups</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">parallel_size</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">mask</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">prefix_product</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">init</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>  <span class="c1"># Exclusive</span>
</span></span><span class="line"><span class="cl">        <span class="n">r</span> <span class="o">=</span> <span class="p">[</span><span class="n">init</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">a</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">init</span> <span class="o">=</span> <span class="n">init</span> <span class="o">*</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">            <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">r</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">inner_product</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">b</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">decompose</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># index: 第几个并行组  # shape: 并行组大小的 list</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function solve the math problem below:
</span></span></span><span class="line"><span class="cl"><span class="s2">            There is an equation: index = sum(idx[i] * stride[i])
</span></span></span><span class="line"><span class="cl"><span class="s2">            And given the value of index, stride.
</span></span></span><span class="line"><span class="cl"><span class="s2">            Return the idx.
</span></span></span><span class="line"><span class="cl"><span class="s2">        This function will used to get the pp/dp/pp_rank from group_index and rank_in_group.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">stride</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">idx</span> <span class="o">=</span> <span class="p">[(</span><span class="n">index</span> <span class="o">//</span> <span class="n">d</span><span class="p">)</span> <span class="o">%</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">)]</span>  <span class="c1">#  计算在每个并行维度上的索引</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># stride is a prefix_product result. And the value of stride[-1]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># is not used.</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stride</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])])</span> <span class="o">==</span> <span class="n">index</span>
</span></span><span class="line"><span class="cl">        <span class="p">),</span> <span class="s2">&#34;idx </span><span class="si">{}</span><span class="s2"> with shape </span><span class="si">{}</span><span class="s2"> mismatch the return idx </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">idx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">masked_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="p">]</span>  <span class="c1"># 需要采取并行的维度</span>
</span></span><span class="line"><span class="cl">    <span class="n">unmasked_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="p">]</span>  <span class="c1"># 不需要的</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">global_stride</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">parallel_size</span><span class="p">)</span>  <span class="c1"># exclusive 前缀积 表示大的并行维度包括几个设备</span>
</span></span><span class="line"><span class="cl">    <span class="n">masked_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_stride</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">m</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">unmasked_stride</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">global_stride</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">m</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">group_size</span> <span class="o">=</span> <span class="n">prefix_product</span><span class="p">(</span><span class="n">masked_shape</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 最大的一个并行维度包括几个设备</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_of_group</span> <span class="o">=</span> <span class="n">world_size</span> <span class="o">//</span> <span class="n">group_size</span>  <span class="c1"># 分成几个大组</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>  
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">group_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_group</span><span class="p">):</span>  <span class="c1"># 遍历每个设备组</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get indices from unmaksed for group_index.</span>
</span></span><span class="line"><span class="cl">        <span class="n">decomposed_group_idx</span> <span class="o">=</span> <span class="n">decompose</span><span class="p">(</span><span class="n">group_index</span><span class="p">,</span> <span class="n">unmasked_shape</span><span class="p">)</span>  <span class="c1"># 得到在不需要采取并行的维度上的索引</span>
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">rank_in_group</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_size</span><span class="p">):</span>  <span class="c1"># 遍历该组中的每个设备 local rank</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># get indices from masked for rank_in_group.</span>
</span></span><span class="line"><span class="cl">            <span class="n">decomposed_rank_idx</span> <span class="o">=</span> <span class="n">decompose</span><span class="p">(</span><span class="n">rank_in_group</span><span class="p">,</span> <span class="n">masked_shape</span><span class="p">)</span>  <span class="c1"># 得到最大并行组的每个设备在采取并行的维度上的索引</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>  <span class="o">//</span> <span class="n">相加得到全局rank</span>
</span></span><span class="line"><span class="cl">                <span class="n">inner_product</span><span class="p">(</span><span class="n">decomposed_rank_idx</span><span class="p">,</span> <span class="n">masked_stride</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">                <span class="o">+</span> <span class="n">inner_product</span><span class="p">(</span><span class="n">decomposed_group_idx</span><span class="p">,</span> <span class="n">unmasked_stride</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">ranks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">ranks</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="hybrid-parallelsim-design">Hybrid Parallelsim Design</h2>
<p>xDiT支持四种并行方式：PipeFusion、Sequence、Data 和 CFG Parallel。其中，Data 和 CFG Parallel在图像间并行相对简单，而 PipeFusion和 Sequence 在图像内部的不同 Patch 间并行则较为复杂。能</p>
<p>PipeFusion 利用 Input Tempor Redundancy特点，使用过时的 KV（Stale KV）进行 Attention 计算，这使得 PipeFusion 无法像大型语言模型那样轻松地实现并行策略的混合。使用标准的序列并行接口，如RingAttention、Ulysses或 USP，无法满足 SP 与PipeFusion混合并行的需求。</p>
<p>我们对这个问题具体说明，下图展示了pipe_degree=4，sp_degree=2的混合并行方法。设置 <code>num_pipeline_patch</code>=4，图片切分为 M=<code>num_pipeline_patch*sp_degree</code>=8 个 Patch，分别是 P0~P7.</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/hybrid_pp_scheme.png" alt="hybrid process group config"  width="60%">
</div>
<p>Standard SP Attention 的输入Q，K，V 和输出 O 都是沿着序列维度切分，且切分方式一致。如果不同 rank 的输入 patch 没有重叠，每个 micro step 计算出 fresh KV 更新的位置在不同 rank 间也没有重叠。如下图所示，standard SP 的 KV Buffer 中黄色部分是 SP0 rank=0 拥有的 fresh KV，绿色部分是 SP1 rank=1 拥有的fresh KV，二者并不相同。在这个 diffusion step 内，device=0 无法拿到 P1,3,5,7 的 fresh KV 进行计算，但是 PipeFusion 则需要在下一个 diffusion step 中，拥有上一个diffusion step 全部的 KV. standard SP 只拥有 1/sp_degree 的 fresh kv buffer，因此无法获得混合并行推理正确的结果。</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/hybrid_workflow.png" alt="hybrid parallel workflow">
</div>
<p>xDiT专门定制了序列并行的实现方式，以适应这种混合并行的需求。xDiT使用 <code>xFuserLongContextAttention</code> 把SP的中间结果存在 KV Buffer 内。效果如下图，每个 micro-step SP 执行完毕后，SP Group 内不同 rank 设备的 fresh KV是 replicate 的。这样一个 diffusion step 后，SP Group 所有设备的 KV Buffer 都更新成最新，供下一个 Diffusion Step 使用。</p>
<div align="center">
    <img src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/kvbuffer_hybrid.png" alt="kvbuffer in hybrid parallel">
</div>
<div class="notice note" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 128 300 300">
  <path d="M150 128c82.813 0 150 67.188 150 150 0 82.813-67.188 150-150 150C67.187 428 0 360.812 0 278c0-82.813 67.188-150 150-150Zm25 243.555v-37.11c0-3.515-2.734-6.445-6.055-6.445h-37.5c-3.515 0-6.445 2.93-6.445 6.445v37.11c0 3.515 2.93 6.445 6.445 6.445h37.5c3.32 0 6.055-2.93 6.055-6.445Zm-.39-67.188 3.515-121.289c0-1.367-.586-2.734-1.953-3.516-1.172-.976-2.93-1.562-4.688-1.562h-42.968c-1.758 0-3.516.586-4.688 1.563-1.367.78-1.953 2.148-1.953 3.515l3.32 121.29c0 2.734 2.93 4.882 6.64 4.882h36.134c3.515 0 6.445-2.148 6.64-4.883Z"/>
</svg>

        </span>Note</p><p>假设一共有 16 个 GPU，索引表示为 g0 &hellip; g15，并行方法和并行度设置如下</p>
<p><code>dp_degree (2) * cfg_degree (2) * pp_degree (2) * sp_degree (2) = 16</code>.</p>
<p>那么一共会创建 2 data parallel-groups, 8 CFG groups, 8 pipeline-parallel groups &amp; 8 sequence-parallel groups:</p>
<ul>
<li>2 data-parallel groups:
[g0, g1, g2, g3, g4, g5, g6, g7],
[g8, g9, g10, g11, g12, g13, g14, g15]</li>
<li>8 CFG-parallel groups:
[g0, g4], [g1, g5], [g2, g6], [g3, g7],
[g8, g12], [g9, g13], [g10, g14], [g11, g15]</li>
<li>8 pipeline-parallel groups:
[g0, g2], [g4, g6], [g8, g10], [g12, g14],
[g1, g3], [g5, g7], [g9, g11], [g13, g15]</li>
<li>8 sequence-parallel groups:
[g0, g1], [g2, g3], [g4, g5], [g6, g7],
[g8, g9], [g10, g11], [g12, g13], [g14, g15]</li>
</ul></div>

<h2 id="convert-model">Convert Model</h2>
<p><a href="https://github.com/xdit-project/xDiT/blob/6f92383e76b5f8bbaf8f45e6863d1e69b0d2f955/xfuser/model_executor/models/transformers/base_transformer.py#L76">_split_transformer_blocks</a> 会对 transformer block 进行分配，如果 parallel_config 指定了 attn_layer_num_for_pp，即存有每个 pipeFusion 的设备被分配的 transformer block 数量的列表，按其进行分配；否则平均分。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_split_transformer_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transformer</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># omit</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># transformer layer split</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_layer_num_for_pp</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># 获取每个 pipeFusion 的设备被分配的 transformer block 数量</span>
</span></span><span class="line"><span class="cl">        <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pp_config</span><span class="o">.</span><span class="n">attn_layer_num_for_pp</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_rank</span> <span class="o">=</span> <span class="n">get_pipeline_parallel_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">pp_world_size</span> <span class="o">=</span> <span class="n">get_pipeline_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attn_layer_num_for_pp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span> <span class="p">:</span> <span class="n">attn_layer_num_for_pp</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">attn_layer_num_for_pp</span><span class="p">[:</span> <span class="n">pp_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="p">:</span> 
</span></span><span class="line"><span class="cl">                                                                            <span class="nb">sum</span><span class="p">(</span><span class="n">attn_layer_num_for_pp</span><span class="p">[:</span><span class="n">pp_rank</span><span class="p">])]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>  <span class="c1"># 没有指定则平均分</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_blocks_per_stage</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">)</span> <span class="o">+</span> <span class="n">pp_world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">pp_world_size</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">pp_rank</span> <span class="o">*</span> <span class="n">num_blocks_per_stage</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">pp_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_blocks_per_stage</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">),)</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">transformer_blocks</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># position embedding</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">norm_out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="o">.</span><span class="n">proj_out</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">transformer</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>同时也会 convert 原先的 transformer backbone 为 <a href="https://github.com/xdit-project/xDiT/blob/main/xfuser/model_executor/models/transformers/pixart_transformer_2d.py#L21">xFuserPixArtTransformer2DWrapper</a>，具体表现为只有 pipeline 的第一阶段进行 position embedding，最后一阶段进行 unpatchify 变为原来的图像形状。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@xFuserTransformerWrappersRegister.register</span><span class="p">(</span><span class="n">PixArtTransformer2DModel</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">xFuserPixArtTransformer2DWrapper</span><span class="p">(</span><span class="n">xFuserTransformerBaseWrapper</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">transformer</span><span class="p">:</span> <span class="n">PixArtTransformer2DModel</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">submodule_classes_to_wrap</span><span class="o">=</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">,</span> <span class="n">PatchEmbed</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">submodule_name_to_wrap</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;attn1&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@xFuserBaseWrapper.forward_check_condition</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">added_cond_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">cross_attention_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">encoder_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">return_dict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>  
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_patch_height_width</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># * only pp rank 0 needs pos_embed (patchify)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">	<span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">	......
</span></span></span><span class="line"><span class="cl"><span class="s1">	&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">	<span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">	    <span class="n">output</span> <span class="o">=</span> <span class="n">hidden_states</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">(</span><span class="n">output</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">Transformer2DModelOutput</span><span class="p">(</span><span class="n">sample</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="pipeline-execution">Pipeline Execution</h1>
<p>在进行 warm up 后便会进行模型推理和采样器的去噪过程。模型推理通过调用 pipeline 的 <code>__call__</code> 方法实现。在原先 diffusers 包中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py">PixaeArtAlphaPipeline</a> 基础上做了一些修改。我们直接看修改的部分。</p>
<p><code>get_runtime_state()</code> 返回 <code>_RUNTIME</code> ，再调用 <code>set_input_parameters</code> 方法，设置输入参数和计算 PipeFusionParallel 中有关 patch 索引的参数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">set_input_parameters</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">height</span><span class="o">=</span><span class="n">height</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">width</span><span class="o">=</span><span class="n">width</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_inference_steps</span><span class="o">=</span><span class="n">num_inference_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该函数会计算</p>
<ul>
<li>pipeline parallel 中每个 patch 的高度，必须是 <code>patch_size * num_sp_patches</code> 的整数倍。</li>
<li>将每个流水线阶段的 patch 高度均匀地分配给 <code>num_sp_patches</code> 个序列并行设备，计算每个设备的 patch 高度和起始索引。</li>
</ul>
<p>然后会对 prompt 嵌入后的正样本和负样本在 cfg parallel 组中的设备进行分割, rank 0 负样本，rank 1 正样本。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">do_classifier_free_guidance</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">prompt_embeds</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">prompt_attention_mask</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_cfg_split_batch</span><span class="p">(</span><span class="n">negative_prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">negative_prompt_attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                                                            <span class="n">prompt_attention_mask</span><span class="p">,)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_process_cfg_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_0_negative</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_0</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_1_negative</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">concat_group_1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,):</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="n">get_classifier_free_guidance_world_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">concat_group_0_negative</span><span class="p">,</span> <span class="n">concat_group_0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">concat_group_1_negative</span><span class="p">,</span> <span class="n">concat_group_1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">get_classifier_free_guidance_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">concat_group_0_negative</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">concat_group_1_negative</span>
</span></span><span class="line"><span class="cl">  <span class="k">elif</span> <span class="n">get_classifier_free_guidance_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_0</span> <span class="o">=</span> <span class="n">concat_group_0</span>
</span></span><span class="line"><span class="cl">      <span class="n">concat_group_1</span> <span class="o">=</span> <span class="n">concat_group_1</span>
</span></span><span class="line"><span class="cl">  <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid classifier free guidance rank&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="k">return</span> <span class="n">concat_group_0</span><span class="p">,</span> <span class="n">concat_group_1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="async-pipeline">Async Pipeline</h1>
<h2 id="initialize-pipeline">Initialize Pipeline</h2>
<p>首先会初始化 pipeline，rank 0 会接收 warmup 阶段的 latents 然后沿着 H 维度进行分块，rank -1 也会沿着 H 维度进行分块。然后为每个 patch 创建接收的任务，注意 rank 0 第一次是从 warmup 阶段接收 latents，所以他的需要接收的 timestep 少一个。
<code>patch_latents</code> 表示当前设备正在处理的 patch 数据，它会在流水线的每一阶段进行处理和传递。<code>last_patch_latents</code> 只在流水线的最后阶段设备中使用，用来存储每个 patch 的最终计算结果。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">latents</span>
</span></span><span class="line"><span class="cl"><span class="n">num_pipeline_patch</span> <span class="o">=</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span>
</span></span><span class="line"><span class="cl"><span class="n">num_pipeline_warmup_steps</span> <span class="o">=</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">warmup_steps</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_async_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_timesteps</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span><span class="o">=</span><span class="n">latents</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_pipeline_warmup_steps</span><span class="o">=</span><span class="n">num_pipeline_warmup_steps</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">last_patch_latents</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># 每个 pipeline group 最后的设备接收所有的 patch</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pipeline_patch</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">is_pipeline_last_stage</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_init_async_pipeline</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_timesteps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_pipeline_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">set_patched_mode</span><span class="p">(</span><span class="n">patch_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># get latents computed in warmup stage</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ignore latents after the last timestep</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents</span> <span class="o">=</span> <span class="p">(</span><span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_recv</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                  <span class="k">if</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">                  <span class="k">else</span> <span class="n">latents</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">latents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_height</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">latents</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_height</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">recv_timesteps</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_timesteps</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="k">else</span> <span class="n">num_timesteps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># construct receive tasks for each patch</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">recv_timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">add_pipeline_recv_task</span><span class="p">(</span><span class="n">patch_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">patch_latents</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="iterate-over-timesteps">Iterate Over Timesteps</h1>
<p>对于每个 <code>timestep</code>（即每个去噪步骤），会对每个 patch 执行：</p>
<ol>
<li>如果当前设备是流水线的最后一阶段 (<code>is_pipeline_last_stage()</code>)，将当前 patch 的数据保存到 <code>last_patch_latents</code> 中。</li>
<li>如果不是第一阶段的第一个时间步 (<code>i == 0</code>)，调用 <code>recv_next()</code> 来异步接收来自上一设备的 patch 数据（非阻塞操作，通过 <code>irecv</code> 完成）。</li>
<li>对每个 patch 执行模型的前向传播 <code>_backbone_forward</code>，根据当前时间步 <code>t</code> 进行推理和计算。</li>
<li>如果当前设备是最后一阶段，调用 <code>_scheduler_step</code> 来根据噪声进行去噪，并将数据发送给下一个设备 <code>pipeline_isend</code>。</li>
<li>对于非最后阶段的设备，继续将当前 patch 的计算结果发送到下一设备。</li>
</ol>
<p><code>get_pp_group().pipeline_isend</code> 用于将当前 patch 发送到下一个设备，使用的是 torch.distributed.isend，这是非阻塞发送。
<code>get_pp_group().recv_next</code> 会准备好接收来自上一个设备的数据，recv_buffer 用来存放接收到的数据。irecv 实现非阻塞接收，可以在等待数据的同时进行其他操作。</p>
<div class="notice warning" >
    <p class="notice-title">
        <span class="icon-notice baseline">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="126 76.5 300 300">
  <path d="M297.431 324.397v-34.255c0-3.245-2.344-5.95-5.358-5.95h-32.146c-3.014 0-5.358 2.705-5.358 5.95v34.255c0 3.245 2.344 5.95 5.358 5.95h32.146c3.014 0 5.358-2.705 5.358-5.95Zm-.335-67.428 3.014-82.753c0-1.081-.502-2.524-1.674-3.425-1.005-.902-2.512-1.983-4.019-1.983h-36.834c-1.507 0-3.014 1.081-4.019 1.983-1.172.901-1.674 2.704-1.674 3.786l2.846 82.392c0 2.344 2.512 4.146 5.693 4.146h30.975c3.013 0 5.525-1.803 5.692-4.146Zm-2.344-168.39L423.34 342.425c3.683 7.032 3.516 15.686-.335 22.717-3.85 7.031-10.883 11.358-18.417 11.358H147.413c-7.534 0-14.566-4.327-18.417-11.358-3.85-7.031-4.018-15.685-.335-22.716L257.248 88.578C260.93 81.188 268.13 76.5 276 76.5c7.87 0 15.069 4.688 18.752 12.08Z"/>
</svg>

        </span>Warning</p><p>scheduler_step 只对单独的 patch 进行，原因未知。</p></div>

<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">first_async_recv</span> <span class="o">=</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">last_patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">first_async_recv</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">recv_next</span><span class="p">()</span>  
</span></span><span class="line"><span class="cl">                <span class="n">first_async_recv</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">get_pipeline_recv_data</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents</span><span class="o">=</span><span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">            <span class="n">prompt_embeds</span><span class="o">=</span><span class="n">prompt_embeds</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">prompt_attention_mask</span><span class="o">=</span><span class="n">prompt_attention_mask</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">added_cond_kwargs</span><span class="o">=</span><span class="n">added_cond_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">guidance_scale</span><span class="o">=</span><span class="n">guidance_scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler_step</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>  <span class="c1"># pred noise</span>
</span></span><span class="line"><span class="cl">                <span class="n">last_patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span>  <span class="c1"># last timestep noise</span>
</span></span><span class="line"><span class="cl">                <span class="n">t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">extra_step_kwargs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_isend</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span> <span class="n">segment_idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">                <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">pipeline_isend</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">],</span> <span class="n">segment_idx</span><span class="o">=</span><span class="n">patch_idx</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">is_pipeline_first_stage</span><span class="p">()</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">pass</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">patch_idx</span> <span class="o">==</span> <span class="n">num_pipeline_patch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">pass</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">get_pp_group</span><span class="p">()</span><span class="o">.</span><span class="n">recv_next</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">next_patch</span><span class="p">()</span>  <span class="c1"># switch to next: (self.pipeline_patch_idx + 1) % self.num_pipeline_patch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">timesteps</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">or</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_warmup_steps</span>
</span></span><span class="line"><span class="cl">        <span class="ow">and</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">order</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">callback</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&#34;callback not supported in async &#34;</span> <span class="s2">&#34;pipeline&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">            <span class="ow">and</span> <span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span> <span class="o">%</span> <span class="n">callback_steps</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">step_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_pipeline_warmup_steps</span><span class="p">)</span> <span class="o">//</span> <span class="nb">getattr</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&#34;order&#34;</span><span class="p">,</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">callback</span><span class="p">(</span><span class="n">step_idx</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">patch_latents</span><span class="p">[</span><span class="n">patch_idx</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="construct-final-latents">Construct Final Latents</h2>
<p>timestep 遍历完成后，仍然有最后的操作要进行，这些操作的主要目的是将流水线并行中各个 patch 的结果拼接起来，形成完整的输出结果。尤其是对于最后一个设备，还需要处理 序列并行（sequence parallelism） 的合并操作。通过 all_gather 操作将每个设备上处理的 patch 结果收集起来，然后从每个设备的 <code>sp_latents_list</code> 中，提取出对应于 <code>pp_patch_idx</code> 的 patch 数据并将它们拼接起来。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">latents</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_pipeline_last_stage</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">patch_latents</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">get_sequence_parallel_world_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">sp_degree</span> <span class="o">=</span> <span class="n">get_sequence_parallel_world_size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">sp_latents_list</span> <span class="o">=</span> <span class="n">get_sp_group</span><span class="p">()</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents</span><span class="p">,</span> <span class="n">separate_tensors</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents_list</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">pp_patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">num_pipeline_patch</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">latents_list</span> <span class="o">+=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">                <span class="n">sp_latents_list</span><span class="p">[</span><span class="n">sp_patch_idx</span><span class="p">][</span>
</span></span><span class="line"><span class="cl">                    <span class="o">...</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_start_idx_local</span><span class="p">[</span><span class="n">pp_patch_idx</span><span class="p">]</span> <span class="p">:</span> <span class="n">get_runtime_state</span><span class="p">()</span><span class="o">.</span><span class="n">pp_patches_start_idx_local</span><span class="p">[</span><span class="n">pp_patch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                    <span class="p">:,</span>
</span></span><span class="line"><span class="cl">                <span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">sp_patch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sp_degree</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">latents_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">latents</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="decode-latents">Decode Latents</h1>
<p>为了避免 VAE 中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/vae.py#L185">Decoder</a> 在对 8192px 分辨率图像进行 conv2D 的过程中出现 OOM 的问题， xDiT 使用了序列并行和 patch 并行的 <a href="https://github.com/xdit-project/DistVAE/blob/a7e7ee7ec222f45af1214984561c8c645be8aece/distvae/models/layers/conv2d.py#L13">PatchConv2d</a> 和 <a href="https://github.com/xdit-project/DistVAE/blob/a7e7ee7ec222f45af1214984561c8c645be8aece/distvae/models/layers/normalization.py#L59">PatchGroupNorm</a> 来替换掉原有 Decoder 中的 <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d_blocks.py#L2682">UpDecoderBlock2D</a> 对应的层。</p>
<h2 id="patchgroupnorm">PatchGroupNorm</h2>
<p>PatchGroupNorm 在 H 维度上划分为多个 patch，每个设备求自己所负责的部分和。
<details class="custom-details">
    <summary class="custom-summary">GroupNorm Principles</summary>
    <div>假设输入张量 x 的形状为 [N, C, H, W]，其中 N 表示批量大小（Batch Size），C 表示通道数（Channels），H 和 W 分别表示高度和宽度。在 GN 中，通道数 C 被划分为 G 组，每个组包含 C/G 个通道。计算每个组内即 [C/G, H, W] 维度上的均值和方差。特别的 G=1 时，GN 退化为 BN。G=C 时，GN 退化为 LN。</div>
</details><br></p>
<ol>
<li>获取高度信息</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchGroupNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; def __init__(self, ...)&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">height</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">height</span><span class="p">)</span>  <span class="c1"># 收集所有进程的高度并汇总。最终每个进程的 height 都将表示全局的高度和。</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>计算每个组的通道数量以及每个进程内的元素数量</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">channels_per_group</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span>  <span class="c1"># 每个组的通道数量</span>
</span></span><span class="line"><span class="cl"><span class="n">nelements_rank</span> <span class="o">=</span> <span class="n">channels_per_group</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 当前进程负责的每个组中的元素总</span>
</span></span><span class="line"><span class="cl"><span class="n">nelements</span> <span class="o">=</span> <span class="n">channels_per_group</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 所有进程的每个组中的元素总数</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>计算每个组的均值</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1">#  [batch_size, num_groups, channels_per_group, height, width]</span>
</span></span><span class="line"><span class="cl"><span class="n">group_sum</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 对每个组的所有元素 (channels_per_group, height, width) 求平均</span>
</span></span><span class="line"><span class="cl"><span class="n">group_sum</span> <span class="o">=</span> <span class="n">group_sum</span> <span class="o">*</span> <span class="n">nelements_rank</span>  <span class="c1"># 加权后的局部和 = 局部均值 * 当前进程的元素数量</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">group_sum</span><span class="p">)</span>  <span class="c1"># 收集并汇总所有进程的局部和，得到全局和</span>
</span></span><span class="line"><span class="cl"><span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_sum</span> <span class="o">/</span> <span class="n">nelements</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># 计算全局的均值 E</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>计算每个组的方差</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 和计算均值同样的操作</span>
</span></span><span class="line"><span class="cl"><span class="n">group_var_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_groups</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">out</span><span class="o">=</span><span class="n">group_var_sum</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="n">group_var_sum</span> <span class="o">=</span> <span class="n">group_var_sum</span> <span class="o">*</span> <span class="n">nelements_rank</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">group_var_sum</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">var</span> <span class="o">=</span> <span class="p">(</span><span class="n">group_var_sum</span> <span class="o">/</span> <span class="n">nelements</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>归一化并缩放 $y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta$</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">E</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="patchconv2d">PatchConv2d</h2>
<p><code>PatchConv2d</code> 将潜在空间中的特征映射分割成多个 patch，跨不同设备进行序列并行 VAE 解码。这种技术将中间激活所需的峰值内存减少到 1/N，其中 N 是所使用的设备数量。对于 VAE 中的卷积算子，需要对如下图所示的 halo 区域数据进行通信。</p>
<p>
<figure class="post-figure">
    <a href="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/patchvaeconv.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://raw.githubusercontent.com/xdit-project/xdit_assets/main/methods/patchvaeconv.png" alt="Patch VAE Conv">
    </a><figcaption>Patch VAE Conv</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>  <span class="c1"># TODO: refine this type</span>
</span></span><span class="line"><span class="cl">        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">assert</span> <span class="n">dilation</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;dilation is not supported in PatchConv2d&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">assert</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&#34;dilation is not supported in PatchConv2d&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>_conv_forward</code> 函数是 <code>PatchConv2d</code> 类的核心，它负责在输入张量上执行卷积操作，特别是在分布式计算场景下处理跨进程的输入切分、halo 区域的传递和计算。以下是使用的辅助函数的简要功能说明</p>
<ul>
<li><code>_get_world_size_and_rank </code>：获取当前分布式环境中的进程总数 <code>world_size</code> 和当前进程的编号 <code>rank</code></li>
<li><code>_calc_patch_height_index</code>：根据每个进程的输入高度，计算所有进程的起始和结束高度索引。</li>
<li><code>_calc_halo_width_in_h_dim</code>：计算当前进程在 h 维度上所需的上方和下方的 halo 区域宽度。</li>
<li><code>_calc_bottom_halo_width</code>：计算当前进程从下方相邻进程需要接收的 halo 区域的宽度。</li>
<li><code>_calc_top_halo_width</code>：计算当前进程从上方相邻进程需要接收的 halo 区域的宽度。</li>
<li><code>_adjust_padding_for_patch</code>：根据当前进程的 <code>rank</code> 和总进程数调整输入数据的填充方式，防止边界重复计算。</li>
</ul>
<ol>
<li>获取输入信息以及通信组信息</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_conv_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_world_size_and_rank</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">world_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># 处理非分布式情况</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                                <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>获取输入的元数据</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">patch_height_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">())]</span>
</span></span><span class="line"><span class="cl"><span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">patch_height_list</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">h</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">))</span>  <span class="c1"># 收集所有进程的输入高度</span>
</span></span><span class="line"><span class="cl"><span class="n">patch_height_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_patch_height_index</span><span class="p">(</span><span class="n">patch_height_list</span><span class="p">)</span>  <span class="c1"># 计算每个进程块的起始高度和结束高度的索引</span>
</span></span><span class="line"><span class="cl"><span class="n">halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_halo_width_in_h_dim</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># 计算当前进程块的上下 halo 区域的宽度</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>计算相邻进程的 halo 区域 (也就是自己需要接发送的部分)</li>
</ol>
<p>通过计算前一个进程的 bottom_halo_width 和后一个进程的 top_halo_width 得出自己需要发送的部分</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">prev_bottom_halo_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="n">next_top_halo_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">prev_bottom_halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_bottom_halo_width</span><span class="p">(</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="n">world_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_top_halo_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_top_halo_width</span><span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">patch_height_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">next_top_halo_width</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">next_top_halo_width</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>进行 halo 区域的发送与接收</li>
</ol>
<p>异步发送，同步接收</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">to_next</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">to_prev</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">top_halo_recv</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="n">bottom_halo_recv</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">next_top_halo_width</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bottom_halo_send</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">next_top_halo_width</span><span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_next</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">bottom_halo_send</span><span class="p">,</span> <span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not rank 0</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_halo_recv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">top_halo_recv</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">prev_bottom_halo_width</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># not rank N-1</span>
</span></span><span class="line"><span class="cl">    <span class="n">top_halo_send</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">prev_bottom_halo_width</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_prev</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">isend</span><span class="p">(</span><span class="n">top_halo_send</span><span class="p">,</span> <span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bottom_halo_recv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">([</span><span class="n">bs</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">dist</span><span class="o">.</span><span class="n">recv</span><span class="p">(</span><span class="n">bottom_halo_recv</span><span class="p">,</span> <span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>拼接 halo 区域</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Remove redundancy at the top of the input</span>
</span></span><span class="line"><span class="cl"><span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="o">-</span><span class="n">halo_width</span><span class="p">[</span><span class="mi">0</span><span class="p">]:,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">top_halo_recv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># concat the halo region to the input tensor </span>
</span></span><span class="line"><span class="cl">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">top_halo_recv</span><span class="p">,</span> <span class="nb">input</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">bottom_halo_recv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="nb">input</span><span class="p">,</span> <span class="n">bottom_halo_recv</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="6">
<li>等待发送完成再开始计算</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">to_next</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_next</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">to_prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">to_prev</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="7">
<li>进行卷积和后处理</li>
</ol>
<p>为了减少 memory spike 一次计算 block_size*block_size 的区域，并将结果拼接起来</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjust_padding_for_patch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reversed_padding_repeated_twice</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">h</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="ow">and</span> <span class="n">w</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                            <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">conv_res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">conv_res</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s2">&#34;zeros&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&#34;constant&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks_in_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>  <span class="c1"># h 维度的 block 数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_chunks_in_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>  <span class="c1"># w ...</span>
</span></span><span class="line"><span class="cl">    <span class="n">unit_chunk_size_h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">//</span> <span class="n">num_chunks_in_h</span>
</span></span><span class="line"><span class="cl">    <span class="n">unit_chunk_size_w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">//</span> <span class="n">num_chunks_in_w</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">idx_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks_in_h</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">inner_output</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">idx_w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_chunks_in_w</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_w</span> <span class="o">=</span> <span class="n">idx_w</span> <span class="o">*</span> <span class="n">unit_chunk_size_w</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_h</span> <span class="o">=</span> <span class="n">idx_h</span> <span class="o">*</span> <span class="n">unit_chunk_size_h</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">unit_chunk_size_w</span>
</span></span><span class="line"><span class="cl">        <span class="n">end_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx_h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">unit_chunk_size_h</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算每个块的开始和结束索引，调整块的边界</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 对当前块执行卷积操作</span>
</span></span><span class="line"><span class="cl">        <span class="n">inner_output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="nb">input</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start_h</span><span class="p">:</span><span class="n">end_h</span><span class="p">,</span> <span class="n">start_w</span><span class="p">:</span><span class="n">end_w</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">weight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">bias</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">inner_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    <item>
      <title>VLLM Sourse Code Reading</title>
      <link>http://localhost:1313/blogs/vllm/</link>
      <pubDate>Sat, 07 Jun 2025 18:15:55 +0800</pubDate>
      <guid>http://localhost:1313/blogs/vllm/</guid>
      <description>vllm structure</description>
      <content:encoded><![CDATA[<h1 id="basic">Basic</h1>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Sample prompts.</span>
</span></span><span class="line"><span class="cl"><span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;Hello, my name is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The president of the United States is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The capital of France is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;The future of AI is&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a sampling params object.</span>
</span></span><span class="line"><span class="cl"><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create an LLM.</span>
</span></span><span class="line"><span class="cl"><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&#34;facebook/opt-125m&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Generate texts from the prompts. The output is a list of RequestOutput objects</span>
</span></span><span class="line"><span class="cl"><span class="c1"># that contain the prompt, generated text, and other information.</span>
</span></span><span class="line"><span class="cl"><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Print the outputs.</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
</span></span><span class="line"><span class="cl">    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="architecture">Architecture</h1>
<p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB713ed0d773cac101706cdaa862d71dda?method=download&amp;shareKey=09c7c358d0427427384e027f0ced662a" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB713ed0d773cac101706cdaa862d71dda?method=download&amp;shareKey=09c7c358d0427427384e027f0ced662a" alt="VLLM Architecture Overview">
    </a><figcaption>VLLM Architecture Overview</figcaption></figure></p>
<ul>
<li>LLM: 最上层的类，构造函数中会根据传入的参数构建 EngineArgs 然后创建 LLMEngine 对象。</li>
<li>LLMEngine: 包含一些组件 InputPreprocessor, ExecutorBase 负责模型推理的最上层的类</li>
<li>ExecutorBase 会初始化 N 个 WorkerWrapperBase (包装实际的 worker，类比成 GPU)
<ul>
<li>Worker: 在 GPU 上执行 (一部分) 模型推理。每个 worker 与一个 GPU 相关联，负责维护 KV Cache 并在 GPU 上执行模型推理。在分布式推理的情况下，每个 worker 被分配模型的一部分。
<ul>
<li>ModelRunner:  执行模型推理并负责采样新 token.</li>
<li>CacheEngine: 负责初始化和管理 GPU 和 CPU KV Cache. 还提供了对 KV Cache 进行操作的方法。通过 <code>initialize_cache()</code> 初始化。</li>
</ul>
</li>
</ul>
</li>
<li>Scheduler: 负责推理时候对请求的调度。组件包括一个 BlockSpaceManager (KV Cache blocks 管理的核心类) 以及三个队列 waiting, running &amp; swapped.</li>
</ul>
<h1 id="llmengine--initialization">LLMEngine  Initialization</h1>
<ul>
<li>InputPreprocessor: 主要是在 <code>add_request()</code> 方法中将输入的 prompt 放入 tokenizer 进行处理。</li>
<li>InputRegistry: 根据目标模型对 InputPreprocessor 之后的数据进行处理。</li>
</ul>
<h2 id="init-executor">Init Executor</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DistributedExecutorBase</span><span class="p">(</span><span class="n">ExecutorBase</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Abstract superclass of distributed executor implementations.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># This is non-None when the execute model loop is running</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># in the parallel workers. It&#39;s a coroutine in the AsyncLLMEngine case.</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_worker_tasks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Awaitable</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>ExecutorBase 的构造函数中会调用 <code>self._init_executor()</code> 对应到具体子类的函数。如果采用 TP 或 PP 的话 对应到的是 RayDistributedExecutor，否则对应到的是 UniProcExecutor. 下面以后者为例。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">UniProcExecutor</span><span class="p">(</span><span class="n">ExecutorBase</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">uses_ray</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_init_executor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;Initialize the worker and load the model.
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">driver_worker</span> <span class="o">=</span> <span class="n">WorkerWrapperBase</span><span class="p">(</span><span class="n">vllm_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                               <span class="n">rpc_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">distributed_init_method</span> <span class="o">=</span> <span class="n">get_distributed_init_method</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">get_ip</span><span class="p">(),</span> <span class="n">get_open_port</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        <span class="n">local_rank</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># set local rank as the device index if specified</span>
</span></span><span class="line"><span class="cl">        <span class="n">device_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="o">.</span><span class="n">device_config</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">device_info</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">device_info</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">rank</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">vllm_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vllm_config</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">local_rank</span><span class="o">=</span><span class="n">local_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">distributed_init_method</span><span class="o">=</span><span class="n">distributed_init_method</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">is_driver_worker</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="ow">or</span> <span class="p">(</span><span class="n">rank</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">tensor_parallel_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;init_worker&#34;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">([</span><span class="n">kwargs</span><span class="p">],</span> <span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;init_device&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="s2">&#34;load_model&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">collective_rpc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                       <span class="n">method</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                       <span class="n">timeout</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                       <span class="n">args</span><span class="p">:</span> <span class="n">Tuple</span> <span class="o">=</span> <span class="p">(),</span>
</span></span><span class="line"><span class="cl">                       <span class="n">kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">        <span class="n">answer</span> <span class="o">=</span> <span class="n">run_method</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">driver_worker</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># 初始化 Worker</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="p">[</span><span class="n">answer</span><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>Executor: 初始化具体的继承自 ExecutorBase 的对象，该对象的初始化过程中会调用 <code>init_worker()</code> 初始化 Worker (被 WorkerWrapperBase 包装)，调用 <code>init_device()</code> 初始化设备，和调用具体 Worker 对象的 model_runner 的 <code>load_model()</code> 将模型加载到设备上。
<ul>
<li>Worker: 构造函数中会初始化 <code>GPUModelRunnerBase</code> 对象，确定计算 attention 使用的 backend 还有 CUDAGraphRunner 用于将模型的计算过程记录为一个静态图，在后续的推理中，通过直接 replay 这个静态图来避免动态调度和重复的内核启动开销。</li>
</ul>
</li>
</ul>
<h2 id="initialize_kv_caches">initialize_kv_caches</h2>
<p>LLMEngine 构造函数在初始化 ExecutorBase 后会调用 <code>initialize_kv_caches()</code> 来初始化 Worker 中的 KV Cache，流程如下:</p>
<ol>
<li>该函数会首先通过 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/neuron_worker.py#L69">Worker.determine_num_available_blocks()</a> 确定 GPU 和 CPU 可用的 block 数量。后者在 <code>memory_profiling</code> 上下文中进行 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/model_runner.py#L1239">profile_run()</a> 模拟模型在最大负载 (max_num_batched_tokens 和 max_num_seqs) 下执行一次推理。测量内存使用并分解为权重、激活张量和非 PyTorch 部分。留给 KV Cache 的内存大小为 <code>total_mem * max_utilization - weight_mem - act_mem - nontorch_mem</code>.  再除以每一个 block 能存储的的 KV Cache 大小 <code>cache_size = Cache_config.block_size * num_attention_layers * 2*num_heads*head_size</code> 即可得到最多能分配多少个 GPU block. 而 CPU block 数量由预设的 <code>swap_size // cache_size</code> 所确定。</li>
<li>确定了 GPU 和 CPU 的 block 数量后会调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/worker.py#L285">Worker.initialize_cache()</a> 方法，里面首先会调用 <code>Worker._init_cache_engine()</code> 根据传入的 GPU block 个数初始化 CacheEngine (初始化 attn_backend，调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/worker/cache_engine.py#L68">CacheEngine._allocate_kv_cache()</a> 为模型的每一层 transformer 开辟 CPU 和 GPU 的 KV Cache 内存)，然后会调用 <a href="https://github.com/vllm-project/vllm/blob/main/vllm/utils.py#L2163">bind_kv_cache()</a> 将 GPU KV Cache Tensor 绑定到对应的模型的注意力层，它筛选需要 KV Cache 的注意力层，按层索引排序并去重后为每个设备绑定对应的 Tensor.</li>
<li>预热之后进行 capture_model 记录计算图。</li>
</ol>
<h2 id="init-scheduler">Init Scheduler</h2>
<p>构造函数中会初始化 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block_manager.py#L61">BlockSpaceManager</a>. 首先会创建一个 <code>CpuGpuBlockAllocator</code>，为 CPU 和 GPU 块维护单独的内存池，并允许在这些内存池中分配、释放、分叉和交换块。它会为 CPU 和 GPU 中的 blocks 分别创建一个 <code>BlockAlloctor</code>. 还会初始化一个空的 <code>Dict[SeqId, BlockTable]</code>， 表示对应 seq 的 KV Cache 所使用的物理内存块。还会初始化一些调度时所需要的数据，后文再谈。</p>
<p>还会初始化 waiting(包含新的或 preempted prefill 请求), running &amp; swapped(被换出的 decoding 请求), 它们是 <code>Deque[SequenceGroup]</code>，其中 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/sequence.py#L633">SequenceGroup(SG)</a> 是一组由同一个 prompt 生成的 Sequences 和对应的采样参数。</p>
<ul>
<li>SequenceGroupOutputProcessor: 抽象基类借接口，会分为 SingleStepOutputProcessor (支持 beam seaching) 和 MultiStepOutputProcessor (支持 speculatice decoding)</li>
</ul>
<h1 id="llm-generate">LLM Generate</h1>
<h2 id="_validate_and_add_requests">_validate_and_add_requests</h2>
<p>里面会调用 <code>_add_request()</code> 给 prompt 分配 reqest_id 后会调用 <code>LLMEngine.add_request()</code> 将其添加到请求池中，并将在调用 <code>LLMEngine.step()</code> 时由调度器处理。确切的调度策略由调度程序确定。主要就是进行 tokenize，然后打包成 SG 后加入 waiting.</p>
<h2 id="__run_engine">__run_engine</h2>
<p>调用 generate 时首先会将 prompt 包装成 SG，它是包含某个 prompt 生成的所有 Sequence，以及一些其他在调度时需要的信息的结构。Scheduler 里面包含三个 <code>Deque[SequenceGroup]</code>: waiting, running &amp; swapped.
generate() &ndash;&gt; _run_engine() &ndash;&gt; step() &ndash;&gt; Scheduler.schedule() &ndash;&gt; Scheduler._schedule()
Scheduler 的一些操作与 BlockManager 息息相关，我们在下面先简要说明逻辑，有关其具体结构和操作流程在后文中解释。</p>
<h2 id="step">step</h2>
<p>执行一次 decoding 迭代并返回新生成的结果。

<figure class="post-figure">
    <a href="https://i.imgur.com/sv2HssD.png" target="_blank" rel="noopener">
        <img loading="lazy" src="https://i.imgur.com/sv2HssD.png" alt="Overview of the step function">
    </a><figcaption>Overview of the step function</figcaption></figure>
主要流程如下</p>
<ol>
<li>调度要在下一次迭代中执行的 seq 和要交换入/出/复制的令牌块。根据调度策略，Sequences 可能被抢占/重新排序。</li>
<li>调用分布式执行器来执行模型。</li>
<li>处理模型输出。主要包括： decoding 相关输出，使用 _beam_search 与否的模型输出更新调度 seq 组和释放已完成的 seq 组。</li>
<li>读取上一次调度的元数据和输出</li>
<li>如果没有剩余步骤且，调用 <code>Scheduler.schedule()</code> 执行新调度，生成 seq 组元数据、调度输出和异步标志。</li>
<li>获取并重置已完成请求 ID，清理内存</li>
<li>如果不允许异步且有输出队列，处理模型输出。</li>
<li>从 Cache 获取上一次迭代的 sampled_token_ids，构造 ExecuteModelRequest 后调用 <code>Executor.execute_model()</code> (最后是由 ModelRunner) 执行模型推理，获取输出。</li>
</ol>
<h2 id="_schedule_prefill">_schedule_prefill()</h2>
<ol>
<li>检查 budget 是否耗尽</li>
<li>取出队列head 部的 SequenceGroup (prefill 阶段 SequenceGroup 只有一个初始 prompt Sequence)</li>
<li>计算 uncached 和 cached 的新 token 数</li>
<li>调用 <code>BlockSpaceManager.can_allocate()</code> 检查是否能分配足够内存。</li>
<li>若能满足 budget，从 waiting 中移除 SequenceGroup. 调用 <code>_allocate_and_set_running()</code> 分配内存并设置为 RUNNING 状态。</li>
</ol>
<h2 id="_schedule_running">_schedule_running()</h2>
<ol>
<li>取出队列head 部 SequenceGroup 并计算其包含 seq 的 #uncached_token. 这里不需要 #cached_token 因为若使用 chunked prefill，该信息已经在第一次 prefill 时使用，如果不使用那么他就是进行 decoding 的 seq ，不需要用到这个信息。</li>
<li>从 running 移除该 SequenceGroup. 循环调用 <code>Scheduler._can_append_slots()</code> 检查是否有足够的空间存储该 SequenceGroup 的 KV Cache，若不能，进入抢占逻辑</li>
<li>从 budget 中减去当前 SequenceGroup 的 token 和 seq 数</li>
<li>若 running 有其他 SequenceGroup，抢占最低优先级（队列尾部）的，若该 SequenceGroup 只有一个正在运行的 Sequence 则抢占模式为 RECOMPUTE 加入到 <code>preempted</code>，否则为 SWAP 加入到 <code>swapped_out</code>.</li>
<li>分配 slot 并更新 blocks_to_copy，根据该 Sequence 处于 decoding(生成 1 个 token 的 KV Cache ) 或者 prefill(生成 #uncached_token 的 KV Cache) 加入到 <code>prefill_seq_group</code> 或者 <code>decode_seq_groups</code>，并更新 budget.</li>
<li>返回 decode_seq_groups：存储 decoding  SequenceGroup. prefill_seq_groups：存储分块 prefill  SequenceGroup. preempted：被抢占需重新计算的 SequenceGroup. swapped_out：被交换到 CPU 的 SequenceGroup. keys_to_swap_out 和 keys_to_copy：内存块交换和复制的映射</li>
</ol>
<h2 id="_schedule_swapepd">_schedule_swapepd()</h2>
<ol>
<li>循环遍历 swapped 队列，取出队列head 部的 SequenceGroup，调用 <code>BlockManager.can_swap_in()</code> (实际上是 SWAPPED 状态的 <code>can_swap</code>)</li>
<li>获取 SequenceGroup 中处于 SWAPPED 的 Sequence 个数和 token 个数，是否满足预算。</li>
<li>调用 <code>_swap_in</code>(实际上是 <code>BlockManager.swap_in()</code>) 执行交换，更新 blocks_to_swap_in，将 Sequence 状态由 SWAPPED 变为 RUNNING.</li>
<li>调用 <code>_append_slots</code> 给被换入的 Sequence 分配 block.</li>
<li>根据 SequenceGroup 的状态添加到不同队列。</li>
<li>返回blocks_to_swap_in：记录需要从 CPU 交换到 GPU 的块映射。blocks_to_copy：记录需要复制的块映射（例如写时复制）。decode_seq_groups 和 prefill_seq_groups：分别存储 decoding 和 prefill  SequenceGroup. infeasible_seq_groups：存储无法调度的 SequenceGroup. swapped_queue：引用交换队列。leftover_swapped：暂存无法立即调度的 SequenceGroup.</li>
</ol>
<h2 id="_schedule_chunked_prefill">_schedule_chunked_prefill()</h2>
<p>主要思想是: 1.安排尽可能多的 decoding 请求。2.调度未完成的 prefill 请求。3.调度交换请求。4.安排新的 prefill 请求。</p>
<ol>
<li>初始化 budget，限制最大批处理 token 数和 seq 数。</li>
<li>从 running 和 waiting 生成 <code>PartialPrefillMetadata</code></li>
</ol>
<ul>
<li>prefills: running 和 waiting 中未完成 prefill 的 #SequenceGroup.</li>
<li>long_prefills: running 中需要进行 prefill 的 token 数很多的 #SequenceGroup.</li>
<li>waiting_long_prefills: waiting 中需要进行且能进行的 (未超过 ScheduleConfig 限制) prefill 的 token 数很多的 #SequenceGroup.</li>
</ul>
<ol start="3">
<li>调用 <code>_schedule_running</code>.</li>
<li>在 running 调度返回中无无抢占或交换时(说明有足够空间) 执行 <code>_schedule_swapped</code></li>
<li>调用 <code>_schedule_prefills</code>.</li>
<li>更新 waiting，添加 running 调度中返回的被抢占的 seq  <code>running_scheduled.preempted</code>.</li>
<li>按优先级更新 running.</li>
<li>swapped_in.decode_seq_groups：交换回来的 decoding 请求。</li>
<li>swapped_in.prefill_seq_groups：交换回来的 prefill 请求。</li>
<li>running_scheduled.decode_seq_groups：运行中的 decoding 请求。</li>
<li>running_scheduled.prefill_seq_groups（按完成顺序）：未完成的分块 prefill 。使用 _order_finishing_prefills_first 确保即将完成的 prefill 优先，便于下一轮转为 decoding.</li>
<li>prefills.seq_groups：新 prefill 请求。</li>
<li>将运行队列中交换出去的 <code>running_scheduled.swapped_out</code> 添加到 swapped.</li>
<li>按顺序组合所有调度的 SequenceGroup: prefill 优先（满足注意力机制假设），decoding 次之。</li>
<li>调整 lookahead_slots 数量。若所有被调度的均为 prefill 且未启用多步调度，设置 num_lookahead_slots = 0(避免推测 decoding 路径). 否则，使用 running 计算的 lookaheadh slots 数量。</li>
</ol>
<h2 id="_schedule_default">_schedule_default</h2>
<p>尽可能多地批处理 prefill 请求，然后调度 decoding 请求. 在 GPU 内存压力下，需要 preempt 或 swap out 运行中的 decoding 请求。</p>
<ol>
<li>swapped 为空则进行 <code>_schedule_prefills</code>.</li>
<li>如果没有调度任何 prefill 请求，调用 <code>_schedule_running</code>.</li>
<li>如果 running 调度结果中没有发生抢占或换出时 (否则说明资源不够)，执行 <code>_schedule_swapped</code>.</li>
<li>更新 waiting, running &amp; swapped 三个队列。</li>
</ol>
<h2 id="after-schedule">After schedule</h2>
<p>调度结果返回后，</p>
<ol>
<li>遍历调度结果中的 SequenceGroup</li>
<li>遍历该 SequenceGroup 中状态为 RUNNING 的 Sequence. 获取其数据，对应的 BlockID 列表，并更新其访问时间。若使用 prefix_caching, 则调用 <code>BlockManager.get_common_computed_block_ids()</code> 获取共享的已计算的部分的 BlockID 列表。</li>
<li>如果该 SequenceGroup 处于 prefill 阶段，则判断这次调度后是否能完成 prefill.</li>
<li>构造返回结果，标记所有调度 SequenceGroup 的 blocks 为已计算。</li>
</ol>
<h1 id="blockspacemanager">BlockSpaceManager</h1>
<p>用于将 SequenceGroup 操作映射到其包含的对应组件的操作。</p>
<ul>
<li>CpuGpuBlockAlloctor: 根据是否采用 prefix caching 分别为 CPU 和 GPU 初始化一个 Alloctor
<ul>
<li><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/prefix_caching_block.py#L53">PrefixCachingBlockAlloctor</a>: 基于哈希值维护 block 的Cache)重用具有相同哈希值的 block，以避免冗余的内存分配。
<ul>
<li><code>Dict[PrefixHash, BlockId]</code> 将用于 prefix caching blocks 的哈希值与其 BlockID 对应。</li>
<li><code>Dict[BlockId, BlockTracker]</code> 为每个物理 block 初始化一个 BlockTracker.</li>
<li><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/naive_block.py#L13">NaiveBlockAllocator</a> 用于分配不作为 prefix caching 的 blocks. 有一个 <code>RefCounter</code> 表示某个物理 block 被多少逻辑 block 指向。</li>
<li><code>Evictor</code> 采用 LRU 策略驱逐已经Cache) blocks.</li>
<li><code>CopyOnWriterTracker</code> 用于将原先的 block ID 映射到目的 block ID.</li>
</ul>
</li>
</ul>
</li>
<li>Dict[SeqId, BlockTable]: <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block/block_table.py#L11">BlockTable</a> 用于将单个 seq 的 KV Cache 映射到物理内存分配。会在调用 <a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/core/block_manager.py#L148">_allocate_sequence()</a> 时被初始化。包含一个 <a href="https://github.com/vllm-project/vllm/blob/main/vllm/core/block/common.py#L231">BlockList</a> (block 列表和一个表示对应 ID 的 int 列表) 和 BlockpaceManager 的 BlockAllocator.</li>
<li>ComputedBlocksTracker: 维护一个 <code>Dict[SeqId, List[int]]</code> ( seq id到 seq 块哈希列表的映射)。Cache)个 seq 的完整块 (块全部被占满) 的哈希值。当一个 seq 进行 decoding 时，也相应更新 seq 的哈希值。还有一个 <code>Dict[int, int]</code> ( seq id到已计算 token 数的映射)</li>
</ul>
<h2 id="can_allocate">can_allocate</h2>
<p>在 <code>_schedule_prefills</code> 中被调用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">can_allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol>
<li>取出该 SequenceGroup 中处于 WAITING 状态的第一个 Sequence (i.e. prompt).</li>
<li>调用 <code>BlockTable.get_num_required_blocks()</code> 计算存储 token 和 lookahead slots 所需的最小 block 数 (假设无 prefix caching), i.e. <code>cdiv(len(token_ids) + num_lookahead_slots, block_size)</code>.</li>
<li>调用 <code>BlockAlloctor.get_num_free_blocks()</code> 获取 GPU 上空闲的 block 数 (非 prefix_caching 中的空闲个数 + 可以被驱逐的个数).</li>
<li>返回分配状态</li>
</ol>
<ul>
<li>NEVER: <code>#total - #required &lt; #watermark</code></li>
<li>OK: <code>#free  - #required &gt;= #watermark</code></li>
<li>LATER: <code>#free  - #required &lt; #watermark</code></li>
</ul>
<h2 id="allocate">allocate</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 <code>_schedule_prefills</code> 中步骤 4 中调用的 <code>_allocate_and_set_running</code> 内部被调用。</p>
<ol>
<li>取出该 SequenceGroup 中处于 WAITING 状态的第一个 Sequence (i.e. prompt).</li>
<li>调用 <code>BlockManager._allocate_sequence()</code> 创建一个 BlockTable，在获取 token_ids 列表后调用 <code>BlockTable.allocate()</code> 为该 Sequence 分配 blocks.</li>
<li>将 token_ids 按 _block_size 大小进行分块。最后一块可能不能占满一个 block.</li>
<li>对于能够占满一个 block 的 token_ids 分块，调用 <code>BlockAlloctor.allocate_immutable_block()</code>. 该函数优先从Cache)查找是否已有相同内容的块，若有则直接复用该块并增加其引用计数；否则调用 <code>BlockAlloctor.allocate_mutable_blocks()</code> 分配一个新的 block，并将 token_ids 添加到该 block 中. 该函数会尝试从非 prefix caching blocks 中分配一个 block_id，若没找到则会驱逐一个。</li>
<li>对于最后一个可能被没占满的 block 调用 <code>BlockAlloctor.allocate_mutable_blocks()</code>.</li>
</ol>
<h2 id="can_append_slots">can_append_slots</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">can_append_slots</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>确定 GPU KV Cache 中是否有足够的空间来继续生成指定的 SequenceGroup. 上层接口为 <code>Scheduler._can_append_slots()</code>，在 <code>_schedule_running</code> 中步骤 2 中确定是否需要进行抢占时被调用。</p>
<ol>
<li>遍历该 Sequence Group 中处于 RUNNING 状态的 Sequence 对应的 BlockTable</li>
<li>调用 <code>BlockTable.get_unseen_token_ids()</code> 获取该 Sequence 还未被Cache) token 部分。</li>
<li>调用 <code>BlockTable.get_num_blocks_touched_by_append_slots()</code> 获取Cache)余部分和 lookahead 部分需要几个 block.</li>
<li>调用 <code>BlockAlloctor.get_num_free_blocks()</code> 获取 GPU 上空闲的 block 数.</li>
<li>需要个数小于空闲个数返回 True.</li>
</ol>
<h2 id="append_slots">append_slots</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">append_slots</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上层接口为 <code>Scheduler._append_slots()</code>. 在 <code>_schedule_running</code> 中检查到有空间添加，<code>_schedule_swapped</code> 中有 budget 进行换入，<code>_schedule_prefills</code> 中允许进行 chunked prefill 时被调用。</p>
<ol>
<li>调用 <code>BlockTable.append_token_ids()</code>. 该方法将 tokens 添加到 BlockTable 中的现有 block 中。会调用 <code>BlockTable.ensure_num_empty_slots()</code>， 它查看当前能够容纳多少个 token. 如果没有足够的空间，则使用 <code>BlockAlloctor.allocate_mutable_block()</code> 方法分配新 block.</li>
<li>调用 <code>BlockAllocator.clear_copy_on_writes()</code> 返回一个映射源 block ID 到当前 COW 的目标 block ID 的元组的列表.</li>
</ol>
<h2 id="_can_swap">_can_swap</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_can_swap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">seq_group</span><span class="p">:</span> <span class="n">SequenceGroup</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">device</span><span class="p">:</span> <span class="n">Device</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">status</span><span class="p">:</span> <span class="n">SequenceStatus</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">num_lookahead_slots</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AllocStatus</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>根据 status 区分上层接口: RUNNING/SWAPPED 表示需要把该 SequenceGroup 处于 RUNNING/SWAPPED 状态的 Sequence 对应的 blocks 从 GPU/CPU 换到 CPU/GPU.</p>
<ol>
<li>获取 SequenceGroup 中符合指定状态的 seq  Sequence，然后根据 SeqID 获取对应的 BlockTable.</li>
<li>调用 <code>BlockTable.get_num_blocks_touched_by_append_slots()</code> 计算添加未存储 token 加上 lookahead_slots 所需的 block 数量。</li>
<li>调用 <code>BlockAlloctor.get_num_full_blocks_touched()</code> 获取当前有被使用的 block 数量。</li>
<li>如果总块数小于被使用的加上需要的 block 数量 返回 Never. 如果空闲块减去 被使用的加上需要的 block 数量后仍大于等于 watermark_blocks，返回 OK. 否则为 LATER.</li>
</ol>
<h2 id="swap_in">swap_in</h2>
<p>调用的是  <code>self.block_allocator.swap(blocks=blocks, src_device=Device.CPU, dst_device=Device.GPU)</code>，即 blocks 从原设备的换出，换入到目的设备。
进一步则是 <code>BlockAlloctor.swap_in()</code>，该函数遍历传入的 blocks，若已经被占满调用 <code>BlockAlloctor.allocate_immutable_block()</code>. 否则调用 <code>BlockAlloctor.allocate_mutable_blocks()</code> 分配一个新的 block 后将原 block的 token 数据追加到新 block.</p>
<h2 id="swap_out">swap_out</h2>
<p>同上，最终调用的是 <code>BlockAlloctor.swap_out()</code>. 该函数对传入的每个 block 调用 <code>_free_block_id</code>，逐个处理释放逻辑。若 block 有哈希值，refcount -1，若减去后为 0 则将 block 信息添加到 evictor 中，从跟踪系统中移除，然后设置 BlockId 为 None. 否则就直接设置为 None. 若无哈希值则释放 BlockId，减去对应的 refcount，但保留 block 对象本身.</p>
<h1 id="attention">Attention</h1>
<p><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/attention/backends/xformers.py#L354">XFormersImpl</a> 中使用了 vllm 自己写的 PagedAttention kernel.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">XFormersImpl</span><span class="p">(</span><span class="n">AttentionImpl</span><span class="p">[</span><span class="n">XFormersMetadata</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">head_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">num_kv_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">alibi_slopes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">      <span class="n">sliding_window</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">      <span class="n">kv_cache_dtype</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">blocksparse_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">logits_soft_cap</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">attn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其中 <code>attn_type</code> 分为四种，下面我们主要分析 DECODER 的情况。</p>
<ul>
<li>DECODER: 使用 decoding 器的 self-attention block table 来Cache)KV(GPT).</li>
<li>ENCODER: 不进行 KV Cache)用于 Encoder-Decoder 模编码器分支。编码器通常一次性处理整个输入 seq 。</li>
<li>ENCODER-ONLY: 不进行 KV Cache)BERT).</li>
<li>ENCODER_DECODER: 用于编码器- decoding 器模型中的交叉注意力部分，其中 KV  seq 长度与编码器 seq 长度一致(T5).</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">layer</span><span class="p">:</span> <span class="n">AttentionLayer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [2, num_blocks, block_size * num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://github.com/vllm-project/vllm/blob/5eeabc2a4400fde9b030f2f72746a2b03db059bd/vllm/attention/backends/.py#L104">AttentionMetadata</a> 类定义如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@dataclass</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">AttentionMetadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Attention metadata for prefill and decode batched together.&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_prefills</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># prefill 请求的总数</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_prefill_tokens</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># 所有 prefill 请求中的 token 总数。</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_decode_tokens</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># decodeing token 的数量，等同于 decoding 请求的数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">slot_mapping</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>  <span class="c1"># (num_tokens,)，指定每个输入 token 存储到 KV cache 中的 slot 索引</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># block_idx = x // block_size, block_offset = x % block_size</span>
</span></span><span class="line"><span class="cl">    <span class="n">multi_modal_placeholder_index_maps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="nb">str</span><span class="p">,</span> <span class="n">MultiModalPlaceholderMap</span><span class="o">.</span><span class="n">IndexMap</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">    <span class="n">enable_kv_scales_calculation</span><span class="p">:</span> <span class="nb">bool</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>forward 方法如下，简化了成了 DECODER 情况的逻辑。
主要流程为</p>
<ol>
<li>调用 <code>PagedAttention.split_kv_cache</code> 分离并 reshape KV Cache 张量后 调用 PagedAttention.write_to_paged_cache`
写入当前 key 和 value 到Cache)。</li>
<li>分离 prefill 和 decoding 的 token，初始化输出。对于 prefill 部分根据是否采用了 prefix_caching 调用 <code>self._run_memory_efficient_xformers_forward</code> 或 <code>PagedAttention.forward_prefix</code> 计算注意力。</li>
<li>调用 <code>get_seq_len_block_table_args</code> 获取 decoding Sequence 对应的 BlockTable后调用 <code>PagedAttention.forward_decode</code> 计算注意力。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">layer</span><span class="p">:</span> <span class="n">AttentionLayer</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>    <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_tokens, num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">kv_cache</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [2, num_blocks, block_size * num_kv_heads * head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">    <span class="c1"># 将 query 重塑为 [num_tokens, num_heads, head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># key 和 value 必须非空（自注意力要求），重塑为 [num_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果 KV Cache)空，处理Cache)辑</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">kv_cache</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 从 kv_cache 分离出 key_cache 和 value_cache</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># key_cache: [num_blocks, num_kv_heads, head_size/x, block_size, x]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># value_cache: [num_blocks, num_kv_heads, head_size, block_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">split_kv_cache</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">kv_cache</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新自注意力的 KV Cache)        # 使用 attn_metadata.slot_mapping 指定 token 存储位置</span>
</span></span><span class="line"><span class="cl">        <span class="n">PagedAttention</span><span class="o">.</span><span class="n">write_to_paged_cache</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">slot_mapping</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取 prefill 和 decoding 阶段的 token 数量</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="n">num_prefill_query_tokens</span><span class="p">,</span> <span class="n">num_prefill_kv_tokens</span><span class="p">,</span> <span class="n">num_decode_query_tokens</span><span class="p">)</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">        <span class="n">get_num_prefill_decode_query_kv_tokens</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 创建输出张量与 query 相同</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 分离 prefill 和 decoding 的 QKV</span>
</span></span><span class="line"><span class="cl">    <span class="n">decode_query</span> <span class="o">=</span> <span class="n">query</span><span class="p">[</span><span class="n">num_prefill_query_tokens</span><span class="p">:]</span>  <span class="c1">#</span>
</span></span><span class="line"><span class="cl">    <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span>     
</span></span><span class="line"><span class="cl">    <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[:</span><span class="n">num_prefill_kv_tokens</span><span class="p">]</span>             
</span></span><span class="line"><span class="cl">    <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[:</span><span class="n">num_prefill_kv_tokens</span><span class="p">]</span>         
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 prefill 阶段（如果存在）</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">prefill_meta</span> <span class="o">:=</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">prefill_metadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">kv_cache</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">block_tables</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 普通注意力（无Cache)缀）</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_memory_efficient_xformers_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="p">,</span> <span class="n">attn_type</span><span class="o">=</span><span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 前缀Cache)意力</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">forward_prefix</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">prefill_meta</span><span class="o">.</span><span class="n">block_tables</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">query_start_loc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">prefill_meta</span><span class="o">.</span><span class="n">seq_lens_tensor</span><span class="p">,</span> <span class="n">prefill_meta</span><span class="o">.</span><span class="n">max_query_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[:</span><span class="n">num_prefill_query_tokens</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 decoding 阶段（如果存在）</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">decode_meta</span> <span class="o">:=</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">decode_metadata</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 获取 decoding 所需的 seq 长度和 BlockTable 参数</span>
</span></span><span class="line"><span class="cl">        <span class="n">seq_lens_arg</span><span class="p">,</span> <span class="n">max_seq_len_arg</span><span class="p">,</span> <span class="n">block_tables_arg</span> <span class="o">=</span> \
</span></span><span class="line"><span class="cl">            <span class="n">get_seq_len_block_table_args</span><span class="p">(</span><span class="n">decode_meta</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 运行 decoding 注意力</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span><span class="p">[</span><span class="n">num_prefill_query_tokens</span><span class="p">:]</span> <span class="o">=</span> <span class="n">PagedAttention</span><span class="o">.</span><span class="n">forward_decode</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">decode_query</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">block_tables_arg</span><span class="p">,</span> <span class="n">seq_lens_arg</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_seq_len_arg</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_k_scale</span><span class="p">,</span> <span class="n">layer</span><span class="o">.</span><span class="n">_v_scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 将输出 reshape 为 [num_tokens, num_heads * head_size]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="write_to_paged_cache">write_to_paged_cache</h2>
<p>调用的是已经注册到 torch.ops 中的 CUDA 函数。其对应的 host 函数为每个 token 分配一个 CUDA block，每个 CUDA block 的线程数被限制在最多 512 个。主要的 kernel 函数如下。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// scalar_t: 输入 key 和 value 的数据类型（如 float、half）
</span></span></span><span class="line"><span class="cl"><span class="c1">// cache_t: Cache)key_cache 和 value_cache 的数据类型（如 half、uint8_t）
</span></span></span><span class="line"><span class="cl"><span class="c1">// kv_dt: KV Cache) FP8 数据类型（如 kAuto 或具体 FP8 格式）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">template</span> <span class="o">&lt;</span><span class="kr">typename</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="kr">typename</span> <span class="kt">cache_t</span><span class="p">,</span> <span class="n">Fp8KVCacheDataType</span> <span class="n">kv_dt</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">reshape_and_cache_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">key</span><span class="p">,</span>    <span class="c1">// [num_tokens, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">value</span><span class="p">,</span>  <span class="c1">// [num_tokens, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">key_cache</span><span class="p">,</span>     <span class="c1">// [num_blocks, num_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">value_cache</span><span class="p">,</span>   <span class="c1">// [num_blocks, num_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">slot_mapping</span><span class="p">,</span>  <span class="c1">// [num_tokens]，指定每个 token 的Cache)置
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">key_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">value_stride</span><span class="p">,</span>  <span class="c1">// key 和 value 在 token 维的步幅
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_heads</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">head_size</span><span class="p">,</span>      <span class="c1">// 注意力head 数和每个head 的维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">block_size</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">x</span><span class="p">,</span>             <span class="c1">// Cache)大小和 key_cache 中 head_size 的拆分因子
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">k_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v_scale</span><span class="p">)</span>    <span class="c1">// key 和 value 的缩放因子，用于数据类型转换
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  <span class="c1">// host 函数定义 block 个数与 token 个数相同
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">slot_idx</span> <span class="o">=</span> <span class="n">slot_mapping</span><span class="p">[</span><span class="n">token_idx</span><span class="p">];</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">// Cache Block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">slot_idx</span> <span class="o">/</span> <span class="n">block_size</span><span class="p">;</span>  <span class="c1">// 块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">block_offset</span> <span class="o">=</span> <span class="n">slot_idx</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">;</span>  <span class="c1">// 块内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">;</span>  <span class="c1">// 每个 token 的维度数目
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// CUDA Block 级别并行，每个线程处理token 的一个维度
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算输入 key 和 value 的源索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">src_key_idx</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">*</span> <span class="n">key_stride</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">src_value_idx</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">*</span> <span class="n">value_stride</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算当前处理的head 索引和head 内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">head_idx</span> <span class="o">=</span> <span class="n">i</span> <span class="o">/</span> <span class="n">head_size</span><span class="p">;</span>      <span class="c1">// 第几个head 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">head_offset</span> <span class="o">=</span> <span class="n">i</span> <span class="o">%</span> <span class="n">head_size</span><span class="p">;</span>   <span class="c1">// head 内的第几个元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 将 head_offset 拆分为 x_idx 和 x_offset（仅用于 key_cache）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">x_idx</span> <span class="o">=</span> <span class="n">head_offset</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>       <span class="c1">// head_size/x 维的索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">x_offset</span> <span class="o">=</span> <span class="n">head_offset</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>    <span class="c1">// x 维的偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算 key_cache 的目标索引，按维度逐步偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">tgt_key_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">head_size</span> <span class="o">/</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>  <span class="c1">// 块偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_idx</span> <span class="o">*</span> <span class="p">(</span><span class="n">head_size</span> <span class="o">/</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>               <span class="c1">// head 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">x_idx</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span>                                    <span class="c1">// head_size/x 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">block_offset</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x_offset</span><span class="p">;</span>                                <span class="c1">// 块内和 x 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 计算 value_cache 的目标索引，按维度逐步偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">tgt_value_idx</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="n">block_idx</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_size</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>            <span class="c1">// 块偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_idx</span> <span class="o">*</span> <span class="n">head_size</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>                         <span class="c1">// head 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">head_offset</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>                                  <span class="c1">// head_size 偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">block_offset</span><span class="p">;</span>                                               <span class="c1">// 块内偏移
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// 从输入张量读取当前元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">scalar_t</span> <span class="n">tgt_key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="n">src_key_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="kt">scalar_t</span> <span class="n">tgt_value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="n">src_value_idx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// 根据 kv_dt 类型决定存储方式
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span> <span class="nf">constexpr</span> <span class="p">(</span><span class="n">kv_dt</span> <span class="o">==</span> <span class="n">Fp8KVCacheDataType</span><span class="o">::</span><span class="n">kAuto</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 如果是 kAuto，直接存储，不进行类型转换
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">key_cache</span><span class="p">[</span><span class="n">tgt_key_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_key</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="n">value_cache</span><span class="p">[</span><span class="n">tgt_value_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tgt_value</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// 否则，使用 scaled_convert 进行类型转换（如 FP8 量化）
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">key_cache</span><span class="p">[</span><span class="n">tgt_key_idx</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">          <span class="n">fp8</span><span class="o">::</span><span class="n">scaled_convert</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="n">kv_dt</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tgt_key</span><span class="p">,</span> <span class="o">*</span><span class="n">k_scale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="n">value_cache</span><span class="p">[</span><span class="n">tgt_value_idx</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">          <span class="n">fp8</span><span class="o">::</span><span class="n">scaled_convert</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="n">kv_dt</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tgt_value</span><span class="p">,</span> <span class="o">*</span><span class="n">v_scale</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="_run_memory_efficient_xformers_forward">_run_memory_efficient_xformers_forward</h2>
<p>也同样简化成 DECODER 的逻辑的情况</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_run_memory_efficient_xformers_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_prefill_tokens, num_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>    <span class="c1"># [num_prefill_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># [num_prefill_tokens, num_kv_heads, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_metadata</span><span class="p">:</span> <span class="s2">&#34;XFormersMetadata&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">original_query</span> <span class="o">=</span> <span class="n">query</span>  <span class="c1"># 保存原始 query，用于最后 reshape 输出</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 处理 GQA/MQA</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape Q to [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                           <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># expand K to [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># expand V to  [num_prefill_tokens, num_kv_heads, num_queries_per_kv, head_size]</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                            <span class="bp">self</span><span class="o">.</span><span class="n">num_queries_per_kv</span><span class="p">,</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 获取或设置 attention bias</span>
</span></span><span class="line"><span class="cl">    <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">_get_attn_bias</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">attn_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>  <span class="c1"># 确保 seq 长度信息存在</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 创建 causal mask</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">BlockDiagonalCausalMask</span><span class="o">.</span><span class="n">from_seqlens</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">query</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="c1"># 如果有滑动窗口，应用局部注意力</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">attn_bias</span><span class="o">.</span><span class="n">make_local_attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="p">[</span><span class="n">attn_bias</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 使用 ALiBi 偏置（线性偏置注意力）</span>
</span></span><span class="line"><span class="cl">            <span class="n">attn_bias</span> <span class="o">=</span> <span class="n">_make_alibi_bias</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                                        <span class="n">query</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">_set_attn_bias</span><span class="p">(</span><span class="n">attn_metadata</span><span class="p">,</span> <span class="n">attn_bias</span><span class="p">,</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">DECODER</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 执行 xFormers 高效注意力计算</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alibi_slopes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 为 QKV 添加 batch</span>
</span></span><span class="line"><span class="cl">        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># ALiBi 模式直接使用 attn_bias</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">original_query</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># xformers 不支持在自定义 bias 的情况下每个 seq 的长度不同</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">attn_metadata</span><span class="o">.</span><span class="n">seq_lens</span><span class="p">):</span> 
</span></span><span class="line"><span class="cl">            <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">            <span class="n">out</span> <span class="o">=</span> <span class="n">xops</span><span class="o">.</span><span class="n">memory_efficient_attention_forward</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">query</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">key</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">value</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">output</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">original_query</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">            <span class="n">start</span> <span class="o">+=</span> <span class="n">seq_len</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 将输出 reshape 为原始 query </span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">original_query</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="forward_prefix">forward_prefix</h2>
<p>不考虑 ALiBi 的情况调用的是 triton 编写的 <a href="https://github.com/vllm-project/vllm/blob/d1695758b2f65fd314d1aee71ba2469ceba67a5b/vllm/attention/ops/prefix_prefill.py#L22">_fwd_kernel()</a> 每个线程块独立处理一个 Q 的一部分，对 KV Cache 和 当前 KV 分别采取 flash-attention 的计算策略。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span><span class="lnt">234
</span><span class="lnt">235
</span><span class="lnt">236
</span><span class="lnt">237
</span><span class="lnt">238
</span><span class="lnt">239
</span><span class="lnt">240
</span><span class="lnt">241
</span><span class="lnt">242
</span><span class="lnt">243
</span><span class="lnt">244
</span><span class="lnt">245
</span><span class="lnt">246
</span><span class="lnt">247
</span><span class="lnt">248
</span><span class="lnt">249
</span><span class="lnt">250
</span><span class="lnt">251
</span><span class="lnt">252
</span><span class="lnt">253
</span><span class="lnt">254
</span><span class="lnt">255
</span><span class="lnt">256
</span><span class="lnt">257
</span><span class="lnt">258
</span><span class="lnt">259
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">triton</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">triton.language</span> <span class="k">as</span> <span class="nn">tl</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nd">@triton.jit</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">_fwd_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 输入张量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">Q</span><span class="p">,</span>  <span class="c1">#  Query 张量: [total_seq_len, num_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># total_seq_len 是所有 batch  seq 长度的总和，当前块为 [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="n">K</span><span class="p">,</span>  <span class="c1"># 键张量（当前输入）: [total_seq_len, num_kv_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">    <span class="n">V</span><span class="p">,</span>  <span class="c1"># 值张量（当前输入）: [total_seq_len, num_kv_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">    <span class="n">K_cache</span><span class="p">,</span>  <span class="c1"># 键Cache) [num_blocks, num_kv_heads, head_dim, block_size, x]</span>
</span></span><span class="line"><span class="cl">              <span class="c1"># 用于存储上下文部分的 K</span>
</span></span><span class="line"><span class="cl">    <span class="n">V_cache</span><span class="p">,</span>  <span class="c1"># 值Cache) [num_blocks, num_kv_heads, head_dim, block_size]</span>
</span></span><span class="line"><span class="cl">              <span class="c1"># 用于存储上下文部分的 V</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Loc</span><span class="p">,</span>  <span class="c1"># 块索引表: [batch_size, max_seq_len // block_size]</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 记录每个 batch 中每个块的块编号</span>
</span></span><span class="line"><span class="cl">    <span class="n">sm_scale</span><span class="p">,</span>  <span class="c1"># softmax 缩放因子，通常为 1/sqrt(head_dim)</span>
</span></span><span class="line"><span class="cl">    <span class="n">k_scale</span><span class="p">,</span>  <span class="c1"># 用于 FP8 精度转换的缩放因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">v_scale</span><span class="p">,</span>  <span class="c1"># 用于 FP8 精度转换的缩放因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Start_Loc</span><span class="p">,</span>  <span class="c1">#  batch 起始位置: [batch_size + 1]</span>
</span></span><span class="line"><span class="cl">                  <span class="c1"># 每个 batch 的全局 seq 起始索引，最后一个元素是总长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">B_Seqlen</span><span class="p">,</span>  <span class="c1">#  batch  seq 长度: [batch_size]</span>
</span></span><span class="line"><span class="cl">               <span class="c1"># 每个 batch 的总 seq 长度（上下文 +  Query ）</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_size</span><span class="p">,</span>  <span class="c1"># 每个Cache)的大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span>  <span class="c1"># K_cache 的额外维度分片因子（通常为 1 或小整数）</span>
</span></span><span class="line"><span class="cl">    <span class="n">Out</span><span class="p">,</span>  <span class="c1"># 输出张量: [total_seq_len, num_heads, head_dim]</span>
</span></span><span class="line"><span class="cl">          <span class="c1"># 存储注意力计算结果</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 步幅参数 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_b_loc_b</span><span class="p">,</span>  <span class="c1"># B_Loc 的 batch 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_b_loc_s</span><span class="p">,</span>  <span class="c1"># B_Loc 的 seq 块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qbs</span><span class="p">,</span>  <span class="c1"># Q 的 batch / seq 步幅，通常为 num_heads * head_dim</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qh</span><span class="p">,</span>   <span class="c1"># Q 的head 步幅，通常为 head_dim</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_qd</span><span class="p">,</span>   <span class="c1"># Q 的head_size步幅，通常为 1</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kbs</span><span class="p">,</span>  <span class="c1"># K 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kh</span><span class="p">,</span>   <span class="c1"># K 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_kd</span><span class="p">,</span>   <span class="c1"># K 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vbs</span><span class="p">,</span>  <span class="c1"># V 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vh</span><span class="p">,</span>   <span class="c1"># V 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_vd</span><span class="p">,</span>   <span class="c1"># V 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_obs</span><span class="p">,</span>  <span class="c1"># Out 的 batch / seq 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_oh</span><span class="p">,</span>   <span class="c1"># Out 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_od</span><span class="p">,</span>   <span class="c1"># Out 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_bs</span><span class="p">,</span>  <span class="c1"># K_cache 的块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_h</span><span class="p">,</span>   <span class="c1"># K_cache 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_d</span><span class="p">,</span>   <span class="c1"># K_cache 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_bl</span><span class="p">,</span>  <span class="c1"># K_cache 的块内偏移步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_k_cache_x</span><span class="p">,</span>   <span class="c1"># K_cache 的额外维度步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_bs</span><span class="p">,</span>  <span class="c1"># V_cache 的块步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_h</span><span class="p">,</span>   <span class="c1"># V_cache 的head 步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_d</span><span class="p">,</span>   <span class="c1"># V_cache 的head_size步幅</span>
</span></span><span class="line"><span class="cl">    <span class="n">stride_v_cache_bl</span><span class="p">,</span>  <span class="c1"># V_cache 的块内偏移步幅</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 超参数 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_queries_per_kv</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>  <span class="c1"># 每个 KV head 对应的 Query head 数量</span>
</span></span><span class="line"><span class="cl">    <span class="n">IN_PRECISION</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 输入精度（例如 tl.float32）</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_M</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#  Query 块大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_DMODEL</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># head 维度大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># head 维度填充到 2 的幂次</span>
</span></span><span class="line"><span class="cl">    <span class="n">BLOCK_N</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># KV 块大小</span>
</span></span><span class="line"><span class="cl">    <span class="n">SLIDING_WINDOW</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 滑动窗口大小，0 表示无窗口</span>
</span></span><span class="line"><span class="cl">    <span class="n">SKIP_DECODE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1"># 是否跳过解码（仅处理上下文）</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 网格定义 ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># grid = (batch_size, num_heads, max_seq_len // BLOCK_M)</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 当前 batch 索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_head</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># 当前head 索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">start_m</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>    <span class="c1"># 当前 Query 块索引</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 KV head 索引 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_kv_head</span> <span class="o">=</span> <span class="n">cur_head</span> <span class="o">//</span> <span class="n">num_queries_per_kv</span>  <span class="c1"># 当前 KV head 索引</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 加载 batch 信息 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_seq_len</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Seqlen</span> <span class="o">+</span> <span class="n">cur_batch</span><span class="p">)</span>  <span class="c1"># 当前 batch 总 seq 长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_in_all_start_index</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Start_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span><span class="p">)</span>  <span class="c1"># 当前 batch 全局起始索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_in_all_stop_index</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Start_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 下一 batch 起始索引</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_query_len</span> <span class="o">=</span> <span class="p">(</span><span class="n">cur_batch_in_all_stop_index</span> <span class="o">-</span> 
</span></span><span class="line"><span class="cl">                          <span class="n">cur_batch_in_all_start_index</span><span class="p">)</span>  <span class="c1"># 当前 batch  Query 长度</span>
</span></span><span class="line"><span class="cl">    <span class="n">cur_batch_ctx_len</span> <span class="o">=</span> <span class="n">cur_batch_seq_len</span> <span class="o">-</span> <span class="n">cur_batch_query_len</span>  <span class="c1"># 上下文长度</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 Query 块起始位置 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_start_loc</span> <span class="o">=</span> <span class="n">BLOCK_M</span> <span class="o">*</span> <span class="n">start_m</span>  <span class="c1"># 当前 Query 块的起始位置</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 初始化索引范围 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>  <span class="c1"># KV 块内偏移: [0, BLOCK_N)</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_d</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">)</span>  <span class="c1"># head_size 偏移: [0, BLOCK_DMODEL_PADDED)</span>
</span></span><span class="line"><span class="cl">    <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">)</span>  <span class="c1">#  Query 块内偏移: [start_m * BLOCK_M, (start_m + 1) * BLOCK_M)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算 Q 的偏移量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q: [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 定位当前 Query 块在 Q 张量中的内存地址</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_q</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">stride_qbs</span> <span class="o">+</span>  <span class="c1">#  batch 和 seq 偏移</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_head</span> <span class="o">*</span> <span class="n">stride_qh</span> <span class="o">+</span>  <span class="c1"># head 偏移</span>
</span></span><span class="line"><span class="cl">        <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_qd</span>  <span class="c1"># head_size偏移</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 示例: 假设 Q [100, 4, 64], stride_qbs=256, stride_qh=64, stride_qd=1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># cur_batch_in_all_start_index=20, cur_head=1, start_m=1, BLOCK_M=16</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># offs_m=[16, 17, ..., 31], offs_d=[0, 1, ..., 63]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q[0, 0] = (20 + 16) * 256 + 1 * 64 + 0 * 1 = 9216 + 64 = 9280</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># off_q[0, 1] = (20 + 16) * 256 + 1 * 64 + 1 * 1 = 9281</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 创建head_size维度掩码 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">dim_mask</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BLOCK_DMODEL</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">int1</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 屏蔽填充部分，例如 BLOCK_DMODEL=64, BLOCK_DMODEL_PADDED=128，则后 64 个值为 0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 加载 Q 数据 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">q</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">off_q</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_M, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加载当前 Query 块，掩码确保不加载超出 Query 长度和填充维度的数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 初始化online softmax 变量 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">m_i</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">-</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;inf&#34;</span><span class="p">)</span>  <span class="c1"># 最大值</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_i</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 归一化因子</span>
</span></span><span class="line"><span class="cl">    <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_DMODEL_PADDED</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>  <span class="c1"># 注意力累加</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算上下文注意力（Q 对 KV Cache) ---</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">start_n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">multiple_of</span><span class="p">(</span><span class="n">start_n</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>  <span class="c1"># 确保 start_n 是 BLOCK_N 的倍数</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 Cache 索引 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">bn</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">B_Loc</span> <span class="o">+</span> <span class="n">cur_batch</span> <span class="o">*</span> <span class="n">stride_b_loc_b</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                     <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">)</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_b_loc_s</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">mask</span><span class="o">=</span><span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">other</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># bn 是当前 KV Cache的块编号</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: B_Loc=[0, 1, 2, ...], cur_batch=0, start_n=16, block_size=16, offs_n=[0, 1, 2, 3]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># bn = B_Loc[0, 1]（若 stride_b_loc_b=8, stride_b_loc_s=1，则地址为 0*8 + 1*1 = 1）</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 K_cache 偏移量 ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># off_k: [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">off_k</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">bn</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_k_cache_bs</span> <span class="o">+</span>  <span class="c1"># 块偏移</span>
</span></span><span class="line"><span class="cl">            <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_k_cache_h</span> <span class="o">+</span>   <span class="c1"># head 偏移</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">//</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_d</span> <span class="o">+</span>  <span class="c1"># head_size偏移（分片）</span>
</span></span><span class="line"><span class="cl">            <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">%</span> <span class="n">block_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_bl</span> <span class="o">+</span>  <span class="c1"># 块内偏移</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">%</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_k_cache_x</span>  <span class="c1"># 额外维度偏移</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: bn=[1], cur_kv_head=1, stride_k_cache_bs=4096, stride_k_cache_h=1024, stride_k_cache_d=16</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># offs_d=[0, 1, ..., 63], start_n=16, offs_n=[0, 1, 2, 3], block_size=16, x=1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># off_k[0, 0] = 1*4096 + 1*1024 + (0//1)*16 + (16+0)%16*256 + (0%1)*1 = 4096 + 1024 = 5120</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 K_cache 数据 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_load</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">K_cache</span> <span class="o">+</span> <span class="n">off_k</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 处理 FP8 精度</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">k_load</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_fp8</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="n">k_load</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_scale</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">k</span> <span class="o">=</span> <span class="n">k_load</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 QK 注意力分数 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">qk</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>  <span class="c1"># [BLOCK_M, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">*=</span> <span class="n">sm_scale</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">SLIDING_WINDOW</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">cur_batch_ctx_len</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">-</span> 
</span></span><span class="line"><span class="cl">                          <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">SLIDING_WINDOW</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="o">-</span><span class="mi">10000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- online softmax 更新 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [BLOCK_M]</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">qk</span> <span class="o">-</span> <span class="n">m_ij</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>  <span class="c1"># [BLOCK_M, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [BLOCK_M]</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i_new</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">m_i</span><span class="p">,</span> <span class="n">m_ij</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_i</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">beta</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_ij</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l_i</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">l_ij</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 更新累加器 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_scale</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">p_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc_scale</span> <span class="o">=</span> <span class="n">l_i</span> <span class="o">/</span> <span class="n">l_i_new</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">acc</span> <span class="o">*</span> <span class="n">acc_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 加载 V_cache</span>
</span></span><span class="line"><span class="cl">        <span class="n">off_v</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">bn</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_v_cache_bs</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_v_cache_h</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_v_cache_d</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">%</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">stride_v_cache_bl</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">v_load</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">V_cache</span> <span class="o">+</span> <span class="n">off_v</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                         <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">cur_batch_ctx_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                         <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">v_load</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_fp8</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">v_load</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_scale</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">v</span> <span class="o">=</span> <span class="n">v_load</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">acc</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新 m_i 和 l_i</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i</span> <span class="o">=</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i</span> <span class="o">=</span> <span class="n">m_i_new</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 计算自注意力（Q 对当前 K 和 V） ---</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算 K 和 V 的初始偏移</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_kbs</span> <span class="o">+</span> <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_kh</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">             <span class="n">offs_d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_kd</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_v</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_vbs</span> <span class="o">+</span> <span class="n">cur_kv_head</span> <span class="o">*</span> <span class="n">stride_vh</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">             <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_vd</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">    <span class="n">k_ptrs</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">off_k</span>  <span class="c1"># 初始指针</span>
</span></span><span class="line"><span class="cl">    <span class="n">v_ptrs</span> <span class="o">=</span> <span class="n">V</span> <span class="o">+</span> <span class="n">off_v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 检查当前 Query 块是否有效</span>
</span></span><span class="line"><span class="cl">    <span class="n">block_mask</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">block_start_loc</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 遍历当前输入的 K 和 V</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">start_n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">block_mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">start_m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">start_n</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">multiple_of</span><span class="p">(</span><span class="n">start_n</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 加载 K 数据 ---</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 全局偏移: (cur_batch_in_all_start_index + start_n) * stride_kbs 定位 batch 和 seq 块</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 示例: K [100, 4, 64], stride_kbs=256, cur_batch_in_all_start_index=20, start_n=8</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 基地址偏移 = (20 + 8) * 256 = 7168</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># k_ptrs[0, 0] = K + 0 + 1*64 + 0*1 + 7168 = K + 7232</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">k_ptrs</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">start_n</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_kbs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_DMODEL_PADDED, BLOCK_N]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 计算 QK 注意力分数 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">BLOCK_M</span><span class="p">,</span> <span class="n">BLOCK_N</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">qk</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">*=</span> <span class="n">sm_scale</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 应用因果掩码</span>
</span></span><span class="line"><span class="cl">        <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]),</span> <span class="n">qk</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&#34;-inf&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">SLIDING_WINDOW</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">qk</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:])</span> <span class="o">&lt;</span> <span class="n">SLIDING_WINDOW</span><span class="p">,</span> <span class="n">qk</span><span class="p">,</span> <span class="o">-</span><span class="mi">10000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- online softmax 更新 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">qk</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">qk</span> <span class="o">-</span> <span class="n">m_ij</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_ij</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i_new</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">m_i</span><span class="p">,</span> <span class="n">m_ij</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_i</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">beta</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">m_ij</span> <span class="o">-</span> <span class="n">m_i_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i_new</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l_i</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">l_ij</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># --- 更新累加器 ---</span>
</span></span><span class="line"><span class="cl">        <span class="n">p_scale</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">/</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="n">p_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc_scale</span> <span class="o">=</span> <span class="n">l_i</span> <span class="o">/</span> <span class="n">l_i_new</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">acc</span> <span class="o">*</span> <span class="n">acc_scale</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">v</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">v_ptrs</span> <span class="o">+</span> <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">start_n</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_vbs</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                    <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">((</span><span class="n">start_n</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># [BLOCK_N, BLOCK_DMODEL_PADDED]</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">acc</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">acc</span><span class="o">=</span><span class="n">acc</span><span class="p">,</span> <span class="n">input_precision</span><span class="o">=</span><span class="n">IN_PRECISION</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 更新 m_i 和 l_i</span>
</span></span><span class="line"><span class="cl">        <span class="n">l_i</span> <span class="o">=</span> <span class="n">l_i_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_i</span> <span class="o">=</span> <span class="n">m_i_new</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># --- 存储输出 ---</span>
</span></span><span class="line"><span class="cl">    <span class="n">off_o</span> <span class="o">=</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">cur_batch_in_all_start_index</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">*</span> <span class="n">stride_obs</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_head</span> <span class="o">*</span> <span class="n">stride_oh</span> <span class="o">+</span> <span class="n">offs_d</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_od</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_ptrs</span> <span class="o">=</span> <span class="n">Out</span> <span class="o">+</span> <span class="n">off_o</span>
</span></span><span class="line"><span class="cl">    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">out_ptrs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">mask</span><span class="o">=</span><span class="n">dim_mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">cur_batch_query_len</span><span class="p">))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="forward_decode">forward_decode</h2>
<p>调用的是 <a href="https://github.com/vllm-project/vllm/blob/400d483e87b71315bbb73edb0da9fd629212ca82/csrc/attention/attention_kernels.cuh#L90">paged_atention_kernel</a>
gridDim = (num_heads, num_seqs, 1). decode 的时候每个 seq 的 Query 的 toekn 数目都是 1，</p>
<ul>
<li>gridDim = (num_heads, num_seqs, 1): 每个线程块负责一个 seq 的 一个 head，函数定义如下</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="n">template</span> <span class="o">&lt;</span><span class="kr">typename</span> <span class="kt">scalar_t</span><span class="p">,</span> <span class="kr">typename</span> <span class="kt">cache_t</span><span class="p">,</span> <span class="kt">int</span> <span class="n">HEAD_SIZE</span><span class="p">,</span> <span class="kt">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>  <span class="c1">// default 16
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>          <span class="kt">int</span> <span class="n">NUM_THREADS</span> <span class="cm">/*=128*/</span><span class="p">,</span> <span class="n">vllm</span><span class="o">::</span><span class="n">Fp8KVCacheDataType</span> <span class="n">KV_DTYPE</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">          <span class="kt">bool</span> <span class="n">IS_BLOCK_SPARSE</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="kt">int</span> <span class="n">PARTITION_SIZE</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>  <span class="c1">// Zero means no partitioning.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">__device__</span> <span class="kt">void</span> <span class="nf">paged_attention_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">exp_sums</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads, max_num_partitions]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">max_logits</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads,
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                     <span class="c1">// max_num_partitions]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">out</span><span class="p">,</span>  <span class="c1">// [num_seqs, num_heads, max_num_partitions, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">q</span><span class="p">,</span>       <span class="c1">// [num_seqs, num_heads, head_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">k_cache</span><span class="p">,</span>  <span class="c1">// [num_blocks, num_kv_heads, head_size/x, block_size, x]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">v_cache</span><span class="p">,</span>  <span class="c1">// [num_blocks, num_kv_heads, head_size, block_size]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_kv_heads</span><span class="p">,</span>               <span class="c1">// [num_heads]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">float</span> <span class="n">scale</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">block_tables</span><span class="p">,</span>  <span class="c1">// [num_seqs, max_num_blocks_per_seq]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">seq_lens</span><span class="p">,</span>      <span class="c1">// [num_seqs]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">max_num_blocks_per_seq</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">alibi_slopes</span><span class="p">,</span>  <span class="c1">// [num_heads]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 矩阵每一维度的 stride，便于移动指针
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">q_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_block_stride</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_head_stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">k_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">v_scale</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">tp_rank</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_local_blocks</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_vert_stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_block_size</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">blocksparse_head_sliding_step</span><span class="p">)</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>首先先计算一下当前线程对应的各种参数，这里根据模板函数定义不使用 PARTITIONING.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// grid = (num_heads, num_seqs, 1) 一个 thread block 处理一个 seq 的 一个 head
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">partition_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">max_num_partitions</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>  <span class="c1">// 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_lens</span><span class="p">[</span><span class="n">seq_idx</span><span class="p">];</span>  <span class="c1">// 该 seq token 数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 计算块范围和 token 范围
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_seq_blocks</span> <span class="o">=</span> <span class="nf">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>  <span class="c1">// seq 要分几块读取
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks_per_partition</span> <span class="o">=</span>  <span class="n">num_seq_blocks</span><span class="p">;</span>  <span class="c1">// 分了几块
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">start_block_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>  <span class="c1">// 起始块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">end_block_idx</span> <span class="o">=</span> <span class="n">num_seq_blocks</span><span class="p">;</span>  <span class="c1">// 结束块索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">end_block_idx</span> <span class="o">-</span> <span class="n">start_block_idx</span><span class="p">;</span>  <span class="c1">// 当前分区块数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">start_token_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>  <span class="c1">// 起始 token 索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">end_token_idx</span> <span class="o">=</span> <span class="nf">MIN</span><span class="p">(</span><span class="n">start_token_idx</span> <span class="o">+</span> <span class="n">num_blocks</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">);</span>  <span class="c1">// 结束 token 索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_tokens</span> <span class="o">=</span> <span class="n">end_token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">;</span>  <span class="c1">// 当前分区 token 数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 线程组织参数
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">THREAD_GROUP_SIZE</span> <span class="o">=</span> <span class="nf">MAX</span><span class="p">(</span><span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// 几个 thread 处理一个 token 32/16=2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_THREAD_GROUPS</span> <span class="o">=</span> <span class="n">NUM_THREADS</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 一个 thread block 被分成几组 128/2=64
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span> <span class="o">=</span> <span class="nf">DIVIDE_ROUND_UP</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">WARP_SIZE</span><span class="p">);</span>  <span class="c1">// 每线程处理的 token 数 16/32=1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_WARPS</span> <span class="o">=</span> <span class="n">NUM_THREADS</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// warp 个数 128/32=4
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">thread_idx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>  <span class="c1">// 线程索引
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">warp_idx</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">/</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程位于第几个 warp
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">lane</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">%</span> <span class="n">WARP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程是该 warp 中的第几个
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">head_idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_heads</span> <span class="o">=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// 考虑 GQA MQA
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span> <span class="n">num_queries_per_kv</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">/</span> <span class="n">num_kv_heads</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">int</span> <span class="n">kv_head_idx</span> <span class="o">=</span> <span class="n">head_idx</span> <span class="o">/</span> <span class="n">num_queries_per_kv</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">float</span> <span class="n">alibi_slope</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">      <span class="n">alibi_slopes</span> <span class="o">==</span> <span class="n">nullptr</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">alibi_slopes</span><span class="p">[</span><span class="n">head_idx</span><span class="p">];</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>定义 thread group ，保证其一次访问的数据为 16 Bytes，需要计算其中每个 thread 处理几个元素。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="c1">// VEC_SIZE 即为一个 thread group 中每个线程需要处理元素个数，
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">VEC_SIZE</span> <span class="o">=</span> <span class="nf">MAX</span><span class="p">(</span><span class="mi">16</span> <span class="o">/</span> <span class="p">(</span><span class="n">THREAD_GROUP_SIZE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">scalar_t</span><span class="p">)),</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// 16/2/2=4 
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">using</span> <span class="n">K_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">using</span> <span class="n">Q_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="n">using</span> <span class="n">Quant_vec</span> <span class="o">=</span> <span class="kr">typename</span> <span class="n">Vec</span><span class="o">&lt;</span><span class="kt">cache_t</span><span class="p">,</span> <span class="n">VEC_SIZE</span><span class="o">&gt;::</span><span class="n">Type</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">=</span> <span class="n">HEAD_SIZE</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 每个 thread 处理几个元素 64/2=32
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">constexpr</span> <span class="kt">int</span> <span class="n">NUM_VECS_PER_THREAD</span> <span class="o">=</span> <span class="n">NUM_ELEMS_PER_THREAD</span> <span class="o">/</span> <span class="n">VEC_SIZE</span><span class="p">;</span>  <span class="c1">// 这几个元素相当于几个向量  32/4=8
</span></span></span><span class="line"><span class="cl"><span class="c1">// thread_idx = thread_group_idx * THREAD_GROUP_SIZE + thread_group_offset
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">thread_group_idx</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">/</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程位于第几个 thread group
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="k">const</span> <span class="kt">int</span> <span class="n">thread_group_offset</span> <span class="o">=</span> <span class="n">thread_idx</span> <span class="o">%</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 线程是该 thread group 中第几个线程
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>下面将 Q 加载进共享内存。

<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB7a7b85b64fbddcf13d703135a4bf6d32?method=download&amp;shareKey=6ca032c977b9f14a0864999633e8e08f" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB7a7b85b64fbddcf13d703135a4bf6d32?method=download&amp;shareKey=6ca032c977b9f14a0864999633e8e08f" alt="loadQ">
    </a><figcaption>loadQ</figcaption></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="k">const</span> <span class="kt">scalar_t</span><span class="o">*</span> <span class="n">q_ptr</span> <span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">q_stride</span> <span class="o">+</span> <span class="n">head_idx</span> <span class="o">*</span> <span class="n">HEAD_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="n">__shared__</span> <span class="n">Q_vec</span> <span class="n">q_vecs</span><span class="p">[</span><span class="n">THREAD_GROUP_SIZE</span><span class="p">][</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>  <span class="c1">// HEAD_SIZE * VEC_SIZE * sizeof(scalar_t) 大小
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_group_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREAD_GROUPS</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// NUM_ELEMS_PER_THREAD / VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// 使得每个 thread group 的线程访问相邻的 vec
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
</span></span><span class="line"><span class="cl">        <span class="o">*</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">Q_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">q_ptr</span> <span class="o">+</span> <span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>假设块不稀疏并且把不采用量化，加载 K 并计算 <a href="mailto:Q@K.T">Q@K.T</a>. 核心思想是一个 thread group 访问 16 Bytes. 一个 thread 访问一个 vec，一个向量包含的元素个数 <code>VEC_SIZE = 16 / sizeof (scalar_t) / THREAD_GROUP_SIZE</code></p>
<ol>
<li>1st for 循环确定的是每次迭代中每个 warp 处理的是哪一个 block，一共要循环 num_seq_blocks / NUM_WARPS 次</li>
<li>2nd for 循环确定的是该 warp 中的每个 thread group 访问的是该 block 的第几个 token. 即每个线程组处理一个 token.</li>
<li>3rd for 循环确定的是该 thread group 中的每个 thread 访问的是第几个 vec. 该循环使得该 thread group 里面的线程读取一个完整的 headsize. 一次迭代读取的大小为 16 Bytes.</li>
</ol>
<p>首先将 block_table 指针移动到存储该 kv cache 的首个 blockID 处，取出实际的物理块 ID，用在第三个 for 循环中将指针移动到该 K cache block 起始处. 由于
k_cache 的 shape 是 <code>[num_blocks, num_kv_heads, head_size/x, block_size, x]</code>，在第三个 for 循环中 k_ptr 被移动到了该 thread_group 要读取的 block 的 token 的 head 处。<code>vec_idx * VEC_SIZE</code> 即为 thread 要读取的元素开始位置，/x 表示对应的是第几个 16Bytes 划分, offset1 移动的是 dim3，offset2 移动的 则是 dim4.</p>
<p>3rd loop 结束后已经读取了一个 K cache 的完整 head_size 到寄存器中，因此 qk 为一个 token 的一个 head 的 Score Matrix. 根据 token_idx 由每个 thread group 里的 第一个线程负责将累加和到 logits 中并更新 qk_max。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="c1">// Memory planning.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">char</span> <span class="n">shared_mem</span><span class="p">[];</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span><span class="o">*</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">shared_mem</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Workspace for reduction.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">red_smem</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">NUM_WARPS</span><span class="p">];</span>  <span class="c1">// 前一半用于存储 qk_max 后一半用于存储 exp_sum
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">  <span class="c1">// x == THREAD_GROUP_SIZE * VEC_SIZE
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="c1">// 每次 thread group 一次取的元素数量 保证为 16 bytes
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">constexpr</span> <span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">cache_t</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="kt">float</span> <span class="n">qk_max</span> <span class="o">=</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// 指针移动到当前 seq 对应的首个 blockID
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">int</span><span class="o">*</span> <span class="n">block_table</span> <span class="o">=</span> <span class="n">block_tables</span> <span class="o">+</span> <span class="n">seq_idx</span> <span class="o">*</span> <span class="n">max_num_blocks_per_seq</span><span class="p">;</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">block_idx</span> <span class="o">=</span> <span class="n">start_block_idx</span> <span class="o">+</span> <span class="n">warp_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">&lt;</span> <span class="n">end_block_idx</span><span class="p">;</span> <span class="n">block_idx</span> <span class="o">+=</span> <span class="n">NUM_WARPS</span><span class="p">)</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">    <span class="c1">// 每个 warp 处理一个 block
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">int64_t</span> <span class="n">physical_block_number</span> <span class="o">=</span> <span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">block_table</span><span class="p">[</span><span class="n">block_idx</span><span class="p">]);</span>  <span class="c1">// 该 warp 当前处理的 block 对应的 id
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">    <span class="c1">// Load a key to registers.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_TOKENS_PER_THREAD_GROUP</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// BLOCK_SIZE(16) / WARP_SIZE(32) = 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">const</span> <span class="kt">int</span> <span class="n">physical_block_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">thread_group_idx</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">BLOCK_SIZE</span><span class="p">;</span>  <span class="c1">// thread group 处理的是该 block 的第几个 token
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="k">const</span> <span class="kt">int</span> <span class="n">token_idx</span> <span class="o">=</span> <span class="n">block_idx</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">+</span> <span class="n">physical_block_offset</span><span class="p">;</span>  <span class="c1">// 该 token 是该 seq 的第几个
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">K_vec</span> <span class="n">k_vecs</span><span class="p">[</span><span class="n">NUM_VECS_PER_THREAD</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">NUM_VECS_PER_THREAD</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// NUM_ELEMS_PER_THREAD(32) / VEC_SIZE(4) = 8
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">cache_t</span><span class="o">*</span> <span class="n">k_ptr</span> <span class="o">=</span> <span class="n">k_cache</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">                                <span class="n">physical_block_number</span> <span class="o">*</span> <span class="n">kv_block_stride</span> <span class="o">+</span>  <span class="c1">// 移动到该 block 起始处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">kv_head_idx</span> <span class="o">*</span> <span class="n">kv_head_stride</span> <span class="o">+</span>  <span class="c1">// 移动到对应的 head 处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                                <span class="n">physical_block_offset</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 移动到对应的 token 处
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">vec_idx</span> <span class="o">=</span> <span class="n">thread_group_offset</span> <span class="o">+</span> <span class="n">j</span> <span class="o">*</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span>  <span class="c1">// 该 thread 要读取 head_size 划分成的第几个 vec
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">offset1</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">/</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 第几个 16Bytes 划分
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">int</span> <span class="n">offset2</span> <span class="o">=</span> <span class="p">(</span><span class="n">vec_idx</span> <span class="o">*</span> <span class="n">VEC_SIZE</span><span class="p">)</span> <span class="o">%</span> <span class="n">x</span><span class="p">;</span>  <span class="c1">// 划分的第几个元素
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nf">constexpr</span> <span class="p">(</span><span class="n">KV_DTYPE</span> <span class="o">==</span> <span class="n">Fp8KVCacheDataType</span><span class="o">::</span><span class="n">kAuto</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">          <span class="n">k_vecs</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="o">*</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="n">K_vec</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">k_ptr</span> <span class="o">+</span> <span class="n">offset1</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">offset2</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="c1">// Compute dot product.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="c1">// This includes a reduction across the threads in the same thread group.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="kt">float</span> <span class="n">qk</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">Qk_dot</span><span class="o">&lt;</span><span class="kt">scalar_t</span><span class="p">,</span> <span class="n">THREAD_GROUP_SIZE</span><span class="o">&gt;::</span><span class="nf">dot</span><span class="p">(</span><span class="n">q_vecs</span><span class="p">[</span><span class="n">thread_group_offset</span><span class="p">],</span> <span class="n">k_vecs</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="c1">// Add the ALiBi bias if slopes are given.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>      <span class="n">qk</span> <span class="o">+=</span> <span class="p">(</span><span class="n">alibi_slope</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">?</span> <span class="n">alibi_slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      <span class="k">if</span> <span class="p">(</span><span class="n">thread_group_offset</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// 每个线程组的第一个线程进行更新 max
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// Store the partial reductions to shared memory.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="c1">// NOTE(woosuk): It is required to zero out the masked logits.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">const</span> <span class="kt">bool</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">token_idx</span> <span class="o">&gt;=</span> <span class="n">seq_len</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span><span class="p">[</span><span class="n">token_idx</span> <span class="o">-</span> <span class="n">start_token_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="mf">0.f</span> <span class="o">:</span> <span class="n">qk</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Update the max value.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="n">qk_max</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">?</span> <span class="nl">qk_max</span> <span class="p">:</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">qk</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">      <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>
<figure class="post-figure">
    <a href="https://note.youdao.com/yws/api/personal/file/WEB36d66a13612972c7f567ed8f20600664?method=download&amp;shareKey=9a305814befc64b17e64feb1c8d76b17" target="_blank" rel="noopener">
        <img loading="lazy" src="https://note.youdao.com/yws/api/personal/file/WEB36d66a13612972c7f567ed8f20600664?method=download&amp;shareKey=9a305814befc64b17e64feb1c8d76b17" alt="load k &amp; QK Mul">
    </a><figcaption>load k &amp; QK Mul</figcaption></figure></p>
<p>上面这一段结束后下面每个 warp 内 thread group 中的第一个线程已经记录了该 group 的 qk_max. 下一步则是在 warp 内进行 qk_max 归约，存储在共享内存 red_smem 中。 由于一个 warp 处理的是一个 block，相当于现在 red_smem 每个元素存储了对应 block 内的 qk_max.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="n">THREAD_GROUP_SIZE</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="nf">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">red_smem</span><span class="p">[</span><span class="n">warp_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">qk_max</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>下一步则是在 thread block 内对所有 warp 进行规约，得到该 seq 最后的 qk_max. 然后广播到所有线程中。之后每个线程计算 exp 存入 logits，每个 warp 内的 exp 求和结果存储在 red_smem 的后一半中。最后则是计算 softmax 存到 logits.</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-C" data-lang="C"><span class="line"><span class="cl">  <span class="n">qk_max</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">&lt;</span> <span class="n">NUM_WARPS</span> <span class="o">?</span> <span class="n">red_smem</span><span class="p">[</span><span class="n">lane</span><span class="p">]</span> <span class="o">:</span> <span class="o">-</span><span class="n">FLT_MAX</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="cp">#pragma unroll
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">NUM_WARPS</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">mask</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">mask</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">fmaxf</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="nf">VLLM_SHFL_XOR_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="n">mask</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="c1">// Broadcast the max qk value to all threads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="n">qk_max</span> <span class="o">=</span> <span class="nf">VLLM_SHFL_SYNC</span><span class="p">(</span><span class="n">qk_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// Get the sum of the exp values.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="kt">float</span> <span class="n">exp_sum</span> <span class="o">=</span> <span class="mf">0.f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="nf">__expf</span><span class="p">(</span><span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">qk_max</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp_sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="n">exp_sum</span> <span class="o">=</span> <span class="n">block_sum</span><span class="o">&lt;</span><span class="n">NUM_WARPS</span><span class="o">&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">red_smem</span><span class="p">[</span><span class="n">NUM_WARPS</span><span class="p">],</span> <span class="n">exp_sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="c1">// Compute softmax.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>  <span class="k">const</span> <span class="kt">float</span> <span class="n">inv_sum</span> <span class="o">=</span> <span class="nf">__fdividef</span><span class="p">(</span><span class="mf">1.f</span><span class="p">,</span> <span class="n">exp_sum</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">thread_idx</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_tokens</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">NUM_THREADS</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">inv_sum</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="nf">__syncthreads</span><span class="p">();</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>加载 v 的逻辑与 k 相同，但没有使用 thread group 概念，而是让一个 thread 一次加载 16 Bytes.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
