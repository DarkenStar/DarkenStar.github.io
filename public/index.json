[{"content":"Github Card this is a github card\rBig Quote Basically, Iâ€™m not interested in doing research and I never have beenâ€¦ Iâ€™m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. â€” David Blackwell\nMargin Note è¿™æ˜¯ä¸€æ®µæ­£å¸¸çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬æ­£åœ¨è®¨è®ºä¸€ä¸ªéå¸¸é‡è¦çš„æ¦‚å¿µã€‚è¿™å°±æ˜¯bilibiliå¯¹é‚£ä¸ªé‡è¦æ¦‚å¿µçš„è§£é‡Šå’Œè¡¥å……è¯´æ˜ã€‚ä½ ç”šè‡³å¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ Markdown è¯­æ³•ï¼\rè¿™ä¸ªæ¦‚å¿µæºäºå¤å¸Œè…Šï¼Œå¯¹åä¸–å½±å“æ·±è¿œã€‚\nç»§ç»­ä½ çš„æ–‡ç« \u0026hellip; å¦ä¸€å¤„éœ€è¦æ³¨è§£çš„åœ°æ–¹ã€‚è¿™æ˜¯ç¬¬äºŒä¸ªæ—æ³¨ï¼Œå®ƒä¼šè‡ªåŠ¨å¯¹é½ï¼Œä¸ä¼šå’Œç¬¬ä¸€ä¸ªé‡å ã€‚\rVarious Notice å…³äºä»¥ä¸‹notice è¯·å‚è€ƒ hugo_notice\rWarning\nThis is a warning notice. Be warned!\nTip\nThis is a very good tip.\nInfo\nThis is a use info.\nNote\nThis is a note.\n","permalink":"http://localhost:1313/blogs/functiontest/","summary":"function test","title":"Functional Test of Hugo"},{"content":"CMake å…¥é—¨æ•™ç¨‹ï¼šä»é¡¹ç›®ç»“æ„åˆ°é“¾æ¥åº“\næ ¸å¿ƒç†å¿µï¼šæºç å¤–æ„å»º (Out-of-Source Builds) åœ¨å¼€å§‹ä¹‹å‰ï¼Œæœ€é‡è¦çš„ä¸€ç‚¹æ˜¯ç†è§£ CMake çš„æ ¸å¿ƒå“²å­¦ï¼šæºç å¤–æ„å»ºã€‚è¿™æ„å‘³ç€æ‰€æœ‰ç”±æ„å»ºè¿‡ç¨‹äº§ç”Ÿçš„æ–‡ä»¶ï¼ˆä¾‹å¦‚ Makefilesã€Visual Studio é¡¹ç›®æ–‡ä»¶ã€ç›®æ ‡æ–‡ä»¶ .oã€å¯æ‰§è¡Œæ–‡ä»¶ .exeã€åº“æ–‡ä»¶ .a æˆ– .soï¼‰éƒ½åº”è¯¥ä¸ä½ çš„æºä»£ç å®Œå…¨åˆ†ç¦»å¼€ã€‚è¿™æ ·åšæœ€å¤§çš„å¥½å¤„æ˜¯èƒ½ä¿æŒä½ çš„æºç ç›®å½•æ°¸è¿œå¹²å‡€æ•´æ´ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª build ç›®å½•æ¥å­˜æ”¾æ‰€æœ‰è¿™äº›ç”Ÿæˆçš„æ–‡ä»¶ã€‚\næ¨èçš„é¡¹ç›®ç›®å½•ç»“æ„ ğŸ“‚ ä¸€ä¸ªè‰¯å¥½ç»„ç»‡çš„ C++ é¡¹ç›®ç»“æ„ä¸ä»…æ¸…æ™°ï¼Œä¹Ÿè®© CMake çš„é…ç½®å·¥ä½œäº‹åŠåŠŸå€ã€‚è¿™æ˜¯ä¸€ä¸ªæ¨èçš„ã€å¯æ‰©å±•çš„ç›®å½•ç»“æ„ï¼šmy_project/\nâ”‚ â”œâ”€â”€ build/ # æ„å»ºç›®å½• (åˆå§‹ä¸ºç©ºï¼Œæ‰€æœ‰ç”Ÿæˆæ–‡ä»¶éƒ½åœ¨æ­¤) â”‚ â”œâ”€â”€ include/ # å­˜æ”¾é¡¹ç›®å…¨å±€å¤´æ–‡ä»¶ â”‚ â””â”€â”€ my_app/ â”‚ â””â”€â”€ my_lib.h â”‚ â”œâ”€â”€ src/ # å­˜æ”¾æ‰€æœ‰æºæ–‡ä»¶ (.cpp) â”‚ â”‚ â”‚ â”œâ”€â”€ main.cpp # ä¸»ç¨‹åºå…¥å£ â”‚ â”‚ â”‚ â””â”€â”€ my_lib/ # ä¸€ä¸ªç‹¬ç«‹çš„åº“æ¨¡å— â”‚ â”œâ”€â”€ CMakeLists.txt # è¿™ä¸ªåº“è‡ªå·±çš„ CMake é…ç½®æ–‡ä»¶ â”‚ â””â”€â”€ my_lib.cpp â”‚ â””â”€â”€ CMakeLists.txt # æ•´ä¸ªé¡¹ç›®çš„é¡¶å±‚ CMake é…ç½®æ–‡ä»¶ build/: è¿™ä¸ªç›®å½•ç”¨äºæ‰§è¡Œæ‰€æœ‰æ„å»ºå‘½ä»¤ï¼Œæºç ä¸ä¼šè¢«æ±¡æŸ“ã€‚include/: å­˜æ”¾å¯ä»¥è¢«é¡¹ç›®å†…å…¶ä»–éƒ¨åˆ†ï¼ˆæˆ–è¢«å…¶ä»–é¡¹ç›®ï¼‰å¼•ç”¨çš„å¤´æ–‡ä»¶ã€‚æŒ‰æ¨¡å—ç»„ç»‡å¯ä»¥é¿å…å¤´æ–‡ä»¶åå†²çªã€‚src/: å­˜æ”¾æ‰€æœ‰ .cpp æºæ–‡ä»¶ã€‚ src/my_lib/: å°†é¡¹ç›®æŒ‰åŠŸèƒ½æ¨¡å—åŒ–æ˜¯ä¸€ç§å¥½ä¹ æƒ¯ã€‚æ¯ä¸ªæ¨¡å—ï¼ˆæ¯”å¦‚ä¸€ä¸ªåº“ï¼‰å¯ä»¥æœ‰è‡ªå·±çš„ CMakeLists.txt æ–‡ä»¶ï¼Œè´Ÿè´£ç®¡ç†è‡ªèº«çš„ç¼–è¯‘ã€‚ CMakeLists.txt (é¡¶å±‚): è¿™æ˜¯æ•´ä¸ªé¡¹ç›®çš„å…¥å£ï¼Œè´Ÿè´£è®¾ç½®å…¨å±€é…ç½®ã€æ‰¾åˆ°å¹¶æ„å»ºæ‰€æœ‰å­æ¨¡å—ï¼Œæœ€åç”Ÿæˆä¸»ç¨‹åºã€‚ ç¼–å†™å„å±‚çº§çš„ CMakeLists.txt ğŸ“æˆ‘ä»¬å°†é‡‡ç”¨â€œè‡ªä¸‹è€Œä¸Šâ€çš„æ–¹å¼æ¥ç¼–å†™é…ç½®æ–‡ä»¶ï¼Œå…ˆä»åº•å±‚çš„åº“å¼€å§‹ï¼Œå†åˆ°é¡¶å±‚çš„é¡¹ç›®ã€‚ ç¬¬ 1 æ­¥: åº“çš„ CMakeLists.txt (src/my_lib/CMakeLists.txt )è¿™ä¸ªæ–‡ä»¶åªè´Ÿè´£ä¸€ä»¶äº‹ï¼šå°† my_lib.cpp å’Œç›¸å…³çš„å¤´æ–‡ä»¶ç¼–è¯‘æˆä¸€ä¸ªåº“ã€‚# æ–‡ä»¶ä½ç½®: src/my_lib/CMakeLists.txt\n# ä½¿ç”¨ add_library å‘½ä»¤åˆ›å»ºä¸€ä¸ªåº“ã€‚ # è¯­æ³•: add_library(\u0026lt;åº“åç§°\u0026gt; [STATIC | SHARED] \u0026lt;æºæ–‡ä»¶...\u0026gt;) # # \u0026lt;åº“åç§°\u0026gt;: æˆ‘ä»¬ç§°ä¹‹ä¸º my_libï¼Œè¿™æ˜¯å…¶ä»–éƒ¨åˆ†é“¾æ¥æ­¤åº“æ—¶ä½¿ç”¨çš„åå­—ã€‚ # STATIC: ç”Ÿæˆé™æ€é“¾æ¥åº“ (.a, .lib)ã€‚ # SHARED: ç”ŸæˆåŠ¨æ€/å…±äº«é“¾æ¥åº“ (.so, .dll)ã€‚ # å¦‚æœä¸æŒ‡å®šï¼Œé»˜è®¤æ˜¯ STATICã€‚ # \u0026lt;æºæ–‡ä»¶\u0026gt;: ç”¨äºç¼–è¯‘è¿™ä¸ªåº“çš„æºæ–‡ä»¶åˆ—è¡¨ã€‚ add_library(my_lib STATIC my_lib.cpp) # ä¸ºè¿™ä¸ªåº“ç›®æ ‡æŒ‡å®šå®ƒéœ€è¦åŒ…å«çš„å¤´æ–‡ä»¶ç›®å½•ã€‚ # è¯­æ³•: target_include_directories(\u0026lt;ç›®æ ‡\u0026gt; \u0026lt;PUBLIC|PRIVATE|INTERFACE\u0026gt; \u0026lt;è·¯å¾„...\u0026gt;) # # \u0026lt;ç›®æ ‡\u0026gt;: å°±æ˜¯æˆ‘ä»¬ä¸Šé¢ç”¨ add_library åˆ›å»ºçš„ my_libã€‚ # PUBLIC: è¡¨ç¤ºæ­¤å¤´æ–‡ä»¶è·¯å¾„ä¸ä»… my_lib è‡ªå·±éœ€è¦ï¼Œä»»ä½•é“¾æ¥äº† my_lib çš„ç›®æ ‡ä¹Ÿéœ€è¦ã€‚ # è¿™æ˜¯æœ€å…³é”®çš„è®¾ç½®ï¼Œå®ƒå®ç°äº†ä¾èµ–çš„è‡ªåŠ¨ä¼ é€’ã€‚ # PRIVATE: è¡¨ç¤ºæ­¤å¤´æ–‡ä»¶è·¯å¾„åªæœ‰ my_lib å†…éƒ¨ç¼–è¯‘æ—¶éœ€è¦ï¼Œä¸ä¼šä¼ é€’ç»™é“¾æ¥å®ƒçš„ç›®æ ‡ã€‚ # INTERFACE:è¡¨ç¤ºæ­¤å¤´æ–‡ä»¶è·¯å¾„åªæœ‰é“¾æ¥å®ƒçš„ç›®æ ‡éœ€è¦ï¼Œmy_lib è‡ªå·±ç¼–è¯‘æ—¶ä¸éœ€è¦ã€‚ target_include_directories(my_lib PUBLIC # ${PROJECT_SOURCE_DIR} æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å†…ç½®å˜é‡ï¼ŒæŒ‡å‘é¡¶å±‚ CMakeLists.txt æ‰€åœ¨çš„ç›®å½•ã€‚ # æˆ‘ä»¬å°†é¡¹ç›®çš„å…¨å±€ include ç›®å½•æš´éœ²å‡ºå»ã€‚ ${PROJECT_SOURCE_DIR}/include ) add_library() å®šä¹‰äº†ä¸€ä¸ªç¼–è¯‘ç›®æ ‡â€”â€”ä¸€ä¸ªåº“ã€‚ target_include_directories() ä¸ºè¿™ä¸ªç›®æ ‡æŒ‡å®šäº†å¤´æ–‡ä»¶æœç´¢è·¯å¾„ã€‚ä½¿ç”¨ PUBLIC å…³é”®å­—è‡³å…³é‡è¦ä½¿å¾—ä»»ä½•é“¾æ¥åˆ° my_lib çš„ç¨‹åºéƒ½èƒ½è‡ªåŠ¨æ‰¾åˆ° my_lib.hï¼Œæ— éœ€åœ¨é“¾æ¥æ–¹å†æ¬¡æ‰‹åŠ¨æ·»åŠ å¤´æ–‡ä»¶è·¯å¾„ã€‚ ç¬¬ 2 æ­¥: é¡¶å±‚çš„ CMakeLists.txt è¿™ä¸ªæ–‡ä»¶æ˜¯æ•´ä¸ªé¡¹ç›®çš„æ€»æŒ‡æŒ¥ï¼Œè´Ÿè´£è®¾ç½®å…¨å±€é…ç½®ã€è°ƒç”¨å­æ¨¡å—ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆçš„å¯æ‰§è¡Œæ–‡ä»¶ã€‚\n# æ–‡ä»¶ä½ç½®: my_project/CMakeLists.txt # 1. æŒ‡å®š CMake çš„æœ€ä½ç‰ˆæœ¬è¦æ±‚ã€‚è¿™æ˜¯æ¯ä¸ªé¡¶å±‚æ–‡ä»¶éƒ½åº”è¯¥æœ‰çš„ç¬¬ä¸€è¡Œã€‚ cmake_minimum_required(VERSION 3.10) # 2. å®šä¹‰é¡¹ç›®ä¿¡æ¯ã€‚ # è¯­æ³•: project(\u0026lt;é¡¹ç›®åç§°\u0026gt; VERSION \u0026lt;ç‰ˆæœ¬å·\u0026gt; LANGUAGES \u0026lt;è¯­è¨€\u0026gt;) # è¿™ä¼šåˆ›å»ºä¸€äº›æœ‰ç”¨çš„å˜é‡ï¼Œæ¯”å¦‚ PROJECT_NAME, PROJECT_SOURCE_DIRã€‚ project(MyApp VERSION 1.0 LANGUAGES CXX) # 3. è®¾ç½® C++ æ ‡å‡† (è¿™æ˜¯ç°ä»£ CMake æ¨èçš„æ–¹å¼)ã€‚ set(CMAKE_CXX_STANDARD 17) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) # 4. æ‰“å°ä¸€æ¡æ¶ˆæ¯ï¼Œæ–¹ä¾¿è°ƒè¯•æ—¶æŸ¥çœ‹å˜é‡å€¼ (å¯é€‰)ã€‚ message(STATUS \u0026#34;Project source directory is: ${PROJECT_SOURCE_DIR}\u0026#34;) # 5. æ·»åŠ å­ç›®å½•ã€‚ # è¿™ä¸ªå‘½ä»¤ä¼šå‘Šè¯‰ CMake å»å¤„ç† src/my_lib ç›®å½•ä¸‹çš„ CMakeLists.txt æ–‡ä»¶ã€‚ # å½“æ‰§è¡Œåˆ°è¿™é‡Œæ—¶ï¼Œä¸Šé¢å®šä¹‰çš„ my_lib åº“ç›®æ ‡å°±ä¼šè¢«åˆ›å»ºå‡ºæ¥ã€‚ add_subdirectory(src/my_lib) # 6. æ·»åŠ å¯æ‰§è¡Œæ–‡ä»¶ã€‚ # è¯­æ³•: add_executable(\u0026lt;å¯æ‰§è¡Œæ–‡ä»¶å\u0026gt; \u0026lt;æºæ–‡ä»¶...\u0026gt;) # æˆ‘ä»¬å°†ä¸»ç¨‹åºå‘½åä¸º appï¼Œå®ƒç”± src/main.cpp ç¼–è¯‘è€Œæ¥ã€‚ add_executable(app src/main.cpp) # 7. é“¾æ¥åº“ï¼è¿™æ˜¯å°†æ‰€æœ‰éƒ¨åˆ†ç»„åˆåœ¨ä¸€èµ·çš„å…³é”®æ­¥éª¤ã€‚ # è¯­æ³•: target_link_libraries(\u0026lt;ç›®æ ‡\u0026gt; \u0026lt;PUBLIC|PRIVATE|INTERFACE\u0026gt; \u0026lt;è¦é“¾æ¥çš„åº“...\u0026gt;) # # \u0026lt;ç›®æ ‡\u0026gt;: æˆ‘ä»¬è¦é“¾æ¥çš„ç›®æ ‡ï¼Œå³ appã€‚ # PRIVATE: è¡¨ç¤º app çš„ç¼–è¯‘éœ€è¦ my_libï¼Œä½†è¿™ä¸ªä¾èµ–å…³ç³»ä¸ä¼šç»§ç»­ä¼ é€’ã€‚ # å¯¹äºå¯æ‰§è¡Œæ–‡ä»¶ï¼Œé€šå¸¸ä½¿ç”¨ PRIVATEã€‚ # \u0026lt;è¦é“¾æ¥çš„åº“\u0026gt;: æˆ‘ä»¬åœ¨å­ç›®å½•ä¸­å®šä¹‰çš„åº“ç›®æ ‡ my_libã€‚ target_link_libraries(app PRIVATE my_lib) add_subdirectory() ä½¿å¾—é¡¶å±‚æ–‡ä»¶ä¿æŒç®€æ´ï¼Œåªè´Ÿè´£â€œæŒ‡æŒ¥â€ï¼Œå…·ä½“å®ç°åˆ™äº¤ç»™å„ä¸ªå­æ¨¡å—ã€‚ target_link_libraries() è´Ÿè´£å°†ä¸åŒçš„ç¼–è¯‘ç›®æ ‡ï¼ˆåº“å’Œå¯æ‰§è¡Œæ–‡ä»¶ï¼‰é“¾æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆä¾èµ–å…³ç³»ã€‚ å¦‚ä½•æ„å»ºé¡¹ç›® ğŸš€ ç°åœ¨å·²ç»å†™å¥½äº†æ‰€æœ‰çš„ CMakeLists.txt æ–‡ä»¶ï¼Œå¯ä»¥å¼€å§‹æ„å»ºäº†ã€‚æ•´ä¸ªè¿‡ç¨‹éƒ½åœ¨ç»ˆç«¯ä¸­å®Œæˆã€‚ # 1. ç¡®ä¿ä½ ä½äºé¡¹ç›®çš„æ ¹ç›®å½• (my_project) cd path/to/my_project # 2. åˆ›å»ºå¹¶è¿›å…¥æˆ‘ä»¬è§„åˆ’å¥½çš„ build ç›®å½• mkdir build cd build # 3. è¿è¡Œ CMake æ¥ç”Ÿæˆæ„å»ºç³»ç»Ÿã€‚ # \u0026#39;..\u0026#39; æŒ‡å‘ä¸Šä¸€çº§ç›®å½•ï¼Œä¹Ÿå°±æ˜¯ my_project/ æ ¹ç›®å½•ï¼ŒCMake ä¼šåœ¨é‚£é‡Œå¯»æ‰¾é¡¶å±‚çš„ CMakeLists.txtã€‚ # -DCMAKE_BUILD_TYPE=Debug æŒ‡å®šäº†æ„å»ºç±»å‹ä¸º Debugï¼Œä¼šåŒ…å«è°ƒè¯•ä¿¡æ¯ã€‚ cmake -DCMAKE_BUILD_TYPE=Debug .. # CMake ä¼šæ‰«æä½ çš„ç³»ç»Ÿï¼Œæ‰¾åˆ° C++ ç¼–è¯‘å™¨ï¼Œç„¶åæ ¹æ® CMakeLists.txt çš„å†…å®¹ # ç”Ÿæˆç‰¹å®šå¹³å°çš„æ„å»ºæ–‡ä»¶ï¼ˆåœ¨ Linux/macOS ä¸Šæ˜¯ Makefileï¼Œåœ¨ Windows ä¸Šæ˜¯ Visual Studio sln æ–‡ä»¶ï¼‰ã€‚ # 4. ç¼–è¯‘é¡¹ç›® # è¿™ä¸ªå‘½ä»¤ä¼šè°ƒç”¨åº•å±‚çš„æ„å»ºå·¥å…·ï¼ˆå¦‚ make æˆ– msbuildï¼‰æ¥æ‰§è¡ŒçœŸæ­£çš„ç¼–è¯‘å’Œé“¾æ¥å·¥ä½œã€‚ # \u0026#39;--build .\u0026#39; æ˜¯ä¸€ä¸ªå¹³å°æ— å…³çš„å‘½ä»¤ï¼Œå‘Šè¯‰ CMake åœ¨å½“å‰ç›®å½•æ‰§è¡Œæ„å»ºã€‚ cmake --build . # æˆ–è€…åœ¨ Linux/macOS ä¸Šï¼Œä½ å¯ä»¥ç›´æ¥è¿è¡Œ: # make # ç¼–è¯‘å®Œæˆåï¼Œä½ ä¼šåœ¨ build ç›®å½•ï¼ˆæˆ–å…¶å­ç›®å½•ï¼‰ä¸‹æ‰¾åˆ°ä½ çš„å¯æ‰§è¡Œæ–‡ä»¶ `app` å’Œåº“æ–‡ä»¶ `libmy_lib.a`ã€‚ ","permalink":"http://localhost:1313/blogs/simple_cmake/","summary":"A Simple Cmake Example","title":"A Simple Cmake Example"},{"content":"What Can git rebase Do rebase çš„å­—é¢æ„æ€æ˜¯â€œå˜åŸºâ€â€”â€”ä¹Ÿå°±æ˜¯æ”¹å˜ä¸€ä¸ªåˆ†æ”¯çš„â€œåŸºç¡€â€æäº¤ç‚¹ã€‚å®ƒçš„ä¸»è¦ç›®æ ‡æ˜¯ï¼šå°†ä¸€ç³»åˆ—çš„æäº¤ä»¥æ›´æ•´æ´ã€çº¿æ€§çš„æ–¹å¼åº”ç”¨åˆ°å¦ä¸€ä¸ªåˆ†æ”¯ä¸Šï¼Œä»è€Œåˆ›é€ ä¸€ä¸ªå¹²å‡€ã€æ²¡æœ‰å¤šä½™åˆå¹¶è®°å½•çš„é¡¹ç›®å†å²ã€‚\nå‡è®¾ä½ çš„é¡¹ç›®å†å²æ˜¯è¿™æ ·çš„ï¼šä½ åœ¨ main åˆ†æ”¯ä¸Šåˆ‡å‡ºäº†ä¸€ä¸ª feature åˆ†æ”¯ï¼Œä¹‹å main åˆ†æ”¯å’Œä½ è‡ªå·±çš„ feature åˆ†æ”¯éƒ½æœ‰äº†æ–°çš„ commits.\nA---B---C \u0026lt;-- feature / D---E---F---G \u0026lt;-- main å¦‚æœä½ åœ¨ feature åˆ†æ”¯ä¸Šè¿è¡Œ git rebase mainï¼ŒGit ä¼šåšä¸€ä»¶éå¸¸ç¥å¥‡çš„äº‹ï¼š\nGit ä¼šæš‚æ—¶â€œæ”¶èµ·â€ feature åˆ†æ”¯ä¸Šçš„æ‰€æœ‰æäº¤ (A, B, C). å°† feature åˆ†æ”¯çš„èµ·ç‚¹ç§»åŠ¨åˆ° main åˆ†æ”¯çš„æœ€æ–°æäº¤ G ä¸Šã€‚ æŠŠåˆšæ‰æ”¶èµ·çš„æäº¤ (A, B, C) ä¾æ¬¡é‡æ–°åº”ç”¨åˆ°æ–°çš„èµ·ç‚¹ä¸Šï¼Œå½¢æˆæ–°çš„æäº¤ A\u0026rsquo;, B\u0026rsquo;, C' A\u0026#39;--B\u0026#39;--C\u0026#39; \u0026lt;-- feature / D---E---F---G \u0026lt;-- main A\u0026rsquo; å’Œ A çš„å†…å®¹è™½ç„¶ä¸€æ ·ï¼Œä½†å®ƒä»¬çš„ Commit ID æ˜¯ä¸åŒçš„ï¼Œå› ä¸ºå®ƒä»¬çš„çˆ¶æäº¤å˜äº†ã€‚rebase ç›¸å½“äºé‡å†™äº†å†å²ã€‚\nç°åœ¨ï¼Œå†åˆ‡æ¢å› main åˆ†æ”¯ï¼Œæ‰§è¡Œ git merge featureï¼Œç”±äº main åˆ†æ”¯çš„æ‰€æœ‰å†å²ç°åœ¨æ˜¯ feature åˆ†æ”¯å†å²çš„å­é›†ï¼ŒGit åªä¼šè¿›è¡Œä¸€æ¬¡ Fast-forward åˆå¹¶ï¼Œä¸ä¼šäº§ç”Ÿæ–°çš„åˆå¹¶æäº¤ã€‚æœ€ç»ˆç»“æœå¦‚ä¸‹\nD---E---F---G---A\u0026#39;--B\u0026#39;--C\u0026#39; \u0026lt;-- main, feature æœ€ç»ˆçš„é¡¹ç›®å†å²æ˜¯ä¸€æ¡å®Œç¾çš„ç›´çº¿ï¼Œéå¸¸æ¸…æ™°ï¼Œå°±åƒæ‰€æœ‰å¼€å‘éƒ½æ˜¯æŒ‰é¡ºåºå‘ç”Ÿçš„ä¸€æ ·ã€‚rebase é‡å†™äº†å†å²ï¼ŒæŠ¹å»äº†åˆ†æ”¯å¼€å‘çš„â€œå¹¶è¡Œâ€ç—•è¿¹ã€‚\nCompared to merge è¦ç†è§£ rebaseï¼Œæœ€å¥½çš„æ–¹æ³•å°±æ˜¯å’Œ merge å¯¹æ¯”ã€‚å¦‚æœåœ¨ main åˆ†æ”¯ä¸Šè¿è¡Œ git merge featureï¼Œç»“æœä¼šæ˜¯è¿™æ ·\nA---B---C / \\ D---E---F---G---H \u0026lt;-- main (H æ˜¯ä¸€ä¸ªåˆå¹¶æäº¤) merge åšçš„äº‹æƒ…æ˜¯ï¼š\næ‰¾åˆ°ä¸¤ä¸ªåˆ†æ”¯çš„å…±åŒç¥–å…ˆ E. å°†ä¸¤ä¸ªåˆ†æ”¯çš„ä¿®æ”¹æ•´åˆèµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå…¨æ–°çš„ Merge Commitï¼Œä¹Ÿå°±æ˜¯ H. è¯¥æäº¤æœ‰ä¸¤ä¸ªçˆ¶æäº¤ç‚¹ C å’Œ G. merge å®Œå…¨å…¨ä¿ç•™äº†å†å²çš„çœŸå®æ€§ã€‚å®ƒæ¸…æ¥šåœ°è®°å½•äº†â€œåœ¨æŸä¸ªæ—¶é—´ç‚¹ï¼Œæˆ‘ä»¬æŠŠä¸€ä¸ªåˆ†æ”¯åˆå¹¶äº†è¿›æ¥â€ã€‚ä½†å¦‚æœé¡¹ç›®é¢‘ç¹åˆå¹¶ï¼Œå†å²è®°å½•ä¼šå……æ»¡å¤§é‡çš„åˆå¹¶æäº¤ï¼Œå½¢æˆä¸€ä¸ªå¤æ‚çš„â€œè±å½¢â€æˆ–â€œæ„å¤§åˆ©é¢æ¡â€å¼çš„ç½‘çŠ¶ç»“æ„ï¼Œéš¾ä»¥é˜…è¯»ã€‚\nHow to use rebase å‡è®¾ä½ æ­£åœ¨ feature-login åˆ†æ”¯ä¸Šå¼€å‘ï¼ŒåŒæ—¶ä¸»åˆ†æ”¯ main ä¹Ÿæœ‰äº†æ–°çš„æ›´æ–°ã€‚\nç¡®ä¿ main åˆ†æ”¯å¤„äºæœ€æ–°çš„çŠ¶æ€ git checkout main git pull origin main åˆ‡æ¢åˆ°ä½ æ­£åœ¨å¼€å‘çš„åˆ†æ”¯ git checkout feature-login æŠŠ main åˆ†æ”¯ä¸Šçš„æœ€æ–°ä¿®æ”¹ rebase åˆ°ä½ å½“å‰çš„ feature-login åˆ†æ”¯ä¸Š git rebase main è§£å†³å†²çª (å¦‚æœæœ‰çš„è¯). å› ä¸º rebase æ˜¯é€ä¸ªåº”ç”¨æäº¤ï¼Œæ‰€ä»¥å¯èƒ½ä¼šåœ¨æŸä¸ªæäº¤åº”ç”¨æ—¶å‘ç”Ÿå†²çªã€‚æ­¤æ—¶ï¼Œrebase ä¼šæš‚åœã€‚ æ‰“å¼€å†²çªæ–‡ä»¶ï¼Œæ‰‹åŠ¨è§£å†³å†²çªï¼ˆå’Œ merge å†²çªä¸€æ ·ï¼‰ã€‚ è§£å†³åï¼Œä½¿ç”¨ git add \u0026lt;filename\u0026gt; å°†æ–‡ä»¶æ ‡è®°ä¸ºå·²è§£å†³ã€‚ ç„¶åï¼Œç»§ç»­ rebase è¿‡ç¨‹ git rebase --continue å¦‚æœä¸­é€”æƒ³æ”¾å¼ƒï¼Œå¯ä»¥å›åˆ° rebase å¼€å§‹å‰çš„çŠ¶æ€ git rebase --abort åˆå¹¶åˆ°ä¸»åˆ†æ”¯ rebase æˆåŠŸåï¼Œä½ çš„ feature-login åˆ†æ”¯å°±å·²ç»åŒ…å«äº† main çš„æ‰€æœ‰æ›´æ–°ï¼Œå¹¶ä¸”ä½ çš„æäº¤éƒ½åœ¨æœ€å‰é¢ã€‚ç°åœ¨å¯ä»¥è¿›è¡Œä¸€æ¬¡å¹²å‡€çš„å¿«è¿›åˆå¹¶ã€‚ git checkout main git merge feature-login When NOT to Use rebase **æ°¸è¿œä¸è¦å¯¹ä¸€ä¸ªå·²ç»æ¨é€åˆ° remoteï¼Œå¹¶ä¸”å¯èƒ½è¢«å›¢é˜Ÿå…¶ä»–äººä½¿ç”¨çš„å…¬å…±åˆ†æ”¯ (å¦‚ main, develop)è¿›è¡Œ rebaseï¼**å› ä¸º rebase ä¼šé‡å†™å†å²ã€‚å¦‚æœä½  rebase äº†ä¸€ä¸ªå…¬å…±åˆ†æ”¯å¹¶å¼ºåˆ¶æ¨é€ (git push --force)ï¼Œé‚£ä¹ˆæ‰€æœ‰å›¢é˜Ÿæˆå‘˜çš„æœ¬åœ°å†å²è®°å½•éƒ½å°†ä¸è¿œç¨‹çš„â€œæ–°å†å²â€äº§ç”Ÿä¸¥é‡åˆ†æ­§ã€‚\næ­£ç¡®ç”¨æ³•æ˜¯åªåœ¨ä½ è‡ªå·±çš„ã€è¿˜æœªä¸ä»–äººåˆ†äº«çš„æœ¬åœ°åˆ†æ”¯ä¸Šä½¿ç”¨ rebaseï¼Œç”¨æ¥æ•´ç†ä½ è‡ªå·±çš„æäº¤è®°å½•ï¼Œä»¥ä¾¿åœ¨åˆå¹¶åˆ°å…¬å…±åˆ†æ”¯å‰æœ‰ä¸€ä¸ªå¹²å‡€çš„å†å²ã€‚\nAdvanced Use git rebase -i git rebase -i å…è®¸ä½ åœ¨ rebase çš„è¿‡ç¨‹ä¸­ï¼Œå¯¹ä½ çš„æäº¤è¿›è¡Œç¼–è¾‘ã€åˆå¹¶ã€æ‹†åˆ†æˆ–åˆ é™¤ã€‚è¿™å¸¸ç”¨äºåœ¨åˆå¹¶åˆ° main åˆ†æ”¯å‰ï¼Œå°†è‡ªå·±æœ¬åœ°å‡Œä¹±çš„æäº¤ï¼ˆå¦‚ \u0026ldquo;ä¿®å¤æ‹¼å†™é”™è¯¯\u0026rdquo;, \u0026ldquo;ä¸´æ—¶æäº¤\u0026rdquo;, \u0026ldquo;åˆæ”¹äº†ä¸€ç‚¹\u0026rdquo;ï¼‰æ•´ç†æˆå‡ ä¸ªæœ‰æ„ä¹‰çš„æäº¤ã€‚\nå‡è®¾ä½ çš„ feature-login åˆ†æ”¯æœ‰ 3 ä¸ªå‡Œä¹±çš„æäº¤ï¼Œä½ æƒ³æŠŠå®ƒä»¬åˆå¹¶æˆä¸€ä¸ªã€‚\nå¯åŠ¨äº¤äº’å¼ rebase git rebase -i HEAD~3. å…¶ä¸­ HEAD~3 è¡¨ç¤ºä»å½“å‰æäº¤ (HEAD) å¾€å‰æ•° 3 ä¸ªæäº¤ã€‚ ç¼–è¾‘ Rebase è„šæœ¬ Git ä¼šæ‰“å¼€ä¸€ä¸ªæ–‡æœ¬ç¼–è¾‘å™¨ï¼Œåˆ—å‡ºè¿™ 3 ä¸ªæäº¤ï¼š pick a31ab34 complete login UI pick 58c34bb fix a button bug pick 948f2cb add backend verify logic åœ¨æ–‡ä»¶ä¸‹æ–¹ä¼šæœ‰æŒ‡ä»¤è¯´æ˜ã€‚ä½ å¯ä»¥ä¿®æ”¹æ¯ä¸€è¡Œå‰é¢çš„ pick å‘½ä»¤ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬æƒ³æŠŠåä¸¤ä¸ªæäº¤åˆå¹¶åˆ°ç¬¬ä¸€ä¸ªé‡Œé¢ï¼š\npick a31ab34 complete login UI squash 58c34bb fix a button bug squash 948f2cb add backend verify logic ä¿å­˜å¹¶é€€å‡ºç¼–è¾‘å™¨ Git ä¼šå¼€å§‹åˆå¹¶æäº¤ï¼Œå¹¶å¼¹å‡ºå¦ä¸€ä¸ªç¼–è¾‘å™¨ï¼Œè®©ä½ ä¸ºè¿™ä¸ªåˆå¹¶åçš„æ–°æäº¤ç¼–å†™ä¸€ä¸ªæ–°çš„ commit message. æ•´ç†å¥½åä¿å­˜é€€å‡ºã€‚ç°åœ¨å†ç”¨ git log æŸ¥çœ‹ï¼Œä½ ä¼šå‘ç°åŸæ¥ 3 ä¸ªå‡Œä¹±çš„æäº¤å·²ç»å˜æˆäº†ä¸€ä¸ªå¹²å‡€ã€å®Œæ•´çš„æäº¤ã€‚ ","permalink":"http://localhost:1313/blogs/git-rebase-flow/","summary":"Use of git rebase","title":"How to Use git rebase"},{"content":"MLIR çš„ä¸»è¦åŸåˆ™ä¹‹ä¸€æ˜¯é€æ­¥ä¸‹é™ï¼Œå³å­˜åœ¨è®¸å¤šçº§åˆ«çš„ IR ç²’åº¦ï¼Œå¹¶ä¸”é€æ­¥ä¸‹é™ IR çš„ä¸åŒéƒ¨åˆ†ï¼Œä»…åœ¨ä¸å†å¯¹ä¼˜åŒ–æœ‰ç”¨æ—¶ä¸¢å¼ƒä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œå°†å®Œæˆå…¶ä¸­çš„ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨æ‰€è°“çš„æ–¹è¨€è½¬æ¢åŸºç¡€è®¾æ–½å°†å¤šæ–¹è¨€ lowering ä¸ºæ ‡å‡†MLIRæ–¹è¨€çš„ç»„åˆã€‚\nThe Type Obstacle å¦‚æœä¸æ˜¯é’ˆå¯¹ç±»å‹ï¼Œæ–¹è¨€è½¬æ¢ (lowering) æœ¬è´¨ä¸Šä¸æ™®é€š pass ç›¸åŒï¼šç¼–å†™ä¸€äº›é‡å†™æ¨¡å¼å¹¶å°†å…¶åº”ç”¨äº IR. å¯¹äºæ¯ä¸ªéœ€è¦ lowering çš„ OP ï¼Œé€šå¸¸ä¼šæœ‰ä¸€ä¸ªé‡å†™æ¨¡å¼ã€‚\nç±»å‹ä½¿è¿™ä¸ªé—®é¢˜å˜å¾—æ›´åŠ å¤æ‚ï¼Œæˆ‘å°†é€šè¿‡polyçš„ç¤ºä¾‹æ¥æ¼”ç¤ºè¿™ä¸ªé—®é¢˜ã€‚\npoly.add å¯¹ä¸¤ä¸ªå¤šé¡¹å¼è¿›è¡Œç›¸åŠ å¹¶è¿”å›ç»“æœå¤šé¡¹å¼ã€‚æˆ‘ä»¬æƒ³ lowering polyã€‚ä¾‹å¦‚ï¼Œæ·»åŠ åˆ° arith.addi ç®—æœ¯è¿ç®—çš„çŸ¢é‡åŒ–å¾ªç¯ä¸­ã€‚ä½† arith å¹¶ä¸çŸ¥é“ poly.poly ç±»å‹çš„å­˜åœ¨ã€‚\nå¦‚æœå¿…é¡»ä½¿æ‰©å±• arith ä»¥äº†è§£polyï¼Œéœ€è¦å¯¹ arith è¿›è¡Œä¸Šæ¸¸æ›´æ”¹ã€‚æ·»åŠ  op çš„ operands ä»¥å…è®¸å®ç°æŸç§æ¥å£çš„ç±»å‹ï¼Œä¾‹å¦‚ integer-like æˆ– containers of integer-like.\næ‰€ä»¥ï¼Œé™¤äº† lowering opï¼Œè¿˜éœ€è¦ lowering poly. poly\u0026lt;N\u0026gt; å˜æˆå¼ é‡ \u0026lt;Nxi32\u0026gt;. è¿™å°±æ˜¯ç±»å‹éšœç¢å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚ä¸€æ—¦æ›´æ”¹äº†ç‰¹å®šå€¼çš„ç±»å‹ï¼Œä¾‹å¦‚ï¼Œåœ¨ lowering ç”Ÿæˆè¯¥å€¼ä½œä¸ºè¾“å‡ºçš„ OP æ—¶ï¼Œé‚£ä¹ˆè¯¥å€¼çš„æ‰€æœ‰ä¸‹æ¸¸ç”¨æˆ·ä»ç„¶æœŸæœ›ä½¿ç”¨æ—§ç±»å‹ï¼Œå¹¶ä¸”åœ¨ lowering å®ƒä»¬ä¹‹å‰åœ¨æŠ€æœ¯ä¸Šæ˜¯æ— æ•ˆçš„ã€‚åœ¨æ¯æ¬¡ä¼ é€’ä¹‹é—´ï¼ŒMLIRè¿è¡ŒéªŒè¯å™¨ä»¥ç¡®ä¿IRæœ‰æ•ˆï¼Œå› æ­¤å¦‚æœæ²¡æœ‰ä¸€äº›ç‰¹æ®Šå¤„ç†ï¼Œè¿™æ„å‘³ç€éœ€è¦åœ¨ä¸€æ¬¡ä¼ é€’ä¸­è½¬æ¢æ‰€æœ‰ç±»å‹å’Œ OP ï¼Œå¦åˆ™è¿™äº›éªŒè¯å™¨å°†å¤±è´¥ã€‚ä½†æ˜¯ç”¨æ ‡å‡†é‡å†™è§„åˆ™ç®¡ç†æ‰€æœ‰è¿™äº›å°†æ˜¯å›°éš¾çš„ï¼šå¯¹äºæ¯ä¸ªé‡å†™è§„åˆ™ï¼Œæ‚¨éƒ½å¿…é¡»ä¸æ–­æ£€æŸ¥å‚æ•°å’Œç»“æœæ˜¯å¦å·²ç»è½¬æ¢ã€‚\nä¾‹å¦‚åœ¨ lowering ä¸€ä¸ªç”Ÿæˆè¯¥å€¼ä½œä¸ºè¾“å‡ºçš„ OP æ—¶ï¼Œæ‰€æœ‰ä¾èµ–è¯¥å€¼çš„ä¸‹æ¸¸ç”¨æˆ·ä»ç„¶æœŸæœ›æ—§çš„ç±»å‹ï¼Œå› æ­¤åœ¨æŠ€æœ¯ä¸Šè¿™äº›ä¸‹æ¸¸ç”¨æˆ·åœ¨æœªè¢« lowering ä¹‹å‰æ˜¯æ— æ•ˆçš„ã€‚MLIR åœ¨æ¯æ¬¡è½¬æ¢ (pass) ä¹‹é—´è¿è¡ŒéªŒè¯å™¨ä»¥ç¡®ä¿ä¸­é—´è¡¨ç¤º (IR) æ˜¯æœ‰æ•ˆçš„ï¼Œå› æ­¤å¦‚æœæ²¡æœ‰ç‰¹æ®Šå¤„ç†ï¼Œè¿™æ„å‘³ç€æ‰€æœ‰ç±»å‹å’Œ OP å¿…é¡»åœ¨ä¸€ä¸ªè½¬æ¢ä¸­å…¨éƒ¨è½¬æ¢ï¼Œå¦åˆ™éªŒè¯å™¨ä¼šå¤±è´¥ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨æ ‡å‡†çš„é‡å†™è§„åˆ™æ¥ç®¡ç†è¿™ä¸€åˆ‡ä¼šå¾ˆå›°éš¾ï¼šå¯¹äºæ¯ä¸ª OP é‡å†™è§„åˆ™ï¼Œä½ éœ€è¦ä¸æ–­åœ°æ£€æŸ¥å‚æ•°å’Œç»“æœæ˜¯å¦å·²ç»è½¬æ¢ã€‚\nMLIR é€šè¿‡ä¸€ä¸ªå›´ç»•æ ‡å‡†è½¬æ¢çš„åŒ…è£…å™¨æ¥å¤„ç†è¿™ç§æƒ…å†µï¼Œè¿™ä¸ªåŒ…è£…å™¨è¢«ç§°ä¸ºæ–¹è¨€è½¬æ¢æ¡†æ¶(dialect conversion framework). ä½¿ç”¨è¿™ä¸ªæ¡†æ¶éœ€è¦ç”¨æˆ·ç»§æ‰¿ä¸åŒçš„ç±»æ¥å®ç°æ™®é€šçš„é‡å†™ï¼Œè®¾ç½®ä¸€äº›é¢å¤–çš„å…ƒæ•°æ®ï¼Œå¹¶ä»¥ç‰¹å®šçš„æ–¹å¼ å°†ç±»å‹è½¬æ¢ä¸ OP è½¬æ¢åˆ†å¼€ï¼Œæˆ‘ä»¬ç¨åä¼šçœ‹åˆ°å…·ä½“æ–¹å¼ã€‚ä½†ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œè¿™ä¸ªæ¡†æ¶é€šè¿‡ä»¥æŸç§æ’åºé¡ºåº lowering OP ã€åŒæ—¶è½¬æ¢ç±»å‹ï¼Œå¹¶è®© OP è½¬æ¢å™¨èƒ½å¤Ÿè®¿é—®æ¯ä¸ª OP çš„åŸå§‹ç±»å‹ä»¥åŠåœ¨ OP è¢«æ¡†æ¶è®¿é—®æ—¶çš„è¿›è¡Œä¸­çš„è½¬æ¢ç±»å‹ã€‚æ¯ä¸ªåŸºäº OP çš„é‡å†™æ¨¡å¼éƒ½æœŸæœ›åœ¨è®¿é—®åä½¿è¯¥ OP çš„ç±»å‹åˆæ³•ï¼Œä½†ä¸éœ€è¦æ‹…å¿ƒä¸‹æ¸¸ OP.\nModes of Conversion å½“å¯¹ä¸€ç»„ OP è¿›è¡Œè½¬æ¢æ—¶ï¼Œæœ‰å‡ ç§ä¸åŒçš„è½¬æ¢æ¨¡å¼å¯ä¾›é€‰æ‹©ï¼š\nPartial Conversion ä½¿å°½å¯èƒ½å¤šçš„å¯¹ç›®æ ‡çš„æ“ä½œåˆæ³•åŒ–ï¼Œä½†å°†å…è®¸æœªæ˜¾å¼æ ‡è®°ä¸ºâ€œéæ³•â€çš„é¢„å…ˆå­˜åœ¨çš„æ“ä½œä¿æŒæœªè½¬æ¢ã€‚è¿™å…è®¸åœ¨å­˜åœ¨æœªçŸ¥æ“ä½œçš„æƒ…å†µä¸‹éƒ¨åˆ†é™ä½è¾“å…¥ã€‚ å¯ä»¥é€šè¿‡ applyPartialConversion è¿›è¡Œéƒ¨åˆ†è½¬æ¢ã€‚ Full Conversion ä½¿æ‰€æœ‰è¾“å…¥æ“ä½œåˆæ³•åŒ–ï¼Œå¹¶ä¸”åªæœ‰å½“æ‰€æœ‰æ“ä½œéƒ½æ­£ç¡®åœ°åˆæ³•åŒ–åˆ°ç»™å®šçš„è½¬æ¢ç›®æ ‡æ—¶æ‰æˆåŠŸã€‚è¿™ç¡®ä¿äº†åœ¨è½¬æ¢è¿‡ç¨‹ä¹‹ååªå­˜åœ¨å·²çŸ¥çš„æ“ä½œã€‚ å¯ä»¥é€šè¿‡ applyFullConversion è¿›è¡Œå®Œæ•´è½¬æ¢ã€‚ Analysis Conversion å¦‚æœè¦åº”ç”¨è½¬æ¢ï¼ŒAnalysis Conversion å°†åˆ†æå“ªäº›æ“ä½œå¯¹ç»™å®šçš„è½¬æ¢ç›®æ ‡æ˜¯åˆæ³•çš„ã€‚è¿™æ˜¯é€šè¿‡æ‰§è¡Œ \u0026lsquo;Partial\u0026rsquo; Conversion å¹¶è®°å½•å“ªäº›æ“ä½œå¦‚æœæˆåŠŸå°†è¢«æˆåŠŸè½¬æ¢æ¥å®Œæˆçš„ã€‚æ³¨æ„ï¼Œæ²¡æœ‰ rewrites æˆ–è½¬æ¢å®é™…åº”ç”¨äºè¾“å…¥æ“ä½œã€‚ å¯ä»¥é€šè¿‡ a pplyAnalysisConversion åº”ç”¨åˆ†æè½¬æ¢ã€‚ Conversion Target è½¬æ¢ç›®æ ‡æ˜¯åœ¨è½¬æ¢è¿‡ç¨‹ä¸­è¢«è®¤ä¸ºæ˜¯åˆæ³•çš„å†…å®¹çš„æ­£å¼å®šä¹‰ã€‚è½¬æ¢æ¡†æ¶ç”Ÿæˆçš„æœ€ç»ˆæ“ä½œå¿…é¡»åœ¨converontargetä¸Šæ ‡è®°ä¸ºåˆæ³•ï¼Œè¿™æ ·é‡å†™æ‰èƒ½æˆåŠŸã€‚æ ¹æ®è½¬æ¢æ¨¡å¼çš„ä¸åŒï¼Œç°æœ‰æ“ä½œä¸ä¸€å®šæ€»æ˜¯åˆæ³•çš„ã€‚æ“ä½œå’Œæ–¹è¨€å¯ä»¥æ ‡è®°ä¸ºä¸‹åˆ—ä»»ä½•è§„å®šçš„åˆæ³•æ€§è¡Œä¸ºï¼š\nLegal: è¡¨æ˜ç»™å®šæ“ä½œçš„æ¯ä¸ªå®ä¾‹éƒ½æ˜¯åˆæ³•çš„ï¼Œå³å±æ€§ã€æ“ä½œæ•°ã€ç±»å‹ç­‰çš„ä»»ä½•ç»„åˆéƒ½æ˜¯æœ‰æ•ˆçš„ã€‚ Dynamic: æ­¤æ“ä½œè¡¨ç¤ºç»™å®šæ“ä½œçš„æŸäº›å®ä¾‹æ˜¯åˆæ³•çš„ã€‚è¿™å…è®¸å®šä¹‰å¾®è°ƒçº¦æŸï¼Œä¾‹å¦‚ï¼Œarith.addi ä»…åœ¨æ“ä½œ32ä½æ•´æ•°æ—¶åˆ- Illegal: æ­¤æ“ä½œè¡¨ç¤ºç»™å®šæ“ä½œçš„å®ä¾‹ä¸åˆæ³•ã€‚ä¸ºä½¿è½¬æ¢æˆåŠŸï¼Œå¿…é¡»å§‹ç»ˆè½¬æ¢æ ‡è®°ä¸ºâ€œéæ³•â€çš„æ“ä½œã€‚æ­¤æ“ä½œè¿˜å…è®¸æœ‰é€‰æ‹©åœ°å°†ç‰¹å®šæ“ä½œæ ‡è®°ä¸ºéæ³•ï¼Œå¦åˆ™å°†æ˜¯åˆæ³•çš„æ–¹è¨€ã€‚ æœªæ˜ç¡®æ ‡è®°ä¸ºåˆæ³•æˆ–éæ³•çš„æ“ä½œå’Œæ–¹è¨€ä¸ä¸Šè¿°ï¼ˆâ€œæœªçŸ¥â€æ“ä½œï¼‰åˆ†å¼€ï¼Œå¹¶è¢«åŒºåˆ«å¯¹å¾…ï¼Œä¾‹å¦‚ï¼Œå‡ºäºä¸Šè¿°éƒ¨åˆ†è½¬æ¢çš„ç›®çš„ã€‚\næœ€åï¼Œæ–¹è¨€è½¬æ¢æ¡†æ¶ä¼šè·Ÿè¸ªä»»ä½•æœªè§£å†³çš„ç±»å‹å†²çªã€‚å¦‚æœåœ¨è½¬æ¢ç»“æŸæ—¶ä»å­˜åœ¨ç±»å‹å†²çªï¼Œä¼šå‘ç”Ÿä»¥ä¸‹ä¸¤ç§æƒ…å†µä¹‹ä¸€ã€‚è½¬æ¢æ¡†æ¶å…è®¸ç”¨æˆ·å¯é€‰åœ°å®ç°ä¸€ä¸ªç§°ä¸ºç±»å‹ç‰©åŒ–å™¨ (type materializer) çš„åŠŸèƒ½ï¼Œå®ƒä¼šæ’å…¥æ–°çš„ä¸­é—´ OP æ¥è§£å†³ç±»å‹å†²çªã€‚å› æ­¤ï¼Œç¬¬ä¸€ç§å¯èƒ½æ˜¯æ–¹è¨€è½¬æ¢æ¡†æ¶ä½¿ç”¨ä½ çš„ç±»å‹ç‰©åŒ–å™¨é’©å­æ¥ä¿®è¡¥ IRï¼Œè½¬æ¢æˆåŠŸç»“æŸã€‚å¦‚æœè¿™äº›é’©å­å¤±è´¥ï¼Œæˆ–è€…ä½ æ²¡æœ‰å®šä¹‰ä»»ä½•é’©å­ï¼Œé‚£ä¹ˆè½¬æ¢ä¼šå¤±è´¥ã€‚\nè¿™ç§åŸºç¡€è®¾æ–½çš„å¤æ‚æ€§éƒ¨åˆ†è¿˜ä¸ä¸Šæ¸¸ MLIR ä¸­ä¸€ä¸ªæ›´å›°éš¾çš„ lowering æµæ°´çº¿æœ‰å…³ï¼šç¼“å†²åŒºåŒ–æµæ°´çº¿ (bufferization pipeline). è¿™ä¸ªæµæ°´çº¿æœ¬è´¨ä¸Šå°†ä½¿ç”¨ value semantics çš„æ“ä½œçš„ IR è½¬æ¢ä¸ºä½¿ç”¨ pointer semantics çš„ä¸­é—´è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œå¼ é‡ç±»å‹ (tensor type) åŠå…¶ç›¸å…³æ“ä½œå…·æœ‰ value semanticsï¼Œè¿™æ„å‘³ç€æ¯ä¸ªæ“ä½œåœ¨è¯­ä¹‰ä¸Šéƒ½ä¼šç”Ÿæˆä¸€ä¸ªå…¨æ–°çš„å¼ é‡ä½œä¸ºè¾“å‡ºï¼Œå¹¶ä¸”æ‰€æœ‰æ“ä½œéƒ½æ˜¯ pure çš„ (æœ‰ä¸€äº›ä¾‹å¤–æƒ…å†µ) ã€‚å¦ä¸€æ–¹é¢ï¼Œ memref å…·æœ‰ pointer semanticsï¼Œæ„å‘³ç€å®ƒæ›´æ¥è¿‘äºå¯¹ç‰©ç†ç¡¬ä»¶çš„å»ºæ¨¡ï¼Œéœ€è¦æ˜¾å¼çš„å†…å­˜åˆ†é…ï¼Œå¹¶æ”¯æŒå¯¹å†…å­˜ä½ç½®è¿›è¡Œå˜åŠ¨çš„æ“ä½œã€‚\nç”±äºç¼“å†²åŒºåŒ–è¿‡ç¨‹å¤æ‚ï¼Œå®ƒè¢«æ‹†åˆ†ä¸º sub-passesï¼Œåˆ†åˆ«å¤„ç†ä¸ä¸Šæ¸¸ MLIR å„ç›¸å…³æ–¹è¨€ç‰¹å®šçš„ç¼“å†²åŒºåŒ–é—®é¢˜ (å‚è§æ–‡æ¡£ï¼Œä¾‹å¦‚ arith-bufferizeã€func-bufferize ç­‰) ã€‚æ¯ä¸ªç¼“å†²åŒºåŒ–è½¬æ¢éƒ½ä¼šäº§ç”Ÿä¸€äº›å†…éƒ¨æ— æ³•è§£å†³çš„ç±»å‹å†²çªï¼Œè¿™äº›å†²çªéœ€è¦è‡ªå®šä¹‰çš„ç±»å‹ç‰©åŒ– (type materializations) æ¥è§£å†³ã€‚ä¸ºäº†åœ¨æ‰€æœ‰ç›¸å…³æ–¹è¨€ä¸­å¤„ç†è¿™äº›é—®é¢˜ï¼ŒMLIR å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªä¸“é—¨çš„æ–¹è¨€ï¼Œç§°ä¸ºç¼“å†²åŒºåŒ–æ–¹è¨€ (bufferization dialect) ï¼Œç”¨æ¥å­˜æ”¾ä¸­é—´æ“ä½œã€‚ä½ ä¼šæ³¨æ„åˆ°åƒ to_memref å’Œ to_tensor è¿™æ ·çš„æ“ä½œï¼Œå®ƒä»¬æ‰®æ¼”äº†è¿™ä¸€è§’è‰²ã€‚ç„¶åè¿˜æœ‰ä¸€ä¸ªæœ€ç»ˆç¼“å†²åŒºåŒ–è½¬æ¢ (finalizing-bufferize pass) ï¼Œå…¶ä½œç”¨æ˜¯æ¸…ç†ä»»ä½•æ®‹ç•™çš„ç¼“å†²åŒºåŒ–æˆ–ç‰©åŒ–æ“ä½œã€‚\nLowering Poly with Type Materializations è·Ÿä¹‹å‰å†™ Pass tablegen çš„æ—¶å€™å¤§åŒå°å¼‚ï¼Œä¸»è¦æ˜¯éœ€è¦å®šä¹‰ dependent dialects. Lowering å¿…é¡»ä»¥è¿™ç§æ–¹å¼ä¾èµ–äºåŒ…å«å°†åˆ›å»ºçš„æ“ä½œæˆ–ç±»å‹çš„ä»»ä½•æ–¹è¨€ï¼Œä»¥ç¡®ä¿ MLIR åœ¨å°è¯•è¿è¡Œ pass ä¹‹å‰åŠ è½½è¿™äº›æ–¹è¨€ã€‚\n// include/Conversion/PolyToStandard/PolyToStandard.td #ifndef LIB_CONVERSION_POLYTOSTANDARD_POLYTOSTANDARD_TD_ #define LIB_CONVERSION_POLYTOSTANDARD_POLYTOSTANDARD_TD_ include \u0026#34;mlir/Pass/PassBase.td\u0026#34; def PolyToStandard : Pass\u0026lt;\u0026#34;poly-to-standard\u0026#34;\u0026gt; { let summary = \u0026#34;Lower `poly` to standard MLIR dialects.\u0026#34;; let description = [{ This pass lowers the `poly` dialect to standard MLIR, a mixture of affine, tensor, and arith. }]; let dependentDialects = [ \u0026#34;mlir::arith::ArithDialect\u0026#34;, \u0026#34;mlir::tutorial::poly::PolyDialect\u0026#34;, \u0026#34;mlir::tensor::TensorDialect\u0026#34;, ]; } #endif // LIB_CONVERSION_POLYTOSTANDARD_POLYTOSTANDARD_TD_ ä¸‹ä¸€æ­¥éœ€è¦å®šä¹‰ ConversionTargetï¼Œå‘Šè¯‰ MLIR å“ªäº› OP éœ€è¦è¿›è¡Œ loweringï¼Œå¯ä»¥å®šä¹‰æ•´ä¸ªéœ€è¦ä¸‹é™çš„ dialect ä¸º illegalï¼Œç¡®ä¿åœ¨è½¬æ¢å®Œæˆåæ²¡æœ‰è¯¥ dialect. è¿™é‡Œä½¿ç”¨ applyPartialConversion è€Œä¸æ˜¯ applyFullConversion çš„åŸå› æ˜¯æŠ¥é”™æ¶ˆæ¯æ›´ç›´è§‚ã€‚Partial Conversion å¯ä»¥çœ‹åˆ°æ­¥éª¤ä»¥åŠæœ€åæ— æ³•ä¿®è¡¥çš„å†²çªç±»å‹ã€‚\n// lib/Conversion/PolyToStandard/PolyToStandard.cpp struct PolyToStandard : impl::PolyToStandardBase\u0026lt;PolyToStandard\u0026gt; { using PolyToStandardBase::PolyToStandardBase; void runOnOperation() override { MLIRContext *context = \u0026amp;getContext(); auto *module = getOperation(); // TODO: implement pass ConversionTarget target(*context); target.addIllegalDialect\u0026lt;PolyDialect\u0026gt;(); // declare an entire dialect as â€œillegalâ€ RewritePatternSet patterns(context); if (failed(applyPartialConversion(module, target, std::move(patterns)))) { signalPassFailure(); } } }; æ¥ä¸‹æ¥éœ€è¦å®šä¹‰ä¸€ä¸ª TypeConverter çš„å­ç±»å°† poly dialect ä¸‹çš„ type è½¬æ¢æˆå…¶ä»–ç±»å‹. å…¶ä¸­ç±»å‹è½¬æ¢å’Œ materialization æ˜¯åˆ†åˆ«é€šè¿‡ addConversion å’Œ addMaterialization å®Œæˆçš„ã€‚è¿™é‡Œæˆ‘ä»¬å°†å±äº poly.poly ç±»å‹çš„ degreBound è½¬æ¢æˆ Tensor.\nclass PolyToStandardTypeConverter : public TypeConverter { public: PolyToStandardTypeConverter(MLIRContext* ctx) { addConversion([](Type type) { return type; }); addConversion([ctx](PolynomialType type) -\u0026gt; Type { int degreeBound = type.getDegreeBound(); IntegerType elementType = IntegerType::get( ctx, 32, IntegerType::SignednessSemantics::Signless); return RankedTensorType::get({degreeBound}, elementType); }); } }; æ¥ä¸‹æ¥å°±æ˜¯è¦è½¬æ¢ Poly ä¸­çš„å„ç§ opï¼Œéœ€è¦ç»§æ‰¿ OpConversionPatternï¼Œé‡å†™é‡Œé¢çš„ matchAndRewrtite æ–¹æ³•. ä»¥ poly.add ä¸ºä¾‹ï¼Œæ ¹æ®çˆ¶ç±»é‡Œçš„å®šä¹‰ï¼Œè¿™é‡Œ OpAdaptor å³ä¸º AddOp:OpAdaptorï¼Œå®ƒä½¿ç”¨ tablegen å®šä¹‰çš„åç§°ä½œä¸º op çš„å‚æ•°å’Œæ–¹æ³•åç§°çš„ç»“æœï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„çš„getOperand. AddOp å‚æ•°åŒ…å«åŸå§‹çš„ã€æœªç±»å‹è½¬æ¢çš„æ“ä½œæ•°å’Œç»“æœã€‚ConversionPatternRewriterç±» ä¼¼äºPatternRewriterï¼Œä½†æœ‰ä¸æ–¹è¨€è½¬æ¢ç›¸å…³çš„å…¶ä»–æ–¹æ³•ï¼Œä¾‹å¦‚ convertRegionTypesï¼Œç”¨äºä¸ºåµŒå¥—åŒºåŸŸçš„æ“ä½œåº”ç”¨ç±»å‹è½¬æ¢ã€‚å¯¹IR\nstruct ConvertAdd : public OpConversionPattern\u0026lt;AddOp\u0026gt; { ConvertAdd(MLIRContext* context) : OpConversionPattern\u0026lt;AddOp\u0026gt;(context) { } using OpConversionPattern::OpConversionPattern; LogicalResult matchAndRewrite( AddOp op, OpAdaptor adaptor, ConversionPatternRewriter\u0026amp; rewriter) const override { auto addOp = rewriter.create\u0026lt;arith::AddIOp\u0026gt;( op-\u0026gt;getLoc(), adaptor.getLhs(), adaptor.getRhs()); rewriter.replaceOp(op.getOperation(), addOp); return success(); } }; ä¸‹é¢æˆ‘ä»¬éœ€è¦å°† ConvertAdd æ·»åŠ è¿› PolyToStandard::runOnOperation ä¸­å®šä¹‰çš„ RewriterPatternSet ä¸­ã€‚\nvoid runOnOperation() { ... RewritePatternSet patterns(context); PolyToStandardTypeConverter typeConverter(context); patterns.add\u0026lt;ConvertAdd\u0026gt;(typeConverter, context); } ","permalink":"http://localhost:1313/blogs/courselearning/mlir/mlir-ch9-dialect-conversion/","summary":"Personal MLIR learning notes 9.","title":"MLIR-Ch9 Dialect Conversion"},{"content":"Why is Canonicalization Needed? è§„èŒƒåŒ–å™¨å¯ä»¥ç”¨æ ‡å‡†çš„æ–¹å¼ç¼–å†™ï¼šåœ¨ tablegen ä¸­å£°æ˜ op å…·æœ‰è§„èŒƒåŒ–å™¨ï¼Œç„¶åå®ç°ç”Ÿæˆçš„ C++å‡½æ•°å£°æ˜ã€‚å®˜ç½‘ä¾‹å­å¦‚ä¸‹\ndef MyOp : ... { // I want to define a fully general set of patterns for this op. let hasCanonicalizer = 1; } def OtherOp : ... { // A single \u0026#34;matchAndRewrite\u0026#34; style RewritePattern implemented as a method // is good enough for me. let hasCanonicalizeMethod = 1; } Canonicalization æ¨¡å¼å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼å®šä¹‰\nvoid MyOp::getCanonicalizationPatterns(RewritePatternSet \u0026amp;patterns, MLIRContext *context) { patterns.add\u0026lt;...\u0026gt;(...); } LogicalResult OtherOp::canonicalize(OtherOp op, PatternRewriter \u0026amp;rewriter) { // patterns and rewrites go here. return failure(); } Canonicalizers in C++ åœ¨ Op å®šä¹‰ä¸­æ·»åŠ  let hasCanonicalizeMethod = 1; åä¼šä¸ºè¯¥ Op ç”Ÿæˆå¦‚ä¸‹çš„å‡½æ•°å£°æ˜ã€‚\nstatic void getCanonicalizationPatterns( ::mlir::RewritePatternSet\u0026amp; results, ::mlir::MLIRContext* context ); è¿™ä¸ªå‡½æ•°éœ€è¦å¯¹ results åŠ å…¥è‡ªå®šä¹‰çš„ OpRewritePattern. ä¾‹å¦‚å¯ä»¥é‡å†™ x^2 - y^2 è¿™ä¸ª SubOp ä¸º (x+y)(x-y)ï¼Œå½“ x^2 å’Œ y^2 åœ¨åç»­æ²¡æœ‰è¢«ä½¿ç”¨æ—¶ã€‚\nstruct DifferenceOfSquares : public OpRewritePattern\u0026lt;SubOp\u0026gt; { DifferenceOfSquares(mlir::MLIRContext* context) : OpRewritePattern\u0026lt;SubOp\u0026gt;(context, 1) { } LogicalResult matchAndRewrite(SubOp op, PatternRewriter\u0026amp; rewriter) const override { Value lhs = op-\u0026gt;getOperand(0); // x^2 Value rhs = op-\u0026gt;getOperand(0); // y^2 // If either arg has another use, then this rewrite is probably less // efficient, because it cannot delete the mul ops. if (!lhs.hasOneUse() || !rhs.hasOneUse()) { return failure(); } auto rhsMul = rhs.getDefiningOp\u0026lt;SubOp\u0026gt;(); auto lhsMul = rhs.getDefiningOp\u0026lt;SubOp\u0026gt;(); if (!rhsMul || !lhsMul) { return failure(); } // check if lhsMul \u0026amp;\u0026amp; rhsMul is squre operation bool rhsMulOpsAgree = rhsMul.getLhs() == rhsMul.getRhs(); bool lhsMulOpsAgree = lhsMul.getLhs() == lhsMul.getRhs(); if (!rhsMulOpsAgree || !lhsMulOpsAgree) { return failure(); } auto x = lhsMul.getLhs(); auto y = rhsMul.getLhs(); auto newAdd = rewriter.create\u0026lt;AddOp\u0026gt;(op-\u0026gt;getLoc(), x, y); auto newSub = rewriter.create\u0026lt;AddOp\u0026gt;(op-\u0026gt;getLoc(), x, y); auto newMul = rewriter.create\u0026lt;AddOp\u0026gt;(op-\u0026gt;getLoc(), newAdd, newSub); rewriter.replaceOp(op, newMul); // We don\u0026#39;t need to remove the original ops because MLIR already has // canonicalization patterns that remove unused ops. return success(); } }; void SubOp::getCanonicalizationPatterns(::mlir::RewritePatternSet\u0026amp; results, ::mlir::MLIRContext* context) { results.add\u0026lt;DifferenceOfSquares\u0026gt;(context); } Canonicalizers in Tablegen ä¸‹é¢åˆ©ç”¨ tablegen å®ç°ä¸€ä¸ªå¤šé¡¹å¼å…±è½­çš„ canonicalizerï¼Œf(conj(z)) = conj(f(z)).\n// PolyPatterns.td def LiftConjThroughEval : Pat\u0026lt;(Poly_EvalOp $f, (ConjOp $z, $fastmath)), (ConjOp (Poly_EvalOp $f, $z), $fastmath)\u0026gt;; è¿™é‡Œçš„ä¹‰äº†é‡å†™æ¨¡å¼çš„ Pat ç±»å’Œå®šä¹‰è¦åŒ¹é…å’Œé‡å†™çš„ IR tree çš„æ‹¬å·. Pattern å’Œ Pat çš„å®šä¹‰å¦‚ä¸‹\nclass Pattern\u0026lt;dag source, list\u0026lt;dag\u0026gt; results, list\u0026lt;dag\u0026gt; preds = [], list\u0026lt;dag\u0026gt; supplemental_results = [], dag benefitAdded = (addBenefit 0)\u0026gt; { dag sourcePattern = source; list\u0026lt;dag\u0026gt; resultPatterns = results; // æ³¨æ„è¿™é‡Œæ˜¯ list\u0026lt;dag\u0026gt; list\u0026lt;dag\u0026gt; constraints = preds; list\u0026lt;dag\u0026gt; supplementalPatterns = supplemental_results; dag benefitDelta = benefitAdded; } class Pat\u0026lt;dag pattern, dag result, list\u0026lt;dag\u0026gt; preds = [], list\u0026lt;dag\u0026gt; supplemental_results = [], dag benefitAdded = (addBenefit 0)\u0026gt; : Pattern\u0026lt;pattern, [result], preds, supplemental_results, benefitAdded\u0026gt;; Pattern ç±»æ¥å—ä¸€ä¸ªåä¸º results çš„æ¨¡æ¿å‚æ•°ï¼Œå®ƒæ˜¯ä¸€ä¸ª list\u0026lt;dag\u0026gt; ç±»å‹ï¼Œå¯ä»¥å®šä¹‰ä¸€ä¸ªæˆ–å¤šä¸ªç»“æœæ¨¡å¼ã€‚è¿™ä½¿å¾— Pattern éå¸¸çµæ´»ï¼Œå¯ä»¥ç”¨äºå¤„ç†ä»¥ä¸‹æƒ…å†µï¼š\næºæ“ä½œäº§ç”Ÿå¤šä¸ªç»“æœï¼Œå¹¶ä¸”æ¯ä¸ªç»“æœéƒ½éœ€è¦è¢«ä¸åŒçš„æ–°æ“ä½œæ›¿æ¢ã€‚ é‡å†™è¿‡ç¨‹éœ€è¦ç”Ÿæˆä¸€äº›è¾…åŠ©æ“ä½œï¼Œè¿™äº›è¾…åŠ©æ“ä½œæœ¬èº«ä¸ç›´æ¥æ›¿æ¢æºæ“ä½œçš„ç»“æœï¼Œä½†æœ‰åŠ©äºæ„å»ºæœ€ç»ˆçš„æ›¿æ¢ç»“æœã€‚ Pat ç±»ç»§æ‰¿è‡ª Pattern ç±»ã€‚è¾“å…¥æ˜¯ä¸¤ä¸ªIR tree å¯¹è±¡ (MLIRç§°ä¹‹ä¸º DAG nodes)ï¼Œæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ç”±æ‹¬å· () æŒ‡å®šï¼Œæ‹¬å·ä¸­çš„ç¬¬ä¸€ä¸ªå€¼æ˜¯æ“ä½œçš„åç§°ï¼Œå…¶ä½™å‚æ•°æ˜¯ op çš„å‚æ•°æˆ–å±æ€§ã€‚å½“èŠ‚ç‚¹å¯ä»¥åµŒå¥—ï¼Œè¿™å¯¹åº”äºåº”ç”¨äºå‚æ•°çš„åŒ¹é…ã€‚å®ƒå°†è¿™ä¸ªå•ä¸€çš„ result DAG åŒ…è£…æˆä¸€ä¸ªåªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„åˆ—è¡¨ [result] ï¼Œç„¶åä¼ é€’ç»™çˆ¶ç±» Pattern çš„ results å‚æ•°ã€‚å› æ­¤ Pat å®é™…ä¸Šæ˜¯ Pattern çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œä¸“é—¨ç”¨äºå®šä¹‰é‚£äº›åªäº§ç”Ÿå•ä¸€ç»“æœæ¨¡å¼çš„é‡å†™è§„åˆ™ã€‚\nç”Ÿæˆçš„ä»£ç å¦‚ä¸‹æ‰€ç¤º\n/* Generated from: /code/sac_mlir_learning/Ch8-DialectConversion/include/mlir-tutorial/Dialect/Poly/PolyPatterns.td:8 */ // å®šä¹‰ä¸€ä¸ªåä¸º LiftConjThroughEval çš„é‡å†™æ¨¡å¼ç»“æ„ä½“ï¼Œç»§æ‰¿è‡ª mlir::RewritePattern struct LiftConjThroughEval : public ::mlir::RewritePattern { // æ„é€ å‡½æ•° LiftConjThroughEval(::mlir::MLIRContext* context) : ::mlir::RewritePattern(\u0026#34;poly.eval\u0026#34;, // æ­¤æ¨¡å¼åŒ¹é…çš„æ ¹æ“ä½œå 2, // æ­¤æ¨¡å¼çš„æ”¶ç›Š (benefit)ï¼Œç”¨äºè§£å†³å¤šä¸ªæ¨¡å¼åŒ¹é…æ—¶çš„ä¼˜å…ˆçº§ context, {\u0026#34;complex.conj\u0026#34;, \u0026#34;poly.eval\u0026#34;} /* ä¾èµ–æˆ–ç”Ÿæˆçš„å…¶ä»–æ“ä½œååˆ—è¡¨ */) { } // æ ¸å¿ƒçš„åŒ¹é…ä¸é‡å†™é€»è¾‘ ::llvm::LogicalResult matchAndRewrite( ::mlir::Operation* op0, // å½“å‰å°è¯•åŒ¹é…çš„æ“ä½œ (op0 é¢„æœŸä¸º poly.eval) ::mlir::PatternRewriter\u0026amp; rewriter) const override { // ç”¨äºæ•è·åŒ¹é…è¿‡ç¨‹ä¸­æ“ä½œæ•°å’Œå±æ€§çš„å˜é‡ ::mlir::Operation::operand_range z; // å°†æ•è· complex.conj çš„æ“ä½œæ•° ::mlir::arith::FastMathFlagsAttr fastmath; // å°†æ•è· complex.conj çš„ fastmath å±æ€§ ::mlir::Operation::operand_range f; // å°†æ•è· poly.eval çš„ç¬¬ä¸€ä¸ªæ“ä½œæ•° (å¤šé¡¹å¼) // ç”¨äºå­˜å‚¨åŒ¹é…åˆ°çš„æ“ä½œï¼Œæ–¹ä¾¿åç»­ç»Ÿä¸€è·å–ä½ç½®ä¿¡æ¯ ::llvm::SmallVector\u0026lt;::mlir::Operation*, 4\u0026gt; tblgen_ops; // --- å¼€å§‹åŒ¹é… --- tblgen_ops.push_back(op0); // å°†æ ¹æ“ä½œ op0 (poly.eval) åŠ å…¥åˆ—è¡¨ // å°è¯•å°† op0 åŠ¨æ€è½¬æ¢ä¸º poly.eval ç±»å‹ auto castedOp0 = ::llvm::dyn_cast\u0026lt;::mlir::tutorial::poly::EvalOp\u0026gt;(op0); (void) castedOp0; // é¿å…æœªä½¿ç”¨è­¦å‘Š (å¦‚æœåç»­ä¸ç›´æ¥ä½¿ç”¨ castedOp0 çš„æŸäº›ç‰¹æ€§) // è·å– poly.eval çš„ç¬¬ä¸€ä¸ªæ“ä½œæ•° (å¤šé¡¹å¼ f) f = castedOp0.getODSOperands(0); { // å†…åµŒä½œç”¨åŸŸï¼Œç”¨äºåŒ¹é… poly.eval çš„ç¬¬äºŒä¸ªæ“ä½œæ•° (æ±‚å€¼ç‚¹ point) // è·å–å®šä¹‰ poly.eval ç¬¬äºŒä¸ªæ“ä½œæ•° (point) çš„é‚£ä¸ªæ“ä½œ (op1) auto* op1 = (*castedOp0.getODSOperands(1).begin()).getDefiningOp(); if (!(op1)) { // å¦‚æœ point ä¸æ˜¯ç”±æŸä¸ªæ“ä½œå®šä¹‰çš„ (ä¾‹å¦‚ï¼Œå®ƒæ˜¯å—å‚æ•°) return rewriter.notifyMatchFailure( castedOp0, [\u0026amp;](::mlir::Diagnostic\u0026amp; diag) { diag \u0026lt;\u0026lt; \u0026#34;There\u0026#39;s no operation that defines operand 1 \u0026#34; \u0026#34;of castedOp0 (the point operand)\u0026#34;; }); } // å°è¯•å°† op1 åŠ¨æ€è½¬æ¢ä¸º complex.conj ç±»å‹ auto castedOp1 = ::llvm::dyn_cast\u0026lt;::mlir::complex::ConjOp\u0026gt;(op1); (void) castedOp1; if (!(castedOp1)) { // å¦‚æœ op1 ä¸æ˜¯ complex.conj æ“ä½œ return rewriter.notifyMatchFailure( op1, [\u0026amp;](::mlir::Diagnostic\u0026amp; diag) { diag \u0026lt;\u0026lt; \u0026#34;Operand 1 of poly.eval is not defined by mlir::complex::ConjOp\u0026#34;; }); } // è·å– complex.conj çš„æ“ä½œæ•° (z) z = castedOp1.getODSOperands(0); { // å†…åµŒä½œç”¨åŸŸï¼Œç”¨äºæå– complex.conj çš„ fastmath å±æ€§ [[maybe_unused]] auto tblgen_attr = // [[maybe_unused]] é¿å…æœªä½¿ç”¨è­¦å‘Š castedOp1.getProperties().getFastmath(); if (!tblgen_attr) // å¦‚æœæ²¡æœ‰æ˜¾å¼è®¾ç½® fastmathï¼Œåˆ™é»˜è®¤ä¸º none tblgen_attr = ::mlir::arith::FastMathFlagsAttr::get( rewriter.getContext(), ::mlir::arith::FastMathFlags::none); fastmath = tblgen_attr; // ä¿å­˜ fastmath å±æ€§ } tblgen_ops.push_back(op1); // å°†åŒ¹é…åˆ°çš„ complex.conj æ“ä½œ (op1) åŠ å…¥åˆ—è¡¨ } // --- åŒ¹é…ç»“æŸ --- // --- å¼€å§‹é‡å†™ --- // ä¸ºæ–°ç”Ÿæˆçš„æ“ä½œåˆ›å»ºä¸€ä¸ªèåˆçš„ä½ç½®ä¿¡æ¯ï¼Œæºè‡ªæ‰€æœ‰åŒ¹é…åˆ°çš„æ“ä½œ auto odsLoc = rewriter.getFusedLoc( {tblgen_ops[0]-\u0026gt;getLoc(), tblgen_ops[1]-\u0026gt;getLoc()}); (void) odsLoc; // é¿å…æœªä½¿ç”¨è­¦å‘Š // ç”¨äºå­˜å‚¨æ›¿æ¢åŸæ“ä½œ op0 çš„æ–°å€¼ ::llvm::SmallVector\u0026lt;::mlir::Value, 4\u0026gt; tblgen_repl_values; // å£°æ˜æ–°çš„ poly.eval æ“ä½œ ::mlir::tutorial::poly::EvalOp tblgen_EvalOp_0; { // åˆ›å»ºæ–°çš„ poly.eval æ“ä½œ: eval(f, z) ::mlir::Value tblgen_value_0 = (*f.begin()); // poly.eval çš„ç¬¬ä¸€ä¸ªæ“ä½œæ•° (å¤šé¡¹å¼ f) ::mlir::Value tblgen_value_1 = (*z.begin()); // poly.eval çš„ç¬¬äºŒä¸ªæ“ä½œæ•° (åŸ conj çš„æ“ä½œæ•° z) tblgen_EvalOp_0 = rewriter.create\u0026lt;::mlir::tutorial::poly::EvalOp\u0026gt;( odsLoc, /*input=*/tblgen_value_0, /*point=*/tblgen_value_1); } // å£°æ˜æ–°çš„ complex.conj æ“ä½œ ::mlir::complex::ConjOp tblgen_ConjOp_1; { // åˆ›å»ºæ–°çš„ complex.conj æ“ä½œ: conj(result of new eval) ::llvm::SmallVector\u0026lt;::mlir::Value, 4\u0026gt; tblgen_values; // æ–° conj çš„æ“ä½œæ•°åˆ—è¡¨ (void) tblgen_values; ::mlir::complex::ConjOp::Properties tblgen_props; // æ–° conj çš„å±æ€§ (void) tblgen_props; // æ–° conj çš„æ“ä½œæ•°æ˜¯æ–°åˆ›å»ºçš„ poly.eval çš„ç»“æœ tblgen_values.push_back( (*tblgen_EvalOp_0.getODSResults(0).begin())); // è®¾ç½®æ–° conj çš„ fastmath å±æ€§ï¼Œä¸åŸ conj ä¿æŒä¸€è‡´ tblgen_props.fastmath = ::llvm::dyn_cast_if_present\u0026lt;decltype(tblgen_props.fastmath)\u0026gt;( fastmath); tblgen_ConjOp_1 = rewriter.create\u0026lt;::mlir::complex::ConjOp\u0026gt;( odsLoc, tblgen_values, tblgen_props); } // å°†æ–°åˆ›å»ºçš„ complex.conj æ“ä½œçš„ç»“æœä½œä¸ºæ›¿æ¢å€¼ for (auto v : ::llvm::SmallVector\u0026lt;::mlir::Value, 4\u0026gt;{ tblgen_ConjOp_1.getODSResults(0)}) { tblgen_repl_values.push_back(v); } // ç”¨æ–°çš„å€¼æ›¿æ¢åŸå§‹æ“ä½œ op0 rewriter.replaceOp(op0, tblgen_repl_values); return ::mlir::success(); // è¡¨ç¤ºåŒ¹é…å’Œé‡å†™æˆåŠŸ } }; void LLVM_ATTRIBUTE_UNUSED populateWithGenerated(::mlir::RewritePatternSet\u0026amp; patterns) { patterns.add\u0026lt;LiftConjThroughEval\u0026gt;(patterns.getContext()); } ç„¶åè·Ÿä¸Šä¸€ä¸ªæ–¹æ³•ä¸€æ ·ï¼Œéœ€è¦æ·»åŠ è¿™ä¸ª canonicalizer.\nvoid EvalOp::getCanonicalizationPatterns(::mlir::RewritePatternSet\u0026amp; results, ::mlir::MLIRContext* context) { populateWithGenerated(results); } åŒæ ·æˆ‘ä»¬å¯ä»¥é€šè¿‡ tablegen çš„æ–¹å¼ç¼–å†™ DifferenceOfSquaresï¼Œä½†ç”±äºå°†ä¸€ä¸ª SubOp æ›¿æ¢æˆäº† 3 ä¸ª Opï¼Œéœ€è¦ç»§æ‰¿ Pattern è€Œä¸æ˜¯ Pat.\n// PolyPatterns.td def HasOneUse: Constraint\u0026lt;CPred\u0026lt;\u0026#34;$_self.hasOneUse()\u0026#34;\u0026gt;, \u0026#34;has one use\u0026#34;\u0026gt;; // Rewrites (x^2 - y^2) as (x+y)(x-y) if x^2 and y^2 have no other uses. def DifferenceOfSquares : Pattern\u0026lt; (Poly_SubOp (Poly_MulOp:$lhs $x, $x), (Poly_MulOp:$rhs $y, $y)), [ (Poly_AddOp:$sum $x, $y), (Poly_SubOp:$diff $x, $y), (Poly_MulOp:$res $sum, $diff), ], [(HasOneUse:$lhs), (HasOneUse:$rhs)] \u0026gt;; ","permalink":"http://localhost:1313/blogs/courselearning/mlir/mlir-ch8-canonicalizers-and-declarative-rewrite-patterns/","summary":"Personal MLIR learning notes 8.","title":"MLIR-Ch8 Canonicalizers and Declarative Rewrite Patterns"},{"content":"Purposes of a Verifier Verifiers ç¡®ä¿å…·ä½“çš„ MLIR ç¨‹åºä¸­çš„ç±»å‹å’Œæ“ä½œæ ¼å¼æ­£ç¡®ã€‚éªŒè¯å™¨ä¼šåœ¨æ¯æ¬¡ä¼˜åŒ– pass ä¹‹å‰å’Œä¹‹åè¿è¡Œï¼Œå¸®åŠ©ç¡®ä¿å•ä¸ª pass, folders, rewrite patterns ç­‰éƒ½èƒ½ç”Ÿæˆæ­£ç¡®çš„ IR. è¿™ä½¿å¾—æ¯ä¸ªæ“ä½œçš„çº¦æŸæ¡ä»¶ï¼ˆinvariantsï¼‰èƒ½å¤Ÿå¾—åˆ°å¼ºåˆ¶æ‰§è¡Œï¼ŒåŒæ—¶ç®€åŒ–äº†ä¼ é€’çš„å®ç°ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä¾èµ–è¿™äº›çº¦æŸæ¡ä»¶ï¼Œä»è€Œé¿å…æ£€æŸ¥è¾¹ç•Œæƒ…å†µã€‚å¤šæ•°æƒ…å†µä¸‹éªŒè¯ä»£ç æ˜¯ç”¨ Traits æ¥å®ç°çš„ã€‚\nTrait-based Verifiers ä¸Šä¸€ç« æˆ‘ä»¬åŠ å…¥äº† SameOperandsAndResultElementType ä»è€Œè®© poly.add çš„è¾“å…¥å¯ä»¥æ—¢æ˜¯ poly æˆ–è€…å¼ é‡ç±»å‹çš„ poly. ä»æŠ€æœ¯ä¸Šè®²ï¼Œè¿™å‘ IR æ·»åŠ äº†ä¸€ä¸ªéªŒè¯å™¨ï¼Œä½†æ˜¯ä¸ºäº†æ›´æ¸…æ¥šåœ°æ¼”ç¤ºè¿™ä¸€ç‚¹ï¼Œè¿™ä¸€ç« å°†é™åˆ¶è¯¥è¡Œä¸ºï¼Œæˆ‘ä»¬å°† Trait æ”¹æˆ SameOperandsAndResultType ä»¥æ–­è¨€è¾“å…¥å’Œè¾“å‡ºç±»å‹å¿…é¡»å…¨éƒ¨ä¸€è‡´ã€‚\nè¿™æ ·ä¼šè‡ªåŠ¨ç”Ÿæˆä¸€äº›æ–°åŠŸèƒ½ã€‚é¦–å…ˆï¼ŒéªŒè¯å¼•æ“ä¼šä½¿ç”¨ verifyTrait æ¥æ£€æŸ¥ç±»å‹æ˜¯å¦ä¸€è‡´ã€‚åœ¨è¿™é‡Œï¼ŒverifyInvariants æ˜¯ Operation åŸºç±»ä¸­çš„ä¸€ä¸ªæ–¹æ³•ï¼Œå½“æŸäº› Traits æ³¨å…¥éªŒè¯é€»è¾‘æ—¶ï¼Œç”Ÿæˆçš„ä»£ç ä¼šè¦†ç›–è¿™ä¸ªæ–¹æ³•ï¼Œç”¨äºæ£€æŸ¥æ“ä½œç±»å‹ä¸Šçš„ç±»å‹çº¦æŸã€‚(å¦‚æœæ˜¯è‡ªå®šä¹‰éªŒè¯å™¨ï¼Œåˆ™ä¼šä½¿ç”¨åä¸º verify çš„æ–¹æ³•ï¼Œä»¥ä¸ verifyInvariants åŒºåˆ†å¼€æ¥) ç”±äº SameOperandsAndResultType æ˜¯ä¸€ä¸ªé€šç”¨æ£€æŸ¥ï¼Œå› æ­¤å®ƒä¸ä¼šå½±å“ç”Ÿæˆçš„ä»£ç ã€‚\nä¸‹é¢å±•ç¤ºäº† AddOp çš„ inferReturnTypes æ–¹æ³•\n::llvm::LogicalResult AddOp::inferReturnTypes( ::mlir::MLIRContext* context, ::std::optional\u0026lt;::mlir::Location\u0026gt; location, ::mlir::ValueRange operands, ::mlir::DictionaryAttr attributes, ::mlir::OpaqueProperties properties, ::mlir::RegionRange regions, ::llvm::SmallVectorImpl\u0026lt;::mlir::Type\u0026gt;\u0026amp; inferredReturnTypes) { inferredReturnTypes.resize(1); // Represent AddOp\u0026#39;s output as a single type. ::mlir::Builder odsBuilder(context); if (operands.size() \u0026lt;= 0) // Check that there is at least one operand. return ::mlir::failure(); ::mlir::Type odsInferredType0 = operands[0].getType(); inferredReturnTypes[0] = odsInferredType0; // Set the output type to the first operand\u0026#39;s type. return ::mlir::success(); } æœ‰äº†ç±»å‹æ¨å¯¼é’©å­ï¼Œæˆ‘ä»¬å¯ä»¥ç®€åŒ–æ“ä½œçš„æ±‡ç¼–æ ¼å¼ï¼Œç±»å‹åªéœ€è¦æŒ‡å®šä¸€æ¬¡ï¼Œè€Œä¸æ˜¯ä¸‰æ¬¡ ((type, type) -\u0026gt; type). åŒæ—¶ä¹Ÿéœ€è¦æ›´æ–°æ‰€æœ‰æµ‹è¯•çš„ mlir ä»¥å¯ç”¨è¿™ä¸ªæ–°çš„ assemblyFormat.\nlet assemblyFormat = \u0026#34;$lhs `,` $rhs attr-dict `:` qualified(type($output))\u0026#34;; æˆ‘ä»¬å¯ä»¥ä» AddOp çš„ build æ–¹æ³•ä¸­çœ‹åˆ°ç°åœ¨ä¸éœ€è¦æŒ‡å®šè¿”å›å€¼ï¼Œè€Œæ˜¯é€šè¿‡ inferReturnTypes æ¥æ¨å¯¼ã€‚\nvoid AddOp::build(::mlir::OpBuilder\u0026amp; odsBuilder, ::mlir::OperationState\u0026amp; odsState, ::mlir::Value lhs, ::mlir::Value rhs) { odsState.addOperands(lhs); odsState.addOperands(rhs); ::llvm::SmallVector\u0026lt;::mlir::Type, 2\u0026gt; inferredReturnTypes; if (::mlir::succeeded(AddOp::inferReturnTypes( odsBuilder.getContext(), odsState.location, odsState.operands, odsState.attributes.getDictionary(odsState.getContext()), odsState.getRawProperties(), odsState.regions, inferredReturnTypes))) odsState.addTypes(inferredReturnTypes); else ::mlir::detail::reportFatalInferReturnTypesError(odsState); } EvalOp æ— æ³•ä½¿ç”¨ SameOperandsAndResultTypeï¼Œå› ä¸ºå®ƒçš„æ“ä½œæ•°éœ€è¦ä¸åŒçš„ç±»å‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ AllTypesMatchï¼Œå®ƒä¼šç”Ÿæˆç±»ä¼¼çš„ä»£ç ï¼Œä½†å°†éªŒè¯é™åˆ¶åœ¨æŸäº›ç‰¹å®šç±»å‹çš„å­é›†ä¸Šã€‚\ndef Poly_EvalOp : Op\u0026lt;Poly_Dialect, \u0026#34;eval\u0026#34;, [AllTypesMatch\u0026lt;[\u0026#34;point\u0026#34;, \u0026#34;output\u0026#34;]\u0026gt;]\u0026gt; { let summary = \u0026#34;Evaluates a Polynomial at a given input value.\u0026#34;; let arguments = (ins Polynomial:$input, AnyInteger:$point); let results = (outs AnyInteger:$output); } å¯ä»¥çœ‹åˆ°ç›¸ä¼¼çš„ inferReturnTypes æ–¹æ³•ï¼Œç”±äº EvalOp æ˜¯è¿”å›å¤šé¡¹å¼åœ¨æŸä¸ªæ•´æ•°ç‚¹ä¸Šçš„å€¼ï¼Œå› æ­¤æ¨æ–­çš„è¿”å›å€¼ç±»å‹éœ€è¦ä¸ç¬¬äºŒä¸ªæ“ä½œæ•°ç±»å‹ä¸€è‡´ã€‚\n::llvm::LogicalResult EvalOp::inferReturnTypes( ::mlir::MLIRContext* context, ::std::optional\u0026lt;::mlir::Location\u0026gt; location, ::mlir::ValueRange operands, ::mlir::DictionaryAttr attributes, ::mlir::OpaqueProperties properties, ::mlir::RegionRange regions, ::llvm::SmallVectorImpl\u0026lt;::mlir::Type\u0026gt;\u0026amp; inferredReturnTypes) { inferredReturnTypes.resize(1); ::mlir::Builder odsBuilder(context); if (operands.size() \u0026lt;= 1) return ::mlir::failure(); ::mlir::Type odsInferredType0 = operands[1].getType(); inferredReturnTypes[0] = odsInferredType0; return ::mlir::success(); } A Custom Verifier å¦‚æœéœ€è¦æ·»åŠ è‡ªå®šä¹‰çš„ verifier æˆ‘ä»¬éœ€è¦åœ¨ def çš„æ—¶å€™æ·»åŠ  let hasVerifier = 1. æˆ‘ä»¬ä¼šå‘ç°ç”Ÿæˆçš„ç±»é‡Œé¢å®šä¹‰äº† verify æ–¹æ³•ã€‚\nclass EvalOp ... { ... ::mlir::LogicalResult verify(); }; å› æ­¤æˆ‘ä»¬éœ€è¦åœ¨ PolyOps.cpp ä¸­å®ç°å®ƒã€‚\n// lib/Dialect/Poly/PolyOps.cpp LogicalResult EvalOp::verify() { return getPoint().getType().isSignlessInteger(32) ? success() : emitError(\u0026#34;argument point must be a 32-bit integer\u0026#34;); } A Trait-based Custom Verifier åœ¨ MLIR ä¸­ï¼Œæ¯ä¸ª Trait éƒ½æœ‰ä¸€ä¸ªå¯é€‰çš„ verifyTrait é’©å­ï¼Œè¿™ä¸ªé’©å­ä¼šåœ¨é€šè¿‡ hasVerifier åˆ›å»ºçš„è‡ªå®šä¹‰éªŒè¯å™¨ä¹‹å‰æ‰§è¡Œã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸ªé’©å­å®šä¹‰é€šç”¨çš„éªŒè¯å™¨ï¼Œä½¿å…¶é€‚ç”¨äºå¤šä¸ªæ“ä½œã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰©å±•ä¸Šä¸€èŠ‚çš„å†…å®¹ï¼Œåˆ›å»ºä¸€ä¸ªé€šç”¨çš„éªŒè¯å™¨ï¼Œç”¨äºæ–­è¨€æ‰€æœ‰æ•´æ•°ç±»å‹çš„æ“ä½œæ•°å¿…é¡»æ˜¯ 32 ä½ã€‚\nå› æ­¤æˆ‘ä»¬å…ˆéœ€è¦ def ä¸€ä¸ªæ–°çš„ Traitï¼Œç„¶åå°†å®ƒåŠ å…¥åˆ° EvalOp ä¸­.\nlet cppNamespace = \u0026#34;::mlir::tutorial::poly\u0026#34;; } æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç”Ÿæˆçš„ä»£ç é‡Œæœ‰ä¸€ä¸ªæ–°ç±»éœ€è¦æˆ‘ä»¬å®ç°\nclass EvalOp : public ::mlir::Op\u0026lt; EvalOp, ::mlir::OpTrait::ZeroRegions, //..., ::mlir::tutorial::poly::Has32BitArguments, //... \u0026gt; { // ... }; æˆ‘ä»¬éœ€è¦æ–°å»ºä¸€ä¸ª PolyTraits.h æ–‡ä»¶å¹¶ä¸”è®© PolyOps.h åŒ…å«å®ƒ\n// // /include/mlir-learning/Dialect/Poly/PolyOps.h #ifndef LIB_DIALECT_POLY_POLYTRAITS_H_ #define LIB_DIALECT_POLY_POLYTRAITS_H_ #include \u0026#34;mlir/include/mlir/IR/OpDefinition.h\u0026#34; namespace mlir::tutorial::poly { template \u0026lt;typename ConcreteType\u0026gt; class Has32BitArguments : public OpTrait::TraitBase\u0026lt;ConcreteType, Has32BitArguments\u0026gt; { public: static LogicalResult verifyTrait(Operation *op) { for (auto type : op-\u0026gt;getOperandTypes()) { // OK to skip non-integer operand types if (!type.isIntOrIndex()) continue; if (!type.isInteger(32)) { return op-\u0026gt;emitOpError() \u0026lt;\u0026lt; \u0026#34;requires each numeric operand to be a 32-bit integer\u0026#34;; } } return success(); } }; } #endif // LIB_DIALECT_POLY_POLYTRAITS_H_ è¿™æ ·åšçš„ä¼˜ç‚¹æ˜¯å…·æœ‰æ›´å¼ºçš„é€šç”¨æ€§ï¼Œä½†ç¼ºç‚¹æ˜¯éœ€è¦è¿›è¡Œç¹ççš„ç±»å‹è½¬æ¢æ¥æ”¯æŒç‰¹å®šçš„æ“ä½œåŠå…¶å‘½åå‚æ•°ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæˆ‘ä»¬æ— æ³•ç›´æ¥è°ƒç”¨ getPointï¼Œé™¤éå¯¹æ“ä½œè¿›è¡ŒåŠ¨æ€è½¬æ¢ä¸º EvalOp.\n","permalink":"http://localhost:1313/blogs/courselearning/mlir/mlir-ch7-verifiers/","summary":"Personal MLIR learning notes 7.","title":"MLIR-Ch7 Verifiers"},{"content":"Constant Propagation vs Canonicalization -sccp Sparse Conditional Constant Propagation æ˜¯ç¨€ç–æ¡ä»¶å¸¸æ•°ä¼ æ’­ï¼Œå®ƒè¯•å›¾æ¨æ–­ op ä½•æ—¶å…·æœ‰å¸¸é‡è¾“å‡ºï¼Œç„¶åç”¨å¸¸é‡å€¼æ›¿æ¢ op ã€‚é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œå®ƒåœ¨ç¨‹åºä¸­å°½å¯èƒ½åœ°â€œä¼ æ’­â€è¿™äº›å¸¸é‡ã€‚\nä¾‹å¦‚å¯¹äºå¦‚ä¸‹çš„å‡½æ•°\nfunc.func @test_arith_sccp() -\u0026gt; i32 { %0 = arith.constant 7 : i32 %1 = arith.constant 8 : i32 %2 = arith.addi %0, %0 : i32 %3 = arith.muli %0, %0 : i32 %4 = arith.addi %2, %3 : i32 return %2 : i32 } -sccp ä¼˜åŒ–åçš„ç»“æœå¦‚ä¸‹ï¼š\nfunc.func @test_arith_sccp() -\u0026gt; i32 { %c63_i32 = arith.constant 63 : i32 %c49_i32 = arith.constant 49 : i32 %c14_i32 = arith.constant 14 : i32 %c8_i32 = arith.constant 8 : i32 %c7_i32 = arith.constant 7 : i32 return %c14_i32 : i32 } éœ€è¦æ³¨æ„çš„æ˜¯ï¼šsccp ä¸ä¼šåˆ é™¤æ­»ä»£ç ï¼›è¿™é‡Œæ²¡æœ‰å±•ç¤ºçš„æ˜¯ sccp çš„ä¸»è¦ä½œç”¨ï¼Œå®ƒå¯ä»¥é€šè¿‡æ§åˆ¶æµ (if æˆ–è€… loop) ä¼ æ’­å¸¸é‡ã€‚\nä¸€ä¸ªç›¸å…³çš„æ¦‚å¿µæ˜¯ canonicalizationï¼Œ--canonicalize pass éšè—äº† MLIR ä¸­çš„è®¸å¤šç¹é‡å·¥ä½œã€‚å®ƒä¸ sccp æœ‰ä¸€ç‚¹é‡å ï¼Œå› ä¸ºå®ƒä¹Ÿè®¡ç®—å¸¸é‡å¹¶åœ¨ IR ä¸­å…·ä½“åŒ–å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸Šé¢çš„ IR ä¸Šä½¿ç”¨ â€”â€”canonicalize pass çš„ç»“æœå¦‚ä¸‹\nfunc.func @test_arith_sccp() -\u0026gt; i32 { %c14_i32 = arith.constant 14 : i32 return %c14_i32 : i32 } ä¸­é—´çš„å¸¸é‡éƒ½è¢«ä¿®å‰ªæ‰äº†ï¼Œå‰©ä¸‹çš„åªæ˜¯è¿”å›å€¼ï¼Œæ²¡æœ‰ä»»ä½• op. è§„èŒƒåŒ–ä¸èƒ½é€šè¿‡æ§åˆ¶æµä¼ æ’­å¸¸é‡ã€‚\nè¿™ä¸¤è€…éƒ½æ˜¯é€šè¿‡æŠ˜å  (folding) æ¥æ”¯æŒçš„ï¼ŒæŠ˜å æ˜¯é‡‡å–ä¸€ç³»åˆ— op å¹¶å°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·ä¸ºæ›´ç®€å•çš„ op çš„è¿‡ç¨‹ã€‚å®ƒè¿˜è¦æ±‚æˆ‘ä»¬çš„æ–¹è¨€å…·æœ‰æŸç§å¸¸é‡ op ï¼Œè¯¥ op ä¸æŠ˜å çš„ç»“æœä¸€èµ·æ’å…¥ã€‚\nä»¥è¿™ç§æ–¹å¼æ”¯æŒæŠ˜å æ‰€éœ€çš„å¤§è‡´æ­¥éª¤æ˜¯ï¼š\næ·»åŠ ä¸€ä¸ªå¸¸é‡ op. æ·»åŠ å®ä¾‹åŒ–é’©å­ã€‚ ä¸ºæ¯ä¸ª op æ·»åŠ  folders. Making a Constant Operation æˆ‘ä»¬ç›®å‰åªæ”¯æŒé€šè¿‡ from_tensor op ä» arith.constant åˆ›å»ºå¸¸é‡ã€‚\n%0 = arith.constant dense\u0026lt;[1, 2, 3]\u0026gt; : tensor\u0026lt;3xi32\u0026gt; %p0 = poly.from_tensor %0 : tensor\u0026lt;3xi32\u0026gt; -\u0026gt; !poly.poly\u0026lt;10\u0026gt; ä¸€ä¸ªå¸¸é‡ op å¯ä»¥å°†ä¸Šè¿°ä¸¤ä¸ªæ“ä½œç®€åŒ–æˆä¸€ä¸ª op. from_tensor op è¿˜å¯ä»¥ç”¨äºæ ¹æ®æ•°æ® (è€Œä¸ä»…ä»…æ˜¯å¸¸æ•°) æ„å»ºä¸€ä¸ªå¤šé¡¹å‡½æ•°ï¼Œå› æ­¤å³ä½¿åœ¨æˆ‘ä»¬å®ç°äº† poly.constant ä¹‹åï¼Œå®ƒä¹Ÿåº”è¯¥ä¿ç•™ã€‚\n%0 = poly.constant dense\u0026lt;[2, 8, 20, 24, 18]\u0026gt; : !poly.poly\u0026lt;10\u0026gt; fold å¯ä»¥ç”¨äºå‘ sccp ç­‰ pass ä¼ é€’ä¿¡å·ï¼Œè¡¨æ˜ op çš„ç»“æœæ˜¯å¸¸é‡ï¼Œæˆ–è€…å®ƒå¯ä»¥ç”¨äºè¯´ op çš„ç»“æœç­‰æ•ˆäºç”±ä¸åŒ op åˆ›å»ºçš„é¢„å…ˆå­˜åœ¨çš„å€¼ã€‚å¯¹äºå¸¸é‡çš„æƒ…å†µï¼Œè¿˜éœ€è¦ä¸€ä¸ª materializeConstant é’©å­æ¥å‘Šè¯‰ MLIR å¦‚ä½•è·å–å¸¸é‡ç»“æœå¹¶å°†å…¶è½¬åŒ–ä¸ºé€‚å½“çš„ IR op. å¸¸é‡ op çš„å®šä¹‰å¦‚ä¸‹\ndef Poly_ConstantOp: Op\u0026lt;Poly_Dialect, \u0026#34;constant\u0026#34;, [Pure, ConstantLike]\u0026gt; {\rlet summary = \u0026#34;Define a constant polynomial via an attribute.\u0026#34;;\rlet arguments = (ins AnyIntElementsAttr:$coefficients);\rlet results = (outs Polynomial:$output);\rlet assemblyFormat = \u0026#34;$coefficients attr-dict `:` type($output)\u0026#34;;\r} ConstantLike trait æ ‡è®°çš„ op è¢«è§†ä¸ºå¸¸é‡å€¼ç”Ÿæˆ op ï¼Œå¯ä»¥åœ¨ç¼–è¯‘æ—¶è¿›è¡Œå¸¸é‡æŠ˜å ç­‰ä¼˜åŒ–ã€‚arguments å®šä¹‰ op çš„è¾“å…¥æ˜¯ä¸€ä¸ªå…·æœ‰ AnyIntElementsAttr çš„å€¼ï¼Œä½¿å¾— op å¯ä»¥å¤„ç†ä»»æ„åŒ…å«æ•´æ•°çš„é›†åˆï¼Œè€Œä¸ä»…ä»…æ˜¯ç‰¹å®šä½å®½çš„æ•´æ•°ã€‚\nAdding Folders æˆ‘ä»¬ä¸ºå®šä¹‰çš„ op éƒ½åŠ ä¸Š let hasFolder = 1; å®ƒåœ¨ .hpp.inc ä¸­æ·»åŠ äº†å¦‚ä¸‹å½¢å¼çš„å£°æ˜ã€‚FoldAdaptor å®šä¹‰ä¸º GenericAdaptor ç±»å‹çš„åˆ«åï¼Œè€Œ GenericAdaptor åŒ…å«äº†ä¸€ä¸ª Attribute æ•°ç»„çš„å¼•ç”¨ï¼Œè¿™ä¸ªæ•°ç»„æä¾›äº†å¯¹ op å±æ€§çš„è®¿é—®æ¥å£ã€‚\nAttribute ç±»çš„æ ¸å¿ƒä½œç”¨æ˜¯ï¼š\nè¡¨ç¤ºå¸¸é‡å€¼ï¼šAttribute ç”¨äºè¡¨ç¤ºæ“ä½œçš„é™æ€ã€ä¸å¯å˜çš„å¸¸é‡å€¼ï¼Œä¾‹å¦‚æ•´æ•°ã€æµ®ç‚¹æ•°ã€å­—ç¬¦ä¸²ã€ç±»å‹ä¿¡æ¯ç­‰ã€‚è¿™äº›å€¼åœ¨ç¼–è¯‘æœŸå·²çŸ¥ä¸”ä¸å¯æ›´æ”¹ã€‚ æ”¯æŒç¼–è¯‘å™¨ä¼˜åŒ–ï¼šé€šè¿‡æä¾›å¸¸é‡å€¼çš„è¡¨ç¤ºï¼ŒAttribute æ”¯æŒ MLIR çš„ä¼˜åŒ–æµç¨‹ï¼Œå¦‚æŠ˜å  (folding) ã€è§„èŒƒåŒ– (canonicalization), å¸¸é‡ä¼ æ’­ (constant propagation) ç­‰ã€‚ è·¨æ–¹è¨€çš„é€šç”¨æ¥å£ï¼šAttribute æ˜¯ä¸€ä¸ªæŠ½è±¡æ¥å£ï¼Œå…è®¸ä¸åŒæ–¹è¨€ (dialects) å®šä¹‰è‡ªå·±çš„å¸¸é‡è¡¨ç¤ºï¼ŒåŒæ—¶é€šè¿‡ç»Ÿä¸€çš„ API è¿›è¡Œæ“ä½œã€‚ è½»é‡çº§å’Œé«˜æ•ˆï¼šAttribute æ˜¯ä¸€ä¸ªå€¼ç±»å‹ (passed by value) ï¼Œå†…éƒ¨ä»…å­˜å‚¨æŒ‡å‘åº•å±‚å­˜å‚¨çš„æŒ‡é’ˆï¼Œä¾èµ– MLIRContext çš„å”¯ä¸€åŒ–æœºåˆ¶ (uniquing) ç¡®ä¿å†…å­˜æ•ˆç‡å’Œä¸€è‡´æ€§ã€‚ using FoldAdaptor = GenericAdaptor\u0026lt;::llvm::ArrayRef\u0026lt;::mlir::Attribute\u0026gt;\u0026gt;; ::mlir::OpFoldResult fold(FoldAdaptor adaptor); æˆ‘ä»¬éœ€è¦åœ¨ PolyOps.cpp ä¸­å®ç°è¿™ä¸ªå‡½æ•°ã€‚å¦‚æœ fold æ–¹æ³•å†³å®š op åº”è¢«æ›¿æ¢ä¸ºä¸€ä¸ªå¸¸é‡ï¼Œåˆ™å¿…é¡»è¿”å›ä¸€ä¸ªè¡¨ç¤ºè¯¥å¸¸é‡çš„ Attributeï¼Œè¯¥å±æ€§å¯ä»¥ä½œä¸º poly.constant æ“ä½œçš„è¾“å…¥ã€‚FoldAdaptor æ˜¯ä¸€ä¸ªé€‚é…å™¨ï¼Œå®ƒå…·æœ‰ä¸æ“ä½œçš„ C++ ç±»å®ä¾‹ç›¸åŒçš„æ–¹æ³•åç§°ï¼Œä½†å¯¹äºé‚£äº›å·²ç»è¢«æŠ˜å çš„å‚æ•°ï¼Œä¼šç”¨è¡¨ç¤ºå…¶æŠ˜å ç»“æœå¸¸é‡çš„ Attribute å®ä¾‹æ›¿æ¢ã€‚è¿™åœ¨æŠ˜å åŠ æ³•å’Œä¹˜æ³•æ“ä½œæ—¶å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºæŠ˜å çš„å®ç°éœ€è¦ç«‹å³è®¡ç®—ç»“æœï¼Œå¹¶ä¸”éœ€è¦è®¿é—®å®é™…çš„æ•°å€¼æ¥å®Œæˆè®¡ç®—ã€‚\nå¯¹äº poly.constant æˆ‘ä»¬åªéœ€è¦è¿”å›è¾“å…¥çš„ attribute.\nOpFoldResult ConstantOp::fold(ConstantOp::FoldAdaptor adaptor) { return adaptor.getCoefficients(); } å¯¹äº from_tensor æˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªé¢å¤–çš„å¼ºåˆ¶è½¬æ¢ä½œä¸ºæ–­è¨€ï¼Œå› ä¸ºå¼ é‡å¯èƒ½æ˜¯ç”¨æˆ‘ä»¬ä¸å¸Œæœ›ä½œä¸ºè¾“å…¥çš„å¥‡æ€ªç±»å‹æ„é€ çš„ã€‚å¦‚æœ dyn_cast ç»“æœæ˜¯ nullptrï¼Œ MLIR å°†å…¶å¼ºåˆ¶è½¬æ¢ä¸ºå¤±è´¥çš„ OpFoldResult.\nOpFoldResult FromTensorOp::fold(FromTensorOp::FoldAdaptor adaptor) { // Returns null if the cast failed, which corresponds to a failed fold. return dyn_cast\u0026lt;DenseIntElementsAttr\u0026gt;(adaptor.getInput()); } BinOp ç¨å¾®å¤æ‚ä¸€äº›ï¼Œå› ä¸ºè¿™äº› fold æ–¹æ³•ä¸­çš„æ¯ä¸€ä¸ª op éƒ½æ¥å—ä¸¤ä¸ª DenseIntElementsAttr ä½œä¸ºè¾“å…¥ï¼Œå¹¶æœŸæœ›æˆ‘ä»¬ä¸ºç»“æœè¿”å›å¦ä¸€ä¸ª DenseIntElementsAttr.\nå¯¹äº elementwise op çš„ add/subï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç°æœ‰çš„æ–¹æ³• constFoldBinaryOpï¼Œå®ƒé€šè¿‡ä¸€äº›æ¨¡æ¿å…ƒç¼–ç¨‹æŠ€å·§ï¼Œå…è®¸æˆ‘ä»¬åªæŒ‡å®šå…ƒç´  op æœ¬èº«ã€‚\nOpFoldResult AddOp::fold(AddOp::FoldAdaptor adaptor) { return constFoldBinaryOp\u0026lt;IntegerAttr, APInt\u0026gt;( adaptor.getOperands(), [\u0026amp;](APInt a, APInt b) { return a + b; }); } å¯¹äº mulï¼Œæˆ‘ä»¬æ‰‹åŠ¨çš„é€šè¿‡å¾ªç¯è®¡ç®—æ¯ä¸ªç³»æ•°ã€‚getResult() æ–¹æ³•æ¥è‡ªäº OneTypedResult ç±»æ¨¡æ¿åŠå…¶å†…éƒ¨ç±» Impl æ˜¯ä¸€ä¸ª MLIR Traitï¼Œå®ƒä¸»è¦ç”¨äºé‚£äº›è¿”å›å•ä¸€ç‰¹å®šç±»å‹ç»“æœçš„ op ã€‚\nOpFoldResult MulOp::fold(MulOp::FoldAdaptor adaptor) { auto lhs = llvm::dyn_cast\u0026lt;DenseIntElementsAttr\u0026gt;(adaptor.getOperands()[0]); auto rhs = llvm::dyn_cast\u0026lt;DenseIntElementsAttr\u0026gt;(adaptor.getOperands()[1]); if (!lhs || !rhs) { return nullptr; } auto degree = mlir::cast\u0026lt;PolynomialType\u0026gt;(getResult().getType()).getDegreeBound(); auto maxIndex = lhs.size() + rhs.size() - 1; SmallVector\u0026lt;llvm::APInt, 8\u0026gt; results; results.reserve(maxIndex); for (int64_t i = 0; i \u0026lt; maxIndex; i++) { results.push_back(APInt((*lhs.begin()).getBitWidth(), 0)); } int64_t i = 0; for (auto lhsIt = lhs.value_begin\u0026lt;APInt\u0026gt;(); lhsIt != lhs.value_end\u0026lt;APInt\u0026gt;(); lhsIt++) { int64_t j = 0; for (auto rhsIt = rhs.value_begin\u0026lt;APInt\u0026gt;(); rhsIt != rhs.value_end\u0026lt;APInt\u0026gt;(); rhsIt++) { results[(i + j) % degree] += (*lhsIt) * (*rhsIt); j++; } i++; } return DenseIntElementsAttr::get( RankedTensorType::get(static_cast\u0026lt;int64_t\u0026gt;(results.size()), mlir::IntegerType::get(getContext(), 32)), results); } Adding a Constant Materializer æœ€åæˆ‘ä»¬æ·»åŠ å¸¸é‡å®ä¾‹åŒ–å‡½æ•°ï¼Œè¿™æ˜¯ä¸€ä¸ª dialect çº§åˆ«çš„ç‰¹æ€§ï¼Œæˆ‘ä»¬åœ¨ PolyDialect.td ä¸­æ·»åŠ  let hasConstantMaterializer = 1; åˆ™ä¼šåœ¨ .hpp.inc ä¸­æ·»åŠ å¦‚ä¸‹å½¢å¼çš„å£°æ˜ã€‚\n::mlir::Operation *materializeConstant(::mlir::OpBuilder \u0026amp;builder, ::mlir::Attribute value, ::mlir::Type type, ::mlir::Location loc) override; è¯¥å‡½æ•°ä½œç”¨æ˜¯å°†ç»™å®š Attribute (ä¸Šé¢æ¯ä¸ªæŠ˜å æ­¥éª¤çš„ç»“æœ) çš„å•ä¸ªå¸¸é‡ op å®ä¾‹åŒ–ä¸ºæ‰€éœ€çš„ç»“æœ Type.\nOperation *PolyDialect::materializeConstant( OpBuilder \u0026amp;builder, Attribute value, Type type, Location loc) { auto coeffs = dyn_cast\u0026lt;DenseIntElementsAttr\u0026gt;(value); if (!coeffs) return nullptr; return builder.create\u0026lt;ConstantOp\u0026gt;(loc, type, coeffs); } ","permalink":"http://localhost:1313/blogs/courselearning/mlir/mlir-ch6-folders-and-constant-propagation/","summary":"Personal MLIR learning notes 6.","title":"MLIR-Ch6 Folders and Constant Propagation"},{"content":"Traits and Loop Invariant Code Motion ä¸ºäº†æé«˜ä»£ç é‡ç”¨æ€§ï¼ŒMLIR æä¾›äº† Traits å’Œ Interfaces Traitsï¼Œç”¨äºå¢å¼º op (Operation) æˆ–ç±»å‹çš„åŠŸèƒ½ï¼Œæä¾›ç»“æ„åŒ–çš„çº¦æŸå’ŒåŠŸèƒ½æ¥å£ï¼Œæ–¹ä¾¿åœ¨ç¼–è¯‘ä¼˜åŒ–å’Œç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œæ›´å¼ºå¤§å’Œçµæ´»çš„ op ã€‚\nTraits æ˜¯ä¸€ç§æœºåˆ¶ï¼Œç”¨äºæŠ½è±¡å‡ºå¤šä¸ªä¸åŒå±æ€§ã€ op æˆ–ç±»å‹ä¹‹é—´å…±åŒçš„å®ç°ç»†èŠ‚å’Œç‰¹æ€§ã€‚å¯ç”¨äºæŒ‡å®šå¯¹è±¡çš„ç‰¹æ®Šå±æ€§å’Œçº¦æŸï¼Œä¾‹å¦‚ op æ˜¯å¦å…·æœ‰å‰¯ä½œç”¨ï¼Œæˆ–å…¶è¾“å‡ºç±»å‹æ˜¯å¦ä¸è¾“å…¥ç±»å‹ç›¸åŒã€‚Traits å°†ç‰¹å®šçš„è¡Œä¸ºæˆ–é™åˆ¶æŠ½è±¡å‡ºæ¥ï¼Œä½¿è¿™äº›è¡Œä¸ºå¯ä»¥å¤ç”¨åœ¨ä¸åŒçš„å¯¹è±¡ä¸Šï¼Œè€Œä¸éœ€è¦åœ¨æ¯ä¸ªå¯¹è±¡ä¸­é‡å¤å®ç°ç›¸åŒçš„é€»è¾‘ã€‚\nInterfaces æ˜¯ä¸€ç§é€šç”¨çš„æœºåˆ¶ï¼Œç”¨äºä¸ IR è¿›è¡Œäº¤äº’ã€‚å®ƒä»¬çš„ç›®æ ‡æ˜¯ä½¿è½¬æ¢æˆ–åˆ†æå¯ä»¥åŸºäºè¿™äº›æ¥å£è¿›è¡Œï¼Œè€Œæ— éœ€äº†è§£å…·ä½“çš„ op æˆ– dialect çš„å†…éƒ¨å®ç°ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œç¼–è¯‘å™¨å¯ä»¥åœ¨å®ç°è½¬æ¢å’Œåˆ†ææ—¶ä¸ä¾èµ–äºç‰¹å®š dialect æˆ– op ï¼Œä»è€Œæ›´è½»æ¾åœ°æ‰©å±•ç¼–è¯‘å™¨çš„åŠŸèƒ½ã€‚\nLoop Invariant Code Motion æ˜¯ MLIR æä¾›çš„ General Transform Passes ä¹‹ä¸€ã€‚å®ƒä¼šæ£€æŸ¥å¾ªç¯ä½“ä¸­çš„ op ï¼Œå¦‚æœå‘ç°æŸäº› op åœ¨å¾ªç¯å†…éƒ¨æ‰§è¡Œæ²¡æœ‰å¿…è¦ï¼ˆå³å®ƒä»¬çš„ç»“æœåœ¨æ¯æ¬¡å¾ªç¯ä¸­ä¿æŒä¸å˜ï¼‰ï¼Œå°±ä¼šå°†è¿™äº› op ç§»å‡ºå¾ªç¯ä½“ã€‚è¿™å¯ä»¥å‡å°‘å¾ªç¯ä¸­çš„é‡å¤è®¡ç®—ï¼Œæé«˜æ•ˆç‡ã€‚\nè¦è®©æŸä¸ªè‡ªå®šä¹‰ op å¯ä»¥è¢«è¿™ç§ pass è¯†åˆ«å¹¶ç§»å‡ºå¾ªç¯ä½“ï¼Œéœ€è¦æ·»åŠ ä¸¤ä¸ªå…³é”®çš„ Traits æ¥è¡¨æ˜è¯¥ op åœ¨å¾ªç¯å¤–æ‰§è¡Œæ˜¯å®‰å…¨çš„ï¼š\nNoMemoryEffect: æ˜¯ MemoryEffect çš„ä¸€ä¸ª empty å®ç°ï¼Œè¡¨ç¤ºè¯¥ op ä¸ä¼šäº§ç”Ÿä»»ä½•ä¸å†…å­˜å†™å…¥ç›¸å…³çš„å‰¯ä½œç”¨ã€‚ AlwaysSpeculatable: æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ª Traits çš„ åˆ—è¡¨ï¼Œå‘Šè¯‰ç¼–è¯‘å™¨è¯¥ op å¯ä»¥åœ¨ä¸å½±å“ç¨‹åºé€»è¾‘çš„å‰æä¸‹ï¼Œå°†å…¶æå‰è®¡ç®—æˆ–ç§»åŠ¨åˆ°å…¶ä»–ä½ç½®ã€‚ åœ¨ MLIR ä¸­ï¼ŒLoop Invariant Code Motion (LICM) ä¼šå°†å…·æœ‰ NoMemoryEffect å’Œ AlwaysSpeculatable è¿™ä¸¤ä¸ª Traits çš„ op ç§»åŠ¨åˆ°å¾ªç¯ä½“å¤–éƒ¨ï¼Œä½†å‰ææ˜¯è¯¥ op çš„ operands åœ¨æ•´ä¸ªå¾ªç¯ä½“ä¸­ä¿æŒä¸å˜ã€‚è¿™æ ·å¯ä»¥é¿å…å¾ªç¯å†…éƒ¨çš„é‡å¤è®¡ç®—ï¼Œä»è€Œä¼˜åŒ–ä»£ç æ‰§è¡Œæ•ˆç‡ã€‚MLIR æä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„ç»„åˆ Trait Pureï¼Œå®ƒåŒ…å«äº† NoMemoryEffect å’Œ AlwaysSpeculatable è¿™ä¸¤ä¸ª Traits. å› æ­¤ï¼Œç›´æ¥æ·»åŠ  Pure Trait åˆ° op çš„å®šä¹‰ä¸­å°±èƒ½è®©ç¼–è¯‘å™¨è‡ªåŠ¨è¯†åˆ«å®ƒä¸ºå¯ç§»åŠ¨åˆ°å¾ªç¯å¤–éƒ¨çš„ op ã€‚\nTypeOrContainer æ˜¯ä¸€ä¸ªç”¨äºå¤„ç† op è¾“å…¥å’Œè¾“å‡ºç±»å‹çš„æœºåˆ¶ï¼Œå®ƒå¯ä»¥åŒ¹é…å•ä¸ªç±»å‹ (å¦‚ f32 æˆ– i32) ä»¥åŠå®¹å™¨ç±»å‹(å¦‚ vector\u0026lt;f32\u0026gt; æˆ– tensor\u0026lt;i32\u0026gt;)ï¼Œä½¿å¾—ä¸€ä¸ª op å¯ä»¥è¢«è®¾è®¡ä¸ºåŒæ—¶æ”¯æŒæ ‡é‡ç±»å‹å’Œé›†åˆç±»å‹ã€‚\ninclude \u0026#34;mlir/Interfaces/SideEffectInterfaces.td\u0026#34;\rdef PolyOrContainer: TypeOrContainer\u0026lt;Polynomial, \u0026#34;poly-or-container\u0026#34;\u0026gt;;\rclass Poly_BinOp\u0026lt;string mnemonic\u0026gt;: Op\u0026lt;Poly_Dialect, mnemonic, [Pure]\u0026gt; {\rlet arguments = (ins PolyOrContainer:$lhs, PolyOrContainer:$rhs);\rlet results = (outs PolyOrContainer:$output);\rlet assemblyFormat = \u0026#34;$lhs `,` $rhs attr-dict `:` `(` type($lhs) `,` type($rhs) `)` `-\u0026gt;` type($output)\u0026#34;;\r} åŠ å…¥ Pure trait åç”Ÿæˆçš„ .hpp.inc ä¸­å…³äº op çš„å®šä¹‰ç»§æ‰¿äº†æ–°çš„å†…å®¹\nclass AddOp : public ::mlir::Op\u0026lt; AddOp, ::mlir::OpTrait::ZeroRegions, ::mlir::OpTrait::OneResult, ::mlir::OpTrait::OneTypedResult\u0026lt;::mlir::tutorial::poly::PolynomialType\u0026gt;::Impl, ::mlir::OpTrait::ZeroSuccessors, ::mlir::OpTrait::NOperands\u0026lt;2\u0026gt;::Impl, ::mlir::OpTrait::OpInvariants, ::mlir::ConditionallySpeculatable::Trait, // \u0026lt;-- new ::mlir::OpTrait::AlwaysSpeculatableImplTrait, // \u0026lt;-- new ::mlir::MemoryEffectOpInterface::Trait\u0026gt; // \u0026lt;--- new NoMemoryEffect interface åˆ™åœ¨ç”Ÿæˆçš„ .cpp.inc ä¸­æ·»åŠ äº†ä¸€ä¸ªç®€å•çš„å‡½æ•°\nvoid AddOp::getEffects( ::llvm::SmallVectorImpl\u0026lt; ::mlir::SideEffects::EffectInstance\u0026lt;::mlir::MemoryEffects::Effect\u0026gt;\u0026gt;\u0026amp; effects) { } æˆ‘ä»¬å¯ä»¥å†™ä¸€ä¸ª .mlir æ¥æµ‹è¯• %2 çš„è®¡ç®—æ˜¯å¦èƒ½ä¼˜åŒ–åˆ°å¾ªç¯å¤–ï¼š\n// RUN: build/Ch4-UsingTraits/tools/ch4-tutorial-opt %s --loop-invariant-code-motion \u0026gt; %t // RUN: FileCheck %s \u0026lt; %t module { // CHECK-LABEL: func.func @test_loop_invariant_code_motion func.func @test_loop_invariant_code_motion() -\u0026gt; !poly.poly\u0026lt;10\u0026gt; { %0 = arith.constant dense\u0026lt;[1,2,3]\u0026gt; : tensor\u0026lt;3xi32\u0026gt; %p0 = poly.from_tensor %0 : tensor\u0026lt;3xi32\u0026gt; -\u0026gt; !poly.poly\u0026lt;10\u0026gt; %1 = arith.constant dense\u0026lt;[9,8,16]\u0026gt; : tensor\u0026lt;3xi32\u0026gt; %p1 = poly.from_tensor %0 : tensor\u0026lt;3xi32\u0026gt; -\u0026gt; !poly.poly\u0026lt;10\u0026gt; // CHECK: poly.mul // CHECK: affine.for %ret_val = affine.for %i = 0 to 100 iter_args(%sum_iter = %p0) -\u0026gt; !poly.poly\u0026lt;10\u0026gt; { // The polt.mul should be hoisted out of the loop. // CHECK-NOT: poly.mul %2 = poly.mul %p0, %p1 : (!poly.poly\u0026lt;10\u0026gt;, !poly.poly\u0026lt;10\u0026gt;) -\u0026gt; !poly.poly\u0026lt;10\u0026gt; %sum_next = poly.add %sum_iter, %2 : (!poly.poly\u0026lt;10\u0026gt;, !poly.poly\u0026lt;10\u0026gt;) -\u0026gt; !poly.poly\u0026lt;10\u0026gt; affine.yield %sum_next : !poly.poly\u0026lt;10\u0026gt; } return %ret_val: !poly.poly\u0026lt;10\u0026gt; } } Passes Already Handled by Pure ç»™æŸä¸ª op åŠ ä¸Š Pure Trait åï¼Œä¸‹åˆ— Pass å°±ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶ä¼˜åŒ–è¯¥ op ï¼š\n--control-flow-sink: å°†åªåœ¨æ¡ä»¶è¯­å¥çš„æŸä¸€ä¸ªåˆ†æ”¯ä¸­ä½¿ç”¨çš„ op ç§»åŠ¨åˆ°å¯¹åº”çš„åˆ†æ”¯ä¸­ï¼Œä»¥å‡å°‘æ— æ•ˆä»£ç çš„æ‰§è¡Œã€‚éœ€è¦ op æ— å†…å­˜å‰¯ä½œç”¨ (memory-effect free)ï¼Œé€šå¸¸å¯ä»¥é€šè¿‡ Pure Trait æ¥æ»¡è¶³ã€‚ --cse (Constant Subexpression Elimination): å¸¸é‡å­è¡¨è¾¾å¼æ¶ˆé™¤ã€‚å½“æŸäº›é‡å¤çš„è®¡ç®—ç»“æœå·²ç»å­˜åœ¨æ—¶ï¼Œæ¶ˆé™¤ä¸å¿…è¦çš„é‡å¤è®¡ç®—ï¼Œæé«˜æ•ˆç‡ã€‚éœ€è¦ op æ²¡æœ‰å†…å­˜å‰¯ä½œç”¨ï¼ˆmemory-effect freeï¼‰ï¼Œå› æ­¤ Pure Trait ä¹Ÿå¯ä»¥æ»¡è¶³è¿™ä¸€è¦æ±‚ã€‚ --inline: å°†å‡½æ•°è°ƒç”¨â€œå†…è”â€åˆ°è°ƒç”¨ä½ç½®ï¼Œä»¥å‡å°‘å‡½æ•°è°ƒç”¨çš„å¼€é”€ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™å¯ä»¥å‡å°‘è°ƒç”¨æ ˆçš„æ·±åº¦æˆ–ä¼˜åŒ–ä»£ç æ‰§è¡Œçš„æ€§èƒ½ã€‚ --mem2reg: å°†å†…å­˜ä¸­çš„å­˜å‚¨/åŠ è½½ op è½¬æ¢ä¸ºå¯¹å®é™…å€¼çš„ç›´æ¥ä½¿ç”¨ï¼Œä»è€Œå‡å°‘å†…å­˜è®¿é—®ï¼Œæé«˜è¿è¡Œæ•ˆç‡ã€‚ --remove-dead-values: ç§»é™¤æœªä½¿ç”¨çš„å‡½æ•°å‚æ•°æˆ–è¿”å›å€¼ï¼Œä»¥å‡å°‘ä¸å¿…è¦çš„æ•°æ®ä¼ é€’æˆ–å†…å­˜å ç”¨ã€‚ --sroa (Scalar Replacement of Aggregates): å°†èšåˆç±»å‹ï¼ˆä¾‹å¦‚æ•°ç»„æˆ–ç»“æ„ä½“ï¼‰æ‹†åˆ†ä¸ºæ ‡é‡å€¼ï¼Œé€šå¸¸ä¼šå¯¹å†…å­˜å¸ƒå±€è¿›è¡Œé‡æ’ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ©ç”¨å†…å­˜ã€‚ --symbol-dce (Symbol Dead Code Elimination): æ¶ˆé™¤ä¸å†ä½¿ç”¨çš„ç§æœ‰å‡½æ•° (æ­»ä»£ç )ï¼Œå‡å°‘ä¸å¿…è¦çš„ä»£ç é‡ã€‚ Elementwise Mappings æœ‰å››ç§ traits å¯ä»¥æŠŠæ ‡é‡è¿ç®—æ‰©å±•åˆ°å¼ é‡è¿ç®—æˆ–è€…åè¿‡æ¥\nElemntwise: æ ‡è®°é€å…ƒç´ çš„ op ï¼Œä»…é€‚ç”¨äºå‘é‡æˆ–å¼ é‡ï¼Œä¸å…è®¸å¹¿æ’­ã€‚\nå¦‚æœä»»ä½•ç»“æœæ˜¯å‘é‡æˆ–å¼ é‡ï¼Œè‡³å°‘æœ‰ä¸€ä¸ª operand å¿…é¡»æ˜¯å‘é‡æˆ–å¼ é‡ã€‚ å¦‚æœä»»ä½• operand æ˜¯å‘é‡æˆ–å¼ é‡ï¼Œè‡³å°‘æœ‰ä¸€ä¸ªç»“æœå¹¶ä¸”æ‰€æœ‰ç»“æœå¿…é¡»æ˜¯å‘é‡æˆ–å¼ é‡ã€‚ æ‰€æœ‰ operand å’Œç»“æœçš„å‘é‡æˆ–å¼ é‡ç±»å‹å¿…é¡»å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å½¢çŠ¶å¯ä»¥æ˜¯åŠ¨æ€çš„ï¼Œä½†å¯¹äºä¸åŒ¹é…çš„å½¢çŠ¶ï¼Œè¡Œä¸ºæ˜¯æœªå®šä¹‰çš„ã€‚ è¯¥ op å¿…é¡»åœ¨ operand å’Œç»“æœä¸Šé€å…ƒç´ è¿›è¡Œï¼Œå³åœ¨å•å…ƒç´ å‘é‡æˆ–å¼ é‡ä¸Šåº”ç”¨æ—¶ï¼Œæ¯ä¸ªå…ƒç´ çš„ç»“æœåº”ç›¸åŒã€‚ Scalarizable: æ ‡è®°å’ŒéªŒè¯æŸäº›æ“ä½œæ˜¯å¦å¯ä»¥è¢«ç³»ç»Ÿæ€§åœ°æ ‡é‡åŒ–ï¼Œå³å°†å…¶åŸºäºå‘é‡æˆ–å¼ é‡çš„æ“ä½œè½¬åŒ–ä¸ºåŸºäºæ ‡é‡çš„æ“ä½œã€‚åªè¦æ“ä½œæ˜¯ Elementwise çš„ï¼ŒScalarizable å°±å¯ä»¥ä½¿ç”¨ã€‚\n%tensor_select = \u0026#34;arith.select\u0026#34;(%pred_tensor, %true_val, %false_val) : (tensor\u0026lt;?xi1\u0026gt;, tensor\u0026lt;?xf32\u0026gt;, tensor\u0026lt;?xf32\u0026gt;) -\u0026gt; tensor\u0026lt;?xf32\u0026gt; // Can be scalarized to %scalar_select = \u0026#34;arith.select\u0026#34;(%pred, %true_val_scalar, %false_val_scalar) : (i1, f32, f32) -\u0026gt; f32 Vectorizable: æä¾›äº†ä¸ Scalarizable ç›¸åçš„ op ã€‚æ‰€æœ‰çš„æ ‡é‡ operand å’Œç»“æœå°†è¢«æ›¿æ¢ä¸ºç›¸åº”çš„å‘é‡ç±»å‹ã€‚å³ï¼Œè¯¥ op è¡¨ç¤ºåŒæ—¶ä½œç”¨äºå¤šä¸ªå…ƒç´ ã€‚å…è®¸é€šè¿‡å¹¿æ’­å°†æ ‡é‡æå‡ä¸ºå‘é‡ï¼Œå†è¿›è¡Œå‘é‡åŒ–æ“ä½œã€‚\nTensorizable: æä¾›äº†ä¸ Scalarizable ç›¸åçš„ op ï¼Œå…è®¸åœ¨å¼ é‡å’Œæ ‡é‡ä¹‹é—´è¿›è¡Œæ¨ç†ã€‚å…è®¸é€šè¿‡å¹¿æ’­å°†æ ‡é‡æå‡ä¸ºå¼ é‡ï¼Œä»¥ä¾¿åœ¨å¼ é‡ op ä¸­ä¿æŒä¸€è‡´çš„ op ç»“æ„ã€‚\n%scalar = \u0026#34;arith.addf\u0026#34;(%a, %b) : (f32, f32) -\u0026gt; f32 // Can be tensorized to %tensor = \u0026#34;arith.addf\u0026#34;(%a, %b) : (tensor\u0026lt;?xf32\u0026gt;, tensor\u0026lt;?xf32\u0026gt;) -\u0026gt; tensor\u0026lt;?xf32\u0026gt; // Also supports broadcasting %scalar_pred = \u0026#34;arith.select\u0026#34;(%pred, %true_val, %false_val) : (i1, tensor\u0026lt;?xf32\u0026gt;, tensor\u0026lt;?xf32\u0026gt;) -\u0026gt; tensor\u0026lt;?xf32\u0026gt; // Can be tensorized to %tensor_pred = \u0026#34;arith.select\u0026#34;(%pred, %true_val, %false_val) : (tensor\u0026lt;?xi1\u0026gt;, tensor\u0026lt;?xf32\u0026gt;, tensor\u0026lt;?xf32\u0026gt;) -\u0026gt; tensor\u0026lt;?xf32\u0026gt; ElementwiseMappable Trait åŒ…å«äº†ä»¥ä¸Šæ‰€æœ‰çš„ Traits. æˆ‘ä»¬å¯ä»¥ä¿®æ”¹ Poly_BinOp å®šä¹‰å¦‚ä¸‹ï¼š\n// PolyOps.td\rdef PolyOrContainer : TypeOrContainer\u0026lt;Polynomial, \u0026#34;poly-or-container\u0026#34;\u0026gt;;\rclass Poly_BinOp\u0026lt;string mnemonic\u0026gt; : Op\u0026lt;Poly_Dialect, mnemonic, [Pure, ElementwiseMappable]\u0026gt; {\rlet arguments = (ins PolyOrContainer:$lhs, PolyOrContainer:$rhs);\rlet results = (outs PolyOrContainer:$output);\r...\r} æ·»åŠ è¿™ä¸ª Trait åï¼Œç”Ÿæˆçš„ .cpp.inc æ–‡ä»¶å®šä¹‰äº†è®¸å¤šæ£€æŸ¥ op æ•°ç±»å‹çš„å‡½æ•°ï¼Œä¸‹é¢æ˜¯å…¶ä¸­ä¸€ä¸ªï¼š\nstatic ::llvm::LogicalResult __mlir_ods_local_type_constraint_PolyOps1( ::mlir::Operation* op, ::mlir::Type type, ::llvm::StringRef valueKind, unsigned valueIndex) { if (!(((::llvm::isa\u0026lt;::mlir::tutorial::poly::PolynomialType\u0026gt;(type))) || (((type.hasTrait\u0026lt;::mlir::ValueSemantics\u0026gt;())) \u0026amp;\u0026amp; ([](::mlir::Type elementType) { return (::llvm::isa\u0026lt;::mlir::tutorial::poly::PolynomialType\u0026gt;( elementType)); }(::llvm::cast\u0026lt;::mlir::ShapedType\u0026gt;(type).getElementType()))))) { return op-\u0026gt;emitOpError(valueKind) \u0026lt;\u0026lt; \u0026#34; #\u0026#34; \u0026lt;\u0026lt; valueIndex \u0026lt;\u0026lt; \u0026#34; must be poly-or-container, but got \u0026#34; \u0026lt;\u0026lt; type; } return ::mlir::success(); } è¯¥å‡½æ•°é¦–å…ˆæ£€æŸ¥ type æ˜¯å¦ä¸º PolynomialTypeï¼›å¦‚æœä¸æ˜¯ï¼Œåˆ™è¿›ä¸€æ­¥æ£€æŸ¥å®ƒæ˜¯å¦å…·æœ‰ ValueSemantics Traitï¼Œå¹¶ä¸”æ˜¯ä¸€ä¸ª ShapedTypeï¼ˆå³å®¹å™¨ç±»å‹ï¼Œå¦‚ vector æˆ– tensorï¼‰ï¼Œå…¶ä¸­åŒ…å«çš„å…ƒç´ ç±»å‹æ˜¯ PolynomialType.\n","permalink":"http://localhost:1313/blogs/courselearning/mlir/mlir-ch5-using-traits/","summary":"Personal MLIR learning notes 5.","title":"MLIR-Ch5 Using Traits"},{"content":"Sketching Out a Dseign TableGen ä¹Ÿå¯ä»¥ç”¨æ¥å®šä¹‰ dialect. æœ¬æ–‡å°†å®šä¹‰ä¸€ä¸ªå•æœªçŸ¥æ•°å¤šé¡¹å¼è¿ç®—çš„ dialectï¼Œç³»æ•°ç”¨ uint32_t ç±»å‹è¡¨ç¤ºã€‚ï¼Œå¹¶æä¾›é€šè¿‡ä»æ ‡å‡† MLIR ç±»å‹æŒ‡å®šå¤šé¡¹å¼ç³»æ•°æ¥å®šä¹‰å¤šé¡¹å¼çš„æ“ä½œï¼Œæå–å…³äºå¤šé¡¹å¼çš„æ•°æ®ä»¥å°†ç»“æœå­˜å‚¨åœ¨æ ‡å‡†MLIRç±»å‹ä¸­ï¼Œä»¥åŠå¯¹å¤šé¡¹å¼è¿›è¡Œç®—æœ¯è¿ç®—ã€‚\nAn Empty Dialect æˆ‘ä»¬é¦–å…ˆç”¨ TableGen å®šä¹‰ä¸€ä¸ªç©ºçš„ dialect. å®ƒå’Œä¸Šä¸€ç« å®šä¹‰ Pass æ²¡ä»€ä¹ˆä¸åŒï¼Œåªä¸è¿‡ include çš„æ˜¯ DialectBase.td æ–‡ä»¶ã€‚åŒæ—¶ä¹Ÿå®šä¹‰äº†å‘½åç©ºé—´ä¸º ::mlir::tutorial::poly.\ninclude \u0026#34;mlir/IR/DialectBase.td\u0026#34; def Poly_Dialect : Dialect { let name = \u0026#34;poly\u0026#34;; let summary = \u0026#34;A dialect for polynomial math\u0026#34;; let description = [{ The poly dialect defines types and operations for single-variable polynomials over integers. }]; let cppNamespace = \u0026#34;::mlir::tutorial::poly\u0026#34;; } æˆ‘ä»¬éœ€è¦åœ¨ include ç›®å½•ä¸‹çš„ CMakeLists.txt æ–‡ä»¶ä¸­æ·»åŠ \nset(TARGET_NAME \u0026#34;${PROJECT_TARGET_PREFIX}-Dialect-PolyDialect-IncGen\u0026#34;) set(LLVM_TARGET_DEFINITIONS mlir-learning/Dialect/Poly/PolyDialect.td) mlir_tablegen(mlir-learning/Dialect/Poly/PolyDialect.hpp.inc --gen-dialect-decls) mlir_tablegen(mlir-learning/Dialect/Poly/PolyDialect.cpp.inc --gen-dialect-defs) add_public_tablegen_target(${TARGET_NAME}) ç„¶ååœ¨ tutorial-opt.cpp ä¸­æ³¨å†Œæ‰€æœ‰ mlir è‡ªå¸¦çš„æ‰€æœ‰ dialect åè¿›è¡Œæ„å»ºï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„ .hpp.inc å’Œ.cpp.inc æ–‡ä»¶ã€‚\nnamespace mlir { namespace tutorial { class PolyDialect : public ::mlir::Dialect { explicit PolyDialect(::mlir::MLIRContext *context); void initialize(); friend class ::mlir::MLIRContext; public: ~PolyDialect() override; static constexpr ::llvm::StringLiteral getDialectNamespace() { return ::llvm::StringLiteral(\u0026#34;poly\u0026#34;); } }; } // namespace tutorial } // namespace mlir MLIR_DECLARE_EXPLICIT_TYPE_ID(::mlir::tutorial::PolyDialect) ç¼–è¯‘å™¨ä¼šæŠ¥é”™ï¼Œå› ä¸º inc ä¸ä¼šåŒ…å« Dialect ç­‰ç±»æ‰€åœ¨çš„å¤´æ–‡ä»¶ã€‚è¿™éœ€è¦æˆ‘ä»¬è‡ªå·±åœ¨ PolyDialect.h æ–‡ä»¶ä¸­è¿›è¡Œ includeï¼Œè¿™æ · å½“é‡æ–°æ„å»ºçš„æ—¶å€™è¯¥æ–‡ä»¶æ³¨å…¥å˜ä¸ä¼šæŠ¥é”™\n// include/mlir-learning/Dialect/Poly/PolyDialect.h #ifndef LIB_DIALECT_POLY_POLYDIALECT_H #define LIB_DIALECT_POLY_POLYDIALECT_H #include \u0026#34;mlir/IR/DialectImplementation.h\u0026#34; // include mannually #include \u0026#34;mlir-learning/Dialect/Poly/PolyDialect.hpp.inc\u0026#34; #endif ç”Ÿæˆçš„ .cpp.inc å¦‚ä¸‹ï¼Œä»–åªåŒ…å«äº†è¯¥ç±»åŸºæœ¬çš„æ„é€ å‡½æ•°å’Œææ„å‡½æ•°ã€‚\nMLIR_DEFINE_EXPLICIT_TYPE_ID(::mlir::tutorial::poly::PolyDialect) namespace mlir { namespace tutorial { namespace poly { PolyDialect::PolyDialect(::mlir::MLIRContext *context) : ::mlir::Dialect(getDialectNamespace(), context, ::mlir::TypeID::get\u0026lt;PolyDialect\u0026gt;()) { initialize(); } PolyDialect::~PolyDialect() = default; } // namespace poly } // namespace tutorial } // namespace mlir ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨ tutorial-opt.cpp ä¸­æ³¨å†Œè¯¥ dialect.\n/* other includes */ #include \u0026#34;mlir-learning/Dialect/Poly/PolyDialect.h\u0026#34; int main(int argc, char** argv) { // Register all built-in MLIR dialects mlir::DialectRegistry registry; // Register our Dialect registry.insert\u0026lt;mlir::tutorial::poly::PolyDialect\u0026gt;(); mlir::registerAllDialects(registry); return mlir::asMainReturnCode( mlir::MlirOptMain(argc, argv, \u0026#34;Tutorial Pass Driver\u0026#34;, registry)); } Adding a Trival Type ä¸‹é¢æˆ‘ä»¬éœ€è¦å®šä¹‰è‡ªå·±çš„ poly.poly ç±»å‹.\n// poly_types.td #ifndef LIB_DIALECT_POLY_POLYTYPES_TD_ #define LIB_DIALECT_POLY_POLYTYPES_TD_ include \u0026#34;mlir-learning/Dialect/Poly/PolyDialect.td\u0026#34; include \u0026#34;mlir/IR/AttrTypeBase.td\u0026#34; // a base class for all types in the dialect class Poly_Type\u0026lt;string name, string typeMnemonic\u0026gt; : TypeDef\u0026lt;Poly_Dialect, name\u0026gt; { let mnemonic = typeMnemonic; } def Polynomial: Poly_Type\u0026lt;\u0026#34;Polynomial\u0026#34;, \u0026#34;poly\u0026#34;\u0026gt; { let summary = \u0026#34;A polynomial with u32 coefficients\u0026#34;; let description = [{ A type for polynomials with integer coefficients in a single-variable polynomial ring. }]; } #endif åœ¨ MLIR çš„ TableGen æ–‡ä»¶ä¸­ï¼Œclass å’Œ def çš„ç”¨æ³•å’Œå«ä¹‰æœ‰æ‰€ä¸åŒ\nclass ç”¨äºå®šä¹‰ä¸€ä¸ªæ¨¡æ¿æˆ–åŸºç±»ï¼Œå¯ä»¥è¢«å…¶ä»–ç±»å‹æˆ–å®šä¹‰ç»§æ‰¿å’Œé‡ç”¨ã€‚å®ƒæœ¬èº«ä¸ä¼šåˆ›å»ºå®é™…çš„å¯¹è±¡æˆ–å…·ä½“ç±»å‹ï¼Œå®ƒåªæ˜¯ä¸€ç§ç»“æ„ï¼Œå¯ä»¥åŒ…å«å‚æ•°å’Œé»˜è®¤å±æ€§ã€‚å…¶ä»–å®šä¹‰å¯ä»¥é€šè¿‡ç»§æ‰¿è¯¥ç±»æ¥è·å¾—å…¶åŠŸèƒ½ã€‚ def ç”¨äºåˆ›å»ºä¸€ä¸ªå…·ä½“çš„å®ä¾‹ï¼Œæ¯”å¦‚ä¸€ä¸ªç±»å‹ã€æ“ä½œæˆ–å±æ€§ã€‚å®ƒä¼šå°†æ‰€å®šä¹‰çš„å†…å®¹åº”ç”¨åˆ° TableGen ä¸­ï¼Œä½¿å…¶æˆä¸ºå¯ç”¨çš„å…·ä½“ç±»å‹æˆ–åŠŸèƒ½ã€‚ è¿™é‡Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸º Poly_Type çš„ç±»ï¼Œå‚æ•°ä¸º nameï¼ˆç±»å‹çš„åç§°ï¼‰å’Œ typeMnemonicï¼ˆç±»å‹çš„ç®€å†™æˆ–åŠ©è®°ç¬¦ï¼‰ã€‚è¿™ä¸ªç±»ç»§æ‰¿è‡ª TypeDef\u0026lt;Poly_Dialect, name\u0026gt;. ç„¶å def ç‰¹å®šçš„å¤šé¡¹å¼ç±»å‹ Polynomialï¼Œç»§æ‰¿è‡ª Poly_Type.\nåœ¨ MLIR çš„ TableGen ä¸­ï¼ŒTypeDef æœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ªç±»ï¼Œå®ƒæ¥å—æ¨¡æ¿å‚æ•°ï¼Œç”¨äºæŒ‡å®šè¯¥ç±»å‹æ‰€å±çš„ dialect å’Œåç§°å­—æ®µã€‚å…¶ä½œç”¨åŒ…æ‹¬å°†ç”Ÿæˆçš„C++ç±»ä¸è¯¥ dialect çš„å‘½åç©ºé—´ç›¸å…³è”ã€‚\nç”Ÿæˆçš„ .hpp.inc æ–‡ä»¶å¦‚ä¸‹ã€‚ç”Ÿæˆçš„ç±» PolynomialType å°±æ˜¯åœ¨æˆ‘ä»¬çš„ TableGen æ–‡ä»¶ä¸­å®šä¹‰çš„ Polynomial ç±»å‹åé¢åŠ ä¸Šäº† Type.\n#ifdef GET_TYPEDEF_CLASSES #undef GET_TYPEDEF_CLASSES namespace mlir { class AsmParser; class AsmPrinter; } // namespace mlir namespace mlir { namespace tutorial { namespace poly { class PolynomialType; class PolynomialType : public ::mlir::Type::TypeBase\u0026lt;PolynomialType, ::mlir::Type, ::mlir::TypeStorage\u0026gt; { public: using Base::Base; static constexpr ::llvm::StringLiteral name = \u0026#34;poly.poly\u0026#34;; static constexpr ::llvm::StringLiteral dialectName = \u0026#34;poly\u0026#34;; static constexpr ::llvm::StringLiteral getMnemonic() { return {\u0026#34;poly\u0026#34;}; } }; } // namespace poly } // namespace tutorial } // namespace mlir MLIR_DECLARE_EXPLICIT_TYPE_ID(::mlir::tutorial::poly::PolynomialType) #endif // GET_TYPEDEF_CLASSES ç”Ÿæˆçš„ .cpp.inc æ–‡ä»¶å¦‚ä¸‹ã€‚TableGen è¯•å›¾ä¸º dialect ä¸­çš„ PolynomialType è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª ç±»å‹è§£æå™¨ (type parser) å’Œç±»å‹æ‰“å°å™¨ (type printer). ä¸è¿‡æ­¤æ—¶è¿™äº›åŠŸèƒ½è¿˜ä¸å¯ç”¨ï¼Œæ„å»ºé¡¹ç›®æ—¶ä¼šçœ‹åˆ°ä¸€äº›ç¼–è¯‘è­¦å‘Šã€‚\nä»£ç ä¸­ä½¿ç”¨äº† å¤´æ–‡ä»¶ä¿æŠ¤ (header guards) æ¥å°† cpp æ–‡ä»¶åˆ†éš”ä¸ºä¸¤ä¸ªå—ä¿æŠ¤çš„éƒ¨åˆ†ã€‚è¿™æ ·å¯ä»¥åˆ†åˆ«ç®¡ç†ç±»å‹å£°æ˜å’Œå‡½æ•°å®ç°ã€‚\nGET_TYPEDEF_LIST åªåŒ…å«ç±»åçš„é€—å·åˆ†éš”åˆ—è¡¨ã€‚åŸå› åœ¨äº PolyDialect.cpp æ–‡ä»¶éœ€è¦è´Ÿè´£å°†ç±»å‹æ³¨å†Œåˆ° dialect ä¸­ï¼Œè€Œè¯¥æ³¨å†Œè¿‡ç¨‹é€šè¿‡åœ¨æ–¹è¨€åˆå§‹åŒ–å‡½æ•°ä¸­å°†è¿™äº› C++ ç±»åä½œä¸ºæ¨¡æ¿å‚æ•°æ¥å®ç°ã€‚æ¢å¥è¯è¯´ï¼ŒGET_TYPEDEF_LIST æä¾›äº†ä¸€ç§ç®€åŒ–æœºåˆ¶ï¼Œä½¿å¾— PolyDialect.cpp å¯ä»¥è‡ªåŠ¨è·å–æ‰€æœ‰ç±»åç§°åˆ—è¡¨ï¼Œä¾¿äºç»Ÿä¸€æ³¨å†Œï¼Œè€Œä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ æ¯ä¸€ä¸ªç±»å‹ã€‚\ngeneratedTypeParser å‡½æ•°æ˜¯ä¸º PolynomialType å®šä¹‰çš„è§£æå™¨ã€‚å½“è§£æå™¨é‡åˆ° PolynomialType çš„åŠ©è®°ç¬¦ï¼ˆpolyï¼‰æ—¶ï¼Œä¼šå°† PolynomialType ç±»å‹å®ä¾‹åŒ–ã€‚KeywordSwitch ä½¿ç”¨ getMnemonic() æ¥åŒ¹é… PolynomialType çš„åŠ©è®°ç¬¦ï¼ˆpolyï¼‰ã€‚å¦‚æœåŒ¹é…æˆåŠŸï¼Œåˆ™è°ƒç”¨ PolynomialType::get() æ¥è·å–ç±»å‹å®ä¾‹ã€‚Default å­å¥åœ¨åŠ©è®°ç¬¦ä¸åŒ¹é…æ—¶æ‰§è¡Œï¼Œè®°å½•æœªçŸ¥çš„åŠ©è®°ç¬¦ï¼Œå¹¶è¿”å› std::nullopt è¡¨ç¤ºè§£æå¤±è´¥ã€‚ generatedTypePrinter å‡½æ•°ä¸º PolynomialType æä¾›äº†æ‰“å°åŠŸèƒ½ã€‚å½“ç±»å‹ä¸º PolynomialType æ—¶ï¼Œæ‰“å°å…¶åŠ©è®°ç¬¦ï¼ˆpolyï¼‰ï¼Œå¦åˆ™è¿”å›å¤±è´¥ã€‚TypeSwitch ç”¨äºæ£€æŸ¥ def ç±»å‹æ˜¯å¦æ˜¯ PolynomialTypeã€‚å¦‚æœæ˜¯ï¼Œæ‰“å°åŠ©è®°ç¬¦ï¼›å¦åˆ™è¿”å›å¤±è´¥ï¼Œè¡¨ç¤ºè¯¥ç±»å‹ä¸å±äºæ­¤æ–¹è¨€ã€‚ PolyDialect::parseType å’Œ PolyDialect::printType ä½œä¸ºæ–¹è¨€æ¥å£è°ƒç”¨è¿™ä¸¤ä¸ªå‡½æ•°ï¼Œä»è€Œå®ç°ç±»å‹çš„è§£æå’Œæ‰“å°åŠŸèƒ½ã€‚ #ifdef GET_TYPEDEF_LIST #undef GET_TYPEDEF_LIST ::mlir::tutorial::poly::PolynomialType #endif // GET_TYPEDEF_LIST #ifdef GET_TYPEDEF_CLASSES #undef GET_TYPEDEF_CLASSES static ::mlir::OptionalParseResult generatedTypeParser(::mlir::AsmParser \u0026amp;parser, ::llvm::StringRef *mnemonic, ::mlir::Type \u0026amp;value) { return ::mlir::AsmParser::KeywordSwitch\u0026lt;::mlir::OptionalParseResult\u0026gt;(parser) .Case(::mlir::tutorial::poly::PolynomialType::getMnemonic(), [\u0026amp;](llvm::StringRef, llvm::SMLoc) { value = ::mlir::tutorial::poly::PolynomialType::get(parser.getContext()); return ::mlir::success(!!value); }) .Default([\u0026amp;](llvm::StringRef keyword, llvm::SMLoc) { *mnemonic = keyword; return std::nullopt; }); } static ::llvm::LogicalResult generatedTypePrinter(::mlir::Type def, ::mlir::AsmPrinter \u0026amp;printer) { return ::llvm::TypeSwitch\u0026lt;::mlir::Type, ::llvm::LogicalResult\u0026gt;(def) .Case\u0026lt;::mlir::tutorial::poly::PolynomialType\u0026gt;([\u0026amp;](auto t) { printer \u0026lt;\u0026lt; ::mlir::tutorial::poly::PolynomialType::getMnemonic(); return ::mlir::success(); }) .Default([](auto) { return ::mlir::failure(); }); } namespace mlir { namespace tutorial { namespace poly { } // namespace poly } // namespace tutorial } // namespace mlir MLIR_DEFINE_EXPLICIT_TYPE_ID(::mlir::tutorial::poly::PolynomialType) namespace mlir { namespace tutorial { namespace poly { /// Parse a type registered to this dialect. ::mlir::Type PolyDialect::parseType(::mlir::DialectAsmParser \u0026amp;parser) const { ::llvm::SMLoc typeLoc = parser.getCurrentLocation(); ::llvm::StringRef mnemonic; ::mlir::Type genType; auto parseResult = generatedTypeParser(parser, \u0026amp;mnemonic, genType); if (parseResult.has_value()) return genType; parser.emitError(typeLoc) \u0026lt;\u0026lt; \u0026#34;unknown type `\u0026#34; \u0026lt;\u0026lt; mnemonic \u0026lt;\u0026lt; \u0026#34;` in dialect `\u0026#34; \u0026lt;\u0026lt; getNamespace() \u0026lt;\u0026lt; \u0026#34;`\u0026#34;; return {}; } /// Print a type registered to this dialect. void PolyDialect::printType(::mlir::Type type, ::mlir::DialectAsmPrinter \u0026amp;printer) const { if (::mlir::succeeded(generatedTypePrinter(type, printer))) return; } } // namespace poly } // namespace tutorial } // namespace mlir #endif // GET_TYPEDEF_CLASSES åœ¨è®¾ç½® C++ æ¥å£ä»¥ä½¿ç”¨ TableGen æ–‡ä»¶æ—¶ï¼Œé€šå¸¸ä¼šæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ¥ç»„ç»‡ä»£ç æ–‡ä»¶å’ŒåŒ…å«å…³ç³»ã€‚\nPolyTypes.h æ˜¯å”¯ä¸€è¢«å…è®¸åŒ…å« PolyTypes.h.inc çš„æ–‡ä»¶ã€‚ PolyTypes.cpp.inc æ–‡ä»¶åŒ…å«äº† TableGen ä¸º PolyDialect ä¸­çš„ç±»å‹ç”Ÿæˆçš„å®ç°ã€‚æˆ‘ä»¬éœ€è¦åœ¨ PolyDialect.cpp ä¸­å°†å…¶åŒ…å«è¿›å»ï¼Œä»¥ç¡®ä¿æ‰€æœ‰å®ç°éƒ½èƒ½åœ¨è¯¥æ–¹è¨€çš„ä¸»æ–‡ä»¶ä¸­ä½¿ç”¨ã€‚ PolyTypes.cpp æ–‡ä»¶åº”è¯¥åŒ…å« PolyTypes.hï¼Œä»¥ä¾¿è®¿é—®ç±»å‹å£°æ˜ï¼Œå¹¶åœ¨è¯¥æ–‡ä»¶ä¸­å®ç°æ‰€æœ‰éœ€è¦çš„é¢å¤–åŠŸèƒ½ã€‚ ./Ch3-DefiningANewDialect/\râ”œâ”€â”€ CMakeLists.txt\râ”œâ”€â”€ include\râ”‚ â”œâ”€â”€ CMakeLists.txt\râ”‚ â””â”€â”€ mlir-tutorial\râ”‚ â””â”€â”€ Dialect\râ”‚ â””â”€â”€ Poly\râ”‚ â”œâ”€â”€ PolyDialect.hpp\râ”‚ â”œâ”€â”€ PolyDialect.td\râ”‚ â”œâ”€â”€ PolyOps.hpp\râ”‚ â”œâ”€â”€ PolyOps.td\râ”‚ â”œâ”€â”€ PolyTypes.hpp\râ”‚ â””â”€â”€ PolyTypes.td\râ”œâ”€â”€ lib\râ”‚ â”œâ”€â”€ CMakeLists.txt\râ”‚ â””â”€â”€ Dialect\râ”‚ â””â”€â”€ Poly\râ”‚ â””â”€â”€ PolyDialect.cpp ä¸ºäº†è®©ç±»å‹è§£æå™¨å’Œæ‰“å°å™¨èƒ½å¤Ÿæ­£ç¡®ç¼–è¯‘å’Œè¿è¡Œï¼Œéœ€è¦æœ€ååœ¨æ–¹è¨€çš„ TableGen æ–‡ä»¶ä¸­æ·»åŠ  let useDefaultTypePrinterParser = 1;ï¼Œè¿™ä¸ªæŒ‡ä»¤å‘Šè¯‰ TableGen ä½¿ç”¨é»˜è®¤çš„ç±»å‹è§£æå’Œæ‰“å°å™¨ã€‚å½“è¿™ä¸ªé€‰é¡¹å¯ç”¨åï¼ŒTableGen ä¼šç”Ÿæˆç›¸åº”çš„è§£æå’Œæ‰“å°ä»£ç ï¼Œå¹¶å°†è¿™äº›å®ç°ä½œä¸º PolyDialect ç±»çš„æˆå‘˜å‡½æ•°ã€‚\n/// Parse a type registered to this dialect. ::mlir::Type parseType(::mlir::DialectAsmParser \u0026amp;parser) const override; /// Print a type registered to this dialect. void printType(::mlir::Type type, ::mlir::DialectAsmPrinter \u0026amp;os) const override; æˆ‘ä»¬å¯ä»¥å†™ä¸€ä¸ª .mlir æ¥æµ‹è¯•å±æ€§æ˜¯æ˜¯å¦è·å–æ­£ç¡®ã€‚åœ¨ MLIR ä¸­è‡ªå®šä¹‰çš„ dialect å‰éƒ½éœ€è¦åŠ ä¸Š !.\n// CHECK-LABEL: test_type_syntax func.func @test_type_syntax(%arg0: !poly.poly\u0026lt;10\u0026gt;) -\u0026gt; !poly.poly\u0026lt;10\u0026gt; { // CHECK: poly.poly return %arg0: !poly.poly\u0026lt;10\u0026gt; } Add a Poly Type Parameter æˆ‘ä»¬éœ€è¦ä¸ºå¤šé¡¹å¼ç±»å‹æ·»åŠ ä¸€ä¸ªå±æ€§ï¼Œè¡¨ç¤ºå®ƒçš„æ¬¡æ•°ä¸Šé™ã€‚\n// include/mlir-tutorial/Dialect/Poly/PolyTypes.td\rlet parameters = (ins \u0026#34;int\u0026#34;:$degreeBound);\rlet assemblyFormat = \u0026#34;`\u0026lt;` $degreeBound `\u0026gt;`\u0026#34;; ç¬¬ä¸€è¡Œå®šä¹‰äº†ç±»å‹çš„ä¸€ä¸ªå‚æ•° degreeBoundï¼Œç±»å‹ä¸º int. è¡¨ç¤ºåœ¨å®ä¾‹åŒ–è¯¥ç±»å‹æ—¶ï¼Œç”¨æˆ·å¯ä»¥æŒ‡å®šä¸€ä¸ªæ•´æ•°å€¼ä½œä¸ºç±»å‹çš„å‚æ•°ã€‚parameters ä¸­çš„ (ins \u0026quot;int\u0026quot;:$degreeBound) æŒ‡å®šäº†è¾“å…¥å‚æ•°çš„ç±»å‹å’Œåç§°ï¼Œå…¶ä¸­ int æ˜¯æ•°æ®ç±»å‹ï¼Œ$degreeBound æ˜¯å‚æ•°çš„å ä½ç¬¦ã€‚assemblyFormat ç”¨äºå®šä¹‰è¯¥ç±»å‹åœ¨ MLIR æ–‡æœ¬æ ¼å¼ä¸­çš„æ‰“å°å’Œè§£ææ ¼å¼ã€‚\u0026quot;\u0026lt;\u0026quot; $degreeBound \u0026quot;\u0026gt;\u0026quot; è¡¨ç¤ºè¯¥ç±»å‹çš„å‚æ•°ä¼šç”¨å°–æ‹¬å·åŒ…è£¹ã€‚ç¬¬äºŒè¡Œæ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºç°åœ¨ä¸€ä¸ª Poly ç±»å‹æœ‰äº†è¿™ä¸ªå…³è”çš„æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦èƒ½å¤Ÿå°†å®ƒæ‰“å°å‡ºæ¥å¹¶ä»æ–‡æœ¬ IR è¡¨ç¤ºä¸­è§£æå®ƒã€‚\nåŠ ä¸Šè¿™ä¸¤è¡Œä»£ç åè¿›è¡Œ build ä¼šå‘ç°å¤šäº†ä¸€äº›æ–°çš„å†…å®¹ã€‚\nPolynomialType æœ‰ä¸€ä¸ªæ–°çš„ int getDegreeBound() æ–¹æ³•ï¼Œä»¥åŠä¸€ä¸ªé™æ€ get å·¥å‚æ–¹æ³•ã€‚ parse å’Œ print å‡çº§ä¸ºæ–°æ ¼å¼ã€‚ æœ‰ä¸€ä¸ªåä¸º typestorage çš„æ–°ç±»ï¼Œå®ƒåŒ…å« int å½¢å‚ï¼Œå¹¶éšè—åœ¨å†…éƒ¨ç»†èŠ‚åç§°ç©ºé—´ä¸­ã€‚ MLIRä¼šè‡ªåŠ¨ç”Ÿæˆç®€å•ç±»å‹çš„ storage ç±»ï¼Œå› ä¸ºå®ƒä»¬ä¸éœ€è¦å¤æ‚çš„å†…å­˜ç®¡ç†ã€‚å¦‚æœå‚æ•°æ›´å¤æ‚ï¼Œå°±éœ€è¦å¼€å‘è€…æ‰‹åŠ¨ç¼–å†™ storage ç±»æ¥å®šä¹‰æ„é€ ã€ææ„å’Œå…¶ä»–è¯­ä¹‰ã€‚å¤æ‚çš„ storage ç±»éœ€è¦å®ç°æ›´å¤šç»†èŠ‚ï¼Œä»¥ç¡®ä¿ç±»å‹èƒ½å¤Ÿåœ¨ MLIR çš„ dialect ç³»ç»Ÿä¸­é¡ºåˆ©è¿è¡Œã€‚\n// include/mlir-learning/Dialect/Poly/PolyTypes.hpp.inc static ::mlir::Type parse(::mlir::AsmParser \u0026amp;odsParser); void print(::mlir::AsmPrinter \u0026amp;odsPrinter) const; int getDegreeBound() const; // include/mlir-learning/Dialect/Poly/PolyTypes.cpp.inc struct PolynomialTypeStorage : public ::mlir::TypeStorage { /* lots of code */ }; PolynomialType PolynomialType::get(::mlir::MLIRContext *context, int degreeBound) { return Base::get(context, std::move(degreeBound)); } ::mlir::Type PolynomialType::parse(::mlir::AsmParser \u0026amp;odsParser) { /* code to parse the type */ } void PolynomialType::print(::mlir::AsmPrinter \u0026amp;odsPrinter) const { ::mlir::Builder odsBuilder(getContext()); odsPrinter \u0026lt;\u0026lt; \u0026#34;\u0026lt;\u0026#34;; odsPrinter.printStrippedAttrOrType(getDegreeBound()); odsPrinter \u0026lt;\u0026lt; \u0026#34;\u0026gt;\u0026#34;; } int PolynomialType::getDegreeBound() const { return getImpl()-\u0026gt;degreeBound; } Adding Some Simple Operations ä¸‹é¢æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç®€å•çš„å¤šé¡¹å¼åŠ æ³•æ“ä½œ\n// include/mlir-tutorial/Dialect/Poly/PolyOps.td\rinclude \u0026#34;PolyDialect.td\u0026#34;\rinclude \u0026#34;PolyTypes.td\u0026#34;\rdef Poly_AddOp : Op\u0026lt;Poly_Dialect, \u0026#34;add\u0026#34;\u0026gt; {\rlet summary = \u0026#34;Addition operation between polynomials.\u0026#34;;\rlet arguments = (ins Polynomial:$lhs, Polynomial:$rhs);\rlet results = (outs Polynomial:$output);\rlet assemblyFormat = \u0026#34;$lhs `,` $rhs attr-dict `:` `(` type($lhs) `,` type($rhs) `)` `-\u0026gt;` type($output)\u0026#34;;\r} å’Œåˆšæ‰å®šä¹‰ types éå¸¸ç›¸è¿‘ï¼Œä½†åŸºç±»æ˜¯ Opï¼Œarguments å¯¹åº”äºæ“ä½œçš„è¾“å…¥ï¼ŒassemblyFormat æ›´å¤æ‚ã€‚ç”Ÿæˆçš„ .hpp.inc å’Œ .cpp.inc éå¸¸å¤æ‚ã€‚æˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ª .mlir æ¥æµ‹è¯•ã€‚\n// CHECK-LABEL: test_add_syntax func.func @test_add_syntax(%arg0: !poly.poly\u0026lt;10\u0026gt;, %arg1: !poly.poly\u0026lt;10\u0026gt;) -\u0026gt; !poly.poly\u0026lt;10\u0026gt; { // CHECK: poly.add %0 = poly.add %arg0, %arg1 : (!poly.poly\u0026lt;10\u0026gt;, !poly.poly\u0026lt;10\u0026gt;) -\u0026gt; !poly.poly\u0026lt;10\u0026gt; return %0 : !poly.poly\u0026lt;10\u0026gt; } ç”Ÿæˆçš„ä»£ç å®šä¹‰äº†ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š\nAdaptor Classes:\nAddOpGenericAdaptorBase å’Œ AddOpAdaptor: æä¾›äº†ä¾¿æ·çš„æ–¹å¼æ¥è®¿é—®æ“ä½œçš„æ“ä½œæ•° (operands) å’Œå±æ€§ (attributes)ã€‚å®ƒä»¬åœ¨ç¼–å†™è½¬æ¢å’Œé‡å†™æ¨¡å¼æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚ Properties Handling:\nè¯¸å¦‚ setPropertiesFromAttr , getPropertiesAsAttr , computePropertiesHash ç­‰å‡½æ•°æ˜¯ MLIR æ“ä½œå±æ€§ç³»ç»Ÿçš„æ¥å£ã€‚è™½ç„¶åœ¨è¿™ä¸ªç‰¹å®šçš„ AddOp å®ç°ä¸­ï¼Œæœ‰äº›å‡½æ•°å¯èƒ½æ˜¯ç©ºå®ç°æˆ–è¿”å›é»˜è®¤å€¼ï¼Œä½†å®ƒä»¬æ˜¯æ“ä½œå®šä¹‰ç»“æ„çš„ä¸€éƒ¨åˆ†ã€‚ Builder Methods:\nå¤šä¸ªé‡è½½çš„ AddOp::build é™æ€æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•ç”¨äºåœ¨ä»£ç ä¸­ä»¥ç¼–ç¨‹æ–¹å¼åˆ›å»º AddOp çš„å®ä¾‹ã€‚ Verification:\nAddOp::verifyInvariantsImpl() å’Œ AddOp::verifyInvariants() : è¿™äº›æ–¹æ³•ç”¨äºæ£€æŸ¥ä¸€ä¸ª AddOp å®ä¾‹æ˜¯å¦ç¬¦åˆå…¶å®šä¹‰ã€‚ä¾‹å¦‚ï¼Œå®ƒä»¬ä¼šéªŒè¯æ“ä½œæ•°çš„æ•°é‡å’Œç±»å‹æ˜¯å¦æ­£ç¡®ï¼Œç»“æœç±»å‹æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚ä»£ç ä¸­è°ƒç”¨äº†åƒ __mlir_ods_local_type_constraint_PolyOps2 è¿™æ ·çš„è¾…åŠ©å‡½æ•°æ¥è¿›è¡Œç±»å‹çº¦æŸæ£€æŸ¥ã€‚ Assembly Format Parsing and Printing:\nAddOp::parse(::mlir::OpAsmParser\u0026amp; parser, ::mlir::OperationState\u0026amp; result) : è¿™ä¸ªæ–¹æ³•å®šä¹‰äº†å¦‚ä½•ä» MLIR çš„æ–‡æœ¬æ±‡ç¼–æ ¼å¼ä¸­è§£æå‡º AddOp ã€‚å½“ MLIR å·¥å…·è¯»å– .mlir æ–‡ä»¶æ—¶ï¼Œä¼šè°ƒç”¨æ­¤æ–¹æ³•ã€‚ AddOp::print(::mlir::OpAsmPrinter\u0026amp; _odsPrinter) : è¿™ä¸ªæ–¹æ³•å®šä¹‰äº†å¦‚ä½•å°† AddOp å®ä¾‹æ‰“å°æˆ MLIR çš„æ–‡æœ¬æ±‡ç¼–æ ¼å¼ã€‚ Type ID Definition:\nMLIR_DEFINE_EXPLICIT_TYPE_ID(::mlir::tutorial::poly::AddOp) : è¿™ä¸ªå®ç”¨äº MLIR çš„è¿è¡Œæ—¶ç±»å‹ä¿¡æ¯ (RTTI) ç³»ç»Ÿï¼Œä¸º AddOp ç±»å‹ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„æ ‡è¯†ç¬¦ã€‚ ","permalink":"http://localhost:1313/blogs/courselearning/mlir/mlir-ch4-defining-a-new-dialect/","summary":"Personal MLIR learning notes 4.","title":"MLIR-Ch4 Defining a New Dialect"},{"content":"What is Tablegen? TableGen æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆä»£ç å’Œæè¿°ç»“æ„çš„ DSL å’Œå·¥å…·ï¼Œæœ€åˆç”± LLVM å¼€å‘ï¼Œåæ¥è¢« MLIR ç»§æ‰¿å¹¶æ‰©å±•ã€‚å®ƒä¸»è¦ç”¨äºä»¥å£°æ˜å¼çš„æ–¹å¼å®šä¹‰å’Œç”Ÿæˆ MLIR çš„å„ç§ç»„ä»¶ï¼Œä¾‹å¦‚ Dialectsã€Operationsã€Attributesã€Types å’Œ Passesï¼Œä»è€Œå‡å°‘æ‰‹åŠ¨ç¼–å†™é‡å¤æ€§ C++ ä»£ç çš„å·¥ä½œé‡ã€‚\nmlir-tablegen å¹¶æ²¡æœ‰æ¸…æ¥šåœ°å‘Šè¯‰ä½ å“ªäº›å‡½æ•°æ²¡æœ‰å®ç°ï¼Œä¹Ÿæ²¡æœ‰è§£é‡Šå¿…é¡»ç¼–å†™çš„å‡½æ•°ã€‚ç¡®å®šç¼ºå¤±å†…å®¹çš„ä¸»è¦æ–¹æ³•æ˜¯å°è¯•ç”¨ä¸€äº›ä½¿ç”¨å®ƒçš„ä»£ç æ¥æ„å»ºç”Ÿæˆçš„ä»£ç ï¼Œç„¶åç­›é€‰æ•°ç™¾è¡Œ c++ ç¼–è¯‘å™¨é”™è¯¯ï¼Œè¿™åè¿‡æ¥åˆéœ€è¦äº†è§£ç”Ÿæˆä»£ç ä¸­çš„å„ç§æ¨¡æ¿æ“ä½œã€‚ç”Ÿæˆçš„ä»£ç å°†ä½¿ç”¨å¿…é¡»çŸ¥é“çš„ç¬¦å·ï¼Œä»¥ä¾¿åœ¨æ­£ç¡®çš„ä½ç½®å¯¼å…¥æˆ–æå‰å£°æ˜ï¼Œå¹¶ä¸”å®ƒè¦æ±‚ç®¡ç†ç”Ÿæˆçš„ä»£ç æ‰€åœ¨çš„åç§°ç©ºé—´ã€‚\nTablegen Files and the mlir-tblgen Binary TableGen å…è®¸ä½ å®šä¹‰å˜é‡ï¼Œå¹¶ä¸”è¿™äº›å˜é‡å¯ä»¥åœ¨å¤šä¸ªå®šä¹‰ä¸­é‡å¤ä½¿ç”¨ã€‚\nTableGenå…è®¸ä½ åœ¨å®šä¹‰ä¸­åµŒå…¥C++ä»£ç ç‰‡æ®µã€‚è¿™äº›ä»£ç ç‰‡æ®µä¼šè¢«æ’å…¥åˆ°TableGenç”Ÿæˆçš„C++ç±»ä¸­ï¼Œå¹¶ä¸”è¿™äº›C++ä»£ç ç‰‡æ®µå¯ä»¥è®¿é—®å‰é¢å®šä¹‰çš„å˜é‡ã€‚è¿™ä½¿å¾—TableGenèƒ½å¤Ÿç”Ÿæˆé«˜åº¦å®šåˆ¶åŒ–çš„C++ä»£ç ã€‚å¦‚æœéœ€è¦ä¸ºä½ çš„ pass ç¼–å†™ç‰¹æ®Šçš„æ„é€ å‡½æ•°ï¼Œå°±å¯ä»¥åœ¨ PassBase.tdä¸­ç”¨ TableGen çš„è¯­æ³•å†™ä¸‹ç›¸åº”çš„ C++ ä»£ç ã€‚\nä¸‹é¢ç»™å‡ºäº†ä¸€ä¸ªä»¥ tablegen è¯­æ³•é‡å†™ä¸Šä¸€ç« çš„ AffineFullUnroll pass çš„ä¾‹å­\n// mlir-learning/Transform/Affine/Pass.td include \u0026#34;mlir/Pass/PassBase.td\u0026#34; def AffineFullUnroll : Pass\u0026lt;\u0026#34;affine-full-unroll\u0026#34;\u0026gt; { let summary = \u0026#34;Fully unroll all affine loops\u0026#34;; let description = [{ Fully unroll all affine loops. (could add more docs here like code examples) }]; let dependentDialects = [\u0026#34;mlir::affine::AffineDialect\u0026#34;]; } TableGen æ‹¥æœ‰ç±»ä¼¼çš„ç±»å’Œç»§æ‰¿çš„æ¦‚å¿µã€‚: Pass\u0026lt;...\u0026gt; è¡¨ç¤ºä¸€ä¸ªç±»ç»§æ‰¿è‡ª PassBase.td æ–‡ä»¶ä¸­å®šä¹‰çš„ Pass åŸºç±»\ndef ç”¨äºå®šä¹‰ä¸€ä¸ªå…·ä½“å®ä¾‹ï¼Œå®ƒä¼šç”Ÿæˆå¯¹åº”çš„ C++ ä»£ç ã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼Œä½¿ç”¨ def å®šä¹‰çš„ç±»å®ä¾‹ä¼šè¢« TableGen å¤„ç†ï¼Œæœ€ç»ˆè½¬æ¢æˆå®é™…çš„ä»£ç ï¼Œè€Œä»…ä»…ä½¿ç”¨ class å®šä¹‰çš„ç±»åˆ™ä¸ä¼šç›´æ¥ç”Ÿæˆä»£ç ï¼Œåªä½œä¸ºæ¨¡æ¿æˆ–åŸºç±»å­˜åœ¨ã€‚\nä¸Šé¢ä»£ç è¯´æ˜ TableGen å…è®¸å®šä¹‰å­—ç¬¦ä¸²å˜é‡å’Œåˆ—è¡¨ã€‚ TableGen è¿˜æœ‰ä¸€ä¸ªé‡è¦åŠŸèƒ½ï¼šå®ƒå…è®¸å®šä¹‰å˜é‡å¹¶åœ¨å¤šä¸ªå®šä¹‰ä¸­å¤ç”¨è¿™äº›å˜é‡ï¼Œè¿˜å¯ä»¥å®šä¹‰ C++ ä»£ç ç‰‡æ®µï¼Œå¹¶å°†è¿™äº›ç‰‡æ®µæ’å…¥åˆ°ç”Ÿæˆçš„ç±»ä¸­ã€‚ è¿™äº› C++ ä»£ç ç‰‡æ®µå¯ä»¥ä½¿ç”¨å‰é¢å®šä¹‰çš„å˜é‡ã€‚ä¾‹å¦‚ PassBase.td ç±»å®šä¹‰äº†ä¸€ä¸ªä»£ç æ„é€ å‡½æ•°å˜é‡ã€‚ å¦‚æœéœ€è¦ä¸ºä½ çš„ Pass ç±»ç¼–å†™ç‰¹æ®Šçš„æ„é€ å‡½æ•°ï¼Œå¯ä»¥åœ¨ PassBase.td ä¸­ç¼–å†™ç›¸åº”çš„ C++ ä»£ç ã€‚ è¿™æ„å‘³ç€ TableGen ä¸ä»…ä»…æ˜¯ç®€å•çš„æ–‡æœ¬æ›¿æ¢ï¼Œå®ƒèƒ½å¤Ÿå¤„ç†æ›´å¤æ‚çš„ä»£ç ç”Ÿæˆé€»è¾‘ï¼ŒåŒ…æ‹¬å˜é‡çš„è·¨å®šä¹‰ä½¿ç”¨å’Œ C++ ä»£ç çš„åµŒå…¥ã€‚\nå’Œä¸Šä¸€ç« ä¸åŒçš„æ˜¯ï¼Œè¿™æ¬¡æˆ‘ä»¬ä¹Ÿéœ€è¦åœ¨ include ç›®å½•ä¸‹å†™ä¸€ä¸ª CMakeLists.txt\nset(TARGET_NAME \u0026#34;${PROJECT_TARGET_PREFIX}-Transform-Affine-Passes-IncGen\u0026#34;) set(LLVM_TARGET_DEFINITIONS mlir-learning/Transform/Affine/Pass.td) mlir_tablegen(mlir-learning/Transform/Affine/Pass.h.inc -gen-pass-decls -name=Affine) mlir_tablegen(mlir-learning/Transform/Affine/Pass.md -gen-pass-doc) add_public_tablegen_target(${TARGET_NAME}) set( ALL_TABLEGEN_TARGETS ${PROJECT_TARGET_PREFIX}-Transform-Affine-Passes-IncGen #${PROJECT_TARGET_PREFIX}-Transform-Arith-Passes-IncGen ) # Add the generated files to a global property, so they can be used in the library set_property( GLOBAL PROPERTY ${PROJECT_TARGET_PREFIX}-TABLEGEN-TARGETS ${ALL_TABLEGEN_TARGETS} ) set(LLVM_TARGET_DEFINITIONS mlir-learning/Transform/Affine/Pass.td): è¿™è¡Œä»£ç è®¾ç½®äº† TableGen çš„è¾“å…¥æ–‡ä»¶ã€‚ mlir_tablegen(mlir-learning/Transform/Affine/Pass.h.inc -gen-pass-decls -name=Affine): è¿™è¡Œè°ƒç”¨äº† mlir_tablegen å‘½ä»¤ï¼Œå®ƒå°† Pass.td æ–‡ä»¶ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆä¸€ä¸ªåä¸º Pass.h.inc çš„å¤´æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å« Pass çš„å£°æ˜ (-gen-pass-decls)ï¼Œå¹¶ä¸”å‘½åç©ºé—´ä¸º Affine (-name=Affine). mlir_tablegen(mlir-learning/Transform/Affine/Pass.md -gen-pass-doc): è¿™è¡ŒåŒæ ·è°ƒç”¨ mlir_tablegenï¼Œç”Ÿæˆä¸€ä¸ªåä¸º Pass.md çš„æ–‡ä»¶ï¼ŒåŒ…å« Pass çš„æ–‡æ¡£ä¿¡æ¯ (-gen-pass-doc). add_public_tablegen_target(${TARGET_NAME}): è¿™è¡Œä»£ç å°† TableGen ç”Ÿæˆçš„ç›®æ ‡æ·»åŠ åˆ° CMake é¡¹ç›®ä¸­ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå…¬å…±ç›®æ ‡ï¼Œå…¶ä»–éƒ¨åˆ†å¯ä»¥ä¾èµ–å®ƒã€‚ set(ALL_TABLEGEN_TARGETS ...): è¿™è¡Œä»£ç å®šä¹‰äº†ä¸€ä¸ªåˆ—è¡¨ ALL_TABLEGEN_TARGETSï¼ŒåŒ…å«æ‰€æœ‰ TableGen ç”Ÿæˆçš„ç›®æ ‡ã€‚ set_property(GLOBAL PROPERTY ...): è¿™è¡Œä»£ç å°†æ‰€æœ‰ TableGen ç”Ÿæˆçš„ç›®æ ‡æ·»åŠ åˆ°å…¨å±€å±æ€§ ${PROJECT_TARGET_PREFIX}-TABLEGEN-TARGETS} ä¸­ã€‚ ä½¿å¾—æ„å»ºç³»ç»Ÿèƒ½å¤Ÿè·Ÿè¸ªå’Œç®¡ç†æ‰€æœ‰ç”± TableGen ç”Ÿæˆçš„æ–‡ä»¶ï¼Œç¡®ä¿å®ƒä»¬è¢«æ­£ç¡®åœ°åŒ…å«åœ¨åº“æˆ–å¯æ‰§è¡Œæ–‡ä»¶ä¸­ã€‚ .inc Files æˆ‘ä»¬åŒæ ·åˆ›å»ºå’Œä¸Šä¸€ç« ç›¸åŒçš„æ–‡ä»¶ (å¯ä»¥å…ˆä¸å†™)ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ç”±äº TableGen ç”Ÿæˆçš„ .inc æ–‡ä»¶ä½äºæ„å»ºç›®å½•ä¸‹ï¼Œåœ¨ lib çš„ CMakeLists.txt ä¸­æˆ‘ä»¬éœ€è¦åœ¨ target_include_directories å‘½ä»¤ä¸­åŠ å…¥ ${CMAKE_OUTPUT_DIR}/include\nä¸‹é¢æˆ‘ä»¬æ¥é€æ®µçœ‹ç”Ÿæˆçš„ .inc æ–‡ä»¶\nå¤´éƒ¨ä¿æŠ¤å’Œæ¡ä»¶ç¼–è¯‘ //===----------------------------------------------------------------------===// // AffineFullUnroll //===----------------------------------------------------------------------===// #ifdef GEN_PASS_DECL_AFFINEFULLUNROLL std::unique_ptr\u0026lt;::mlir::Pass\u0026gt; createAffineFullUnroll(); #undef GEN_PASS_DECL_AFFINEFULLUNROLL #endif // GEN_PASS_DECL_AFFINEFULLUNROLL è¿™éƒ¨åˆ†ä»£ç ä½¿ç”¨äº†é¢„å¤„ç†å® GEN_PASS_DECL_AFFINEFULLUNROLLã€‚ å¦‚æœè¿™ä¸ªå®è¢«å®šä¹‰ï¼Œåˆ™ç¼–è¯‘å™¨ä¼šç”Ÿæˆ createAffineFullUnroll() å‡½æ•°çš„å£°æ˜ã€‚\nPass çš„å®ç° #ifdef GEN_PASS_DEF_AFFINEFULLUNROLL namespace impl { std::unique_ptr\u0026lt;::mlir::Pass\u0026gt; createAffineFullUnroll(); } // namespace impl namespace impl { template \u0026lt;typename DerivedT\u0026gt; class AffineFullUnrollBase : public ::mlir::OperationPass\u0026lt;\u0026gt; { // ... (Pass çš„æ–¹æ³•å®šä¹‰) ... }; } // namespace impl std::unique_ptr\u0026lt;::mlir::Pass\u0026gt; createAffineFullUnroll() { return impl::createAffineFullUnroll(); } #undef GEN_PASS_DEF_AFFINEFULLUNROLL #endif // GEN_PASS_DEF_AFFINEFULLUNROLL è¿™éƒ¨åˆ†æ˜¯ Pass çš„ä¸»è¦å®ç°ã€‚å®ƒä½¿ç”¨äº† GEN_PASS_DEF_AFFINEFULLUNROLL å®æ¥æ§åˆ¶ç¼–è¯‘ã€‚å¦‚æœè¯¥å®è¢«å®šä¹‰ï¼Œåˆ™ç¼–è¯‘å™¨ä¼šç¼–è¯‘ AffineFullUnrollBase ç±»ä»¥åŠ createAffineFullUnroll å‡½æ•°ã€‚\nAffineFullUnrollBase æ˜¯ä¸€ä¸ªåŸºç±»æ¨¡æ¿ï¼Œä½¿ç”¨ CRTP (Curiously Recurring Template Pattern) æŠ€æœ¯ï¼Œå…è®¸æ´¾ç”Ÿç±»é€šè¿‡ DerivedT è·å–è‡ªèº«çš„ç±»å‹ä¿¡æ¯ã€‚ è¿™æ˜¯ä¸€ç§å¸¸è§çš„ C++ è®¾è®¡æ¨¡å¼ï¼Œç”¨äºå®ç°é™æ€å¤šæ€ã€‚å®ƒå®šä¹‰äº† Pass çš„åŸºæœ¬ä¿¡æ¯ï¼Œä¾‹å¦‚åç§°ã€æè¿°ã€å‘½ä»¤è¡Œå‚æ•°ã€ä¾èµ–çš„ Dialect (è¿™é‡Œæ˜¯ mlir::affine::AffineDialect). createAffineFullUnroll å‡½æ•°è´Ÿè´£åˆ›å»º AffineFullUnroll Pass çš„å®ä¾‹ã€‚ å®ƒä½¿ç”¨äº† impl å‘½åç©ºé—´ï¼Œè¿™æ˜¯ä¸€ç§å¸¸è§çš„ C++ ä»£ç ç»„ç»‡æ–¹å¼ï¼Œç”¨äºéšè—å®ç°ç»†èŠ‚ã€‚ Pass æ³¨å†Œ #ifdef GEN_PASS_REGISTRATION //===----------------------------------------------------------------------===// // AffineFullUnroll Registration //===----------------------------------------------------------------------===// inline void registerAffineFullUnroll() { ::mlir::registerPass([]() -\u0026gt; std::unique_ptr\u0026lt;::mlir::Pass\u0026gt; { return createAffineFullUnroll(); }); } // Old registration code, kept for temporary backwards compatibility. inline void registerAffineFullUnrollPass() { ::mlir::registerPass([]() -\u0026gt; std::unique_ptr\u0026lt;::mlir::Pass\u0026gt; { return createAffineFullUnroll(); }); } //===----------------------------------------------------------------------===// // Affine Registration //===----------------------------------------------------------------------===// inline void registerAffinePasses() { registerAffineFullUnroll(); } #undef GEN_PASS_REGISTRATION #endif // GEN_PASS_REGISTRATION Complete .hpp \u0026amp; .cpp TableGenæ ¹æ® .tdæ–‡ä»¶ç”ŸæˆPassçš„ä»£ç ï¼Œç”Ÿæˆçš„ä»£ç åŒ…å«æ³¨å†Œå‡½æ•°ï¼Œè¿™äº›æ³¨å†Œå‡½æ•°æœ€ç»ˆä¼šè¢«è°ƒç”¨ï¼Œå°†Passæ³¨å†Œåˆ°MLIRç³»ç»Ÿä¸­ã€‚ æˆ‘ä»¬å¯ä»¥é€šè¿‡å†™ä¸€ä¸ª Passes.hæ–‡ä»¶é›†ä¸­ç®¡ç†æ‰€æœ‰Passçš„æ³¨å†Œï¼Œç®€åŒ–æ„å»ºè¿‡ç¨‹ã€‚\n// include/mlir-learning/Transform/Affine/Pass.h #include \u0026#34;mlir-learning/Transform/Affine/AffineFullUnroll.h\u0026#34; namespace mlir::tutorial { #define GEN_PASS_REGISTRION #include \u0026#34;mlir-learning/Transform/Affine/Pass.h.inc\u0026#34; } ç„¶åå†å¯¹åº”çš„ AffineFullUnroll.hpp ä¸­å®šä¹‰ GEN_PASS_DECL_AFFINEFULLUNROLL å®ï¼Œä»¥å®ç°åˆ›å»º Pass å‡½æ•°çš„å£°æ˜ã€‚\n#pragma once #include \u0026#34;mlir/Pass/Pass.h\u0026#34; namespace mlir::tutorial { #define GEN_PASS_DECL_AFFINEFULLUNROLL #include \u0026#34;mlir-learning/Transform/Affine/Pass.h.inc\u0026#34; } // namespace mlir::tutorial åŒæ ·åœ¨ cpp ä¸­éœ€è¦å®šä¹‰ GEN_PASS_DEF_AFFINEFULLUNROLL å®ï¼Œç„¶åå†™ä½ å¯¹åº”çš„å®ç° (ä¸ä¸Šä¸€ç« ç›¸åŒ). é—®é¢˜æ˜¯ä»…ä»…æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç å¹¶ä¸èƒ½ç›´æ¥çœ‹å‡ºè¿˜éœ€è¦å®ç°å“ªäº›å‡½æ•°ï¼Œéœ€è¦é€šè¿‡å…¶ä»–æ–¹æ³•æ¥ç¡®å®šã€‚\nç¼–è¯‘å¹¶æŸ¥çœ‹ç¼–è¯‘å™¨é”™è¯¯ä¿¡æ¯: æœ€ç›´æ¥çš„æ–¹æ³•æ˜¯å°è¯•ç¼–è¯‘ä»£ç ã€‚ç¼–è¯‘å™¨ä¼šæŒ‡å‡ºå“ªäº›å‡½æ•°æ²¡æœ‰å®ç°ï¼Œä»è€Œå‘Šè¯‰ä½ éœ€è¦å®ç°å“ªäº›å‡½æ•°ã€‚ ä¸åŸºç±»è¿›è¡Œæ¯”è¾ƒ: å¯ä»¥å°†ç”Ÿæˆçš„ä»£ç ä¸åŸºç±»ï¼ˆOperationPasså’Œ Passï¼‰è¿›è¡Œæ¯”è¾ƒã€‚é€šè¿‡æ¯”è¾ƒï¼Œå¯ä»¥å‘ç°å”¯ä¸€éœ€è¦å®ç°çš„å‡½æ•°æ˜¯ runOnOperation()ã€‚ è¿™éœ€è¦ä½ ç†Ÿæ‚‰MLIR Passçš„ç»§æ‰¿ç»“æ„å’Œå„ä¸ªå‡½æ•°çš„ä½œç”¨ã€‚ è§‚å¯Ÿç¼ºå¤±çš„å‡½æ•°: å¦‚æœä¹‹å‰å·²ç»ä»åŸå§‹APIæ‰‹åŠ¨å®ç°è¿‡ç±»ä¼¼çš„Passï¼Œå¯ä»¥è§‚å¯Ÿç”Ÿæˆçš„ä»£ç ä¸­å“ªäº›å‡½æ•°å·²ç»å­˜åœ¨ï¼ˆä¾‹å¦‚ getArgumentï¼‰ï¼Œå“ªäº›å‡½æ•°ç¼ºå¤±ï¼ˆä¾‹å¦‚ runOnOperationï¼‰ã€‚ é€šè¿‡å¯¹æ¯”ï¼Œå¯ä»¥ç¡®å®šè¿˜éœ€è¦å®ç°å“ªäº›å‡½æ•°ã€‚ å…·ä½“çš„å®ç°ä¸ä¸Šä¸€ç« ç›¸åŒï¼Œè¿™é‡Œæˆ‘ä»¬è¦ç»§æ‰¿ .inc æ–‡ä»¶ä¸­ç”Ÿæˆçš„ç±»\n#include \u0026#34;mlir-learning/Transform/Affine/AffineFullUnroll.h\u0026#34; #include \u0026#34;mlir/Dialect/Affine/IR/AffineOps.h\u0026#34; #include \u0026#34;mlir/Dialect/Affine/LoopUtils.h\u0026#34; #include \u0026#34;mlir/Pass/Pass.h\u0026#34; namespace mlir::tutorial { #define GEN_PASS_DEF_AFFINEFULLUNROLL #include \u0026#34;mlir-learning/Transform/Affine/Pass.h.inc\u0026#34; using mlir::affine::AffineForOp; using mlir::affine::loopUnrollFull; class AffineFullUnroll : public impl::AffineFullUnrollBase\u0026lt;AffineFullUnroll\u0026gt; { public: using AffineFullUnrollBase::AffineFullUnrollBase; void runOnOperation() override { getOperation()-\u0026gt;walk([\u0026amp;](AffineForOp op) { if (failed(loopUnrollFull(op))) { op.emitError(\u0026#34;unrolling failed\u0026#34;); signalPassFailure(); } }); } }; } // namespace mlir::tutorial æœ€ååœ¨ tutorial.cpp ä¸­ä½¿ç”¨ .inc æ–‡ä»¶ç”Ÿæˆçš„ registerAffinePasses\n#include \u0026#34;mlir/IR/DialectRegistry.h\u0026#34; #include \u0026#34;mlir/InitAllDialects.h\u0026#34; #include \u0026#34;mlir/Pass/PassManager.h\u0026#34; #include \u0026#34;mlir/Pass/PassRegistry.h\u0026#34; #include \u0026#34;mlir/Tools/mlir-opt/MlirOptMain.h\u0026#34; #include \u0026#34;mlir-learning/Transform/Affine/Pass.h\u0026#34; int main(int argc, char** argv) { // Register all built-in MLIR dialects mlir::DialectRegistry registry; mlir::registerAllDialects(registry); mlir::tutorial::registerAffinePasses(); return mlir::asMainReturnCode( mlir::MlirOptMain(argc, argv, \u0026#34;Tutorial Pass Driver\u0026#34;, registry)); } ","permalink":"http://localhost:1313/blogs/courselearning/mlir/mlir-ch3-using-tablegen-for-passes/","summary":"Personal MLIR learning notes 3.","title":"MLIR-Ch3 Using Tablegen for Passes"},{"content":"Tutorial-opt and Project Organization ç¼–è¯‘å™¨å¯èƒ½å°† mlir-opt ä½œä¸ºå­ä¾‹ç¨‹åœ¨å‰ç«¯ (c++ -\u0026gt; æŸäº›MLIRæ–¹è¨€) å’Œåç«¯ (MLIR çš„ LLVM æ–¹è¨€ -\u0026gt; LLVM -\u0026gt; æœºå™¨ç ) ä¹‹é—´è¿è¡Œã€‚ (æˆ‘å°†å®ƒå‘½åä¸º tutorial-opt).\nå…¸å‹çš„ MLIR ä»£ç åº“å°†ä»£ç åˆ†æˆå…·æœ‰å¤§è‡´ç›¸åŒå±‚æ¬¡ç»“æ„çš„ç›®å½•ï¼š\ninclude/ ç›®å½•ç”¨äºå­˜æ”¾å¤´æ–‡ä»¶å’Œtablegen æ–‡ä»¶ï¼Œ lib/ ç›®å½•ç”¨äºå­˜æ”¾å®ç°ä»£ç ã€‚å¯èƒ½ä¼šæœ‰ Transform/ å­ç›®å½•ç”¨äºå­˜å‚¨åœ¨æ–¹è¨€ä¸­è½¬æ¢ä»£ç çš„ passï¼ŒConversion/ å­ç›®å½•ç”¨äºåœ¨æ–¹è¨€ä¹‹é—´è½¬æ¢çš„ pass ï¼ŒAnalysis/ å­ç›®å½•ç”¨äºåˆ†æ passï¼Œç­‰ç­‰ã€‚è¿™äº›ç›®å½•ä¸­çš„æ¯ä¸€ä¸ªéƒ½å¯èƒ½æœ‰å®ƒä»¬æ‰€æ“ä½œçš„ç‰¹å®šæ–¹è¨€çš„å­ç›®å½•ã€‚ test/ ç”¨äºå­˜æ”¾éœ€è¦æµ‹è¯•çš„ mlir æ–‡ä»¶ã€‚ tools/ å­˜æ”¾ç”¨äºæ³¨å†Œ pass çš„ä¸»æ–‡ä»¶ ./Ch1-WritingOurFirstPass/ â”œâ”€â”€ CMakeLists.txt â”œâ”€â”€ include â”‚ â””â”€â”€ mlir-tutorial â”œâ”€â”€ lib â”‚ â”œâ”€â”€ CMakeLists.txt â”‚ â””â”€â”€ Transform â”œâ”€â”€ tests â”‚ â”œâ”€â”€ Output â”‚ â”œâ”€â”€ affine_loop_unroll.mlir â”‚ â”œâ”€â”€ lit.cfg.py â”‚ â””â”€â”€ mul_to_add.mlir â””â”€â”€ tools â”œâ”€â”€ CMakeLists.txt â””â”€â”€ tutorial-opt.cpp å°½ç®¡ MLIR æä¾›äº†è®¸å¤šå®šä¹‰å¾ªç¯å’Œæ§åˆ¶æµçš„æœºåˆ¶ï¼Œæœ€é«˜çº§çš„æ˜¯ affine dialect. å®ƒè¢«è®¾è®¡ç”¨æ¥è¿›è¡Œå¤šé¢ä½“å¾ªç¯åˆ†æ (polyhedral loop analysis).\nPolyhedral Loop Analysis å¤šé¢ä½“å¾ªç¯åˆ†æçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†ç¨‹åºä¸­çš„å¾ªç¯å’Œæ•°ç»„è®¿é—®æŠ½è±¡ä¸ºæ•°å­¦å½¢å¼ï¼Œä½¿å¾—å¯ä»¥åº”ç”¨å‡ ä½•å˜æ¢æ¥ä¼˜åŒ–ä»£ç ã€‚è¿™ç§æ•°å­¦å½¢å¼é€šå¸¸è¡¨ç¤ºä¸º æ•´æ•°çº¿æ€§ä¸ç­‰å¼çš„é›†åˆ ï¼Œè¿™äº›ä¸ç­‰å¼å®šä¹‰äº†å¾ªç¯è¿­ä»£ç©ºé—´å’Œæ•°ç»„è®¿é—®çš„èŒƒå›´ã€‚\nè¿­ä»£ç©ºé—´ï¼ˆIteration Spaceï¼‰ ï¼šç¨‹åºä¸­çš„å¾ªç¯åµŒå¥—å¯ä»¥è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªå¤šç»´çš„è¿­ä»£ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä¸€ä¸ªåŒå±‚åµŒå¥—å¾ªç¯ï¼š for (i = 0; i \u0026lt; N; i++) { for (j = 0; j \u0026lt; M; j++) { A[i][j] = A[i][j] + 1; } } è¿™é‡Œçš„è¿­ä»£ç©ºé—´æ˜¯äºŒç»´çš„ï¼Œç”± (i, j) æ„æˆã€‚\nè®¿é—®å…³ç³»ï¼ˆAccess Relationsï¼‰ ï¼šæ¯ä¸ªæ•°ç»„çš„è®¿é—®æ¨¡å¼ï¼ˆä¾‹å¦‚ A[i][j]ï¼‰ä¹Ÿå¯ä»¥è¢«è¡¨ç¤ºä¸ºå‡ ä½•å…³ç³»ã€‚è¿™ç§å…³ç³»å®šä¹‰äº†å“ªäº›è¿­ä»£å˜é‡è®¿é—®å“ªäº›æ•°ç»„å…ƒç´ ã€‚ å¤šé¢ä½“è¡¨ç¤ºï¼ˆPolyhedral Representationï¼‰ ï¼šåœ¨å¤šé¢ä½“å¾ªç¯åˆ†æä¸­ï¼Œå¾ªç¯çš„è¿­ä»£ç©ºé—´å’Œæ•°ç»„è®¿é—®æ¨¡å¼å¯ä»¥ç”¨æ•´æ•°çº¿æ€§ä¸ç­‰å¼æ¥è¡¨ç¤ºï¼Œä»è€Œå½¢æˆä¸€ä¸ªå¤šé¢ä½“ã€‚ä¾‹å¦‚ï¼Œ0\u0026lt;=i\u0026lt;N å’Œ 0\u0026lt;=j\u0026lt;M æ˜¯ä¸¤ä¸ªç®€å•çš„çº¿æ€§ä¸ç­‰å¼ï¼Œå®ƒä»¬è¡¨ç¤ºå¾ªç¯çš„è¾¹ç•Œã€‚ ä¸€ä¸ªç®€å•çš„å¯¹æ•°ç»„æ±‚å’Œçš„å‡½æ•°å¦‚ä¸‹: affine.for å®šä¹‰ä¸€ä¸ªå¾ªç¯ï¼Œè¿­ä»£å˜é‡ä¸º %iï¼ŒèŒƒå›´ [0,4)ï¼Œå³å¾ªç¯ 4 æ¬¡ã€‚ iter_args(%sum_iter = %sum_0) è¡¨ç¤ºå¾ªç¯ç»´æŠ¤ä¸€ä¸ªè¿­ä»£å˜é‡ %sum_iterï¼Œåˆå§‹å€¼ä¸º %sum_0.\nfunc.func @sum_buffer(%buffer: memref\u0026lt;4xi32\u0026gt;) -\u0026gt; i32 { %sum_0 = arigh.constant 0 : i32 %sum = affine.for %i = 0 to 4 iter_args(%sum_iter = %sum_0) -\u0026gt; (i32) { %t = affine.load %buffer[%i] : memref\u0026lt;4xi32\u0026gt; %sum_next = arith.addi %sum_iter, %t : i32 affine.yield %sum_next : i32 } return %sum: i32 } MLIR é«˜çº§ç»“æ„ åŸºäºå›¾æ•°æ®ç»“æ„ï¼Œå…¶èŠ‚ç‚¹ç§°ä¸º Operationsï¼Œè¾¹ç§°ä¸º Valuesã€‚æ¯ä¸ª Value éƒ½æ˜¯ä¸€ä¸ª Operation æˆ– Block Argument çš„ç»“æœï¼Œå¹¶å…·æœ‰ç”±ç±»å‹ç³»ç»Ÿå®šä¹‰çš„ Value Typeã€‚Operations åŒ…å«åœ¨ Blocks ä¸­ï¼ŒBlocks åŒ…å«åœ¨ Regions ä¸­ã€‚Operations åœ¨å…¶æ‰€åœ¨çš„ Block ä¸­æ˜¯æœ‰åºçš„ï¼ŒBlocks åœ¨å…¶æ‰€åœ¨çš„ Region ä¸­ä¹Ÿæ˜¯æœ‰åºçš„ï¼Œå°½ç®¡è¿™ç§é¡ºåºåœ¨ç‰¹å®šç±»å‹çš„ Region ä¸­å¯èƒ½å…·æœ‰æˆ–ä¸å…·æœ‰è¯­ä¹‰æ„ä¹‰ã€‚Operations è¿˜å¯ä»¥åŒ…å« Regionsï¼Œä»è€Œèƒ½å¤Ÿè¡¨ç¤ºå±‚æ¬¡åŒ–çš„ç»“æ„ã€‚\nOperations å¯ä»¥è¡¨ç¤ºå¤šç§ä¸åŒçš„æ¦‚å¿µï¼Œä»é«˜çº§æ¦‚å¿µå¦‚å‡½æ•°å®šä¹‰ã€å‡½æ•°è°ƒç”¨ã€ç¼“å†²åŒºåˆ†é…ã€ç¼“å†²åŒºçš„è§†å›¾æˆ–åˆ‡ç‰‡ã€è¿›ç¨‹åˆ›å»ºï¼Œåˆ°ä½çº§æ¦‚å¿µå¦‚ç›®æ ‡æ— å…³çš„ç®—æœ¯è¿ç®—ã€ç›®æ ‡ç‰¹å®šçš„æŒ‡ä»¤ã€é…ç½®å¯„å­˜å™¨å’Œé€»è¾‘é—¨ã€‚è¿™äº›ä¸åŒçš„æ¦‚å¿µåœ¨ MLIR ä¸­ç”±ä¸åŒçš„ Operations è¡¨ç¤ºï¼Œå¹¶ä¸” MLIR ä¸­å¯ç”¨çš„ Operations é›†å¯ä»¥ä»»æ„æ‰©å±•ã€‚\nMLIR è¿˜æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºå¯¹ Operations è¿›è¡Œè½¬æ¢ï¼Œä½¿ç”¨ç†Ÿæ‚‰çš„ç¼–è¯‘å™¨ Passes æ¦‚å¿µã€‚åœ¨ä»»æ„ Operations é›†ä¸Šå¯ç”¨ä»»æ„ Passes é›†ä¼šå¸¦æ¥æ˜¾è‘—çš„æ‰©å±•æ€§æŒ‘æˆ˜ï¼Œå› ä¸ºæ¯ä¸ªè½¬æ¢å¯èƒ½éœ€è¦è€ƒè™‘ä»»ä½• Operation çš„è¯­ä¹‰ã€‚MLIR é€šè¿‡å…è®¸ä½¿ç”¨ Traits å’Œ Interfaces æŠ½è±¡åœ°æè¿° Operation çš„è¯­ä¹‰æ¥è§£å†³è¿™ç§å¤æ‚æ€§ï¼Œä»è€Œä½¿è½¬æ¢èƒ½å¤Ÿæ›´é€šç”¨åœ°æ“ä½œ Operationsã€‚Traits é€šå¸¸æè¿°å¯¹æœ‰æ•ˆ IR çš„éªŒè¯çº¦æŸï¼Œèƒ½å¤Ÿæ•è·å’Œæ£€æŸ¥å¤æ‚çš„ä¸å˜æ€§ã€‚ï¼ˆå‚è§ Op vs Operationï¼‰\nMLIR çš„è¡¨ç¤ºåŸºäº SSA çš„ IRï¼Œä¾‹å¦‚ LLVM core IRï¼Œé€šè¿‡é€‚å½“é€‰æ‹© Operation ç±»å‹æ¥å®šä¹‰ Modulesã€Functionsã€Branchesã€Memory Allocationï¼Œä»¥åŠéªŒè¯çº¦æŸä»¥ç¡®ä¿ SSA Dominance å±æ€§ã€‚MLIR åŒ…å«ä¸€ç»„ Dialectsï¼Œå®šä¹‰äº†æ­¤ç±»ç»“æ„ã€‚\nAffine Full Unroll Pass MLIR æä¾›äº†ä¸€ä¸ªæ–¹æ³• loopUnrollFull æ¥è¿›è¡Œå¾ªç¯å±•å¼€ï¼Œå› æ­¤æˆ‘ä»¬çš„ pass å°†æ˜¯å¯¹è¿™ä¸ªå‡½æ•°è°ƒç”¨çš„ä¸€ä¸ªåŒ…è£…ï¼Œç›´æ¥è°ƒç”¨ C++ API å®ç°ã€‚\n// include/mlir-learning/Transform/Affine/AffineFullUnroll.h class AffineFullUnrollPass : public PassWrapper\u0026lt;AffineFullUnrollPass, OperationPass\u0026lt;mlir::FuncOp\u0026gt;\u0026gt; { private: void runOnOperation() override; StringRef getArgument() const final {return \u0026#34;affine-full-unroll\u0026#34;;} StringRef getDescription() const final { return \u0026#34;Perform full unrolling of all affine.for loops\u0026#34;; } }; // lib/Transform/Affine/AffineFullUnroll.cpp using mlir::affine::AffineForOp; using mlir::affine::loopUnrollFull; void AffineFullUnrollPass::runOnOperation() { getOperation().walk( [\u0026amp;](AffineForOp op) { if (failed(loopUnrollFull(op))) { op.emitError(\u0026#34;unrolling failed\u0026#34;); signalPassFailure(); } }); } è¯¥ç±»çš„å®šä¹‰ä½¿ç”¨äº†å¥‡å¼‚é€’å½’æ¨¡æ¿æ¨¡å¼ (Curiously Recurring Template Pattern, CRTP). PassWrapper æ˜¯ MLIR æ¡†æ¶ä¸­çš„ä¸€ä¸ªæ¨¡æ¿ç±»ï¼Œä¸ºå®šä¹‰çš„ Pass æä¾›é€šç”¨åŠŸèƒ½ (å¦‚ç±»å‹æ£€æŸ¥ã€åç§°è·å–ã€å…‹éš†)ã€‚å¼€å‘è€…åªéœ€ä¸“æ³¨äº Pass çš„æ ¸å¿ƒé€»è¾‘ï¼ˆå¦‚ runOnOperationï¼‰ï¼Œè€Œæ— éœ€æ‰‹åŠ¨å®ç°ç±»å‹æ ‡è¯†ã€å…‹éš†ç­‰è¾…åŠ©åŠŸèƒ½ã€‚\nrunOnOperation ä¸­è°ƒç”¨äº† getOperation æ–¹æ³•ï¼Œå®ƒæ˜¯ MLIR ä¸­ Pass ç±»æä¾›çš„ä¸€ä¸ªæ–¹æ³•ï¼Œè¿”å›å½“å‰æ“ Operation. walk æ–¹æ³•æ˜¯ MLIR æä¾›çš„ä¸€ä¸ªéå†æ–¹æ³•ï¼Œç”¨æ¥éå†æ“ä½œæ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ã€‚å®ƒä¼šé€’å½’åœ°éå†æ“ä½œæ ‘ä¸­çš„æ‰€æœ‰å­æ“ä½œï¼Œå¹¶å¯¹æ¯ä¸ªæ“ä½œåº”ç”¨ä¼ å…¥çš„å›è°ƒå‡½æ•° (lambda func). å½“è¿è¡Œè¿™ä¸ª Pass æ—¶ï¼Œå®ƒä¼šåœ¨æ¯ä¸€ä¸ª AffineForOp ç±»å‹çš„æ“ä½œä¸Šæ‰§è¡Œ runOnOperation å‡½æ•°ã€‚ getArgument æ–¹æ³•è¿”å› Pass çš„å‘½ä»¤è¡Œå‚æ•°ã€‚è¿™ä¸ªè¿”å›å€¼ affine-full-unroll è¡¨ç¤ºè¿™ä¸ª Pass çš„åç§°ï¼Œå¯ä»¥åœ¨è¿è¡Œæ—¶é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ˜¯å¦å¯ç”¨è¯¥ Pass. getDescription æ–¹æ³•ä¼šåœ¨è°ƒç”¨åƒ mlir-opt è¿™æ ·çš„å·¥å…·æ—¶è‹¥æœ‰ --help å‚æ•°åˆ™è¿”å› Pass çš„æè¿°ä¿¡æ¯ã€‚ Callback Function å›è°ƒå‡½æ•° (Callback Function) æ˜¯ä¸€ç§é€šè¿‡å°†å‡½æ•°ä½œä¸ºå‚æ•°ä¼ é€’ç»™å¦ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å®ç°æŸäº›ç‰¹å®šæ“ä½œçš„æœºåˆ¶ã€‚å›è°ƒå‡½æ•°é€šå¸¸åœ¨æŸä¸ªäº‹ä»¶å‘ç”Ÿæˆ–æŸä¸ªç‰¹å®šæ¡ä»¶æ»¡è¶³æ—¶è¢«è°ƒç”¨ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå›è°ƒå‡½æ•°å°±æ˜¯è¢«è°ƒç”¨çš„å‡½æ•°ï¼Œå®ƒä¼šåœ¨ç‰¹å®šçš„æ—¶æœºè¢«æ‰§è¡Œã€‚\nåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒinvokeCallback å‡½æ•°æ¥æ”¶åˆ° printMessage å‡½æ•°çš„åœ°å€ï¼Œå¹¶åœ¨ main å‡½æ•°ä¸­è°ƒç”¨å®ƒã€‚\n#include \u0026lt;iostream\u0026gt; // å›è°ƒå‡½æ•°çš„å®šä¹‰ void printMessage() { std::cout \u0026lt;\u0026lt; \u0026#34;Hello, World!\u0026#34; \u0026lt;\u0026lt; std::endl; } // æ¥å—å›è°ƒå‡½æ•°ä½œä¸ºå‚æ•°çš„å‡½æ•° void invokeCallback(void (*callback)()) { // è°ƒç”¨å›è°ƒå‡½æ•° callback(); } int main() { // å°†å›è°ƒå‡½æ•°ä¼ é€’ç»™å¦ä¸€ä¸ªå‡½æ•° invokeCallback(printMessage); return 0; } åœ¨ç°ä»£ C++ ä¸­ï¼Œå›è°ƒå‡½æ•°é€šå¸¸é€šè¿‡ lambda è¡¨è¾¾å¼ä¼ é€’ã€‚ä¸‹é¢çš„ä¾‹å­ä¸­ invokeCallback å‡½æ•°æ¥å—ä¸€ä¸ª std::function\u0026lt;void()\u0026gt; ç±»å‹çš„å›è°ƒå‡½æ•°å‚æ•°ã€‚åœ¨ main å‡½æ•°ä¸­ï¼Œä¼ å…¥äº†ä¸€ä¸ª Lambda è¡¨è¾¾å¼ä½œä¸ºå›è°ƒå‡½æ•°ã€‚\n#include \u0026lt;iostream\u0026gt; void invokeCallback(std::function\u0026lt;void()\u0026gt; callback) { callback(); } int main() { // ä½¿ç”¨ Lambda è¡¨è¾¾å¼ä½œä¸ºå›è°ƒå‡½æ•° invokeCallback([](){ std::cout \u0026lt;\u0026lt; \u0026#34;Hello from Lambda!\u0026#34; \u0026lt;\u0026lt; std::endl; }); return 0; } Registering the Pass æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åœ¨ tutorial.cpp ä¸­æ³¨å†Œè¿™ä¸ª Passã€‚\n#include \u0026#34;mlir-learning/Transform/Affine/AffineFullUnroll.h\u0026#34; #include \u0026#34;mlir/InitAllDialects.h\u0026#34; #include \u0026#34;mlir/Pass/PassManager.h\u0026#34; #include \u0026#34;mlir/Pass/PassRegistry.h\u0026#34; #include \u0026#34;mlir/Tools/mlir-opt/MlirOptMain.h\u0026#34; int main(int argc, char** argv) { mlir::DialectRegistry registry; mlir::registerAllDialects(registry); mlir::PassRegistration\u0026lt;mlir::tutorial::AffineFullUnrollPass\u0026gt;(); return mlir::asMainReturnCode( mlir::MlirOptMain(argc, argv, \u0026#34;Tutorial Pass Driver\u0026#34;, registry)); } mlir::registerAllDialects(registry); ä¼šè°ƒç”¨ MLIR åº“çš„å‡½æ•°ï¼Œå°†æ‰€æœ‰å¯ç”¨çš„æ–¹è¨€æ³¨å†Œåˆ° registry ä¸­ã€‚æ–¹è¨€æ˜¯ MLIR ä¸­ç”¨æ¥å®šä¹‰å„ç§ä¸­é—´è¡¨ç¤ºçš„æŠ½è±¡ï¼Œå¯ä»¥ç†è§£ä¸ºä¸åŒç±»å‹çš„ IR. mlir::PassRegistration\u0026lt;mlir::tutorial::AffineFullUnrollPass\u0026gt;(); å°†è‡ªå®šä¹‰çš„ AffineFullUnrollPass æ³¨å†Œåˆ° MLIR çš„ Pass ç³»ç»Ÿä¸­ã€‚ MlirOptMain æ˜¯ MLIR æä¾›çš„ä¸€ä¸ªå‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶æ‰§è¡Œç›¸åº”çš„ Pass. argc å’Œ argvï¼šæ¥è‡ªå‘½ä»¤è¡Œçš„å‚æ•°ã€‚ \u0026ldquo;Tutorial Pass Driver\u0026rdquo;ï¼šè¿™æ˜¯ä¸€ä¸ªç¨‹åºæè¿°å­—ç¬¦ä¸²ï¼Œé€šå¸¸æ˜¯ç»™ç”¨æˆ·çš„ä¿¡æ¯ã€‚ registryï¼šä¹‹å‰åˆ›å»ºçš„ DialectRegistryï¼Œå®ƒåŒ…å«äº†æ‰€æœ‰å·²æ³¨å†Œçš„æ–¹è¨€ã€‚ mlir::asMainReturnCode(...) å°† MlirOptMain çš„è¿”å›å€¼è½¬æ¢ä¸ºæ ‡å‡†çš„é€€å‡ºä»£ç  (0 è¡¨ç¤ºæˆåŠŸï¼Œéé›¶å€¼è¡¨ç¤ºå¤±è´¥). Test the Pass æˆ‘ä»¬å†™ä¸€ä¸ª .mlir æ¥æµ‹è¯•æˆ‘ä»¬çš„ Passï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹æ•°ç»„è¿›è¡Œç´¯åŠ çš„å‡½æ•°ã€‚FileCheck æ£€æŸ¥ç»è¿‡ Pass åå‡½æ•°ä¸­ä¸ä¼šå­˜åœ¨ affine.for æŒ‡ä»¤ã€‚\n// RUN: /leaning/build/chapter2/tools/02-tutorial-opt %s --affine-full-unroll \u0026gt; %t // RUN: FileCheck %s \u0026lt; %t func.func @test_single_nested_loop(%buffer: memref\u0026lt;4xi32\u0026gt;) -\u0026gt; (i32) { %sum_0 = arith.constant 0 : i32 // CHECK-NOT: affine.for %sum = affine.for %i = 0 to 4 iter_args(%sum_iter = %sum_0) -\u0026gt; i32 { %t = affine.load %buffer[%i] : memref\u0026lt;4xi32\u0026gt; %sum_next = arith.addi %sum_iter, %t : i32 affine.yield %sum_next : i32 } return %sum : i32 } ç»è¿‡ä¼˜åŒ–åçš„å‡½æ•°å¦‚ä¸‹\n#map = affine_map\u0026lt;(d0) -\u0026gt; (d0 + 1)\u0026gt; #map1 = affine_map\u0026lt;(d0) -\u0026gt; (d0 + 2)\u0026gt; #map2 = affine_map\u0026lt;(d0) -\u0026gt; (d0 + 3)\u0026gt; module { func.func @test_single_nested_loop(%arg0: memref\u0026lt;4xi32\u0026gt;) -\u0026gt; i32 { %c0 = arith.constant 0 : index %c0_i32 = arith.constant 0 : i32 %0 = affine.load %arg0[%c0] : memref\u0026lt;4xi32\u0026gt; %1 = arith.addi %c0_i32, %0 : i32 %2 = affine.apply #map(%c0) %3 = affine.load %arg0[%2] : memref\u0026lt;4xi32\u0026gt; %4 = arith.addi %1, %3 : i32 %5 = affine.apply #map1(%c0) %6 = affine.load %arg0[%5] : memref\u0026lt;4xi32\u0026gt; %7 = arith.addi %4, %6 : i32 %8 = affine.apply #map2(%c0) %9 = affine.load %arg0[%8] : memref\u0026lt;4xi32\u0026gt; %10 = arith.addi %7, %9 : i32 return %10 : i32 } } A Rewrite Pattern Version å½“æƒ³è¦å¯¹ä¸€ä¸ªç»™å®šçš„ IR å­ç»“æ„é‡å¤åº”ç”¨ç›¸åŒçš„å˜æ¢å­é›†ï¼Œç›´åˆ°è¯¥å­ç»“æ„è¢«å®Œå…¨å»é™¤æ—¶ï¼Œéœ€è¦å†™ä¸€ä¸ªé‡å†™æ¨¡å¼å¼•æ“ã€‚é‡å†™æ¨¡å¼æ˜¯ OpRewritePattern çš„å­ç±»ï¼Œå®ƒæœ‰ä¸€ä¸ªåä¸º matchAndRewrite çš„æ–¹æ³•æ¥æ‰§è¡Œè½¬æ¢ã€‚\n// chapter2/lib/Transform/Affine/AffineFullUnroll.cpp struct AffineFullUnrollPattern : public mlir::OpRewritePattern\u0026lt;AffineForOp\u0026gt; { AffineFullUnrollPattern(mlir::MLIRContext* context) : mlir::OpRewritePattern\u0026lt;AffineForOp\u0026gt;(context, 1) { } // ä¸€èˆ¬åœ¨ OpRewritePattern ä¸­ï¼ŒIR çš„æ›´æ”¹è¦é€šè¿‡ PatternRewriter // PatternRewriter å¤„ç† OpRewritePatternä¸­å‘ç”Ÿçš„çªå˜çš„åŸå­æ€§ LogicalResult matchAndRewrite(AffineForOp op, PatternRewriter\u0026amp; rewriter) const override{ return loopUnrollFull(op); } }; AffineFullUnrollPattern ç»§æ‰¿è‡ª OpRewritePattern\u0026lt;AffineForOp\u0026gt;ï¼ŒOpRewritePattern æ˜¯ MLIR ä¸­ç”¨äºå¯¹ç‰¹å®šæ“ä½œç±»å‹ (åœ¨è¿™é‡Œæ˜¯ AffineForOp) è¿›è¡Œæ¨¡å¼åŒ¹é…å’Œé‡å†™çš„åŸºç±»ã€‚æ¨¡æ¿å‚æ•° AffineForOp è¡¨ç¤ºæˆ‘ä»¬è¦ä¸º AffineForOp è¿™ä¸ªæ“ä½œåˆ›å»ºä¸€ä¸ªæ¨¡å¼ã€‚ æ„é€ å‡½æ•°åˆå§‹åŒ–äº†åŸºç±» OpRewritePattern\u0026lt;AffineForOp\u0026gt;ï¼Œå¹¶ä¼ é€’äº†ä¸¤ä¸ªå‚æ•° contextï¼šMLIRContext æ˜¯ MLIR çš„ä¸Šä¸‹æ–‡ï¼Œä¿å­˜ç€æ‰€æœ‰çš„æ“ä½œã€æ–¹è¨€å’Œç±»å‹ç­‰ä¿¡æ¯ã€‚åœ¨è¿™é‡Œï¼Œcontext ç”¨æ¥åˆå§‹åŒ–æ¨¡å¼å¯¹è±¡ã€‚ benefit æ˜¯ä¸€ä¸ªè¡¨ç¤ºæ¨¡å¼åŒ¹é…ä¼˜å…ˆçº§çš„æ•´æ•°å€¼ï¼Œä¼˜å…ˆçº§è¶Šé«˜çš„æ¨¡å¼è¶Šå…ˆåº”ç”¨ã€‚ matchAndRewrite æ˜¯åœ¨ MLIR ä¸­è¿›è¡Œæ¨¡å¼é‡å†™çš„æ ¸å¿ƒæ–¹æ³•ã€‚å®ƒçš„ç›®çš„æ˜¯ï¼šæ£€æŸ¥æŸä¸ªæ“ä½œæ˜¯å¦ç¬¦åˆå½“å‰æ¨¡å¼çš„è¦æ±‚ã€‚å¦‚æœæ“ä½œåŒ¹é…æ¨¡å¼ï¼Œåˆ™æ‰§è¡Œé‡å†™æ“ä½œï¼Œé€šå¸¸ä¼šç”¨æ–°çš„ IR æ›¿æ¢åŸæ¥çš„ IRã€‚ AffineForOp op è¡¨ç¤ºè¦è¿›è¡Œæ¨¡å¼åŒ¹é…çš„ AffineForOp æ“ä½œã€‚ PatternRewriter \u0026amp;rewriter æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆæ–°çš„ MLIR æ“ä½œçš„å·¥å…·ï¼Œå®ƒå¯ä»¥ä¿®æ”¹ IR. æˆ‘ä»¬åŒæ ·è¦åƒä¸Šä¸€èŠ‚ä¸€æ ·åœ¨å¤´æ–‡ä»¶ä¸­å£°æ˜ä¸€ä¸ª AffineFullUnrollPassAsPatternRewrite ç±»ï¼Œç„¶åå®ç°å…¶ runOnOperation æ–¹æ³•ã€‚\n// chapter2/lib/Transform/Affine/AffineFullUnroll.cpp void AffineFullUnrollPassAsPatternRewrite::runOnOperation() { mlir::RewritePatternSet patterns(\u0026amp;getContext()); patterns.add\u0026lt;AffineFullUnrollPattern\u0026gt;(\u0026amp;getContext()); (void) applyPatternsGreedily(getOperation(), std::move(patterns)); } RewritePatternSet æ˜¯ MLIR ä¸­ä¸€ä¸ªå®¹å™¨ï¼Œç”¨äºå­˜å‚¨å¤šä¸ª Rewrite Pattern. æ¯ä¸ªæ¨¡å¼éƒ½æ˜¯é’ˆå¯¹æŸç§ç‰¹å®šæ“ä½œè¿›è¡Œçš„ä¼˜åŒ–è§„åˆ™ã€‚RewritePatternSet ä¼šæŠŠæ‰€æœ‰è¿™äº›è§„åˆ™èšåˆåœ¨ä¸€èµ·ï¼Œæ–¹ä¾¿åœ¨åç»­çš„æ­¥éª¤ä¸­æ‰¹é‡åº”ç”¨ã€‚ ç„¶åé€šè¿‡ patterns.add\u0026lt;AffineFullUnrollPattern\u0026gt;ï¼Œå°†ä¸€ä¸ª Rewrite Pattern (è¿™é‡Œæ˜¯ä¸Šé¢å®šä¹‰çš„ AffineFullUnrollPattern) æ·»åŠ åˆ° patterns é›†åˆä¸­ã€‚ applyPatternsGreedilyæ˜¯ MLIR æä¾›çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå°†å®šä¹‰çš„æ¨¡å¼åº”ç”¨åˆ°ç»™å®šçš„æ“ä½œ (getOperation()) ä¸Šã€‚è¿™ä¸ªå‡½æ•°ä½¿ç”¨è´ªå¿ƒç­–ç•¥ï¼Œåœ¨ä¸€æ¬¡éå†ä¸­å°½å¯èƒ½å¤šåœ°åº”ç”¨æ¨¡å¼ï¼Œç›´åˆ°æ— æ³•å†åº”ç”¨ä¸ºæ­¢ã€‚ std::move std::move æ˜¯ C++11 å¼•å…¥çš„ä¸€ä¸ªæ ‡å‡†åº“å‡½æ•°ï¼Œå®ƒçš„ä¸»è¦ä½œç”¨æ˜¯å°†ä¸€ä¸ªå¯¹è±¡è½¬æ¢ä¸ºå³å€¼å¼•ç”¨ï¼Œä»¥ä¾¿å¯ç”¨ç§»åŠ¨è¯­ä¹‰ (Move Semantics). ç®€å•æ¥è¯´ï¼Œstd::move æœ¬èº«å¹¶ä¸å®é™…ç§»åŠ¨å¯¹è±¡ï¼Œè€Œæ˜¯ä¸ºå¯¹è±¡æä¾›ä¸€ä¸ªæŒ‡ç¤ºï¼Œå‘Šè¯‰ç¼–è¯‘å™¨è¯¥å¯¹è±¡å¯ä»¥è¢«ç§»åŠ¨è€Œä¸æ˜¯å¤åˆ¶ã€‚\nåœ¨ C++ ä¸­ï¼Œæœ‰ä¸¤ç§ä¸»è¦çš„å€¼ç±»åˆ«:\nå·¦å€¼ (Lvalue) ï¼šè¡¨ç¤ºå¯ä»¥å–åœ°å€çš„å¯¹è±¡ï¼Œå¯ä»¥ç†è§£ä¸ºæ‹¥æœ‰æŒä¹…ç”Ÿå‘½å‘¨æœŸçš„å¯¹è±¡ã€‚å®ƒé€šå¸¸æ˜¯å˜é‡ã€æ•°ç»„å…ƒç´ ã€å¯¹è±¡æˆå‘˜ç­‰ã€‚ å³å€¼ (Rvalue) ï¼šè¡¨ç¤ºä¸´æ—¶å¯¹è±¡ã€éæŒä¹…ç”Ÿå‘½å‘¨æœŸçš„å¯¹è±¡ï¼Œé€šå¸¸æ˜¯è¿”å›å€¼ã€å­—é¢å¸¸é‡ç­‰ã€‚ #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;utility\u0026gt; // std::move class MyClass { public: MyClass() { std::cout \u0026lt;\u0026lt; \u0026#34;Constructor\\n\u0026#34;; } MyClass(const MyClass\u0026amp; other) { std::cout \u0026lt;\u0026lt; \u0026#34;Copy Constructor\\n\u0026#34;; } MyClass(MyClass\u0026amp;\u0026amp; other) noexcept { std::cout \u0026lt;\u0026lt; \u0026#34;Move Constructor\\n\u0026#34;; } MyClass\u0026amp; operator=(const MyClass\u0026amp; other) { std::cout \u0026lt;\u0026lt; \u0026#34;Copy Assignment\\n\u0026#34;; return *this; } MyClass\u0026amp; operator=(MyClass\u0026amp;\u0026amp; other) noexcept { std::cout \u0026lt;\u0026lt; \u0026#34;Move Assignment\\n\u0026#34;; return *this; } }; int main() { MyClass obj1; // Constructor MyClass obj2 = std::move(obj1); // Move Constructor MyClass obj3; obj3 = std::move(obj2); // Move Assignment } A proper greedy RewritePattern æ¥ä¸‹æ¥å†™ä¸€ä¸ªç”¨é‡å†™æ¨¡å¼å®šä¹‰çš„ MulToAddPassï¼Œå®ƒä¼šå°† y=C*x å½¢å¼çš„ä¹˜æ³•è½¬æ¢ä¸º y=C/2*x+C/2*x å½¢å¼çš„åŠ æ³•å½“ C æ˜¯å¶æ•°ã€‚å¦åˆ™è½¬æ¢æˆ y=1+(C-1)/2*x+(C-1)/2*x å½¢å¼çš„åŠ æ³•ã€‚\nPowerOfTwoExpand è·å–äº† rhs çš„å®šä¹‰æ“ä½œï¼ˆrhs.getDefiningOp\u0026lt;arith::ConstantIntOp\u0026gt;()ï¼‰ï¼Œä»¥ç¡®ä¿å³æ“ä½œæ•°æ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚ å¦‚æœå³æ“ä½œæ•°çš„å€¼æ˜¯ 2 çš„å¹‚ï¼Œå³ (value \u0026amp; (value - 1)) == 0ï¼Œåˆ™è¿›è¡Œä¼˜åŒ–ã€‚ å°† value é™¤ä»¥ 2 ç„¶åç”Ÿæˆæ–°çš„å¸¸æ•° newConstantã€‚ è®¡ç®—æ–°çš„ä¹˜æ³• lhs * newConstantï¼Œå¹¶å°†å…¶åŠ å€ï¼ˆé€šè¿‡ AddIOp æ¥å®ç° lhs * valueï¼‰ã€‚ æœ€ç»ˆç”¨æ–°çš„åŠ æ³•æ›¿ä»£åŸæ¥çš„ä¹˜æ³•ã€‚ struct PowerOfTwoExpand : public OpRewritePattern\u0026lt;MulIOp\u0026gt; { PowerOfTwoExpand(MLIRContext* context) : OpRewritePattern\u0026lt;MulIOp\u0026gt;(context, 2) { } LogicalResult matchAndRewrite(MulIOp op, PatternRewriter\u0026amp; rewriter) const override { // Value represents an instance of an SSA value in the MLIR system Value lhs = op-\u0026gt;getOperand(0); Value rhs = op-\u0026gt;getOperand(1); auto rhsDefiningOp = rhs.getDefiningOp\u0026lt;arith::ConstantIntOp\u0026gt;(); if (!rhsDefiningOp) { return failure(); } int64_t value = rhsDefiningOp.value(); bool is_power_of_two = (value \u0026amp; (value - 1)) == 0; if (!is_power_of_two) { return failure(); } auto newConstant = rewriter.create\u0026lt;ConstantOp\u0026gt;( rhsDefiningOp-\u0026gt;getLoc(), rewriter.getIntegerAttr(rhs.getType(), value / 2)); auto newMul = rewriter.create\u0026lt;MulIOp\u0026gt;(op-\u0026gt;getLoc(), lhs, newConstant); auto newAdd = rewriter.create\u0026lt;AddIOp\u0026gt;(op-\u0026gt;getLoc(), newMul, newMul); rewriter.replaceOp(op, newAdd); rewriter.eraseOp(rhsDefiningOp); return success(); } }; PeelFromMul è¿™ä¸ª Pass çš„ç›®æ ‡æ˜¯å°†ä¸€ä¸ªå¸¸æ•°ä¹˜æ³•è½¬åŒ–ä¸ºåŠ æ³•å½¢å¼ï¼Œé€‚ç”¨äºå¸¸æ•°å€¼ rhs ä¸ä¸º 2 çš„å¹‚æ—¶ã€‚\nå°† rhs å‡å» 1ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªæ–°çš„å¸¸æ•° newConstantï¼ˆå³ value - 1ï¼‰ã€‚ ç”¨ lhs * newConstant è¿›è¡Œè®¡ç®—ï¼Œå¹¶å°†ç»“æœåŠ ä¸Š lhsï¼ˆå³ lhs * value è½¬åŒ–ä¸º (lhs * (value - 1)) + lhsï¼‰ã€‚ æœ€ç»ˆç”¨æ–°çš„åŠ æ³•æ›¿ä»£åŸæ¥çš„ä¹˜æ³•ã€‚ struct PeelFromMul : public OpRewritePattern\u0026lt;MulIOp\u0026gt; { PeelFromMul(MLIRContext* context) : OpRewritePattern\u0026lt;MulIOp\u0026gt;(context, 1) { } LogicalResult matchAndRewrtite(MulIOp op, PatternRewriter\u0026amp; rewriter) const { Value lhs = op-\u0026gt;getOperand(0); Value rhs = op-\u0026gt;getOperand(1); auto rhsDefiningOp = rhs.getDefiningOp\u0026lt;arith::ConstantIntOp\u0026gt;(); if (!rhsDefiningOp) { return failure(); } int64_t value = rhsDefiningOp.value(); // Beacause PowerOfTwoExpand has higher benefit, // value must not be power of 2 auto newConstant = rewriter.create\u0026lt;ConstantOp\u0026gt;( rhsDefiningOp-\u0026gt;getLoc(), rewriter.getIntegerAttr(rhs.getType(), value - 1)); auto newMul = rewriter.create\u0026lt;MulIOp\u0026gt;(op.getLoc(), lhs, newConstant); auto newAdd = rewriter.create\u0026lt;AddIOp\u0026gt;(op.getLoc(), newMul, lhs); rewriter.replaceOp(op, newAdd); rewriter.eraseOp(rhsDefiningOp); return success(); } }; Add the Pass ä¹‹åæˆ‘ä»¬åŒæ ·åœ¨ runOnOperation æ–¹æ³•ä¸­æ³¨å†Œ PowerOfTwoExpand å’Œ PeelFromMul ä¸¤ä¸ªæ¨¡å¼ã€‚\nvoid MulToAddPass::runOnOperation() { mlir::RewritePatternSet patterns(\u0026amp;getContext()); patterns.add\u0026lt;PowerOfTwoExpand\u0026gt;(\u0026amp;getContext()); patterns.add\u0026lt;PeelFromMul\u0026gt;(\u0026amp;getContext()); (void) applyPatternsAndFoldGreedily(getOperation(), std::move(patterns)); } Lit, FileCheck LLVM å’Œ MLIR ä½¿ç”¨çš„æ˜¯åŒä¸€ä¸ªæµ‹è¯•æ¡†æ¶ï¼Œåˆ†ä¸ºä¸¤ä¸ªæµ‹è¯•æ­¥éª¤ã€‚\nlit (LLVM Integratesd Tester) è´Ÿè´£å‘ç°ã€ç»„ç»‡å’Œè¿è¡Œæµ‹è¯•ï¼Œå¹¶æŠ¥å‘Šæµ‹è¯•ç»“æœã€‚æµ‹è¯•æ–‡ä»¶ä¸­é€šå¸¸åŒ…å« RUN: æŒ‡ä»¤ï¼Œå‘Šè¯‰ lit å¦‚ä½•è¿è¡Œæµ‹è¯•ã€‚ FileCheck é€šè¿‡æ¨¡å¼åŒ¹é…çš„æ–¹å¼ï¼ŒéªŒè¯è¾“å‡ºæ˜¯å¦åŒ…å«ç‰¹å®šçš„å­—ç¬¦ä¸²æˆ–ç»“æ„ã€‚ # lit.cfg.py # CMD: llvm-lit -v path/to/test_files import os from os import path as osp from lit.formats import ShTest config.name = \u0026#34;MLIR-LEARN\u0026#34; config.test_format = ShTest() config.suffixes = [\u0026#34;.mlir\u0026#34;] current_path = os.getcwd() tool_path = \u0026#34;path/to/build/opt_executable\u0026#34; config.environment[\u0026#34;PATH\u0026#34;] = ( osp.join(current_path, tool_path) + \u0026#34;:\u0026#34; + os.environ[\u0026#34;PATH\u0026#34;] ) Test the Pass æˆ‘ä»¬åŒæ ·åˆ›å»ºä¸€ä¸ª .mlir æ–‡ä»¶æ¥æµ‹è¯•æˆ‘ä»¬çš„ Pass. æˆ‘ä»¬å¸Œæœ› Pass èƒ½å¤Ÿå°†é€’å½’åœ°å°†ä¹˜æ³•è½¬åŒ–ä¸ºåŠ æ³•å½¢å¼ï¼Œ\n// RUN: /leaning/build/chapter2/tools/02-tutorial-opt %s --mul-to-add \u0026gt; %t // RUN: FileCheck %s \u0026lt; %t func.func @just_power_of_two(%arg0: i32) -\u0026gt; i32 { %0 = arith.constant 8: i32 %1 = arith.muli %arg0, %0: i32 func.return %1: i32 } // CHECK-LABEL: func.func @just_power_of_two( // CHECK-SAME: %[[ARG:.*]]: i32 // CHECK-SAME: ) -\u0026gt; i32 { // CHECK: %[[SUM_0:.*]] = arith.addi %[[ARG]], %[[ARG]] // CHECK: %[[SUM_1:.*]] = arith.addi %[[SUM_0]], %[[SUM_0]] // CHECK: %[[SUM_2:.*]] = arith.addi %[[SUM_1]], %[[SUM_1]] // CHECK: return %[[SUM_2]] : i32 // CHECK: } func.func @power_of_two_plus_one(%arg: i32) -\u0026gt; i32 { %0 = arith.constant 9 : i32 %1 = arith.muli %arg, %0 : i32 func.return %1 : i32 } // CHECK-LABEL: func.func @power_of_two_plus_one( // CHECK-SAME: %[[ARG:.*]]: i32 // CHECK-SAME: ) -\u0026gt; i32 { // CHECK: %[[SUM_0:.*]] = arith.addi %[[ARG]], %[[ARG]] // CHECK: %[[SUM_1:.*]] = arith.addi %[[SUM_0]], %[[SUM_0]] // CHECK: %[[SUM_2:.*]] = arith.addi %[[SUM_1]], %[[SUM_1]] // CHECK: %[[SUM_3:.*]] = arith.addi %[[SUM_2]], %[[ARG]] // CHECK: return %[[SUM_3]] : i32 // CHECK: } ç»è¿‡ä¼˜åŒ–åçš„ä»£ç å¦‚ä¸‹ï¼š\nmodule { func.func @just_power_of_two(%arg0: i32) -\u0026gt; i32 { %0 = arith.addi %arg0, %arg0 : i32 %1 = arith.addi %0, %0 : i32 %2 = arith.addi %1, %1 : i32 return %2 : i32 } func.func @power_of_two_plus_one(%arg0: i32) -\u0026gt; i32 { %0 = arith.addi %arg0, %arg0 : i32 %1 = arith.addi %0, %0 : i32 %2 = arith.addi %1, %1 : i32 %3 = arith.addi %2, %arg0 : i32 return %3 : i32 } } Summary ä½¿ç”¨æ¨¡å¼é‡å†™å¼•æ“é€šå¸¸æ¯”ç¼–å†™éå†ASTçš„ä»£ç æ›´å®¹æ˜“ã€‚ä¸éœ€è¦å¤§å‹ case/switch è¯­å¥æ¥å¤„ç† IR ä¸­å¯èƒ½å‡ºç°çš„æ‰€æœ‰å†…å®¹ã€‚å› æ­¤å¯ä»¥å•ç‹¬ç¼–å†™æ¨¡å¼ï¼Œå¹¶ç›¸ä¿¡å¼•æ“ä¼šé€‚å½“åœ°ç»„åˆå®ƒä»¬ã€‚\n","permalink":"http://localhost:1313/blogs/courselearning/mlir/mlir-ch2-writing-our-first-pass/","summary":"Personal MLIR learning notes 2.","title":"MLIR-Ch2 Writing Our First Pass"},{"content":"15 Graph Traversal å›¾æ˜¯ä¸€ç§è¡¨ç¤ºå®ä½“ä¹‹é—´å…³ç³»çš„æ•°æ®ç»“æ„ã€‚æ‰€æ¶‰åŠçš„å®ä½“è¡¨ç¤ºä¸ºé¡¶ç‚¹ï¼Œå…³ç³»è¡¨ç¤ºä¸ºè¾¹ã€‚å›¾çš„éå†æ˜¯æŒ‡ä»ä¸€ä¸ªé¡¶ç‚¹å‡ºå‘ï¼Œä¾æ¬¡è®¿é—®å›¾ä¸­æ‰€æœ‰ä¸ä¹‹ç›¸é‚»çš„é¡¶ç‚¹ï¼Œç›´åˆ°æ‰€æœ‰é¡¶ç‚¹éƒ½è¢«è®¿é—®è¿‡ä¸ºæ­¢ã€‚\n15.1 Background ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªæœ‰å‘çš„ç®€å•å›¾çš„ä¾‹å­ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªé¡¶ç‚¹åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•°å­—ï¼Œç§°ä¸ºé¡¶ç‚¹ç¼–å· (vertex id).\nA Simple Graph Example with 9 Vertices and 15 Directional Edges\nå›¾çš„ç›´è§‚è¡¨ç¤ºæ˜¯é‚»æ¥çŸ©é˜µ (adjacency matrix). å¦‚æœå­˜åœ¨ä¸€æ¡ä»æºé¡¶ç‚¹ i åˆ°ç›®çš„é¡¶ç‚¹ j çš„è¾¹ï¼Œåˆ™é‚»æ¥çŸ©é˜µå…ƒç´  a[i][j] çš„å€¼ä¸º 1ï¼Œå¦åˆ™ä¸º 0. ä¸‹å›¾å±•ç¤ºäº†å¯¹åº”çš„é‚»æ¥çŸ©é˜µã€‚\nAdjacent Matrix Representation of the Example Graph\nç¨€ç–è¿æ¥çš„å›¾å¯ä»¥ç”¨ç¨€ç–çŸ©é˜µè¡¨ç¤ºï¼Œä¸‹å›¾å±•ç¤ºäº†ç”¨ä¸‰ç§ä¸åŒå­˜å‚¨æ ¼å¼çš„é‚»æ¥çŸ©é˜µ: CSR, CSC å’Œ COO. æˆ‘ä»¬å°†è¡Œä¸‹æ ‡å’ŒæŒ‡é’ˆæ•°ç»„åˆ†åˆ«ç§°ä¸º src å’Œ srcPtrs æ•°ç»„ï¼Œåˆ—ä¸‹æ ‡å’ŒæŒ‡é’ˆæ•°ç»„åˆ†åˆ«ç§°ä¸º dst å’Œ dstPtrs æ•°ç»„ã€‚åœ¨å›¾çš„ CSR è¡¨ç¤ºä¸­ï¼Œæ¯ä¸ªæºé¡¶ç‚¹æŒ‡é’ˆ(srcPtrs) ç»™å‡ºé¡¶ç‚¹å‡ºè¾¹çš„èµ·å§‹ä½ç½®ã€‚åœ¨å›¾çš„ CSC è¡¨ç¤ºä¸­ï¼Œæ¯ä¸ªç›®çš„é¡¶ç‚¹æŒ‡é’ˆ (dstPtrs) ç»™å‡ºé¡¶ç‚¹å…¥è¾¹çš„èµ·å§‹ä½ç½®ã€‚åœ¨å›¾çš„ COO è¡¨ç¤ºä¸­ï¼Œsrc å’Œ dst æ•°ç»„åˆ†åˆ«å­˜å‚¨æºé¡¶ç‚¹å’Œç›®çš„é¡¶ç‚¹çš„ç¼–å·ã€‚\nThree Sparse Matrix Representations of the Adjacency Matrix\n15.2 Breadth-first Search (BFS) BFS é€šå¸¸ç”¨äºæ‰¾åˆ°ä»å›¾çš„ä¸€ä¸ªé¡¶ç‚¹åˆ°å¦ä¸€ä¸ªé¡¶ç‚¹æ‰€éœ€éå†çš„æœ€çŸ­è¾¹æ•°ã€‚ä¸€ç§æ–¹æ³•æ˜¯ï¼Œç»™å®šä¸€ä¸ªè¢«ç§°ä¸ºæ ¹çš„é¡¶ç‚¹ï¼Œç”¨ä»æ ¹åˆ°æŸä¸ªé¡¶ç‚¹æ‰€éœ€è¦éå†çš„æœ€å°è¾¹æ•°æ¥æ ‡è®°æ¯ä¸ªé¡¶ç‚¹ã€‚\nä¸‹å›¾(A)å±•ç¤ºç¤ºäº†ä»¥é¡¶ç‚¹ 0 ä¸ºæ ¹çš„ BFS ç»“æœã€‚å¦‚æœå¦ä¸€ä¸ªé¡¶ç‚¹ä½œä¸ºæ ¹ï¼ŒBFS çš„ç»“æœå°†å®Œå…¨ä¸åŒã€‚ä¸‹å›¾(B)æ˜¯ä¸ºä»¥é¡¶ç‚¹ 2 ä¸ºæ ¹çš„ BFS çš„ç»“æœã€‚å¯ä»¥å°† BFS çš„æ ‡è®°æ“ä½œçœ‹ä½œæ˜¯æ„å»ºä¸€ä¸ªæœç´¢æ ¹èŠ‚ç‚¹çš„ BFS æ ‘ã€‚æ ‘ç”±æ‰€æœ‰æ ‡è®°çš„é¡¶ç‚¹å’Œåœ¨æœç´¢è¿‡ç¨‹ä¸­ä»ä¸€ä¸ªé¡¶ç‚¹åˆ°ä¸‹ä¸€ä¸ªé¡¶ç‚¹çš„éå†çš„è¾¹ç»„æˆã€‚\n(A and B) Two Examples of BFS Results for Two Different Root Vertices\nä¸‹å›¾å±•ç¤ºäº† BFS åœ¨è®¡ç®—æœºè¾…åŠ©è®¾è®¡ (Computer-Aided Design, CAD) ä¸­çš„ä¸€ä¸ªé‡è¦åº”ç”¨ã€‚è¿·å®«è·¯ç”± (maze routing) å°†èŠ¯ç‰‡è¡¨ç¤ºä¸ºå›¾ã€‚è·¯ç”±å—æ˜¯é¡¶ç‚¹ã€‚ä»é¡¶ç‚¹ i åˆ°é¡¶ç‚¹ j çš„è¾¹è¡¨ç¤ºå¯ä»¥å°†ä¸€æ¡çº¿ä»å— i å»¶ä¼¸åˆ°å— j.\nMaze Routing in Integrated Circuits\n15.3 Vertex-centric Parallelization of BFS ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å¹¶è¡Œå®ç°å°†çº¿ç¨‹åˆ†é…ç»™é¡¶ç‚¹ï¼Œå¹¶è®©æ¯ä¸ªçº¿ç¨‹å¯¹å…¶é¡¶ç‚¹æ‰§è¡Œæ“ä½œï¼Œè¿™é€šå¸¸æ¶‰åŠè¿­ä»£è¯¥é¡¶ç‚¹çš„é‚»å±…ã€‚å½“å¤„ç†ä¸åŒå±‚çº§çš„è¿­ä»£æ—¶ï¼Œå¹¶è¡Œå®ç°éµå¾ªç›¸åŒçš„ç­–ç•¥ã€‚ä¸ºæ¯ä¸€å±‚è°ƒç”¨ä¸€ä¸ªå•ç‹¬çš„å†…æ ¸çš„åŸå› æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦ç­‰å¾…å‰ä¸€å±‚çš„æ‰€æœ‰é¡¶ç‚¹éƒ½è¢«æ ‡è®°ï¼Œç„¶åå†ç»§ç»­æ ‡è®°ä¸‹ä¸€å±‚çš„é¡¶ç‚¹ã€‚ä¸‹é¢å®ç°äº†ä¸€ä¸ª BFS å†…æ ¸ï¼Œæ ¹æ®å‰ä¸€ä¸ªå±‚çº§çš„é¡¶ç‚¹æ ‡ç­¾æ¥æ ‡è®°å±äºè¯¥å±‚çº§çš„æ‰€æœ‰é¡¶ç‚¹ã€‚è¯¥å†…æ ¸å°†æ¯ä¸ªçº¿ç¨‹åˆ†é…ç»™ä¸€ä¸ªé¡¶ç‚¹ï¼Œæ£€æŸ¥å…¶é¡¶ç‚¹æ˜¯å¦å±äºå‰ä¸€å±‚ã€‚å¦‚æœæ˜¯ï¼Œçº¿ç¨‹å°†éå†å‡ºè¾¹ï¼Œå°†æ‰€æœ‰æœªè®¿é—®çš„é‚»å±…æ ‡è®°ä¸ºå±äºå½“å‰çº§åˆ«ã€‚è¿™ç§ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å®ç°é€šå¸¸è¢«ç§°ä¸ºè‡ªé¡¶å‘ä¸‹æˆ– push å®ç°ï¼Œå› ä¸ºå…¶éœ€è¦è®¿é—®ç»™å®šæºé¡¶ç‚¹çš„å‡ºè¾¹ã€‚å¤šä¸ªçº¿ç¨‹å¯ä»¥å°†è¯¥æ ‡å¿—èµ‹å€¼ä¸º 1ï¼Œä»£ç ä»ç„¶å¯ä»¥æ­£ç¡®æ‰§è¡Œã€‚è¿™ä¸ªæ€§è´¨ç§°ä¸ºå¹‚ç­‰æ€§ (idempotence).\nstruct CSRGRAPH { int numVertices; int* scrPtrs; // Strating outgoing edge index of each vertex int* dstList; // Destination vertex index of each edge }; __global__ void bfs_kernel_csr(CSRGRAPH graph, unsigned int* level, unsigned int* visited, unsigned int currLevel) { unsigned vertexId = blockIdx.x * blockDim.x + threadIdx.x; if (vertexId \u0026lt; graph.numVertices) { if (level[vertexId] == currLevel - 1) { for (int i = graph.scrPtrs[vertexId]; i \u0026lt; graph.scrPtrs[vertexId + 1]; i++) { unsigned int neighbor = graph.dstList[i]; if (level[neighbor] == 0xFFFFFFFF) { // unvisited neighbor level[neighbor] = currLevel; visited[neighbor] = 1; *visited = 1; // flag to indicate whether reached the end of the graph } } } } } ä¸‹å›¾å±•ç¤ºäº†è¯¥å†…æ ¸å¦‚ä½•æ‰§è¡Œä»ç¬¬ 1 å±‚ (currLevel-1) åˆ°ç¬¬ 2 å±‚ (currLevel) çš„éå†ã€‚\nExample of a Vertex-centric Push BFS Traversal from Level 1 to Level 2\nç¬¬äºŒä¸ªä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å¹¶è¡Œå®ç°å°†æ¯ä¸ªçº¿ç¨‹åˆ†é…ç»™ä¸€ä¸ªé¡¶ç‚¹ï¼Œè¿­ä»£é¡¶ç‚¹çš„å…¥è¾¹ã€‚æ¯ä¸ªçº¿ç¨‹é¦–å…ˆæ£€æŸ¥å…¶é¡¶ç‚¹æ˜¯å¦å·²è¢«è®¿é—®ã€‚å¦‚æœæ²¡è¢«è®¿é—®ï¼Œçº¿ç¨‹å°†éå†å…¥è¾¹ï¼Œå¦‚æœçº¿ç¨‹æ‰¾åˆ°ä¸€ä¸ªå±äºå‰ä¸€å±‚çš„é‚»å±…ï¼Œçº¿ç¨‹å°†æŠŠå®ƒçš„é¡¶ç‚¹æ ‡è®°ä¸ºå±äºå½“å‰å±‚ã€‚è¿™ç§ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å®ç°é€šå¸¸è¢«ç§°ä¸ºè‡ªåº•å‘ä¸Šæˆ– pull å®ç°ã€‚å®ç°è¦æ±‚èƒ½è®¿é—®ç»™å®šç›®æ ‡é¡¶ç‚¹çš„å…¥è¾¹ï¼Œå› æ­¤è¦é‡‡ç”¨ CSC è¡¨ç¤ºã€‚ ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„ pull å®ç°çš„å†…æ ¸ä»£ç å¦‚ä¸‹ï¼Œå¯¹äºä¸€ä¸ªçº¿ç¨‹æ¥è¯´ï¼Œè¦ç¡®å®šå®ƒçš„é¡¶ç‚¹å¤„äºå½“å‰å±‚ï¼Œåªéœ€è¦è¯¥é¡¶ç‚¹æœ‰ä¸€ä¸ªé‚»å±…så±äºå‰ä¸€å±‚ä¸­å°±è¶³å¤Ÿäº†ã€‚\nstruct CSCGRAPH { int numVertices; int* dstPtrs; // Starting incoming edge index of each vertex int* scrList; // Source vertex index of each edge }; __global__ void bfs_kernel_csc(CSCGRAPH graph, unsigned int* level, unsigned int* visited, unsigned int currLevel) { unsigned vertexId = blockIdx.x * blockDim.x + threadIdx.x; if (vertexId \u0026lt; graph.numVertices) { if (level[vertexId] == 0xFFFFFFF) { // loop through its incoming edges if not visited for (int i = graph.dstPtrs[vertexId]; i \u0026lt; graph.dstPtrs[vertexId + 1]; i++) { unsigned int neighbor = graph.scrList[i]; if (level[neighbor] == currLevel - 1) { level[vertexId] = currLevel; *visited = 1; // flag to indicate whether reached the end of the graph break; // Only need 1 neighbor in previous level to identify the vetex is currLevel } } } } } ä¸‹å›¾å±•ç¤ºäº†è¿™ä¸ªå†…æ ¸å¦‚ä½•æ‰§è¡Œä»ç¬¬ 1 å±‚åˆ°ç¬¬ 2 å±‚çš„éå†ã€‚\nExample of a Vertex-centric Pull (bottom-up) Traversal from Level 1 to Level 2\nåœ¨æ¯”è¾ƒæ¨å’Œæ‹‰ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å¹¶è¡Œå®ç°æ—¶ï¼Œéœ€è¦è€ƒè™‘ä¸¤ä¸ªå¯¹æ€§èƒ½æœ‰é‡è¦å½±å“çš„å…³é”®å·®å¼‚ã€‚\nåœ¨ push å®ç°ä¸­ï¼Œçº¿ç¨‹åœ¨å…¶é¡¶ç‚¹çš„å¾ªç¯éå†æ‰€æœ‰é‚»å±…ï¼›è€Œåœ¨ pull å®ç°ä¸­ï¼Œçº¿ç¨‹å¯èƒ½ä¼šæå‰è·³å‡ºå¾ªç¯ã€‚ åœ¨ push å®ç°ä¸­ï¼Œåªæœ‰è¢«æ ‡è®°ä¸ºå‰ä¸€å±‚çš„é¡¶ç‚¹çš„çº¿ç¨‹åœ¨éå†å…¶é‚»å±…åˆ—è¡¨ï¼›è€Œåœ¨ pull å®ç°ä¸­ï¼Œä»»ä½•è¢«æ ‡è®°ä¸ºæœªè®¿é—®é¡¶ç‚¹çš„çº¿ç¨‹ä¼šéå†å…¶é‚»å±…åˆ—è¡¨ã€‚ åŸºäºä¸¤ç§å®ç°çš„å·®å¼‚ï¼Œå¸¸è§çš„ä¼˜åŒ–æ–¹æ³•æ˜¯å¯¹ä½å±‚çº§ä½¿ç”¨ push å®ç°ï¼Œç„¶åå¯¹è¾ƒé«˜å±‚çº§ä½¿ç”¨ pull å®ç°ã€‚è¿™ç§æ–¹æ³•é€šå¸¸è¢«ç§°ä¸ºæ–¹å‘ä¼˜åŒ– (directional optimization) å®ç°ã€‚é€‰æ‹©ä½•æ—¶åˆ‡æ¢é€šå¸¸å–å†³äºå›¾çš„ç±»å‹ã€‚ä½åº¦å›¾é€šå¸¸æœ‰å¾ˆå¤šå±‚ï¼›é«˜åº¦å›¾ä¸­ï¼Œä»ä»»ä½•é¡¶ç‚¹åˆ°ä»»ä½•å…¶ä»–é¡¶ç‚¹åªéœ€è¦å¾ˆå°‘çš„å±‚ã€‚å› æ­¤å¯¹äºé«˜åº¦å›¾æ¥è¯´ä» push å®ç°åˆ‡æ¢åˆ° pull å®ç°é€šå¸¸æ¯”ä½åº¦å›¾è¦æ—©å¾—å¤šã€‚ å¦‚æœè¦ä½¿ç”¨æ–¹å‘ä¼˜åŒ–çš„å®ç°ï¼Œåˆ™å›¾çš„ CSR å’Œ CSC è¡¨ç¤ºéƒ½éœ€è¦å‚¨å­˜ã€‚ä½†å¯¹äºæ— å‘å›¾æ¥è¯´ï¼Œå…¶é‚»æ¥çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œå› æ­¤ CSR å’Œ CSC è¡¨ç¤ºæ˜¯ç›¸åŒçš„çš„ï¼Œåªéœ€è¦å­˜å‚¨å…¶ä¸­ä¸€ä¸ªï¼Œå°±å¯ä»¥è¢«ä¸¤ä¸ªå®ç°ä½¿ç”¨ã€‚ 15.4 Edge-centric Parallelization of BFS åœ¨è¿™ä¸ªå®ç°ä¸­ï¼Œæ¯ä¸ªçº¿ç¨‹è¢«åˆ†é…åˆ°ä¸€æ¡è¾¹ã€‚å®ƒæ£€æŸ¥è¾¹çš„æºé¡¶ç‚¹æ˜¯å¦å±äºå‰ä¸€å±‚ä»¥åŠè¾¹çš„ç›®æ ‡é¡¶ç‚¹æ˜¯å¦æœªè¢«è®¿é—®ã€‚ ä»¥è¾¹ä¸ºä¸­å¿ƒçš„å¹¶è¡Œå®ç°çš„å†…æ ¸ä»£ç å¦‚ä¸‹ã€‚æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ COO src æ•°ç»„æ‰¾åˆ°å…¶è¾¹ç¼˜çš„æºé¡¶ç‚¹ï¼Œå¹¶æ£€æŸ¥é¡¶ç‚¹æ˜¯å¦å±äºå‰ä¸€çº§ã€‚é€šè¿‡æ­¤æ£€æŸ¥çš„çº¿ç¨‹å°†ä½¿ç”¨ COO dst æ•°ç»„ç¡®å®šè¾¹çš„ç›®çš„é¡¶ç‚¹ï¼Œå¹¶æ£€æŸ¥å…¶æ˜¯å¦æœªè¢«è®¿é—®è¿‡ã€‚\nstruct COOGRAPH { int numVertices; int numEdges; int* srcList; // Source vertex index of each edge int* dstList; // Destination vertex index of each edge }; __global__ void bfs_kernel_coo(COOGRAPH graph, unsigned int* level, unsigned int* visited, unsigned int currLevel) { unsigned edgeId = blockIdx.x * blockDim.x + threadIdx.x; if (edgeId \u0026lt; graph.numEdges) { unsigned int src = graph.srcList[edgeId]; if (level[src] == currLevel - 1) { unsigned int neighbor = graph.dstList[edgeId]; if (level[neighbor] == 0xFFFFFFFF) { // unvisited neighbor level[neighbor] = currLevel; visited[neighbor] = 1; *visited = 1; // flag to indicate whether reached the end of the graph } } } } ä¸‹å›¾å±•ç¤ºäº†è¯¥å†…æ ¸å¦‚ä½•æ‰§è¡Œä»ä»ç¬¬ 1 å±‚åˆ°ç¬¬ 2 å±‚çš„éå†ã€‚\nExample of an Edge-centric Traversal from Level 1 to Level 2\nä¸ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å¹¶è¡Œå®ç°ç›¸æ¯”ï¼Œä»¥è¾¹ä¸ºä¸­å¿ƒçš„å¹¶è¡Œå®ç°çš„ä¼˜ç‚¹å¦‚ä¸‹\næœ‰æ›´å¤šçš„å¹¶è¡Œæ€§ã€‚åœ¨ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å®ç°ä¸­ï¼Œå¦‚æœé¡¶ç‚¹çš„æ•°é‡å¾ˆå°‘ï¼Œå¯èƒ½ä¸ä¼šå¯åŠ¨è¶³å¤Ÿçš„çº¿ç¨‹æ¥å®Œå…¨å ç”¨è®¾å¤‡ã€‚å› ä¸ºä¸€ä¸ªå›¾é€šå¸¸æœ‰æ¯”é¡¶ç‚¹æ›´å¤šçš„è¾¹ï¼Œä»¥è¾¹ä¸ºä¸­å¿ƒçš„å®ç°å¯ä»¥å¯åŠ¨æ›´å¤šçš„çº¿ç¨‹ã€‚ å…·æœ‰è¾ƒå°çš„è´Ÿè½½ä¸å¹³è¡¡å’Œæ§åˆ¶å‘æ•£ã€‚åœ¨ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å®ç°ä¸­ï¼Œæ¯ä¸ªçº¿ç¨‹è¿­ä»£ä¸åŒæ•°é‡çš„è¾¹ã€‚ç›¸åï¼Œåœ¨ä»¥è¾¹ä¸ºä¸­å¿ƒçš„å®ç°ä¸­ï¼Œæ¯ä¸ªçº¿ç¨‹åªéå†ä¸€ä¸ªè¾¹ã€‚ ä»¥è¾¹ä¸ºä¸­å¿ƒçš„å®ç°çš„ç¼ºç‚¹å¦‚ä¸‹ éœ€è¦æ£€æŸ¥å›¾ä¸­çš„æ¯æ¡è¾¹ã€‚ç›¸åï¼Œä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å®ç°ä¸­ï¼Œå¦‚æœç¡®å®šé¡¶ç‚¹ä¸å½“å‰å±‚çº§æ— å…³ï¼Œåˆ™ä¼šè·³è¿‡æ•´ä¸ªè¾¹åˆ—è¡¨ã€‚ ä½¿ç”¨ COO æ ¼å¼å­˜å‚¨å›¾ï¼Œä¸ä»¥é¡¶ç‚¹ä¸ºä¸­å¿ƒçš„å®ç°ä½¿ç”¨çš„ CSR å’Œ CSC ç›¸æ¯”ï¼Œå®ƒéœ€è¦æ›´å¤šçš„å­˜å‚¨ç©ºé—´æ¥å­˜å‚¨è¾¹ã€‚ 15.5 Improving efficiency with frontiers åœ¨å‰ä¸¤èŠ‚ä¸­çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä¼šæ£€æŸ¥æ¯ä¸ªé¡¶ç‚¹æˆ–æ¯æ¡è¾¹æ˜¯å¦å±å’Œå½“å‰å±‚æœ‰å…³ã€‚è¿™ç§ç­–ç•¥çš„ä¼˜ç‚¹æ˜¯å†…æ ¸æ˜¯é«˜åº¦å¹¶è¡Œçš„ï¼Œå¹¶ä¸”ä¸éœ€è¦è·¨çº¿ç¨‹è¿›è¡Œä»»ä½•åŒæ­¥ã€‚ç¼ºç‚¹æ˜¯å¯åŠ¨äº†è®¸å¤šä¸å¿…è¦çš„çº¿ç¨‹ï¼Œå¹¶æ‰§è¡Œäº†å¤§é‡æ— ç”¨çš„å·¥ä½œã€‚æˆ‘ä»¬å¯ä»¥è®©å¤„ç†å‰ä¸€å±‚é¡¶ç‚¹çš„çº¿ç¨‹å°†å®ƒä»¬è®¿é—®çš„é¡¶ç‚¹ä½œä¸º frontier. å› æ­¤ï¼Œå¯¹äºå½“å‰å±‚çº§ï¼Œåªéœ€è¦ä¸ºè¯¥ frontier ä¸­çš„é¡¶ç‚¹å¯åŠ¨çº¿ç¨‹ã€‚\nExample of a Vertex-centric Push (top-down) BFS Traversal from Level 1 to Level 2 with Frontiers\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹ã€‚é¦–å…ˆä¸º frontier çš„æ¯ä¸ªå…ƒç´ åˆ†é…ä¸€ä¸ªçº¿ç¨‹ï¼Œä½¿ç”¨ CSR srcPtrs æ•°ç»„æ¥å®šä½é¡¶ç‚¹çš„å‡ºè¾¹å¹¶è¿›è¡Œè¿­ä»£ã€‚å¯¹äºæ¯ä¸ªå‡ºè¾¹ï¼Œçº¿ç¨‹ä½¿ç”¨ CSR dst æ•°ç»„ç¡®å®šå…¶ç›®çš„é¡¶ç‚¹ï¼Œè‹¥æœªè¢«è®¿é—®è¿‡ï¼Œå¹¶å°†å…¶æ ‡è®°ä¸ºå±äºå½“å‰å±‚çº§ã€‚ä¸ºäº†é¿å…å¤šä¸ªçº¿ç¨‹å°†é‚»å±…è§†ä¸ºæœªè®¿é—®ï¼Œåº”è¯¥ä»¥åŸå­æ–¹å¼æ‰§è¡Œé‚»å±…æ ‡ç­¾çš„æ£€æŸ¥å’Œæ›´æ–°ã€‚atomicCAS å†…ç½®å‡½æ•°æä¾› compare-and-swap çš„åŸå­æ“ä½œã€‚å¦‚æœæ¯”è¾ƒæˆåŠŸ,ä¸å…¶ä»–åŸå­æ“ä½œä¸€æ ·ï¼ŒatomicCAS è¿”å›å­˜å‚¨çš„æ—§å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¯”è¾ƒè¿”å›å€¼ä¸è¢«æ¯”è¾ƒçš„å€¼æ¥æ£€æŸ¥è¯¥é¡¶ç‚¹æ˜¯å¦è¢«è®¿é—®è¿‡ã€‚\n__global__ void frontier_bfs_kernel(CSRGRAPH graph, unsigned int* level, unsigned int* prevFroniter, unsigned int* currFroniter, unsigned int numPrevFroniter, unsigned int* numCurrFroniter, unsigned int* currLevel) { // Each thread processes a node in prevFroniter. unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; numPrevFroniter) { unsigned int vertexId = prevFroniter[i]; // All its neighbouring nodes are traversed. for (unsigned int edge = graph.scrPtrs[vertexId]; edge \u0026lt; graph.scrPtrs[vertexId + 1]; edge++) { unsigned int neighbor = graph.dstList[edge]; if (atomicCAS(level + neighbor, 0xFFFFFFFF, currLevel) == 0xFFFFFFFF) { // check if neighbor is unvisited unsigned int currFroniterIndex = atomicAdd(numCurrFroniter, 1); currFroniter[currFroniterIndex] = neighbor; } } } } è¿™ç§åŸºäº frontier çš„æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºï¼Œå®ƒé€šè¿‡åªå¯åŠ¨å¤„ç†ç›¸å…³é¡¶ç‚¹çš„çº¿ç¨‹å‡å°‘äº†å†—ä½™å·¥ä½œã€‚ç¼ºç‚¹æ˜¯é•¿å»¶è¿ŸåŸå­æ“ä½œçš„å¼€é”€ï¼Œç‰¹åˆ«æ˜¯å½“è¿™äº›æ“ä½œç«äº‰è®¿é—®ç›¸åŒçš„åœ°å€æ—¶ã€‚å¯¹äº atomicAdd æ“ä½œäº‰ç”¨ä¼šå¾ˆé«˜ï¼Œå› ä¸ºæ‰€æœ‰çº¿ç¨‹éƒ½å¢åŠ åŒä¸€ä¸ªè®¡æ•°å™¨ã€‚\n15.6 Reducing Contention with Privatization ç§æœ‰åŒ–å¯ä»¥åº”ç”¨äºå¯¹ numCurrFrontier çš„å¢åŠ ï¼Œä»¥å‡å°‘æ’å…¥ frontier æ—¶çš„äº‰ç”¨ã€‚æˆ‘ä»¬å¯ä»¥è®©æ¯ä¸ªçº¿ç¨‹å—åœ¨æ•´ä¸ªè®¡ç®—è¿‡ç¨‹ä¸­ç»´æŠ¤è‡ªå·±çš„æœ¬åœ° frontierï¼Œå¹¶åœ¨å®Œæˆåæ›´æ–°å…¨å±€ frontier. æœ¬åœ° frontier åŠå…¶è®¡æ•°å™¨å¯ä»¥å­˜å‚¨åœ¨å…±äº«å†…å­˜ä¸­ï¼Œä»è€Œæ”¯æŒå¯¹è®¡æ•°å™¨å’Œå­˜å‚¨åˆ°æœ¬åœ°è¾¹ç•Œçš„ä½å»¶è¿ŸåŸå­æ“ä½œã€‚æ­¤å¤–ï¼Œå½“å°†å…±äº«å†…å­˜ä¸­çš„ frontier å­˜å‚¨åˆ°å…¨å±€å†…å­˜ä¸­çš„å…¬å…± frontier æ—¶ï¼Œè®¿é—®å¯ä»¥åˆå¹¶ã€‚\nä¸‹å›¾è¯´æ˜äº† frontier ç§æœ‰åŒ–çš„æ‰§è¡Œæƒ…å†µã€‚\nPrivatization of Frontiers Example\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹ã€‚æ³¨æ„åˆ°å…¬å…± frontiner çš„ç´¢å¼• currFrontierIdx æ˜¯ç”¨ currFrontierIdx_s è¡¨ç¤ºçš„ï¼Œè€Œ currFrontierIdx_s æ˜¯ç”¨ threadIdx.x è¡¨ç¤ºçš„ã€‚å› æ­¤ï¼Œç›¸é‚»çº¿ç¨‹å­˜å‚¨åˆ°è¿ç»­çš„å…¨å±€å†…å­˜ä½ç½®ï¼Œè¿™æ„å‘³ç€å†…å­˜è®¿é—®æ˜¯åˆå¹¶çš„ã€‚\n#define LOCAL_FRONTIER_SIZE 4 __global__ void private_frontier_bfs_kernel(CSRGRAPH graph, unsigned int* level, unsigned int* prevFroniter, unsigned int* currFroniter, unsigned int numPrevFroniter, unsigned int* numCurrFroniter, unsigned int* currLevel) { // Initialize privative frontier __shared__ unsigned int currFrontier_s[LOCAL_FRONTIER_SIZE]; __shared__ unsigned int numCurrFrontier_s; if (threadIdx.x == 0) { numCurrFrontier_s = 0; } __syncthreads(); // Perform BFS on private frontier unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; numPrevFroniter) { unsigned int vertexId = prevFroniter[i]; for (unsigned int edge = graph.scrPtrs[vertexId]; edge \u0026lt; graph.scrPtrs[vertexId + 1]; edge++) { unsigned int neighbor = graph.dstList[edge]; if (atomicCAS(level + neighbor, 0xFFFFFFFF, currLevel) == 0xFFFFFFFF) { // Once a new frontier node is found, unsigned currFroniterIndex = atomicAdd(\u0026amp;numCurrFrontier_s, 1); if (currFroniterIndex \u0026lt; LOCAL_FRONTIER_SIZE) { // Try to add it to the private frontier (currFrontier_s) currFrontier_s[currFroniterIndex] = neighbor; } else { numCurrFrontier_s = LOCAL_FRONTIER_SIZE; // frontier is full, stop adding new elements unsigned int currFrontierIdx = atomicAdd(numCurrFroniter, 1); currFroniter[currFrontierIdx] = neighbor; } } } } // Copy private frontier to global frontier __syncthreads(); __shared__ unsigned int currFrontierStartIdx; // Start index of private frontier in global frontier if (threadIdx.x == 0) { currFrontierStartIdx = atomicAdd(numCurrFroniter, numCurrFrontier_s); } __syncthreads(); // Commit private frontier to global frontier for (unsigned int j = threadIdx.x; j \u0026lt; numCurrFrontier_s; j += blockDim.x) { unsigned int currFroniterIdx = currFrontierStartIdx + j; currFroniter[currFroniterIdx] = currFrontier_s[j]; } } ","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch15/","summary":"Personal notebook 15 of Programming Massively Parallel","title":"PMPP Learning-Chapter 15 Graph traversal"},{"content":"14 Sparse Matrix Computation åœ¨ç¨€ç–çŸ©é˜µä¸­ï¼Œå¤§å¤šæ•°å…ƒç´ æ˜¯é›¶ã€‚å­˜å‚¨å’Œå¤„ç†è¿™äº›é›¶å…ƒç´ åœ¨å†…å­˜å®¹é‡ã€å†…å­˜å¸¦å®½ã€æ—¶é—´å’Œèƒ½é‡æ–¹é¢æ˜¯æµªè´¹çš„ã€‚\n14.1 Background çŸ©é˜µå¸¸ç”¨äºæ±‚è§£ N ä¸ªæœªçŸ¥æ•° N ä¸ªæ–¹ç¨‹çš„çº¿æ€§ç³»ç»Ÿï¼Œå…¶å½¢å¼ä¸º AX+Y = 0ï¼Œå…¶ä¸­Aæ˜¯ä¸€ä¸ª NxN çŸ©é˜µï¼ŒX æ˜¯ä¸€ä¸ª N ç»´çš„æœªçŸ¥æ•°å‘é‡ï¼ŒY æ˜¯ä¸€ä¸ª N ç»´çš„å¸¸æ•°å‘é‡ã€‚æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„çš„è¿­ä»£æ–¹æ³•ä¸­æœ€è€—æ—¶çš„éƒ¨åˆ†æ˜¯å¯¹è®¡ç®— AX+Yï¼Œè¿™æ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µå‘é‡çš„ä¹˜æ³•å’Œç´¯åŠ ã€‚ åˆ é™¤æ‰€æœ‰çš„é›¶å…ƒç´ ä¸ä»…èŠ‚çœäº†å­˜å‚¨ç©ºé—´ï¼Œè€Œä¸”æ¶ˆé™¤äº†ä»å†…å­˜ä¸­è·å–è¿™äº›é›¶å…ƒç´ å¹¶å¯¹å®ƒä»¬æ‰§è¡Œæ— ç”¨çš„ä¹˜æ³•æˆ–åŠ æ³•æ“ä½œçš„å†—ä½™æ­¥éª¤ã€‚ ä»¥ä¸‹æ˜¯ä¸€äº›åœ¨ç¨€ç–çŸ©é˜µå­˜å‚¨æ ¼å¼çš„ç»“æ„ä¸­çš„å…³é”®è€ƒè™‘å› ç´ å¦‚ä¸‹:\nç©ºé—´æ•ˆç‡ (Space efficiency): ä½¿ç”¨å­˜å‚¨æ ¼å¼è¡¨ç¤ºçŸ©é˜µæ‰€éœ€çš„å†…å­˜å®¹é‡ã€‚ çµæ´»æ€§ (Flexibility): é€šè¿‡æ·»åŠ æˆ–åˆ é™¤éé›¶æ¥ä¿®æ”¹çŸ©é˜µçš„å­˜å‚¨æ ¼å¼çš„æ–¹ä¾¿ç¨‹åº¦â€¢ å¯è®¿é—®æ€§ (Accessibility): å­˜å‚¨æ ¼å¼æ˜¯å¦æ˜“äºè®¿é—®æ•°æ®ã€‚ å†…å­˜è®¿é—®æ•ˆç‡ (Memory access efficiency): å­˜å‚¨æ ¼å¼åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¸ºç‰¹å®šè®¡ç®—å®ç°äº†æœ‰æ•ˆçš„å†…å­˜è®¿é—®æ¨¡å¼ (æ­£åˆ™åŒ–çš„ä¸€ä¸ªæ–¹é¢). è´Ÿè½½å¹³è¡¡ (Load balancing): å­˜å‚¨æ ¼å¼åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¸ºç‰¹å®šè®¡ç®—åœ¨ä¸åŒçº¿ç¨‹ä¹‹é—´å¹³è¡¡è´Ÿè½½ (æ­£åˆ™åŒ–çš„å¦ä¸€ä¸ªæ–¹é¢). 14.2 A simple SpMV kernel with the COO format å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ COO (COOrdinate) æ ¼å¼æ˜¯ä¸€ç§ç¨€ç–çŸ©é˜µçš„å­˜å‚¨æ ¼å¼ï¼Œå…¶ä¸­çŸ©é˜µå…ƒç´ ä»¥ä¸‰å…ƒç»„çš„å½¢å¼å­˜å‚¨ï¼Œå³ (i, j, a_ij). ã€\nExample of the Coordinate List (COO) Format\nä½¿ç”¨ä»¥ COO æ ¼å¼è¡¨ç¤ºçš„ç¨€ç–çŸ©é˜µå¹¶è¡Œæ‰§è¡Œ SpMV (Sparse Matrix Vector Multiplication) çš„ä¸€ç§æ–¹æ³•æ˜¯ä¸ºçŸ©é˜µä¸­çš„æ¯ä¸ªéé›¶å…ƒç´ åˆ†é…ä¸€ä¸ªçº¿ç¨‹ï¼Œä¸‹å›¾æ˜¯å…¶ç¤ºæ„å›¾ã€‚\nExample of Parallelizing SpMV with the COO Format\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼Œå®ƒåœ¨åˆ—ç´¢å¼•å¯¹åº”çš„ä½ç½®æŸ¥æ‰¾è¾“å…¥å‘é‡å€¼ï¼Œå°†å…¶ä¹˜ä»¥éé›¶å€¼ï¼Œç„¶åå°†ç»“æœç´¯åŠ åˆ°å¯¹åº”çš„è¡Œç´¢å¼•å¤„çš„è¾“å‡ºå€¼ã€‚\nstruct COOMATRIX { int* rowIdx; int* colIdx; float* val; int numNonZeros; }; __global__ void spmv_coo_kernel(COOMATRIX m, float* x, float* y) { // Assign a thread to each nonzero element unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; if (i \u0026lt; m.numNonZeros) { int row = m.rowIdx[i]; int col = m.colIdx[i]; int val = m.val[i]; atomicAdd(\u0026amp;y[row], val * x[col]); // Perform the matrix-vector multiplication } } ä¸‹é¢æ¥åˆ†æ COO æ ¼å¼åœ¨å‡ ä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚\nç©ºé—´æ•ˆç‡ï¼šCOO éœ€è¦ä¸‰ä¸ªæ•°ç»„ï¼ŒrowIdx, colIdx å’Œ valueï¼Œæ¯ä¸ªæ•°ç»„çš„å…ƒç´ æ•°é‡ä¸éé›¶å…ƒç´ çš„æ•°é‡ç›¸åŒã€‚ çµæ´»æ€§ï¼šåªè¦ä»¥ç›¸åŒçš„æ–¹å¼é‡æ–°æ’åº rowIdx, colIdx å’Œ value æ•°ç»„ï¼Œå°±å¯ä»¥åœ¨ä¸ä¸¢å¤±ä»»ä½•ä¿¡æ¯çš„æƒ…å†µä¸‹ä»»æ„åœ°ä»¥ COO æ ¼å¼é‡æ–°æ’åºå…ƒç´ ã€‚ å¯è®¿é—®æ€§æ–¹é¢ï¼šCOO ä¸æ˜“è®¿é—®æŸä¸€è¡Œæˆ–æŸä¸€åˆ—ä¸­çš„æ‰€æœ‰éé›¶å…ƒç´ ã€‚ å†…å­˜è®¿é—®æ•ˆç‡ï¼šç›¸é‚»çº¿ç¨‹è®¿é—® COO æ ¼å¼çš„æ¯ä¸ªæ•°ç»„ä¸­çš„ç›¸é‚»å…ƒç´ ã€‚å› æ­¤ï¼Œé€šè¿‡ SpMV/COO å¯¹çŸ©é˜µçš„è®¿é—®æ˜¯å†…å­˜åˆå¹¶çš„ã€‚ è´Ÿè½½å¹³è¡¡ï¼šç”±äºæ¯ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®—ä¸€ä¸ªéé›¶å…ƒç´ ï¼Œæ‰€æœ‰çº¿ç¨‹è´Ÿè´£ç›¸åŒæ•°é‡çš„å·¥ä½œã€‚ SpMV/COO çš„ä¸»è¦ç¼ºç‚¹æ˜¯éœ€è¦ä½¿ç”¨åŸå­æ“ä½œï¼Œéå¸¸è€—æ—¶ã€‚ 14.3 Grouping Row Nonzeros with the CSR Format å¦‚æœå°†åŒä¸€è¡Œä¸­çš„æ‰€æœ‰éé›¶éƒ½åˆ†é…ç»™åŒä¸€ä¸ªçº¿ç¨‹ï¼Œé‚£ä¹ˆè¯¥çº¿ç¨‹å°†æ˜¯å”¯ä¸€æ›´æ–°ç›¸åº”è¾“å‡ºå€¼çš„çº¿ç¨‹ï¼Œåˆ™å¯ä»¥é¿å…åŸå­æ“ä½œã€‚è¿™ç§å¯è®¿é—®æ€§å¯ä»¥é€šè¿‡ CSR (Compressed Sparse Row ) å­˜å‚¨æ ¼å¼å®ç°ã€‚ä¸‹å›¾è¯´æ˜äº†å¦‚ä½•ä½¿ç”¨ CSR æ ¼å¼å­˜å‚¨ 14.1 èŠ‚ä¸­çš„çŸ©é˜µã€‚CSR ä¹Ÿå°†éé›¶å€¼å­˜å‚¨åœ¨ä¸€ç»´æ•°ç»„ä¸­ï¼Œä½†è¿™äº›éé›¶å€¼æ˜¯æŒ‰è¡Œåˆ†ç»„çš„ã€‚COO æ ¼å¼å’Œ CSR æ ¼å¼ä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºï¼ŒCSR æ ¼å¼ç”¨ rowPtrs æ•°ç»„æ›¿æ¢äº† rowIdx æ•°ç»„ï¼ŒrowPtrs æ•°ç»„å­˜å‚¨äº† colIdx å’Œ value æ•°ç»„ä¸­æ¯è¡Œéé›¶çš„èµ·å§‹åç§»é‡ï¼Œæ¯è¡Œä¸­çš„éé›¶å…ƒç´ ä¸ä¸€å®šæŒ‰åˆ—ç´¢å¼•æ’åºã€‚\nExample of Compressed Sparse Row (CSR) Format\nå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¦ä½¿ç”¨ä»¥ CSR æ ¼å¼è¡¨ç¤ºçš„ç¨€ç–çŸ©é˜µå¹¶è¡Œæ‰§è¡Œ SpMVï¼Œå¯ä»¥ä¸ºçŸ©é˜µçš„æ¯ä¸€è¡Œåˆ†é…ä¸€ä¸ªçº¿ç¨‹ã€‚ç”±äºä¸€ä¸ªçº¿ç¨‹éå†ä¸€è¡Œï¼Œæ‰€ä»¥æ¯ä¸ªçº¿ç¨‹å°†è¾“å‡ºå†™å…¥ä¸åŒçš„å†…å­˜ä½ç½®ã€‚\nExample of Parallelizing SpMV with the CSR Format\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹ï¼Œæ¯ä¸ªçº¿ç¨‹ç¡®å®šå®ƒè´Ÿè´£çš„è¡Œï¼Œå¾ªç¯éå†è¯¥è¡Œçš„éé›¶å…ƒç´ æ¥æ‰§è¡Œç‚¹ç§¯ã€‚çº¿ç¨‹åœ¨ rowPtrs æ•°ç»„ä¸­ç¡®å®šå®ƒä»¬çš„èµ·å§‹ç´¢å¼• (rowPtrs[row])å’Œé€šè¿‡ä¸‹ä¸€è¡Œéé›¶çš„èµ·å§‹ç´¢å¼• (rowPtrs[row+1]) æ¥ç¡®å®šç»“æŸä½ç½®ã€‚\nstruct CSRMatrix { int* rowPtrs; int* colIdx; float* val; int numRows; }; __global__ void spmv_csr_kernel(CSRMatrix m, float* x, float* y) { // Assign a thread to each row unsigned int row = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026lt; m.numRows) { int start = m.rowPtrs[row]; int end = m.rowPtrs[row + 1]; float sum = 0.0f; for (int i = start; i \u0026lt; end; i++) { int col = m.colIdx[i]; float val = m.val[i]; sum += val * x[col]; } y[row] = sum; // Perform the matrix-vector multiplication } } ä¸‹é¢æ¥åˆ†æ CSR æ ¼å¼åœ¨å‡ ä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚\nç©ºé—´æ•ˆç‡ï¼šCSR éœ€è¦ä¸‰ä¸ªæ•°ç»„ï¼Œå…¶ä¸­ colIdx å’Œ value çš„ç»´åº¦å’Œéé›¶å…ƒç´ çš„æ•°é‡ä¸€æ ·ã€‚rowPtrs ç»´åº¦ç­‰äºè¡Œæ•°åŠ  1. çµæ´»æ€§ï¼šCSR æ ¼å¼ä¸­è¦æ·»åŠ çš„éé›¶å¿…é¡»æ·»åŠ åˆ°å®ƒæ‰€å±çš„ç‰¹å®šè¡Œä¸­ã€‚è¿™æ„å‘³ç€åé¢è¡Œçš„éé›¶å…ƒç´ éƒ½éœ€è¦ç§»åŠ¨ï¼Œåé¢è¡Œçš„è¡ŒæŒ‡é’ˆéƒ½éœ€è¦ç›¸åº”å¢åŠ ã€‚ å¯è®¿é—®æ€§ï¼šCSR å¯ä»¥å¾ˆå®¹æ˜“åœ°è®¿é—®ç»™å®šè¡Œçš„éé›¶å…ƒç´ ï¼Œå…è®¸åœ¨ SpMV/CSR ä¸­è·¨è¡Œå¹¶è¡Œã€‚ å†…å­˜è®¿é—®æ•ˆç‡ï¼šCSR è®¿é—®æ¨¡å¼ä½¿å¾—è¿ç›¸é‚»ç¨‹è®¿é—®çš„æ•°æ®ç›¸è·å¾ˆè¿œï¼Œå¹¶ä¸èƒ½è¿›è¡Œå†…å­˜åˆå¹¶ã€‚ è´Ÿè½½å¹³è¡¡ï¼šçº¿ç¨‹åœ¨ç‚¹ç§¯å¾ªç¯ä¸­è¿›è¡Œçš„è¿­ä»£æ¬¡æ•°å–å†³äºåˆ†é…ç»™çº¿ç¨‹çš„è¡Œä¸­éé›¶å…ƒç´ çš„æ•°é‡ï¼Œå› æ­¤å¤§å¤šæ•°ç”šè‡³æ‰€æœ‰çº¿ç¨‹ä¸­éƒ½å­˜åœ¨æ§åˆ¶å‘æ•£ã€‚ 14.4 Improving Memory Coalescing with the ELL Format ELL å­˜å‚¨æ ¼å¼é€šè¿‡å¯¹ç¨€ç–çŸ©é˜µæ•°æ®è¿›è¡Œå¡«å……å’Œè½¬ç½®ï¼Œå¯ä»¥è§£å†³éåˆå¹¶å†…å­˜è®¿é—®çš„é—®é¢˜ã€‚å®ƒçš„åå­—æ¥æºäº ELLPACK ä¸­çš„ç¨€ç–çŸ©é˜µåŒ…ï¼Œä¸€ä¸ªç”¨äºæ±‚è§£æ¤­åœ†è¾¹å€¼é—®é¢˜çš„åŒ…ã€‚ ä¸€ä¸ªç”¨ ELL æ ¼å¼å­˜å‚¨çš„ä¾‹å­å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä»æŒ‰è¡Œåˆ†ç»„éé›¶çš„ CSR æ ¼å¼ä¸­ç¡®å®šå…·æœ‰æœ€å¤§éé›¶å…ƒç´ æ•°é‡çš„è¡Œã€‚ç„¶ååœ¨æ‰€æœ‰å…¶ä»–è¡Œçš„éé›¶å…ƒç´ ä¹‹åçš„æ·»åŠ å¡«å……å…ƒç´ ï¼Œä½¿å®ƒä»¬ä¸æœ€å¤§è¡Œé•¿åº¦ç›¸åŒã€‚æœ€åæŒ‰åˆ—ä¸»å…ƒç´ é¡ºåºå­˜å‚¨å¡«å……çŸ©é˜µã€‚\nExample of ELL Storage Format\nä¸‹å›¾ä½¿ç”¨ ELL æ ¼å¼å¹¶è¡ŒåŒ– SpMVã€‚ä¸ CSR ä¸€æ ·ï¼Œæ¯ä¸ªçº¿ç¨‹è¢«åˆ†é…åˆ°çŸ©é˜µçš„ä¸åŒè¡Œã€‚\nExample of Parallelizing SpMV with the ELL Format\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹ï¼Œç‚¹ç§¯å¾ªç¯éå†æ¯è¡Œçš„éé›¶å…ƒç´ ã€‚SpMV/ELL å†…æ ¸å‡è®¾è¾“å…¥çŸ©é˜µæœ‰ä¸€ä¸ªå‘é‡ ellMatrix.nnzPerRow è®°å½•æ¯è¡Œä¸­éé›¶çš„æ•°é‡ï¼Œæ¯ä¸ªçº¿ç¨‹åªè¿­ä»£å…¶åˆ†é…çš„è¡Œä¸­çš„éé›¶å…ƒç´ ã€‚\nstruct ELLMATRIX { int* nnzPerRow; // Number of nonzeros per row int* colIdx; // Column indices of nonzeros float* val; // Nonzero values int numRows; // Number of rows }; __global__ void spmv_ell_kernel(ELLMATRIX m, float* x, float* y) { unsigned int row = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026lt; m.numRows) { float sum = 0.0f; for (unsigned int i = 0; i \u0026lt; m.nnzPerRow[row]; i++) { // ell matrix stores values in column-major order unsigned int col = m.colIdx[i * m.numRows + row]; float val = m.val[i * m.numRows + row]; sum += val * x[col]; } y[row] = sum; // Perform the matrix-vector multiplication } } ä¸‹é¢æ¥åˆ†æ CSR æ ¼å¼åœ¨å‡ ä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚\nç©ºé—´æ•ˆç‡ï¼šç”±äºå¡«å……å…ƒç´ çš„ç©ºé—´å¼€é”€ï¼ŒELLæ ¼å¼çš„ç©ºé—´æ•ˆç‡ä½äºCSRæ ¼å¼ã€‚ çµæ´»æ€§ï¼šELL æ ¼å¼çš„æ¯” CSR æ ¼å¼æœ‰æ›´é«˜çš„çµæ´»æ€§ã€‚åªè¦ä¸€è¡Œæ²¡æœ‰è¾¾åˆ°çŸ©é˜µä¸­éé›¶çš„æœ€å¤§æ•°ç›®ï¼Œå°±å¯ä»¥é€šè¿‡ç®€å•åœ°ç”¨å®é™…å€¼æ›¿æ¢å¡«å……å…ƒç´ æ¥å‘è¯¥è¡Œæ·»åŠ éé›¶ã€‚ å¯è®¿é—®æ€§ï¼šELL å¯ä»¥è®¿é—®æŸä¸€è¡Œçš„éé›¶å…ƒç´ ã€‚ELL è¿˜å…è®¸åœ¨ç»™å®šéé›¶å…ƒç´ çš„ç´¢å¼•åå¾—åˆ°è¯¥å…ƒç´ çš„è¡Œå’Œåˆ—ç´¢å¼•ï¼Œå› ä¸º i = col*m.numRows + row, é€šè¿‡ i % m.numRows å°±å¯ä»¥å¾—åˆ°æ‰€åœ¨çš„è¡Œã€‚ å†…å­˜è®¿é—®æ•ˆç‡ï¼šç”±äºå…ƒç´ æŒ‰åˆ—ä¸»åºæ’åˆ—ï¼Œæ‰€æœ‰ç›¸é‚»çš„çº¿ç¨‹ç°åœ¨éƒ½è®¿é—®ç›¸é‚»çš„å†…å­˜ä½ç½®ã€‚ è´Ÿè½½å¹³è¡¡ï¼šSpMV/ELL ä»ç„¶å’Œ SpMV/CSR å…·æœ‰ç›¸åŒçš„è´Ÿè½½ä¸å¹³è¡¡é—®é¢˜ï¼Œå› ä¸ºæ¯ä¸ªçº¿ç¨‹å¾ªç¯æ¬¡æ•°ä»å–å†³å®ƒè´Ÿè´£çš„è¡Œä¸­çš„éé›¶å…ƒç´ æ•°é‡ã€‚ 14.5 Regulating Padding with the Hybrid ELL-COO Format åœ¨ ELL æ ¼å¼ä¸­ï¼Œå½“ä¸€è¡Œæˆ–å°‘æ•°è¡Œå…·æœ‰éå¸¸å¤šçš„éé›¶å…ƒç´ æ—¶ï¼Œç©ºé—´æ•ˆç‡ä½å’Œæ§åˆ¶å‘æ•£çš„é—®é¢˜æœ€ä¸ºæ˜æ˜¾ã€‚COO æ ¼å¼å¯ç”¨äºé™åˆ¶ ELL æ ¼å¼ä¸­çš„è¡Œé•¿åº¦ã€‚åœ¨å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸º ELL ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥ä»å…·æœ‰å¤§é‡éé›¶å…ƒç´ çš„è¡Œä¸­å–å‡ºä¸€äº›å…ƒç´ ï¼Œå¹¶å°†è¿™äº›å…ƒç´ ç”¨å•ç‹¬çš„ COO æ ¼å¼å­˜å‚¨ã€‚ ä¸‹å›¾å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ··åˆ ELL-COO æ ¼å¼å­˜å‚¨å›¾ä¸­çŸ©é˜µã€‚ä» ELL æ ¼å¼ä¸­åˆ é™¤ç¬¬äºŒè¡Œçš„æœ€å 3 ä¸ªéé›¶å…ƒç´ å’Œç¬¬å…­è¡Œçš„æœ€å 2 ä¸ªéé›¶å…ƒç´ ï¼Œå¹¶å°†å®ƒä»¬ç§»åŠ¨åˆ°å•ç‹¬çš„ COO æ ¼å¼ä¸­ã€‚\nHybrid ELL-COO Example\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹ï¼Œç‚¹ç§¯å°†è¢«åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†å¤„ç†ï¼Œä¸€éƒ¨åˆ†è´Ÿè´£å¤„ç† ELL æ ¼å¼çš„éé›¶å…ƒç´ ï¼Œå¦ä¸€éƒ¨åˆ†è´Ÿè´£å¤„ç† COO æ ¼å¼ä¸­ rowIdx ä¸ row ç›¸åŒçš„éé›¶å…ƒç´ ã€‚\n__global__ void spmv_hybrid_ell_coo_kernel(ELLMATRIX ell, COOMATRIX coo, float* x, float* y) { unsigned int row = blockIdx.x * blockDim.x + threadIdx.x; float sum = 0.0f; // ELL part if (row \u0026lt; ell.numRows) { for (int i = 0; i \u0026lt; ell.nnzPerRow[row]; i++) { unsigned int col = ell.colIdx[i * ell.numRows + row]; float val = ell.val[col]; sum += val * x[col]; } } y[row] = sum; // Perform the matrix-vector multiplication // COO part for (int i = 0; i \u0026lt; coo.numNonZeros; i++) { int col = coo.colIdx[i]; float val = coo.val[i]; sum += val * x[col]; atomicAdd(\u0026amp;y[row], val * x[col]); } } ä¸‹é¢æ¥åˆ†ææ··åˆ ELL-COO æ ¼å¼åœ¨å‡ ä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚\nç©ºé—´æ•ˆç‡ï¼šå› ä¸ºå‡å°‘äº†å¡«å……å…ƒç´ ï¼Œæ··åˆ ELL-COO æ ¼å¼æ¯”å•ç‹¬ä½¿ç”¨ ELL æ ¼å¼çš„ç©ºé—´æ•ˆç‡æ›´é«˜ã€‚ çµæ´»æ€§ï¼šæ··åˆ COO-ELL æ—¢å¯ä»¥é€šè¿‡æ›¿æ¢å¡«å……å…ƒç´ æ¥æ·»åŠ éé›¶ã€‚å¦‚æœè¯¥è¡Œæ²¡æœ‰ä»»ä½•å¯ä»¥åœ¨ ELL éƒ¨åˆ†ä¸­æ›¿æ¢çš„å¡«å……å…ƒç´ ï¼Œä¹Ÿå¯ä»¥åœ¨æ ¼å¼çš„ COO éƒ¨åˆ†æ·»åŠ ã€‚ å¯è®¿é—®æ€§ï¼šè®¿é—®ç»™å®šè¡Œä¸­æ‰€æœ‰çš„éé›¶å…ƒç´ åªèƒ½ç”¨äºé€‚åˆç”¨ ELL æ ¼å¼å­˜å‚¨çš„éƒ¨åˆ†è¡Œã€‚ å†…å­˜è®¿é—®æ•ˆç‡ï¼šSpMV/ELL å’Œ SpMV/COO éƒ½èƒ½å¯¹ç¨€ç–çŸ©é˜µè¿›è¡Œåˆå¹¶å†…å­˜è®¿é—®ã€‚å› æ­¤ï¼Œå®ƒä»¬çš„ç»„åˆä¹Ÿå°†æ˜¯åˆå¹¶è®¿é—®æ¨¡å¼ã€‚ è´Ÿè½½å¹³è¡¡ï¼šä»ELL æ ¼å¼éƒ¨åˆ†ç§»é™¤ä¸€äº›éé›¶å…ƒç´ å¯ä»¥å‡å°‘ SpMV/ELL å†…æ ¸çš„æ§åˆ¶å‘æ•£ã€‚è¿™äº›éé›¶å…ƒç´ è¢«æ”¾åœ¨ COO æ ¼å¼éƒ¨åˆ†ï¼Œä¸ä¼šå‡ºç°æ§åˆ¶å‘æ•£ã€‚ 14.6 Reducing Control Divergence with the JDS Format æ ¹æ®çŸ©é˜µä¸­è¡Œçš„éé›¶å…ƒç´ å¤ºå°‘è¿›è¡Œé™åºæ’åºä¹‹åçŸ©é˜µåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šçœ‹èµ·æ¥åƒä¸‰è§’å½¢çŸ©é˜µï¼Œå› æ­¤è¿™ç§æ ¼å¼é€šå¸¸è¢«ç§°ä¸º JDS (Jagged Diagonal Storage) æ ¼å¼ã€‚ ä¸‹å›¾å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ JDS æ ¼å¼å­˜å‚¨çŸ©é˜µã€‚é¦–å…ˆï¼Œä¸ CSR å’Œ ELL æ ¼å¼ä¸€æ ·å°†éé›¶å…ƒç´ æŒ‰è¡Œåˆ†ç»„ã€‚æ¥ä¸‹æ¥ï¼ŒæŒ‰æ¯è¡Œä¸­éé›¶çš„ä¸ªæ•°ä»å¤§åˆ°å°æ’åºã€‚value æ•°ç»„ä¸­çš„éé›¶å€¼åŠå…¶å­˜å‚¨å…¶å¯¹åº”åˆ—ç´¢å¼•çš„ colIdx æ•°ç»„æŒ‰åˆ—ä¸»å…ƒç´ é¡ºåºå­˜å‚¨ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­æ·»åŠ ä¸€ä¸ª iterPtr æ•°ç»„æ¥è·Ÿè¸ªéé›¶å…ƒç´ çš„å¼€å§‹ä½ç½®ã€‚å¹¶ä¸”ç»´æŠ¤ä¸€ä¸ªä¿ç•™åŸå§‹è¡Œç´¢å¼•çš„ rowIdx æ•°ç»„ã€‚\nExample of JDS Storage Format\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¸€å…±è¦è¿­ä»£ maxNumNonZerosPerRow æ¬¡ï¼Œæ¯æ¬¡è¿­ä»£ä¸­æ¯ä¸ªçº¿ç¨‹åˆ¤æ–­è‡ªå·±è´Ÿè´£çš„è¡Œæ˜¯å¦è¿˜å­˜åœ¨éé›¶å…ƒç´ ã€‚\nstruct JDSMATRIX { int* iterPtr; // Pointer to the start of each row in the JDS format int* colIdx; // Column indices of nonzeros float* val; // Nonzero values int* rowIdx; // Original row indices int numRows; int maxNumNonZerosPerRow; }; __global__ void spmv_jds_kernel(JDSMATRIX m, float* x, float* y) { unsigned int row = blockIdx.x * blockDim.x + threadIdx.x; if (row \u0026lt; m.numRows) { float sum = 0.0f; for (int i = 0; i \u0026lt; m.maxNumNonZerosPerRow + 1; i++) { int start = m.iterPtr[i]; int end = m.iterPtr[i + 1]; if (row + i * blockDim.x \u0026gt;= end) { break; } else { sum += m.val[row + i * blockDim.x]; } } y[m.rowIdx[row]] = sum; // Perform the matrix-vector multiplication } } ä¸‹é¢æ¥åˆ†æ JDS æ ¼å¼åœ¨å‡ ä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚\nç©ºé—´æ•ˆç‡ï¼šå› ä¸ºé¿å…äº†å¡«å…… JDS æ ¼å¼æ¯” ELL æ ¼å¼æ•ˆç‡æ›´é«˜ã€‚ çµæ´»æ€§ï¼šJDS æ ¼å¼çš„çµæ´»æ€§è¾ƒå·®ï¼Œå› ä¸ºæ·»åŠ éé›¶ä¼šæ”¹å˜è¡Œå¤§å°ï¼Œè¿™å¯èƒ½éœ€è¦é‡æ–°å¯¹è¡Œè¿›è¡Œæ’åºã€‚ å¯è®¿é—®æ€§ï¼šJDS æ ¼å¼ç±»ä¼¼äºCSRæ ¼å¼ï¼Œå…è®¸åœ¨ç»™å®šè¡Œç´¢å¼•çš„æƒ…å†µä¸‹è®¿é—®è¯¥è¡Œçš„éé›¶å…ƒç´ ã€‚ å†…å­˜è®¿é—®æ•ˆç‡ï¼šJDS æ ¼å¼çš„å†…å­˜è®¿é—®æ•ˆç‡æ¯” ELL æ ¼å¼é«˜ï¼Œå› ä¸ºå®ƒå¯ä»¥å¯¹ç¨€ç–çŸ©é˜µè¿›è¡Œåˆå¹¶è®¿é—®ã€‚ è´Ÿè½½å¹³è¡¡ï¼šJDS æ ¼å¼å¯¹çŸ©é˜µçš„è¡Œè¿›è¡Œæ’åºï¼Œä½¿å¾—ç›¸é‚»çº¿ç¨‹éé•¿åº¦æ¥è¿‘çš„è¡Œã€‚å› æ­¤ï¼ŒJDS æ ¼å¼èƒ½å‡å°‘æ§åˆ¶å‘æ•£ã€‚ ","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch14/","summary":"Personal notebook 14 of Programming Massively Parallel","title":"PMPP Learning-Chapter 14 Sparse Matrix Computation"},{"content":"13 Sorting æ’åºç®—æ³•å°†åˆ—è¡¨ä¸­çš„æ•°æ®å…ƒç´ æŒ‰ä¸€å®šçš„é¡ºåºæ’åˆ—ã€‚\n13.1 Background ä»»ä½•æ’åºç®—æ³•éƒ½å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶:\nè¾“å‡ºæ˜¯éé€’å‡é¡ºåºæˆ–éé€’å¢é¡ºåºã€‚ è¾“å‡ºæ˜¯è¾“å…¥çš„ä¸€ç§æ’åˆ— (permutation). æ’åºç®—æ³•å¯ä»¥åˆ†ä¸ºç¨³å®šç®—æ³•å’Œä¸ç¨³å®šç®—æ³•ã€‚å½“ä¸¤ä¸ªå…ƒç´ å…·æœ‰ç›¸åŒçš„é”®å€¼æ—¶ï¼Œç¨³å®šçš„æ’åºç®—æ³•ä¿ç•™äº†åŸå§‹çš„å‡ºç°é¡ºåºã€‚ æ’åºç®—æ³•ä¹Ÿå¯ä»¥åˆ†ä¸ºåŸºäºæ¯”è¾ƒçš„ç®—æ³•å’ŒéåŸºäºæ¯”è¾ƒçš„ç®—æ³•ã€‚åŸºäºæ¯”è¾ƒçš„æ’åºç®—æ³•æ— æ³•è¾¾åˆ°æ¯” O(NlogN) æ›´å¥½çš„å¤æ‚åº¦ï¼Œå› ä¸ºå®ƒä»¬å¿…é¡»åœ¨å…ƒç´ ä¹‹é—´æ‰§è¡Œæœ€å°‘æ¬¡æ•°çš„æ¯”è¾ƒã€‚\n13.2 Radix Sort åŸºæ•°æ’åºæ˜¯ä¸€ç§åŸºäºéæ¯”è¾ƒçš„æ’åºç®—æ³•ï¼Œå…¶å·¥ä½œåŸç†æ˜¯æ ¹æ®åŸºæ•°å€¼å°†è¦æ’åºçš„é”®åˆ†å¸ƒåˆ°æ¡¶ (bucket) ä¸­ã€‚å¦‚æœé”®ç”±å¤šä¸ªæ•°å­—ç»„æˆï¼Œåˆ™é‡å¤å¯¹æ¯ä¸ªæ•°å­—é‡å¤åˆ†é…æ¡¶ï¼Œç›´åˆ°è¦†ç›–æ‰€æœ‰æ•°å­—ã€‚ ä¸‹å›¾å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ 1 ä½åŸºæ•°å¯¹ 4 ä½æ•´æ•°åˆ—è¡¨è¿›è¡ŒåŸºæ•°æ’åºã€‚\nA Radix Sort Example\n13.3 Parallel Radix Sort åŸºæ•°æ’åºçš„æ¯æ¬¡è¿­ä»£éƒ½ä¾èµ–äºå‰ä¸€æ¬¡è¿­ä»£çš„æ•´ä¸ªç»“æœã€‚å› æ­¤ï¼Œè¿­ä»£æ˜¯ç›¸å¯¹äºå½¼æ­¤é¡ºåºæ‰§è¡Œçš„ã€‚æˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨æ‰§è¡Œå•ä¸ªåŸºæ•°æ’åºè¿­ä»£çš„å†…æ ¸çš„å®ç°ï¼Œå¹¶å‡è®¾ä¸»æœºä»£ç æ¯æ¬¡è¿­ä»£è°ƒç”¨è¯¥å†…æ ¸ä¸€æ¬¡ã€‚ åœ¨ GPU ä¸Šå¹¶è¡ŒåŒ–åŸºæ•°æ’åºè¿­ä»£çš„ä¸€ç§ç›´æ¥æ–¹æ³•æ˜¯è®©æ¯ä¸ªçº¿ç¨‹è´Ÿè´£è¾“å…¥åˆ—è¡¨ä¸­çš„ä¸€ä¸ªé”®ã€‚çº¿ç¨‹å¿…é¡»ç¡®å®šé”®åœ¨è¾“å‡ºåˆ—è¡¨ä¸­çš„ä½ç½®ï¼Œç„¶åå°†é”®å­˜å‚¨åˆ°è¯¥ä½ç½®ã€‚ ä¸‹å›¾å±•ç¤ºäº†è¿™ç§å¹¶è¡ŒåŒ–æ–¹æ³•ç¬¬ä¸€æ¬¡è¿­ä»£çš„æ‰§è¡Œæƒ…å†µã€‚å¯¹äºæ˜ å°„åˆ° 0 æ¡¶çš„é”®ï¼Œç›®æ ‡ç´¢å¼•å¯ä»¥é€šè¿‡å¦‚ä¸‹å…¬å¼è®¡ç®—ï¼š $$\r\\begin{align*} \\text{destination of a zero} \u0026= \\text{\\#zeros before} \\\\\r\u0026=\\text{\\#keys before} - \\text{\\#ones before} \\\\\r\u0026=\\text{key index}-\\text{\\#ones before}\r\\end{align*}\r$$å¯¹äºæ˜ å°„åˆ° 1 æ¡¶çš„é”®ï¼Œç›®æ ‡ç´¢å¼•å¦‚ä¸‹æ‰€ç¤º:\n$$\r\\begin{align*}\r\\text{destination of a one}\u0026=\\text{\\#zeros in total}+\\text{\\#ones before} \\\\\r\u0026=(\\text{\\#keys in total}-\\text{\\#ones in total})+\\text{\\#ones before} \\\\\r\u0026=\\text{input size}-\\text{\\#ones in total}+\\text{\\#ones before}\r\\end{align*}\r$$\rParallelizing a Radix Sort Iteration by Assigning One Input Key to Each Thread\nä¸‹å›¾å±•ç¤ºäº†æ¯ä¸ªçº¿ç¨‹æŸ¥æ‰¾å…¶é”®çš„ç›®æ ‡ç´¢å¼•æ‰€æ‰§è¡Œçš„æ“ä½œã€‚\nFinding the Destination of Each Input Key\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚åœ¨æ¯ä¸ªçº¿ç¨‹ç¡®å®šè‡ªå·±çš„ç´¢å¼•å¹¶æå–å‡ºå¯¹åº”çš„ bit åï¼Œå› ä¸ºè¿™äº›ä½ä¸æ˜¯ 0 å°±æ˜¯ 1ï¼Œæ‰€ä»¥æ’é™¤æ‰«æçš„ç»“æœå°±ç­‰äºç´¢å¼•å‰é¢ 1 çš„ä¸ªæ•°ã€‚\n__global__ void exclusiveScan(unsigned int* bits, int N) { extern __shared__ unsigned int temp[]; int thid = threadIdx.x; int offset = 1; // Load input into shared memory temp[2 * thid] = (2 * thid \u0026lt; N) ? bits[2 * thid] : 0; temp[2 * thid + 1] = (2 * thid + 1 \u0026lt; N) ? bits[2 * thid + 1] : 0; // Build sum in place up the tree for (int d = N \u0026gt;\u0026gt; 1; d \u0026gt; 0; d \u0026gt;\u0026gt;= 1) { __syncthreads(); if (thid \u0026lt; d) { int ai = offset * (2 * thid + 1) - 1; int bi = offset * (2 * thid + 2) - 1; temp[bi] += temp[ai]; } offset *= 2; } // Clear the last element if (thid == 0) { temp[N - 1] = 0; } // Traverse down the tree for (int d = 1; d \u0026lt; N; d *= 2) { offset \u0026gt;\u0026gt;= 1; __syncthreads(); if (thid \u0026lt; d) { int ai = offset * (2 * thid + 1) - 1; // left child index of the thread int bi = offset * (2 * thid + 2) - 1; // right unsigned int t = temp[ai]; temp[ai] = temp[bi]; temp[bi] += t; } } // Write results to output array __syncthreads(); if (2 * thid \u0026lt; N) bits[2 * thid] = temp[2 * thid]; if (2 * thid + 1 \u0026lt; N) bits[2 * thid + 1] = temp[2 * thid + 1]; } __global__ void radix_sort_iter(unsigned int* input, unsigned int* output, unsigned int* bits, int N, unsigned int iter) { unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; unsigned int key, bit; if (i \u0026lt; N) { key = input[i]; bit = (key \u0026gt;\u0026gt; iter) \u0026amp; 1; bits[i] = bit; } exclusiveScan(bits, N); // # ones before if (i \u0026lt; N) { unsigned int numberOnesBefore = bits[i]; unsigned int numberOnesTotal = bits[N]; unsigned int dst = (bit == 0) ? (i - numberOnesBefore) : (N - numberOnesTotal - numberOnesBefore); output[dst] = key; } } 13.4 Optimizing for Memory Coalescing ä¸Šé¢æ–¹æ³•æ•ˆç‡ä½ä¸‹çš„ä¸€ä¸ªä¸»è¦åŸå› æ˜¯ï¼Œå¯¹è¾“å‡ºæ•°ç»„çš„å†™å…¥æ˜¾ç¤ºå‡ºä¸èƒ½ä»¥å†…å­˜åˆå¹¶çš„æ¨¡å¼è®¿é—®ã€‚æ”¹è¿›åçš„ç®—æ³•å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ¯ä¸ªå—ä¸­çš„çº¿ç¨‹å°†é¦–å…ˆæ‰§è¡Œå—çº§åˆ«çš„å±€éƒ¨æ’åºï¼Œä»¥åˆ†ç¦»å…±äº«å†…å­˜ä¸­æ˜ å°„åˆ° 0 bucket çš„é”®å’Œæ˜ å°„åˆ° 1 bucket çš„é”®ã€‚æ­¤ä¼˜åŒ–ä¸­çš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ¯ä¸ªçº¿ç¨‹å—åœ¨å…¨å±€ bucket ä¸­ç¡®å®šå…¶ä½ç½®ã€‚çº¿ç¨‹å—çš„ 0 æ¡¶çš„ä½ç½®åœ¨å‰é¢çº¿ç¨‹å—çš„æ‰€æœ‰ 0 æ¡¶ä¹‹åã€‚å¦ä¸€æ–¹é¢ï¼Œçº¿ç¨‹å—çš„ 1 æ¡¶çš„ä½ç½®åœ¨æ‰€æœ‰çº¿ç¨‹å—çš„ 0 æ¡¶å’Œä¹‹å‰çº¿ç¨‹å—çš„æ‰€æœ‰ 1 æ¡¶ä¹‹åã€‚\nOptimizing for Memory Coalescing by Sorting Locally in Shared Memory\nä¸‹å›¾å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ’é™¤æ‰«ææ¥æŸ¥æ‰¾æ¯ä¸ªçº¿ç¨‹å—çš„æœ¬åœ°æ¡¶çš„ä½ç½®çš„ã€‚åœ¨å®Œæˆå±€éƒ¨åŸºæ•°æ’åºä¹‹åï¼Œæ¯ä¸ªçº¿ç¨‹å—æ ‡è¯†å…¶æ¯ä¸ªè‡ªå·±æ¡¶ä¸­é”®çš„æ•°é‡ã€‚ç„¶åæ¯ä¸ªå—å°†ç»“æœè®°å½•åœ¨å¦‚å›¾ä¸­æ‰€ç¤ºçš„è¡¨ä¸­ï¼Œè¯¥è¡¨æŒ‰è¡Œä¸»é¡ºåºå­˜å‚¨ï¼Œå¯¹çº¿æ€§åŒ–çš„è¡¨æ‰§è¡Œæ’é™¤æ‰«æï¼Œç»“æœè¡¨ç¤ºçº¿ç¨‹å—çš„æœ¬åœ° bucket çš„èµ·å§‹ä½ç½®ã€‚\nFinding the Destination of Each Thread Block\u0026#39;s Local Buckets\n#define SECTION_SIZE 32 __global__ void memory_coalescing_radix_sort(unsigned int* input, unsigned int* output, unsigned int* bits, unsigned int* table, int N, int iter) { __shared__ unsigned int input_s[SECTION_SIZE]; __shared__ unsigned int output_s[SECTION_SIZE]; // Load input into shared memory unsigned int globalIdx = blockIdx.x * blockDim.x + threadIdx.x; if (globalIdx \u0026lt; N) { input_s[threadIdx.x] = input[globalIdx]; } __syncthreads(); // Sort each section radix_sort_iter(input_s, output_s, bits + blockIdx.x * SECTION_SIZE, SECTION_SIZE, iter); __syncthreads(); // Store local bucket num if (threadIdx.x == 0) { unsigned int numberOnesTotal = 0; unsigned int numberZerosTotal = 0; for (int i = 0; i \u0026lt; SECTION_SIZE; ++i) { numberOnesTotal += bits[blockIdx.x * SECTION_SIZE + i]; } numberZerosTotal = SECTION_SIZE - numberOnesTotal; table[blockIdx.x] = numberZerosTotal; table[blockIdx.x + gridDim.x] = numberOnesTotal; } __syncthreads(); // Exclusive prefix sum to determine output index exclusiveScan(table, 2 * gridDim.x); // Write results to output array if (globalIdx \u0026lt; N) { int zeroOffset = table[blockIdx.x]; int oneOffset = table[blockIdx.x + gridDim.x]; unsigned int bit = bits[blockIdx.x * SECTION_SIZE + threadIdx.x]; unsigned int dst = (bit == 0) ? (globalIdx - zeroOffset) : (N - oneOffset); output[dst] = input[globalIdx]; } } 13.5 Choice of Radix Value ä½¿ç”¨ 2 bit çš„åŸºæ•°æ—¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ¯æ¬¡è¿­ä»£ä½¿ç”¨ä¸¤ä¸ªæ¯”ç‰¹å°†é”®åˆ†å‘åˆ°å­˜å‚¨æ¡¶ã€‚å› æ­¤ï¼Œä¸¤æ¬¡è¿­ä»£å°±å¯ä»¥å¯¹ 4 bit é”®è¿›è¡Œå®Œå…¨æ’åºã€‚\nRadix Sort Example with 2-bit Radix\nä¸ºäº†å†…å­˜åˆå¹¶è®¿é—®ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ¯ä¸ªçº¿ç¨‹å—å¯ä»¥åœ¨å…±äº«å†…å­˜ä¸­å¯¹å…¶é”®è¿›è¡Œæœ¬åœ°æ’åºï¼Œç„¶åå°†æ¯ä¸ªæœ¬åœ°æ¡¶ä¸­çš„é”®çš„æ•°é‡å†™å…¥è¡¨ä¸­ã€‚å’Œ 13.4 èŠ‚ä¸€æ ·ï¼Œå¯¹äº r ä½åŸºæ•°ï¼Œå¯¹å…·æœ‰ 2^r è¡Œçš„è¡¨æ‰§è¡Œæ’é™¤æ‰«ææ“ä½œã€‚æœ€åä»¥åˆå¹¶çš„æ–¹å¼å°†æœ¬åœ° bucket å†™å…¥å…¨å±€å†…å­˜ã€‚\nOptimizing 2-bit Radix Sorting for Memory Coalescing Using the Shared Memory\nä½¿ç”¨æ›´å¤§çš„åŸºæ•°ä¹Ÿæœ‰ç¼ºç‚¹\næ¯ä¸ªçº¿ç¨‹å—æœ‰æ›´å¤šçš„æœ¬åœ°æ¡¶ï¼Œæ¯ä¸ªæ¡¶æœ‰æ›´å°‘çš„é”®ã€‚è¿™æ ·å°±ä¼šå‘å¤šä¸ªå…¨å±€å†…å­˜å—è¿›è¡Œå†™å…¥ï¼Œä½†æ¯ä¸€éƒ¨åˆ†å†™å…¥çš„æ•°æ®å˜å°‘ï¼Œä¸åˆ©äºå†…å­˜åˆå¹¶ã€‚ è¿›è¡Œæ’é™¤æ‰«æçš„è¡¨ä¼šéšç€åŸºæ•°çš„å¢å¤§è€Œå˜å¤§ï¼Œæ‰«æçš„å¼€é”€éšç€åŸºæ•°çš„å¢åŠ è€Œå¢åŠ ã€‚ Finding the Destination of Each Block\u0026#39;s Local Buckets for a 2-bit Radix\n13.6 Thread Coarsening to Improve Coalescing è·¨å¤šä¸ªçº¿ç¨‹å—å¹¶è¡ŒåŒ–åŸºæ•°æ’åºçš„ä¸€ä¸ªä»£ä»·æ˜¯å¯¹å…¨å±€å†…å­˜çš„å†™çš„è®¿é—®åˆå¹¶å¾ˆå·®ã€‚æ¯ä¸ªçº¿ç¨‹å—éƒ½æœ‰è‡ªå·±çš„æœ¬åœ°æ¡¶ï¼Œå¹¶å°†å…¶å†™å…¥å…¨å±€å†…å­˜ã€‚æ‹¥æœ‰æ›´å¤šçš„çº¿ç¨‹å—æ„å‘³ç€æ¯ä¸ªçº¿ç¨‹å—æ‹¥æœ‰æ›´å°‘çš„é”®ï¼Œè¿™æ„å‘³ç€æœ¬åœ°å­˜å‚¨æ¡¶å°†æ›´å°ï¼Œä»è€Œåœ¨å°†å®ƒä»¬å†™å…¥å…¨å±€å†…å­˜æ—¶åˆå¹¶æœºä¼šæ›´å°‘ã€‚å¦ä¸€ä¸ªä»£ä»·æ˜¯æ‰§è¡Œå…¨å±€æ’é™¤æ‰«æä»¥è¯†åˆ«æ¯ä¸ªçº¿ç¨‹å—çš„æœ¬åœ°æ¡¶çš„å­˜å‚¨ä½ç½®çš„å¼€é”€ã€‚é€šè¿‡åº”ç”¨çº¿ç¨‹ç²—åŒ–ï¼Œå¯ä»¥å‡å°‘å—çš„æ•°é‡ï¼Œä»è€Œå‡å°‘è¡¨çš„å¤§å°å’Œæ’é™¤æ‰«ææ“ä½œçš„å¼€é”€ã€‚ ä¸‹å›¾å±•ç¤ºäº†å¦‚ä½•å°†çº¿ç¨‹ç²—åŒ–åº”ç”¨äº 2 ä½åŸºæ•°æ’åºã€‚æ¯ä¸ªçº¿ç¨‹è¢«åˆ†é…ç»™è¾“å…¥åˆ—è¡¨ä¸­çš„å¤šä¸ªé”®ã€‚\nRadix Sort for a 2-bit Radix with Thread Coarsening\n13.7 Parallel Merge Sort ","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch13/","summary":"Personal notebook 13 of Programming Massively Parallel","title":"PMPP Learning-Chapter 13 Sorting"},{"content":"12 Merge-An Introduction to Dynamic Input Data Identification æœ‰åºå½’å¹¶æ“ä½œæ¥å—ä¸¤ä¸ªæœ‰åºåˆ—è¡¨å¹¶ç”Ÿæˆä¸€ä¸ªåˆå¹¶åçš„æœ‰åºåˆ—è¡¨ã€‚\n12.1 Background å‡è®¾æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æœ‰ä¸€ä¸ªé”®å¹¶ä¸”é”®å®šä¹‰äº†ä¸€ä¸ªç”¨ â‰¤ è¡¨ç¤ºçš„é¡ºåºå…³ç³»ã€‚ä¸‹å›¾å±•ç¤ºäº†åŸºäºæ•°å­—æ’åºå…³ç³»çš„ç®€å•å½’å¹¶å‡½æ•°çš„æ“ä½œã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœé”®å€¼ç›¸ç­‰çš„å…ƒç´ åœ¨è¾“å‡ºä¸­çš„é¡ºåºä¸å…¶åœ¨è¾“å…¥ä¸­çš„é¡ºåºç›¸åŒï¼Œåˆ™ç§°æ’åºæ“ä½œæ˜¯ç¨³å®šçš„ã€‚\nExample of a Merge Operation\n12.2 A Sequential Merge Algorithm å½’å¹¶æ“ä½œå¯ä»¥ç”¨å¦‚ä¸‹ä¸€ä¸ªç®€å•çš„é¡ºåºç®—æ³•æ¥å®ç°ã€‚é¡ºåºå½’å¹¶å‡½æ•°è®¿é—® A å’Œ B çš„æ¯ä¸ªè¾“å…¥å…ƒç´ ä¸€æ¬¡ï¼Œå¹¶å‘ C ä¸­æ¯ä¸ªä½ç½®å†™å…¥ä¸€æ¬¡ã€‚å…¶ç®—æ³•å¤æ‚åº¦ä¸º O(m+n).\nvoid merge_sequential(int* A, int* B, int* C, int m, int n) { int i = 0, j = 0, k = 0; // Indices for A, B, and C while (i \u0026lt; m \u0026amp;\u0026amp; j \u0026lt; n) { if (A[i] \u0026lt; B[j]) { C[k++] = A[i++]; } else { C[k++] = B[j++]; } if (i == m) { // Done with A[], handling remaining B while (j \u0026lt; n) { C[k++] = B[j++]; } } else { // Done with B[], handling remaining A while (i \u0026lt; m) { C[k++] = A[i++]; } } } } 12.3 A Parallelization Approach æ¯ä¸ªçº¿ç¨‹é¦–å…ˆç¡®å®šå®ƒå°†è¦è´Ÿè´£çš„è¾“å‡ºä½ç½®èŒƒå›´ï¼Œå¹¶ä½¿ç”¨è¯¥è¾“å‡ºèŒƒå›´ä½œä¸º co-rank å‡½æ•°çš„è¾“å…¥ï¼Œä»¥ç¡®å®šæ‰€è´Ÿè´£ C è¾“å‡ºèŒƒå›´çš„å¯¹åº”çš„ A å’Œ B è¾“å…¥èŒƒå›´ã€‚è¿™æ ·æ¯ä¸ªçº¿ç¨‹åœ¨å®ƒä»¬çš„å­æ•°ç»„ä¸Šæ‰§è¡Œé¡ºåºåˆå¹¶å‡½æ•°ï¼Œä»è€Œå¹¶è¡Œåœ°è¿›è¡Œåˆå¹¶ã€‚\nExamples of Observations\nObservation 1ï¼šå­æ•°ç»„ C[0]-C[k-1] (k ä¸ªå…ƒç´ ) æ˜¯ A[0]-A[i-1] (i ä¸ªå…ƒç´ ) å’Œ B[0]-B[k-i-1] (k-i ä¸ªå…ƒç´ ) çš„å½’å¹¶ç»“æœã€‚ Observation 2ï¼šå¯¹äºä»»æ„æ»¡è¶³ 0â‰¤kâ‰¤m+n çš„ kï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°å”¯ä¸€çš„ i å’Œ j ä½¿å¾— k=i+j, 0â‰¤iâ‰¤m, 0â‰¤jâ‰¤nï¼Œå¹¶ä¸”å­æ•°ç»„ C[0]-C[k-1] æ˜¯å­æ•°ç»„ A[0]-A[i-1] å’Œå­æ•°ç»„ B[0]-B[j-1] åˆå¹¶çš„ç»“æœã€‚å”¯ä¸€çš„ç´¢å¼• i å’Œ j è¢«ç§° C[k] çš„ co-rank. æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†è¾“å‡ºæ•°ç»„åˆ’åˆ†ä¸ºå­æ•°ç»„ï¼Œå¹¶è®©æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ä¸€ä¸ªå­æ•°ç»„çš„ç”Ÿæˆæ¥åˆ’åˆ†å·¥ä½œã€‚ç”±äºå¹¶è¡Œå½’å¹¶ç®—æ³•ä¸­æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨çš„è¾“å…¥å…ƒç´ çš„èŒƒå›´å–å†³äºå®é™…çš„è¾“å…¥å€¼ä½¿å¾—æˆ‘ä»¬éœ€è¦è¾…åŠ©å‡½æ•°æ¥å®Œæˆã€‚\n12.4 Co-rank Function Implementation å°† co-rank å‡½æ•°å®šä¹‰ä¸ºæ¥å—è¾“å‡ºæ•°ç»„ C ä¸­å…ƒç´ çš„ä½ç½® k å’Œä¸¤ä¸ªè¾“å…¥æ•°ç»„ A å’Œ Bçš„ä¿¡æ¯ï¼Œå¹¶è¿”å›è¾“å…¥æ•°ç»„ A å¯¹åº”çš„ co-rank å€¼ i. ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼Œå‡è®¾çº¿ç¨‹ 1 çš„ co-rank å‡½æ•°çš„ç›®æ ‡æ˜¯ä¸ºå…¶ç§© k1=4 ç¡®å®š co-rankå€¼ i1=3 å’Œ j1=1. ä¹Ÿå°±æ˜¯è¯´ï¼Œä» C[4] å¼€å§‹çš„å­æ•°ç»„å°†ç”±ä» A[3] å’Œ B[1] å¼€å§‹çš„å­æ•°ç»„åˆå¹¶ç”Ÿæˆã€‚æˆ‘ä»¬å¯ä»¥å‘ç°çº¿ç¨‹ t ä½¿ç”¨çš„è¾“å…¥å­æ•°ç»„ç”±çº¿ç¨‹ t å’Œçº¿ç¨‹ t+1 çš„ co-rank ç¡®å®šã€‚\nExample of co-rank Function Execution\nç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿å¾— A[i - 1] \u0026lt;= B[j] å¹¶ä¸” B[j - 1] \u0026lt;= A[i] çš„ç´¢å¼•ã€‚\nå¦‚æœ A[i-1] \u0026gt; B[j]ï¼Œè¯´æ˜ A[i] å¤ªå¤§ï¼Œéœ€è¦å‡å°‘ iï¼Œå¹¶å¢åŠ  jã€‚ å¦‚æœ B[j-1] \u0026gt; A[i]ï¼Œè¯´æ˜ B[j] å¤ªå¤§ï¼Œéœ€è¦å‡å°‘ jï¼Œå¹¶å¢åŠ  iã€‚ æ¯æ¬¡è°ƒæ•´æ—¶ï¼Œi å’Œ j éƒ½æŒ‰ç…§äºŒåˆ†æ–¹å¼è°ƒæ•´ï¼Œå³è°ƒæ•´çš„æ­¥é•¿æ˜¯ delta / 2. i å’Œ i_low ç¡®å®šäº†å½“å‰æ­£åœ¨æœç´¢çš„æ•°ç»„ A çš„èŒƒå›´ã€‚ int co_rank(int k, int* A, int m, int* B, int n) { // C[k] comes from A[i] of B[j] // k = i + j int i = k \u0026lt; m ? k : m; // max starting search value for A, i.e. A[k-1] \u0026lt; B[0] int i_low = 0 \u0026gt; (k - n) ? 0 : k - n; // when B is done, min starting search value for A is k-n int j = k - i; int j_low = 0 \u0026gt; (k - m) ? 0 : (k - m); int delta; bool active = true; while (active) { // Binary search for C[k] if (i \u0026gt; 0 \u0026amp;\u0026amp; j \u0026lt; n \u0026amp;\u0026amp; A[i - 1] \u0026gt; B[j]) { delta = (i - i_low + 1) \u0026gt;\u0026gt; 1; j_low = j; j += delta; i -= delta; } else if (j \u0026gt; 0 \u0026amp;\u0026amp; i \u0026lt; m \u0026amp;\u0026amp; B[j - 1] \u0026gt; A[i]) { delta = (j - j_low + 1) \u0026gt;\u0026gt; 1; i_low = i; i += delta; j -= delta; } else { // Found the correct position for C[k] active = false; } return i; } } 12.5 A Basic Parallel Merge Kernel åœ¨å‰©ä¸‹çš„å°èŠ‚é‡Œï¼Œæˆ‘ä»¬å‡è®¾è¾“å…¥æ•°ç»„ A å’Œ B å­˜å‚¨åœ¨å…¨å±€å†…å­˜ä¸­ï¼Œä¸€ä¸ªå†…æ ¸è¢«å¯åŠ¨ç”¨æ¥åˆå¹¶ä¸¤ä¸ªè¾“å…¥æ•°ç»„ï¼Œè¾“å‡ºä¸€ä¸ªåŒæ ·ä½äºå…¨å±€å†…å­˜ä¸­çš„æ•°ç»„ C. ä¸‹é¢å†…æ ¸æ˜¯å¹¶è¡Œå½’å¹¶çš„ç›´æ¥å®ç°ã€‚å®ƒé¦–å…ˆé€šè¿‡è®¡ç®—å½“å‰çº¿ç¨‹ (k_curr) å’Œä¸‹ä¸€ä¸ªçº¿ç¨‹ (k_next) äº§ç”Ÿçš„è¾“å‡ºå­æ•°ç»„çš„èµ·ç‚¹æ¥ç¡®å®šè´Ÿè´£è¾“å‡ºçš„èŒƒå›´ã€‚ç„¶ååˆ†åˆ«è°ƒç”¨è‡ªå·±å’Œåä¸€ä¸ªçº¿ç¨‹çš„ co_rank å‡½æ•°æ¥ç¡®å®šå¯¹åº”çš„ A å’Œ B è¾“å…¥å­æ•°ç»„çš„èŒƒå›´ã€‚æœ€åè°ƒç”¨é¡ºåºåˆå¹¶å‡½æ•°æ¥åˆå¹¶ä¸¤ä¸ªè¾“å…¥å­æ•°ç»„ï¼Œå¹¶å°†ç»“æœå†™å…¥è¾“å‡ºå­æ•°ç»„ã€‚\n__global__ void mergre_basic_kernel(int* A, int* B, int* C, int m, int n) { // Each thread handles a section of C int tid = blockIdx.x * blockDim.x + threadIdx.x; int elementsPerThread = ceil(m + n) / (blockDim.x * gridDim.x); int start = tid * elementsPerThread; int end = std::min(start + elementsPerThread, m + n); // Determin the range of A and B to be merged for this thread int i_curr = co_rank(start, A, m, B, n); int i_next = co_rank(end, A, m, B, n); int j_curr = start - i_curr; int j_next = end - i_next; merge_sequential(A + i_curr, B + j_curr, C + start, i_next - i_curr, j_next - j_curr); } ä¸Šé¢çš„åŸºæœ¬å½’å¹¶å†…æ ¸æœ‰ 2 ä¸ªé—®é¢˜ï¼š\nwarp ä¸­çš„ç›¸é‚»çº¿ç¨‹åœ¨è¯»å†™è¾“å…¥å’Œè¾“å‡ºå­æ•°ç»„å…ƒç´ æ—¶ä¸ä¼šè®¿é—®ç›¸é‚»çš„å†…å­˜ä½ç½®ã€‚ çº¿ç¨‹åœ¨æ‰§è¡Œ co-rank å‡½æ•°æ—¶è¿˜éœ€è¦ä»å…¨å±€å†…å­˜è®¿é—® A å’Œ B çš„å…ƒç´ ã€‚ 12.6 A Tiled Merge Kernel to Improve Coalescing æ³¨æ„åˆ°ç›¸é‚»çº¿ç¨‹ä½¿ç”¨çš„ A å’Œ B å­æ•°ç»„åœ¨å†…å­˜ä¸­å½¼æ­¤ç›¸é‚»ã€‚æˆ‘ä»¬å¯ä»¥ä¸ºä¸ºæ¯ä¸ªå—è°ƒç”¨ co-rank å‡½æ•°æ¥è·å¾—å…¶ A å’Œ B å­æ•°ç»„çš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚ Info\nå›å¿†ä¸€ä¸‹æ”¹è¿›å†…æ ¸å†…å­˜åˆå¹¶çš„ä¸»è¦ç­–ç•¥æœ‰ä¸‰ç§:\né‡æ–°ç»„ç»‡çº¿ç¨‹åˆ°æ•°æ®çš„æ˜ å°„ã€‚ é‡æ–°ç»„ç»‡æ•°æ®æœ¬èº«ã€‚ ä»¥åˆå¹¶çš„æ–¹å¼åœ¨å…¨å±€å†…å­˜å’Œå…±äº«å†…å­˜ä¹‹é—´ä¼ è¾“æ•°æ®ï¼Œå¹¶åœ¨å…±äº«å†…å­˜ä¸­æ‰§è¡Œä¸è§„åˆ™è®¿é—®ã€‚ ä¸‹å›¾å±•ç¤ºäº†åˆ†æ®µåˆå¹¶å†…æ ¸çš„å—çº§åˆ«è®¾è®¡ã€‚A_S å’Œ B_S å¯èƒ½æ— æ³•è¦†ç›–å—çš„æ•´ä¸ªè¾“å…¥å­æ•°ç»„ï¼Œå› æ­¤åœ¨æ¯æ¬¡è¿­ä»£æœŸé—´ï¼Œå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹å°†åä½œä»å—çš„ A å’Œ B å­æ•°ç»„ä¸­åŠ è½½ x ä¸ªå…ƒç´ ã€‚è¿™æ ·æ¯ä¸ªå—æœ‰è¶³å¤Ÿçš„è¾“å…¥å…ƒç´ æ¥ç”Ÿæˆè‡³å°‘ x ä¸ªè¾“å‡ºæ•°ç»„å…ƒç´  (åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œå½“å‰è¾“å‡ºéƒ¨åˆ†çš„æ‰€æœ‰å…ƒç´ å¯èƒ½éƒ½æ¥è‡ª A æˆ– B çš„å­æ•°ç»„)ã€‚å‡è®¾æ¯ä¸ªå—è´Ÿè´£ y ä¸ªè¾“å‡ºå…ƒç´ ï¼Œåˆ™éœ€è¦è¿›è¡Œ y/x æ¬¡å½’å¹¶ã€‚æ¯ä¸ªå—ä¸­çš„çº¿ç¨‹å°†åœ¨æ¯æ¬¡è¿­ä»£ä¸­ä½¿ç”¨ A_S çš„ä¸€éƒ¨åˆ†å’Œ B_S çš„ä¸€éƒ¨åˆ† (æ·±ç°è‰²éƒ¨åˆ†)\nDesign of a Tiled Merge Kernel\nä¸‹é¢æ˜¯åˆ†æ®µåˆå¹¶å†…æ ¸çš„å®ç°çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚æœ¬è´¨ä¸Šæ˜¯çº¿ç¨‹çº§åŸºæœ¬åˆå¹¶å†…æ ¸çš„å—çº§ç‰ˆæœ¬çš„ä»£ç ã€‚æ¯ä¸ªå—çš„ç¬¬ä¸€ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®—å½“å‰å—å’Œä¸‹ä¸€ä¸ªå—çš„å¼€å§‹è¾“å‡ºç´¢å¼•çš„ä½ç½®ä»¥åŠä»–ä»¬çš„ co-rank. ç»“æœè¢«æ”¾å…¥å…±äº«å†…å­˜ä¸­ï¼Œä»¥ä¾¿å—ä¸­çš„æ‰€æœ‰çº¿ç¨‹éƒ½å¯ä»¥çœ‹åˆ°ã€‚\n__global__ void merge_tiled_kernel(int* A, int* B, int* C, int m, int n, int tile_size) { /* Part 1: Identify block-level output \u0026amp; input subarrays */ // Use extern keywords to determine // the shared memory size at runtime rather than compilation extern __shared__ int shared_AB[]; int* A_s = \u0026amp;shared_AB[0]; // Start index of ShareA int* B_s = \u0026amp;shared_AB[tile_size]; // Start index of ShareB int C_curr = blockIdx.x * ceil((m + n) / gridDim.x); // Start index of C for this block int C_next = std::min(C_curr + int(ceil((m + n) / gridDim.x)), m + n); // End index of C for this block if (threadIdx.x == 0) { A_s[0] = co_rank(C_curr, A, m, B, n); // Make block level co-rank values visible A_s[1] = co_rank(C_next, A, m, B, n); // Next threads co-rank values in the block } __synctyhreads(); int A_curr = A_s[0]; int A_next = A_s[1]; int B_curr = C_curr - A_curr; int B_next = C_next - A_next; ç¬¬äºŒéƒ¨åˆ†çº¿ç¨‹ä½¿ç”¨å®ƒä»¬çš„ threadIdx.x çš„å€¼æ¥ç¡®å®šè¦åŠ è½½çš„å…ƒç´ ï¼Œå› æ­¤è¿ç»­çš„çº¿ç¨‹åŠ è½½è¿ç»­çš„å…ƒç´ ï¼Œå†…å­˜è®¿é—®æ˜¯åˆå¹¶çš„ã€‚æ¯æ¬¡è¿­ä»£ä» A å’Œ B æ•°ç»„ä¸­åŠ è½½å½“å‰tileçš„èµ·å§‹ç‚¹å–å†³äºå—çš„æ‰€æœ‰çº¿ç¨‹åœ¨ä¹‹å‰çš„è¿­ä»£ä¸­æ¶ˆè€—çš„ A å’Œ B å…ƒç´ çš„æ€»æ•°ã€‚ä¸‹å›¾è¯´æ˜äº† while å¾ªç¯ç¬¬äºŒæ¬¡è¿­ä»£çš„ç´¢å¼•è®¡ç®—ã€‚æ¯ä¸ªå—åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­æ¶ˆè€—çš„ A å…ƒç´ éƒ¨åˆ† ä¸º A å­æ•°ç»„å¼€å¤´çš„ç™½è‰²å°éƒ¨åˆ† (ç”¨ç«–æ¡æ ‡è®°)ã€‚if è¯­å¥ç¡®ä¿çº¿ç¨‹åªåŠ è½½ A å­æ•°ç»„å‰©ä½™éƒ¨åˆ†ä¸­çš„å…ƒç´ ã€‚\n/* Part 2: Loading A \u0026amp; B elements into the shared memory */ int counter = 0; int lenC = C_next - C_curr; int lenA = A_next - A_curr; int lenB = B_next - B_curr; int num_iterations = ceil(lenC / tile_size); // index of completed merge in int C_completed = 0; int A_completed = 0; int B_completed = 0; while (counter \u0026lt; num_iterations) { // Each iter threads in a block will generate tile_size C elements // Loading tile_size A and B elements into shared memory for (int i = 0; i \u0026lt; tile_size; i += blockDim.x) { // Coalecsing loading from global memory if (i + threadIdx.x \u0026lt; lenA - A_completed) { A_s[i + threadIdx.x] = A[i + threadIdx.x + A_curr + A_completed]; } if (i + threadIdx.x \u0026lt; lenB - B_completed) { B_s[i + threadIdx.x] = B[i + threadIdx.x + B_curr + B_completed]; } } __syncthreads(); ç¬¬ä¸‰éƒ¨åˆ†åˆ™æ˜¯æ¯ä¸ªå—çš„çº¿ç¨‹å¯¹å…±äº«å†…å­˜çš„æ•°ç»„è¿›è¡Œå½’å¹¶ã€‚åœ¨æ›´æ–°ç´¢å¼•çš„éƒ¨åˆ†ä¸­æœ€åä¸€æ¬¡è¿­ä»£ä¸­ A_s å’Œ B_s å¯èƒ½æ²¡æœ‰ tile_size ä¸ªå…ƒç´ ï¼Œè°ƒç”¨ co-rank å¯èƒ½ä¼šå¾—åˆ°é”™è¯¯ç»“æœã€‚ä½†æ˜¯ï¼Œç”±äº while å¾ªç¯ä¸ä¼šè¿›ä¸€æ­¥è¿­ä»£ï¼Œå› æ­¤ä¸ä¼šä½¿ç”¨ç»“æœï¼Œå› æ­¤ä¸ä¼šé€ æˆä»»ä½•å½±å“ã€‚\n/* Part 3: All threads merge their subarrays in prallel */ int c_curr = threadIdx.x * (tile_size / blockDim.x); // Output index in shared memory int c_next = c_curr + (tile_size / blockDim.x); c_curr = (c_curr \u0026lt;= lenC - C_completed) ? c_curr : lenC - C_completed; c_next = (c_next \u0026lt;= lenC - C_completed) ? c_next : lenC - C_completed; // find co-rank for c_curr and c_next int a_curr = co_rank(c_curr, A_s, std::min(tile_size, lenA - A_completed), B_s, std::min(tile_size, lenB - B_completed)); int b_curr = c_curr - a_curr; int a_next = co_rank(c_next, A_s, std::min(tile_size, lenA - A_completed), B_s, std::min(tile_size, lenB - B_completed)); int b_next = c_next - a_next; // merge the subarrays merge_sequential(A_s + a_curr, B_s + b_curr, C + C_curr + C_completed + c_curr, a_next - a_curr, b_next - b_curr); // Update completed indices C_completed += tile_size; A_completed += co_rank(tile_size, A_s, tile_size, B_s, tile_size); // Idx of A_s to generate tile_size Idx of merged A_s and B_s B_completed += tile_size - A_completed; } } 12.7 A Circular Buffer Merge Kernel ä¸Šä¸€èŠ‚çš„å†…æ ¸ä¸æ˜¯é‚£ä¹ˆé«˜æ•ˆå› ä¸ºä¸‹ä¸€æ¬¡è¿­ä»£ tile çš„ä¸€éƒ¨åˆ†å·²ç»è¢«åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­ï¼Œä½†æ˜¯æˆ‘ä»¬æ¯æ¬¡è¿­ä»£ä»å…¨å±€å†…å­˜ä¸­é‡æ–°åŠ è½½æ•´ä¸ªå—ï¼Œå¹¶è¦†ç›–æ‰å‰ä¸€æ¬¡è¿­ä»£ä¸­çš„è¿™äº›å…ƒç´ ã€‚ä¸‹å›¾å±•ç¤ºäº† merge_circular_buffer_kernel çš„ä¸»è¦æ€æƒ³ï¼Œæ·»åŠ äº†ä¸¤ä¸ªé¢å¤–çš„å˜é‡ A_S_start å’ŒB_S_startï¼Œä½¿å¾— while å¾ªç¯çš„æ¯æ¬¡è¿­ä»£åŠ¨æ€ç¡®å®šä» A å’Œ B çš„å“ªä¸ªä½ç½®å¼€å§‹åŠ è½½ï¼Œè¿™æ ·å¯ä»¥åˆ©ç”¨å‰ä¸€æ¬¡è¿­ä»£ä¸­å‰©ä½™çš„ A_s å’Œ B_s å…ƒç´ ã€‚ä¿®æ”¹åæ¯ä¸ª for å¾ªç¯éƒ½åªåŠ è½½ A_S_consumed è¡¨ç¤ºçš„å¡«å…… tile æ‰€éœ€çš„å…ƒç´ æ•°é‡ã€‚å› æ­¤ï¼Œçº¿ç¨‹åœ¨ç¬¬ i æ¬¡ for å¾ªç¯è¿­ä»£ä¸­åŠ è½½çš„A å…ƒç´ æ˜¯ A[A_curr+A_S_consumed+i+threadIdx.x]. å–æ¨¡(%) æ“ä½œæ£€æŸ¥ç´¢å¼•å€¼æ˜¯å¦å¤§äºæˆ–ç­‰äº tile_size.\n!A Circular Buffer Scheme for Managing the Shared Memory Tiles\n12.8 Thread Coarsening for Merge å¤šä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œå½’å¹¶çš„ä»£ä»·æ˜¯æ¯ä¸ªçº¿ç¨‹å¿…é¡»æ‰§è¡Œè‡ªå·±çš„äºŒè¿›åˆ¶æœç´¢æ“ä½œæ¥è¯†åˆ«å…¶è¾“å‡ºç´¢å¼•çš„ co-rank. æœ¬ç« ä¸­ä»‹ç»çš„æ‰€æœ‰å†…æ ¸éƒ½å·²ç»åº”ç”¨äº†çº¿ç¨‹ç²—åŒ–ï¼Œå› ä¸ºå®ƒä»¬éƒ½æ˜¯ä¸ºæ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªå…ƒç´ è€Œè®¾è®¡çš„ã€‚åœ¨å®Œå…¨æœªç²—åŒ–çš„å†…æ ¸ä¸­ï¼Œæ¯ä¸ªçº¿ç¨‹å°†è´Ÿè´£å•ä¸ªè¾“å‡ºå…ƒç´ ã€‚\n","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch12/","summary":"Personal notebook 12 of Programming Massively Parallel","title":"PMPP Learning-Chapter 12 Merge-An Introduction to Dynamic Input Data Identification"},{"content":"11 Prefix sum (scan)-An Introduction to Work Efficiency in Parallel Algorithms ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœè®¡ç®—æœ¬è´¨ä¸Šå¯ä»¥è¢«æè¿°ä¸ºæ•°å­¦é€’å½’ï¼Œå³åºåˆ—ä¸­çš„æ¯ä¸€é¡¹éƒ½æ˜¯æ ¹æ®å‰ä¸€é¡¹å®šä¹‰çš„ï¼Œé‚£ä¹ˆå®ƒå¯èƒ½è¢«å¹¶è¡ŒåŒ–ä¸ºå¹¶è¡Œæ‰«æ (parallel scan) è¿ç®—ã€‚\n11.1 Background åŒ…å«æ‰«æ (inclusive scan) æ“ä½œæ¥æ”¶ä¸€ä¸ªäºŒå…ƒå¯äº¤æ¢è¿ç®—ç¬¦ $\\oplus$ å’Œä¸€ä¸ªåŒ…å« n ä¸ªå…ƒç´ çš„è¾“å…¥æ•°ç»„ $[x_0,x_1,\\ldots,x_{n-1}]$ï¼Œè¾“å‡ºæ•°ç»„ $[x_0,(x_0\\oplus x_1),\\ldots,(x_0\\oplus x_1\\oplus\\ldots\\oplus x_{n-1})]$ . åŒ…å«æ‰«æçš„åç§°ä½“ç°åœ¨è¾“å‡ºæ•°ç»„æ¯ä¸ªä½ç½®çš„ç»“æœéƒ½æœ‰å¯¹åº”è¾“å…¥å…ƒç´ å‚ä¸ã€‚è€ƒè™‘åŒ…å«æ‰«æçš„ä¸€ç§ç›´è§‚æ–¹å¼æ˜¯ï¼Œæ¥æ”¶ä¸€ç»„æ‰€éœ€é¦™è‚ çš„é•¿åº¦çš„è®¢å•ï¼Œå¹¶ä¸€æ¬¡æ€§å¾—å‡ºæ‰€æœ‰æ‰€æœ‰è®¢å•å¯¹åº”çš„åˆ‡å‰²ç‚¹ã€‚ æ’é™¤æ‰«ææ“ä½œç±»ä¼¼äºåŒ…å«æ‰«ææ“ä½œï¼Œåªæ˜¯è¾“å‡ºæ•°ç»„çš„æ’åˆ—ç•¥æœ‰ä¸åŒ: $[i,x_0,(x_0\\oplus x_1),\\ldots,(x_0\\oplus x_1\\oplus\\ldots\\oplus x_{n-2})]$ . æ¯ä¸ªè¾“å‡ºå…ƒç´ çš„è®¡ç®—éƒ½ä¸ç›¸åº”è¾“å…¥å…ƒç´ æ— å…³ã€‚ ç”¨åŒ…å«æ‰«æå‡½æ•°è®¡ç®—æ’é™¤æ‰«æçš„ç»“æœæ—¶ï¼Œåªéœ€å°†æ‰€æœ‰å…ƒç´ å‘å³ç§»åŠ¨ï¼Œå¹¶ä¸ºç¬¬ 0 ä¸ªå…ƒç´ å¡«å……æ’ç­‰å€¼ã€‚åä¹‹ï¼Œåªéœ€è¦å°†æ‰€æœ‰å…ƒç´ å‘å·¦ç§»åŠ¨ï¼Œå¹¶ç”¨æ’é™¤æ‰«æç»“æœçš„æœ€åä¸€ä¸ªå…ƒç´  $\\oplus$ æœ€åä¸€ä¸ªè¾“å…¥å…ƒç´ æ¥å¡«å……æœ€åä¸€ä¸ªå…ƒç´ ã€‚\n11.2 Parallel Scan with the Kogge-Stone Algorithm è®¡ç®—ä½ç½® i çš„è¾“å‡ºå…ƒç´  éœ€è¦è¿›è¡Œ i æ¬¡åŠ æ³•è¿ç®—ï¼Œå› æ­¤é™¤éæ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥å…±äº«ä¸åŒè¾“å‡ºå…ƒç´ çš„å½’çº¦æ ‘çš„éƒ¨åˆ†å’Œï¼Œå¦åˆ™è¿™ç§æ–¹æ³•è®¡ç®—å¤æ‚åº¦ä¸º $O(N^2)$. Kogge-Stone ç®—æ³•æœ€åˆæ˜¯ä¸ºäº†è®¾è®¡å¿«é€ŸåŠ æ³•å™¨ç”µè·¯è€Œå‘æ˜çš„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå®ƒæ˜¯ä¸€ç§å°±åœ°æ‰«æç®—æ³•ï¼Œå®ƒå¯¹æœ€åˆåŒ…å«è¾“å…¥å…ƒç´ çš„æ•°ç»„ XY è¿›è¡Œæ“ä½œã€‚ç»è¿‡ k æ¬¡è¿­ä»£åï¼ŒXY[i] å°†åŒ…å«åœ¨è¯¥ä½ç½®åŠä¹‹å‰çš„æœ€å¤š 2^k ä¸ªè¾“å…¥å…ƒç´ çš„å’Œã€‚\nA Parallel Inclusive Scan Algorithm Based on Kogge-Stone Adder Design\nå¯¹åº”çš„å†…æ ¸å‡½æ•°å¦‚ä¸‹ï¼Œå‡è®¾è¾“å…¥æœ€åˆä½äºå…¨å±€å†…å­˜æ•°ç»„ X ä¸­ã€‚è®©æ¯ä¸ªçº¿ç¨‹è®¡ç®—å…¶å…¨å±€æ•°æ®ç´¢å¼•ï¼Œå³å…¶è´Ÿè´£è®¡ç®—è¾“å‡ºæ•°ç»„çš„ä½ç½®ã€‚æ¯ä¸ªä¸ªæ´»åŠ¨çº¿ç¨‹é¦–å…ˆå°†å…¶ä½ç½®çš„éƒ¨åˆ†å’Œå­˜å‚¨åˆ°ä¸€ä¸ªä¸´æ—¶å˜é‡ä¸­(åœ¨å¯„å­˜å™¨ä¸­)ã€‚å½“æ­¥å¹…å€¼å¤§äº threadIdx.x æ—¶ï¼Œæ„å‘³ç€çº¿ç¨‹åˆ†é…çš„ XY ä½ç½®å·²ç»ç´¯åŠ äº†æ‰€æœ‰æ‰€éœ€çš„è¾“å…¥å€¼ï¼Œé€€å‡ºæ´»åŠ¨çŠ¶æ€ã€‚éœ€è¦é¢å¤–çš„ temp å’Œ __syncthreads() å› ä¸ºæ›´æ–°ä¸­å­˜åœ¨è¯»åå†™æ•°æ®ä¾èµ–ç«äº‰å…³ç³»ã€‚\n#define SECTION_SIZE 32 __global__ void Kogge_Stone_Scan_Kernel(int* X, int* Y, unsigned int N) { __shared__ float XY[SECTION_SIZE]; unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; /* Exclusive kernel initilization if (i \u0026lt; N \u0026amp;\u0026amp; threadIdx.x != 0) { XY[threadIdx.x] = X[i]; } else { XY[threadIdx.x] = 0.0f; } */ if (i \u0026lt; N) { XY[threadIdx.x] = X[i]; } else { XY[threadIdx.x] = 0.0f; } for (unsigned stride = 1; stride \u0026lt; blockDim.x; stride *= 2) { __syncthreads(); float temp; if (threadIdx.x \u0026gt;= stride) { temp = XY[threadIdx.x] + XY[threadIdx.x - stride]; } __syncthreads(); // write-after-read dependence if (threadIdx.x \u0026gt;= stride) { // Only N - stride threads are active XY[threadIdx.x] = temp; } } if (i \u0026lt; N) { Y[i] = XY[threadIdx.x]; } } Kogge-Stone ç®—æ³•é‡ç”¨äº†æ¨ªè·¨å½’çº¦æ ‘çš„éƒ¨åˆ†å’Œæ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚åœ¨ä¸Šä¸€ç« çš„å½’çº¦å†…æ ¸ä¸­ï¼Œæ´»åŠ¨çº¿ç¨‹åœ¨è¿­ä»£ä¸­å†™å…¥çš„å…ƒç´ ä¸ä¼šåœ¨åŒä¸€è¿­ä»£ä¸­è¢«ä»»ä½•å…¶ä»–æ´»åŠ¨çº¿ç¨‹è¯»å–ï¼Œå› æ­¤ä¸å­˜åœ¨è¯»åå†™ç«äº‰æ¡ä»¶ã€‚å¦‚æœå¸Œæœ›é¿å…åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½æœ‰ barrier åŒæ­¥ï¼Œé‚£ä¹ˆå…‹æœç«äº‰æ¡ä»¶çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä¸ºè¾“å…¥å’Œè¾“å‡ºä½¿ç”¨å•ç‹¬çš„æ•°ç»„ã€‚è¿™ç§æ–¹æ³•éœ€è¦ä¸¤ä¸ªå…±äº«å†…å­˜ç¼“å†²åŒºã€‚äº¤æ›¿å˜åŒ–ä¸èƒ½è¾“å…¥/è¾“å‡ºç¼“å†²åŒºçš„è§’è‰²ï¼Œç›´åˆ°è¿­ä»£å®Œæˆã€‚è¿™ç§ä¼˜åŒ–ç§°ä¸ºåŒç¼“å†² (double buffering).\n#define SECTION_SIZE 32 __global__ void DF_Kogge_Stone_Scan_Kernel(int* X, int* Y, unsigned int N) { __shared__ float XY_in[SECTION_SIZE]; __shared__ float XY_out[SECTION_SIZE]; unsigned int i = blockIdx.x * blockDim.x + threadIdx.x; // Initialization if (i \u0026lt; N) { XY_in[threadIdx.x] = X[i]; } else { XY_in[threadIdx.x] = 0.0f; } bool read_in = true; // Alternating ther role of XY_in and XY_out for (unsigned stride = 1; stride \u0026lt; blockDim.x; stride *= 2) { if (read_in) { if (threadIdx.x \u0026gt;= stride) { XY_out[threadIdx.x] = XY_in[threadIdx.x] + XY_in[threadIdx.x - stride]; } else { XY_out[threadIdx.x] = XY_in[threadIdx.x]; } } else { if (threadIdx.x \u0026gt;= stride) { XY_in[threadIdx.x] = XY_out[threadIdx.x] + XY_out[threadIdx.x - stride]; } else { XY_in[threadIdx.x] = XY_out[threadIdx.x]; } } read_in = !read_in; // åˆ‡æ¢æ•°ç»„ } // å°†ç»“æœå†™å›å…¨å±€å†…å­˜ if (i \u0026lt; N) { if (read_in) { Y[i] = XY_in[threadIdx.x]; } else { Y[i] = XY_out[threadIdx.x]; } } } 11.3 Speed and Work Efficiency Consideration ç®—æ³•çš„å·¥ä½œæ•ˆç‡ï¼ˆwork efficiencyï¼‰æ˜¯æŒ‡ç®—æ³•æ‰€å®Œæˆçš„å·¥ä½œæ¥è¿‘äºè®¡ç®—æ‰€éœ€çš„æœ€å°å·¥ä½œé‡çš„ç¨‹åº¦ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œéæ´»åŠ¨çº¿ç¨‹çš„æ•°é‡ç­‰äºæ­¥é•¿ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºå·¥ä½œé‡ä¸º\n$$\r\\sum_{stride}(N-\\mathrm{stride}), \\text{for strides} 1, 2, 4, \\ldots N/2(\\mathrm{log}_2N \\text{terms}) = N\\log_2N - (N-1)\r$$å› æ­¤ï¼ŒKogge-Stone ç®—æ³•çš„è®¡ç®—å¤æ‚åº¦ä¸º $O(N\\log_2N)$.\nä½¿ç”¨è®¡ç®—æ­¥æ•° (compute steps) çš„æ¦‚å¿µä½œä¸ºæ¯”è¾ƒæ‰«æç®—æ³•çš„è¿‘ä¼¼æŒ‡æ ‡ã€‚é¡ºåºæ‰«æç”¨ N-1 æ­¥æ¥å¤„ç† N ä¸ªè¾“å…¥å…ƒç´ ï¼›è‹¥ CUDA è®¾å¤‡æœ‰ P ä¸ªæ‰§è¡Œå•å…ƒï¼ŒKogge-Stone å†…æ ¸æ‰§è¡Œéœ€è¦æ­¥æ•°ä¸º $O(N\\log_2N)/P$. Kogge-Stone å†…æ ¸ç›¸æ¯”ä¸²è¡Œä»£ç æ‰€åšçš„é¢å¤–å·¥ä½œæœ‰ä¸¤ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œä½¿ç”¨ç¡¬ä»¶æ‰§è¡Œå¹¶è¡Œå†…æ ¸çš„æ•ˆç‡è¦ä½å¾—å¤šã€‚ç¬¬äºŒï¼Œæ‰€æœ‰é¢å¤–çš„å·¥ä½œæ¶ˆè€—é¢å¤–çš„èƒ½é‡ï¼Œä¸åˆ©äºç§»åŠ¨åº”ç”¨ç­‰åœºæ™¯ã€‚Kogge-Stone å†…æ ¸çš„å¼ºå¤§ä¹‹å¤„åœ¨äºï¼Œå½“æœ‰è¶³å¤Ÿçš„ç¡¬ä»¶èµ„æºæ—¶ï¼Œå®ƒå¯ä»¥è¾¾åˆ°éå¸¸å¥½çš„æ‰§è¡Œé€Ÿåº¦ã€‚\n11.4 Parallel Scan with the Brent-Kung Algorithm å¯¹ä¸€ç»„å€¼è¿›è¡Œæ±‚å’Œæœ€å¿«çš„æ–¹æ³•æ˜¯ä½¿ç”¨å½’çº¦æ ‘ï¼Œå¦‚æœæœ‰è¶³å¤Ÿçš„æ‰§è¡Œå•å…ƒï¼Œå°±å¯ä»¥åœ¨ $O(N\\log_2N)$ æ—¶é—´å†…è®¡ç®— N ä¸ªå€¼çš„æ±‚å’Œç»“æœã€‚è¯¥æ ‘è¿˜å¯ä»¥ç”Ÿæˆå‡ ä¸ªå­åºåˆ—çš„å’Œï¼Œå®ƒä»¬å¯ç”¨äºè®¡ç®—æŸäº›æ‰«æè¾“å‡ºå€¼ã€‚ ä¸‹å›¾å±•ç¤ºäº†åŸºäº Brent-Kung åŠ æ³•å™¨è®¾è®¡çš„å¹¶è¡ŒåŒ…å«æ‰«æç®—æ³•çš„æ­¥éª¤ã€‚å›¾ä¸­ä¸ŠåŠéƒ¨åˆ†ï¼ŒèŠ± 4 æ­¥è®¡ç®—æ‰€æœ‰ 16 ä¸ªå…ƒç´ çš„å’Œã€‚ä¸‹åŠéƒ¨åˆ†æ˜¯ä½¿ç”¨åå‘æ ‘å°†éƒ¨åˆ†å’Œåˆ†é…åˆ°å¯ä»¥ä½¿ç”¨éƒ¨åˆ†å’Œçš„ä½ç½®ï¼Œä»¥è®¡ç®—è¿™äº›ä½ç½®çš„ç»“æœã€‚çº¦ç®€æ ‘ä¸­çš„æ±‚å’Œæ€»æ˜¯åœ¨å¯¹ä¸€ä¸ªè¿ç»­çš„èŒƒå›´å†…çš„è¾“å…¥å…ƒç´ è¿›è¡Œã€‚å› æ­¤ï¼Œæ±‚å’Œç´¯ç§¯åˆ° XY çš„æ¯ä¸ªä½ç½®çš„å€¼æ€»æ˜¯å¯ä»¥è¡¨ç¤ºä¸ºè¾“å…¥å…ƒç´ çš„ä¸€ä¸ª xiâ€¦xj çš„èŒƒå›´ï¼Œå…¶ä¸­ xi æ˜¯å¼€å§‹ä½ç½®ï¼Œ xj æ˜¯ç»“æŸä½ç½® (åŒ…æ‹¬)ã€‚\nA Parallel Inclusive Scan Algorithm Based on the Brentâ€“Kung Adder Design\nä¸‹å›¾å±•ç¤ºäº†åå‘æ ‘ä¸­æ¯ä¸ªä½ç½® (åˆ—) çš„çŠ¶æ€ï¼ŒåŒ…æ‹¬å·²ç»ç´¯ç§¯åˆ°è¯¥ä½ç½®çš„å€¼ä»¥åŠåœ¨åå‘æ ‘çš„æ¯çº§ (è¡Œ) ä¸Šéœ€è¦çš„é¢å¤–è¾“å…¥å…ƒç´ å€¼ (æµ…ç°è‰²è¡¨ç¤º 2ï¼Œæ·±ç°è‰²è¡¨ç¤º 1ï¼Œé»‘è‰²è¡¨ç¤º 0).\nProgression of Values in XY After Each Level of Additions in the Reverse Tree.\nä¸ŠåŠéƒ¨åˆ†å½’çº¦æ ‘çš„å†…æ ¸ä»£ç å¦‚ä¸‹ï¼Œå’Œç¬¬åç« ä¸åŒçš„æ˜¯\næˆ‘ä»¬æŠŠæ±‚å’Œç»“æœå†™åˆ°æœ€å¤§ç´¢å¼•çš„ä½ç½®ã€‚ æˆ‘ä»¬å°†çº¿ç¨‹ç´¢å¼•ç»„ç»‡æˆ $2^n-1$ çš„å½¢å¼ (n ä¸ºæ ‘çš„é«˜åº¦)ã€‚ for (unsigned int stride = 1; stride \u0026lt; blockDim.x; stride *= 2) { __syncthreads(); if ((threadIdx.x + 1) % (2 * stride) == 0) { XY[threadIdx.x] += XY[threadIdx.x - stride]; } } è¿™ç§å½’çº¦æ–¹å¼çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯å­˜åœ¨æ§åˆ¶å‘æ•£é—®é¢˜ã€‚å› æ­¤éœ€è¦å°†çº¿ç¨‹çš„è¿ç»­éƒ¨åˆ†æ˜ å°„åˆ°ç´¢å¼•ä¸º $k*2^n-1$ å½¢å¼çš„ XY ä½ç½®ã€‚\n// Mapping a continous section of threads to the XY positions for (unsigned int stride = 1; stride \u0026lt;= blockDim.x; stride *= 2) { __syncthreads(); unsigned int index = (threadIdx.x + 1) * 2 * stride - 1; // index of the left child if (index \u0026lt; SECTION_SIZE) { XY[index] += XY[index - stride]; } } åå‘æ ‘çš„å®ç°è¦å¤æ‚ä¸€äº›ã€‚æ­¥é•¿ä» SECTION_SIZE/4 å‡å°åˆ° 1. åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°† XY å…ƒç´ ç´¢å¼•å€¼ä»æ­¥é•¿å‡å» 1 åçš„ä¸¤å€çš„ä½ç½®å‘å³æ¨åˆ°è·ç¦»å…¶ä¸€ä¸ªæ­¥é•¿çš„ä½ç½®ã€‚\n// Reverse tree stride value decreases from SECTION_SIZE / 4 to 1 for (unsigned int stride = SECTION_SIZE / 4; stride \u0026gt; 0; stride /= 2) { __syncthreads(); unsigned int index = (threadIdx.x + 1) * 2 * stride - 1; // index of the left child if (index + stride \u0026lt; SECTION_SIZE) { XY[index + stride] += XY[index]; } } æˆ‘ä»¬å¯ä»¥çœ‹åˆ° Brent-Kung ç®—æ³•æ— è®ºåœ¨å½’çº¦é˜¶æ®µè¿˜æ˜¯åˆ†å‘é˜¶æ®µï¼Œéƒ½ä¸éœ€è¦è¶…è¿‡ SECTION_SIZE/2 çš„çº¿ç¨‹ã€‚å¹¶è¡Œæ‰«æä¸­çš„è¿ç®—æ€»æ•°ï¼ŒåŒ…æ‹¬å½’çº¦æ ‘ (N-1 æ¬¡) å’Œåå‘æ ‘ ( $N-1-log_2N$ æ¬¡) é˜¶æ®µï¼Œæ€»å…± $2N-2-log_2N$ æ¬¡ã€‚å½“è¾“å…¥é•¿åº¦å˜å¤§æ—¶ï¼ŒBrent-Kung ç®—æ³•æ‰§è¡Œçš„æ“ä½œæ•°é‡æ°¸è¿œä¸ä¼šè¶…è¿‡é¡ºåºç®—æ³•æ‰§è¡Œçš„æ“ä½œæ•°é‡çš„ 2 å€ã€‚\nBrent-Kung ç®—æ³•çš„æ´»åŠ¨çº¿ç¨‹çš„æ•°é‡é€šè¿‡å½’çº¦æ ‘æ¯” Kogge-Stone ç®—æ³•ä¸‹é™å¾—å¿«å¾—å¤šã€‚ç„¶è€Œï¼Œä¸€äº›éæ´»åŠ¨çº¿ç¨‹å¯èƒ½ä»ç„¶ä¼šæ¶ˆè€— CUDA è®¾å¤‡ä¸­çš„æ‰§è¡Œèµ„æºï¼Œå› ä¸ºå®ƒä»¬é€šè¿‡ SIMD ç»‘å®šåˆ°å…¶ä»–æ´»åŠ¨çº¿ç¨‹ã€‚è¿™ä½¿å¾—åœ¨ CUDA è®¾å¤‡ä¸Šå‰è€…å·¥ä½œæ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ä¸é‚£ä¹ˆæ˜æ˜¾ã€‚åœ¨æœ‰å……è¶³æ‰§è¡Œèµ„æºçš„æƒ…å†µä¸‹ï¼Œç”±äºéœ€è¦é¢å¤–çš„æ­¥éª¤æ¥æ‰§è¡Œåå‘æ ‘é˜¶æ®µï¼ŒBrent-Kung çš„æ—¶é—´æ˜¯ Kogge-Stone çš„ä¸¤å€ã€‚\n11.5 Coarsening for Even More Work Efficiency å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç²—åŒ–æ‰«æåˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬è®©æ¯ä¸ªçº¿ç¨‹å¯¹å…¶ç›¸é‚»çš„å­çº¿ç¨‹æ‰§è¡Œä¸²è¡Œæ‰«æã€‚éœ€è¦æ³¨æ„å¦‚æœæ¯ä¸ªçº¿ç¨‹é€šè¿‡è®¿é—®å…¨å±€å†…å­˜çš„è¾“å…¥ç›´æ¥æ‰§è¡Œæ‰«æï¼Œåˆ™å®ƒä»¬çš„è®¿é—®ä¸ä¼šåˆå¹¶ã€‚æ‰€ä»¥æˆ‘ä»¬ä»¥åˆå¹¶çš„æ–¹å¼åœ¨å…±äº«å†…å­˜å’Œå…¨å±€å†…å­˜ä¹‹é—´ä¼ è¾“æ•°æ®ï¼Œå¹¶åœ¨å…±äº«å†…å­˜ä¸­æ‰§è¡Œä¸æ˜¯é‚£ä¹ˆå¥½çš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ¯ä¸ªå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹åä½œå¹¶å¯¹ç”±æ¯ä¸ªéƒ¨åˆ†çš„æœ€åä¸€ä¸ªå…ƒç´ ç»„æˆçš„é€»è¾‘æ•°ç»„æ‰§è¡Œæ‰«ææ“ä½œã€‚åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œæ¯ä¸ªçº¿ç¨‹å°†å…¶å‰ä¸€ä¸ªéƒ¨åˆ†çš„æœ€åä¸€ä¸ªå…ƒç´ çš„æ–°å€¼ä¸è‡ªèº«éƒ¨åˆ†é™¤æœ€åä¸€ä¸ªçš„æ‰€æœ‰å…ƒç´ ç›¸åŠ ã€‚å¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹ã€‚\nA Three-phase Parallel Scan for Higher Work Efficiency\n#define CORASE_FACTOR 4 #define SUBSECTION_SIZE (SECTION_SIZE / CORASE_FACTOR) __global__ void Corasened_Scan_Kernel(int* X, int* Y, unsigned int N) { // Partition X into blockDim.x subsections // Load X into shared memory in coalesced fashion __shared__ float XY[SECTION_SIZE]; __shared__ float subXY[SUBSECTION_SIZE]; for (int i = 0; i \u0026lt; SECTION_SIZE; i+= blockDim.x) { XY[threadIdx.x + i] = X[threadIdx.x + i]; } __syncthreads(); // Part 1: Compute prefix sum of each subsection in sequenial for (int i = 1; i \u0026lt; SUBSECTION_SIZE; i++) { XY[threadIdx.x * SUBSECTION_SIZE + i] += XY[threadIdx.x * SUBSECTION_SIZE + i - 1]; } __syncthreads(); // Part 2: Compute prefix sum of the last element of each subsection in parallel unsigned int lastElemId = (blockIdx.x + 1) * blockDim.x * CORASE_FACTOR - 1; subXY[threadIdx.x] = XY[(threadIdx.x + 1) * SUBSECTION_SIZE - 1]; float temp = 0.0f; for (int stride = 1; stride \u0026lt; SUBSECTION_SIZE; stride *= 2) { __syncthreads(); if (threadIdx.x \u0026gt;= stride) { temp = subXY[threadIdx.x] + subXY[threadIdx.x - stride]; } __syncthreads(); if (threadIdx.x \u0026gt;= stride) { subXY[threadIdx.x] = temp; } } __syncthreads(); // Part 3: Add the reduction sum of the previous subsection to the current subsection (except the last element) for (int i = 1; i \u0026lt; SUBSECTION_SIZE - 1; i++) { XY[threadIdx.x * SUBSECTION_SIZE + i] += subXY[threadIdx.x]; } __syncthreads(); // Store back to Y for (int i = 0; i \u0026lt; SECTION_SIZE; i+= blockDim.x) { Y[threadIdx.x + i] = XY[threadIdx.x + i]; } } 11.6 Segmented Parallel Scan for Arbitrary-length Inputs å¯¹äºé•¿åº¦å¾ˆå¤§çš„è¾“å…¥æ•°æ®ï¼Œæˆ‘ä»¬é¦–å…ˆå°†å…¶åˆ’åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œä»¥ä¾¿æ¯ä¸ªéƒ¨åˆ†éƒ½å¯ä»¥æ”¾å…¥æµå¤šå¤„ç†å™¨çš„å…±äº«å†…å­˜ä¸­ï¼Œå¹¶ç”±å•ä¸ªå—è¿›è¡Œå¤„ç†ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç¬¬ä¸€æ­¥åœ¨æ¯ä¸ªå—å†…éƒ¨å…ˆè¿›è¡Œæ‰«æï¼Œå®Œæˆåæ¯ä¸ªæ‰«æå—çš„æœ€åä¸€ä¸ªè¾“å‡ºå…ƒç´ ä¸ºè¯¥æ‰«æå—çš„æ‰€æœ‰è¾“å…¥å…ƒç´ çš„å’Œã€‚ç¬¬äºŒæ­¥å°†æ¯ä¸ªæ‰«æå—çš„æœ€åä¸€ä¸ªç»“æœå…ƒç´ æ”¶é›†åˆ°ä¸€ä¸ªæ•°ç»„ä¸­ï¼Œå¹¶å¯¹è¿™äº›è¾“å‡ºå…ƒç´ æ‰§è¡Œæ‰«æã€‚ç¬¬ä¸‰æ­¥å°†ç¬¬äºŒæ­¥æ‰«æè¾“å‡ºå€¼ä¸å…¶å¯¹åº”æ‰«æå—çš„å€¼ç›¸åŠ ã€‚\nA Hierarchical Scan for Arbitrary Length Inputs\næˆ‘ä»¬å¯ä»¥ç”¨ä¸‰ä¸ªå†…æ ¸å®ç°åˆ†æ®µæ‰«æã€‚ç¬¬ä¸€ä¸ªå†…æ ¸ä¸ 11.5 èŠ‚çš„å†…æ ¸åŸºæœ¬ç›¸åŒï¼Œç¬¬äºŒä¸ªå†…æ ¸åªæ˜¯å•ä¸ªçº¿ç¨‹å—çš„å¹¶è¡Œæ‰«æå†…æ ¸ï¼Œç¬¬ä¸‰ä¸ªå†…æ ¸å°† S æ•°ç»„å’Œ Y æ•°ç»„ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶è¾“å‡ºå†™å› Y.\n11.7 Single-pass Scan for Memory Access Efficiency ","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch11/","summary":"Personal notebook 11 of Programming Massively Parallel","title":"PMPP Learning-Chapter 11 Prefix sum (scan)-An Introduction to Work Efficiency in Parallel Algorithms"},{"content":"10 Reduction and Minimizing Divergence å½’çº¦ (Reduction) æ˜¯ä»è¾“å…¥æ•°ç»„è®¡ç®—å‡ºä¸€ä¸ªæ•°çš„è¿ç®—ã€‚\n10.1 Background å½’çº¦æ˜¯ä»è¾“å…¥æ•°ç»„è®¡ç®—å‡ºä¸€ä¸ªæ•°çš„è¿ç®—ï¼Œé€šå¸¸æ˜¯é€šè¿‡å¯¹æ•°ç»„ä¸­çš„å…ƒç´ è¿›è¡ŒæŸç§äºŒå…ƒè¿ç®—æ¥å®ç°çš„ã€‚å¦‚æœäºŒå…ƒæ“ä½œç¬¦å…·æœ‰å®šä¹‰è‰¯å¥½çš„æ’ç­‰å€¼ (ä¾‹å¦‚åŠ æ³•ä¸­çš„ 0ï¼Œä¹˜æ³•ä¸­çš„ 1)ï¼Œåˆ™å¯ä»¥ä¸ºåŸºäºè¯¥æ“ä½œç¬¦è¿›è¡Œè¿ç®—çš„ä¸€ä¸ªæ•°ç»„ä¸­çš„å€¼å®šä¹‰å½’çº¦æ“ä½œã€‚å¯ä»¥é€šè¿‡é¡ºåºéå†æ•°ç»„çš„æ¯ä¸ªå…ƒç´ æ¥è¿›è¡Œå½’çº¦ã€‚ä¸‹é¢ä¼ªä»£ç ä¸ºè¿ç®—ç¬¦çš„ä¸€èˆ¬å½’çº¦å½¢å¼ï¼Œå®ƒè¢«å®šä¹‰ä¸ºæ¥å—ä¸¤ä¸ªè¾“å…¥å¹¶è¿”å›ä¸€ä¸ªå€¼çš„å‡½æ•°ã€‚\nacc = IDENTITY; for (i = 0; i \u0026lt; n; i++) { acc = Operator(acc, input[i]); } 10.2 Reduction Trees å¹¶è¡Œå½’çº¦çš„åŸºæœ¬æ€æƒ³å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ—¶é—´ç«–ç›´å‘ä¸‹å¢åŠ ï¼Œæ°´å¹³æ–¹å‘ä¸ºçº¿ç¨‹åœ¨æ¯ä¸ªæ—¶é—´ç‚¹å¹¶è¡Œæ‰§è¡Œçš„æ´»åŠ¨ã€‚å¹¶è¡Œçº¦ç®€å‡å®šè¾“å‡ºä¸éšç€è¾“å…¥å€¼è¿›è¡Œè¿ç®—çš„é¡ºåºè€Œæ”¹å˜ (å³å…·æœ‰äº¤æ¢å¾‹)ã€‚\nA Parallel Max Reduction Tree\nä¸Šå›¾ä¸­çš„å¹¶è¡Œå½’çº¦æ¨¡å¼è¢«ç§°ä¸ºå½’çº¦æ ‘ (reduction tree)ï¼Œå› ä¸ºå®ƒçœ‹èµ·æ¥åƒä¸€æ£µå¶å­æ˜¯åŸå§‹è¾“å…¥å…ƒç´ ï¼Œæ ¹æ˜¯æœ€ç»ˆç»“æœçš„æ ‘ã€‚å½’çº¦æ ‘çš„è¾¹æ˜¯æ— å®é™…æ„ä¹‰ï¼Œåªæ˜¯åæ˜ äº†ä»ä¸€ä¸ªæ—¶é—´æ­¥æ‰§è¡Œçš„æ“ä½œåˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥æ‰§è¡Œçš„æ“ä½œçš„ä¿¡æ¯æµã€‚æ‰§è¡Œçš„æ“ä½œæ€»æ•°æ˜¯ä¸€ä¸ªå‡ ä½•çº§æ•° $\\frac{1}{2}N + \\frac{1}{2^2}N + \\cdots + \\frac{1}{N}N = N-1$. å½’çº¦æ ‘éœ€è¦ $log_{2}{N}$ æ­¥éª¤æ¥å®Œæˆã€‚å®Œæˆè®¡ç®—æ‰€éœ€çš„èµ„æºæ•°é‡éšç€æ—¶é—´æ­¥çš„å¢åŠ è€Œè¿…é€Ÿå‡å°‘ï¼Œæ¯ä¸ªæ—¶é—´æ­¥çš„å¹¶è¡Œåº¦ä¸æ‰€éœ€çš„æ‰§è¡Œå•å…ƒæ•°é‡ç›¸åŒã€‚å¹¶è¡Œåº¦å’Œèµ„æºæ¶ˆè€—éšç€æ—¶é—´æ­¥é•¿çš„å‰§çƒˆå˜åŒ–è®©å½’çº¦æ ‘æˆä¸ºä¸€ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„å¹¶è¡Œæ¨¡å¼ã€‚\n10.3 A Simple Reduction Kernel ä»å®ç°ä¸€ä¸ªåœ¨å•ä¸ªçº¿ç¨‹å—å†…æ‰§è¡Œæ±‚å’Œå½’çº¦æ ‘çš„å†…æ ¸å¼€å§‹ã€‚å…¶å¹¶è¡Œæ‰§è¡Œçš„æƒ…å†µå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå‡è®¾è¾“å…¥æ•°ç»„ä½äºå…¨å±€å†…å­˜ä¸­ï¼Œå¹¶ä¸”åœ¨è°ƒç”¨å†…æ ¸å‡½æ•°æ—¶å°†å…¶æŒ‡é’ˆä½œä¸ºè¾“å…¥å‚æ•°ä¼ å…¥ã€‚æ¯ä¸ªçº¿ç¨‹è¢«åˆ†é…åˆ°ç´¢å¼•2*threadIdx.x å¤„ï¼Œæ¯ä¸€æ­¥å½’çº¦çš„ç»“æœä¹Ÿä¼šè¢«å†™å…¥æ­¤å¤„ã€‚\nThreads Arrangment of the Input Array in the Simple Kernel\nå¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼Œfor å¾ªç¯ä¸­çš„ __syncthreads() ç¡®ä¿ä»»ä½•ä¸€ä¸ªçº¿ç¨‹å¼€å§‹ä¸‹ä¸€æ¬¡è¿­ä»£ä¹‹å‰ï¼Œæ‰€æœ‰çº¿ç¨‹éƒ½å·²ç»å®Œæˆäº†ä¸Šä¸€æ¬¡è¿­ä»£çš„è®¡ç®—ã€‚\n__global__ void SimpleReductionKernel(float* input, float* output) { // launch single block with 1/2 #elements threads unsigned int i = threadIdx.x * 2; for (unsigned int stride = 1; stride = blockDim.x; stride *= 2) { if (threadIdx.x % 2 == 0) { input[i] += input[i + stride]; } __syncthreads(); // Ensure partial sums have been written to the destinition. } if (threadIdx.x == 0) { *output = input[0]; } } 10.4 Minimizing Control Divergence ä¸Šé¢ä»£ç åœ¨æ¯æ¬¡è¿­ä»£ä¸­å¯¹æ´»åŠ¨å’Œéæ´»åŠ¨çº¿ç¨‹çš„ç®¡ç†å¯¼è‡´äº†æ§åˆ¶å‘æ•£ã€‚åªæœ‰é‚£äº›çº¿ç¨‹çš„ threadIdx.x ä¸ºå¶æ•°çš„çº¿ç¨‹åœ¨ç¬¬äºŒæ¬¡è¿­ä»£ä¸­æ‰§è¡ŒåŠ æ³•æ“ä½œã€‚ç”±äºæ§åˆ¶å‘æ•£é€ æˆçš„æ‰§è¡Œèµ„æºæµªè´¹éšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ è€Œå¢åŠ ï¼Œç¬¬äºŒæ¬¡è¿­ä»£ä¸­æ¯ä¸ª warp åªæœ‰ä¸€åŠçš„çº¿ç¨‹æ‰§è¡ŒåŠ æ³•æ“ä½œï¼Œä½†æ¶ˆè€—çš„è®¡ç®—èµ„æºå´æ˜¯ç›¸åŒçš„ã€‚å¦‚æœè¾“å…¥æ•°ç»„çš„å¤§å°å¤§äº32ï¼Œæ•´ä¸ª warp å°†åœ¨ç¬¬äº”æ¬¡è¿­ä»£åä¸å†æ‰§è¡ŒåŠ æ³•æ“ä½œã€‚æ¶ˆè€—çš„æ‰§è¡Œèµ„æºçš„æ€»æ•°ä¸æ‰€æœ‰è¿­ä»£ä¸­æ´»åŠ¨ warp çš„æ€»æ•°æˆæ­£æ¯”ï¼Œè®¡ç®—æ–¹å¼å¦‚ä¸‹ã€‚\n$$\\text{active warps} = (5+\\frac{1}{2}+\\frac{1}{4}+\\cdots+1)*\\frac{N}{64}*32$$å…¶ä¸­ N/64 ä»£è¡¨å¯åŠ¨çš„ warp æ€»æ•°ã€‚æ¯ä¸ª warp åœ¨å‰äº”æ¬¡è¿­ä»£ä¸­éƒ½å¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œä¹‹åæ¯æ¬¡è¿­ä»£éƒ½åªæœ‰ä¸Šæ¬¡ä¸€åŠçš„çº¿ç¨‹åœ¨æ´»åŠ¨çŠ¶æ€ï¼Œç›´åˆ°åªå‰©æœ€åä¸€ä¸ªã€‚ æ¯æ¬¡è¿­ä»£ä¸­æ´»åŠ¨çº¿ç¨‹è®¡ç®—å‡ºçš„ç»“æœä¸ªæ•°ç­‰äºæ´»åŠ¨çº¿ç¨‹çš„æ€»æ•°\n$$\\text{active threads} = \\frac{N}{64}*(32+16+8+4+2+1)+\\frac{N}{64}*\\frac{1}{2}*1+\\frac{N}{64}*\\frac{1}{4}*1+\\cdots+1$$æ¯ä¸ª warp åœ¨å‰äº”æ¬¡è¿­ä»£ä¸­å¤„äºæ´»åŠ¨çŠ¶æ€çš„çº¿ç¨‹æ•°å‡åŠï¼Œä¹‹åæ¯æ¬¡è¿­ä»£ä¸­æ¯ä¸ªå¤„äºæ´»åŠ¨çŠ¶æ€çš„ warp åªæœ‰ä¸€ä¸ªçº¿ç¨‹å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚è¿™ä¸ªç»“æœåº”è¯¥éå¸¸ç›´è§‚çš„ï¼Œå› ä¸ºå…¶æ­£ç­‰äºå®Œæˆå½’çº¦æ‰€éœ€çš„æ“ä½œæ€»æ•°ã€‚ ç”±æ­¤æˆ‘ä»¬å¯ä»¥å¾—å‡ºå½“è¾“å…¥å¤§å°ä¸º 256 æ—¶ï¼Œæ‰§è¡Œèµ„æºåˆ©ç”¨ç‡ä¸º 255/736 = 0.35. å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸ºäº†å‡å°‘æ§åˆ¶åˆ†æ•£åº”è¯¥å®‰æ’çº¿ç¨‹å’Œå®ƒä»¬è®¡ç®—çš„ä½ç½®ä½¿å¾—èƒ½å¤Ÿéšç€æ—¶é—´çš„æ¨ç§»è€Œå½¼æ­¤é è¿‘ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›æ­¥å¹…éšç€æ—¶é—´çš„æ¨ç§»è€Œå‡å°‘ï¼Œè€Œä¸æ˜¯å¢åŠ ã€‚ä¿®æ”¹åçš„å†…æ ¸å‡½æ•°å¦‚ä¸‹ï¼Œæ¯æ¬¡è¿­ä»£ä¸­æ‰§è¡ŒåŠ æ³•æ“ä½œçš„çº¿ç¨‹æ•°æ˜¯ç›¸åŒçš„ï¼Œä½†ç›´åˆ°åŒæ—¶è¿›è¡ŒåŠ æ³•çš„çº¿ç¨‹æ•°å°äº 32 ä¹‹å‰ï¼Œä¸€ä¸ª warp çš„çº¿ç¨‹æ•°æ‰€èµ°çš„åˆ†æ”¯ç›¸åŒã€‚\nArrangement with Less Control Divergence\n__global__ void ConvergentSumReductionKernel(float* input, float* output) { unsigned int i = threadIdx.x; for (unsigned int stride = blockDim.x; stride \u0026gt;= 1; stride /= 2) { // Decrease stride to reduce control divergence if (threadIdx.x \u0026lt; stride) { input[i] += input[i + stride]; } __syncthreads(); } if (threadIdx.x == 0) { *output = input[0]; } } è¿™ç§æƒ…å†µä¸‹çš„è¿›è¡Œè§„çº¦æ“ä½œæ¶ˆè€—çš„è®¡ç®—èµ„æºæ€»æ•°ä¸º $$(\\frac{N}{64}*1 + \\frac{N}{64}*\\frac{1}{2}*1 + \\frac{N}{64}*\\frac{1}{4}*1 + \\cdots + 1 + 5*1) * 32 $$ 5*1 ä»£è¡¨æœ€åçš„äº”æ¬¡è¿­ä»£ï¼Œåªæœ‰ä¸€ä¸ªæ´»åŠ¨çš„warpï¼Œå¹¶ä¸”å®ƒçš„æ‰€æœ‰32ä¸ªçº¿ç¨‹éƒ½æ¶ˆè€—æ‰§è¡Œèµ„æºï¼Œå³ä½¿åªæœ‰ä¸€å°éƒ¨åˆ†çº¿ç¨‹æ˜¯æ´»åŠ¨çŠ¶æ€ã€‚æ‰§è¡Œèµ„æºçš„åˆ©ç”¨ç‡ä¸º 255/384 = 0.66.\n10.5 Minimizing Memory Divergence ä¸Šé¢çš„å†…æ ¸è¿˜æœ‰å†…å­˜åˆ†æ•£çš„é—®é¢˜ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ¯ä¸ªçº¿ç¨‹å¯¹å…¨å±€å†…å­˜æ‰§è¡Œ 2 æ¬¡è¯»å–å’Œ 1 æ¬¡å†™å…¥ã€‚ç¬¬ä¸€æ¬¡ä»è‡ªå·±çš„ä½ç½®è¯»å–ï¼Œç¬¬äºŒæ¬¡ä»ç¦»è‡ªå·± stride çš„ä½ç½®è¯»å–ï¼Œç›¸åŠ åå†™å…¥åˆ°è‡ªå·±çš„ä½ç½®ã€‚ 10.3 èŠ‚çš„å†…æ ¸ä»£ç ä¸­ï¼Œç¬¬ä¸€æ¬¡è¿­ä»£æ¯ä¸ª warp ä¸­çš„ç›¸é‚»çº¿ç¨‹é—´éš” 2 ä¸ªå…ƒç´ ï¼Œå› æ­¤è¦è®¿é—® 2 ä¸ªå†…å­˜ä½ç½®ï¼Œæ­¤åæ¯æ¬¡è¿­ä»£ stride éƒ½å¢åŠ ï¼Œç›´åˆ°ç¬¬å…­æ¬¡è¿­ä»£æ—¶ï¼Œæ¯ä¸ª warp éƒ½åªæœ‰ä¸€ä¸ªçº¿ç¨‹å¤„äºæ´»åŠ¨çŠ¶æ€ï¼Œåªç”¨è®¿é—® 1 ä¸ªä½ç½®ã€‚å› æ­¤è¿›è¡Œå†…å­˜è®¿é—®çš„æ€»æ¬¡æ•°ä¸º $$(5*\\frac{N}{64}*2+\\frac{N}{64}*1+\\frac{N}{64}*\\frac{1}{2}*1+\\cdots+1)*3$$ 10.4 èŠ‚çš„å†…æ ¸ä»£ç ä¸­ï¼Œæ¯ä¸ª warp åœ¨ä»»ä½•è¯»æˆ–å†™æ—¶åªè¿›è¡Œä¸€ä¸ªå…¨å±€å†…å­˜è¯·æ±‚ï¼Œç›´åˆ°è¯¥ warp ä¸­çš„æ‰€æœ‰çº¿ç¨‹éƒ½å¤„äºéæ´»åŠ¨çŠ¶æ€ã€‚æœ€åäº”æ¬¡è¿­ä»£çš„çº¿ç¨‹éƒ½ä½äºä¸€ä¸ª warp ä¸­ï¼Œå› æ­¤è¿›è¡Œå†…å­˜è®¿é—®çš„æ€»æ¬¡æ•°ä¸º $$((\\frac{N}{64}*1+\\frac{N}{64}*\\frac{1}{2}*1+\\frac{N}{64}*\\frac{1}{4}*1+\\cdots+1)+5)*3$$ å¯¹äºé•¿åº¦ä¸º 2048 çš„è¾“å…¥ï¼Œå‰è€…å’Œåè€…å…¨å±€å†…å­˜è¯·æ±‚çš„æ€»æ•°åˆ†åˆ«ä¸º 1149 å’Œ 204. åè€…åœ¨ä½¿ç”¨ DRAM å¸¦å®½æ–¹é¢ä¹Ÿå…·æœ‰æ›´é«˜çš„æ•ˆç‡ã€‚\n10.6 Minimizing Global Memory Accesses é€šè¿‡ä½¿ç”¨å…±äº«å†…å­˜ï¼Œå¯ä»¥è¿›ä¸€æ­¥æ”¹è¿› 10.4 èŠ‚çš„å†…æ ¸ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œçº¿ç¨‹å°†å®ƒä»¬çš„éƒ¨åˆ†å’Œç»“æœå€¼å†™å…¥å…¨å±€å†…å­˜ï¼Œè¿™äº›å€¼åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­ç”±ç›¸åŒçš„çº¿ç¨‹å’Œå…¶ä»–çº¿ç¨‹é‡æ–°è¯»å–ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé€šè¿‡å°†éƒ¨åˆ†å’Œç»“æœä¿å­˜åœ¨å…±äº«å†…å­˜ä¸­ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ‰§è¡Œé€Ÿåº¦ã€‚\nUse Shared Memory to Reduce Accesses from the Global Memory å¯¹åº”çš„ä»£ç å¦‚ä¸‹ï¼Œæ¯ä¸ªçº¿ç¨‹ä»å…¨å±€å†…å­˜åŠ è½½å¹¶ 2 ä¸ªè¾“å…¥å…ƒç´ å¹¶å°†éƒ¨åˆ†å’Œå†™å…¥å…±äº«å†…å­˜ã€‚å‰©ä¸‹çš„æ‰€æœ‰è¿­ä»£ä¸­çš„è®¡ç®—éƒ½åœ¨å…±äº«å†…å­˜ä¸­è¿›è¡Œã€‚\n#define BLOCK_DIM 512 __global__ void SharedMemoryReductionKernel(float* input) { __shared__ float input_s[BLOCK_DIM]; unsigned int i = threadIdx.x; input_s[i] = input[i] + input[i + blockDim.x]; // Partial sum of first iteration for (unsigned int stride = blockDim.x / 2; stride \u0026gt;= 1; stride /= 2) { __syncthreads(); // Ensure all partial sums have been written to shared memory if (threadIdx.x \u0026lt; stride) { input_s[i] += input_s[i + stride]; // Partial sum of subsequent iterations } } if (threadIdx.x == 0) { input[0] = input_s[0]; // Write final sum to output } } å…¨å±€å†…å­˜è®¿é—®çš„æ¬¡æ•°å‡å°‘åˆ°åˆå§‹åŠ è½½è¾“å…¥æ•°ç»„å’Œæœ€ç»ˆå†™å…¥ input[0]ï¼Œæ€»å…±åªæœ‰ (N/32) + 1 ä¸ªå…¨å±€å†…å­˜è¯·æ±‚ã€‚\n10.7 Hierarchical Reduction for Arbitrary Input Length ç”±äº __syncthreads() åªå¯¹åŒä¸€å—ä¸­çš„çº¿ç¨‹æœ‰æ•ˆï¼Œå› æ­¤æ— æ³•åœ¨ä¸åŒå—ä¹‹é—´åŒæ­¥ã€‚ä¸‹å›¾å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨åˆ†çº§å½’çº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…¶æ€æƒ³æ˜¯å°†è¾“å…¥æ•°ç»„åˆ’åˆ†ä¸ºå¤šä¸ªé€‚åˆäºçº¿ç¨‹å—å¤§å°çš„æ®µã€‚ç„¶åï¼Œæ‰€æœ‰å—éƒ½ç‹¬ç«‹åœ°æ‰§è¡Œå½’çº¦æ ‘ï¼Œå¹¶ä½¿ç”¨åŸå­åŠ æ³•æ“ä½œå°†å®ƒä»¬çš„ç»“æœç´¯ç§¯åˆ°æœ€ç»ˆè¾“å‡ºã€‚\nSegmented Multiblock Reduction Using Atomic Operations å¯¹åº”çš„å†…æ ¸ä»£ç å¦‚ä¸‹ã€‚æ¯ä¸ªçº¿ç¨‹å—å¤„ç† 2*blockDim.x ä¸ªå…ƒç´ ã€‚åœ¨æ¯ä¸ªçº¿ç¨‹å—å†…ï¼Œæˆ‘ä»¬é€šè¿‡çº¿ç¨‹æ‰€å±å—çš„æ®µèµ·å§‹ä½ç½®åŠ ä¸Š threadIdx.x ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ†é…å…¶è¾“å…¥å…ƒç´ çš„ä½ç½®ã€‚\n__global__ void SegmentedSumReductionKernel(float* input, float* output) { __shared__ float input_s[BLOCK_DIM]; unsigned int segment = blockIdx.x * blockDim.x * 2; // Each block processes 2*blockDim.x elements unsigned int i = segment + threadIdx.x; unsigned int t = threadIdx.x; input_s[t] = input[t + blockDim.x]; // Partial sum of first iteration of each block for (unsigned int stride = blockDim.x / 2; stride \u0026gt;= 1; stride /= 2) { __syncthreads(); // Ensure all partial sums have been written to shared memory if (t \u0026lt; stride) { input_s[t] += input_s[t + stride]; // Partial sum of subsequent iterations } } if (t == 0) { atomicAdd(\u0026amp;output, input_s[0]); // Write final sum to output } } 10.8 Thread Coarsening for Reduced Overhead åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿‡çš„å½’çº¦å†…æ ¸éƒ½è¯•å›¾é€šè¿‡ä½¿ç”¨å°½å¯èƒ½å¤šçš„çº¿ç¨‹æ¥æœ€å¤§åŒ–å¹¶è¡Œæ€§ã€‚è‹¥çº¿ç¨‹å—å¤§å°ä¸º 1024 ä¸ªçº¿ç¨‹ï¼Œåˆ™éœ€è¦å¯åŠ¨çš„çº¿ç¨‹å—æ•°é‡ä¸º N/2048. ä¸‹å›¾å±•ç¤ºäº†å¦‚ä½•å°†çº¿ç¨‹ç²—åŒ–ã€‚çº¿ç¨‹ç‹¬ç«‹åœ°æ·»åŠ å®ƒä»¬è´Ÿè´£çš„å››ä¸ªå…ƒç´ ï¼Œå®ƒä»¬ä¸éœ€è¦åŒæ­¥ï¼Œç›´åˆ°å°†æ‰€æœ‰çš„å››ä¸ªå…ƒç´ ç›¸åŠ ä¹‹åæ‰èƒ½å°†éƒ¨åˆ†å’Œç»“æœå†™å…¥å…±äº«å†…å­˜ã€‚å‰©ä¸‹çš„æ­¥éª¤ä¸ 10.7 èŠ‚åç»­ç›¸åŒã€‚\nThread Coarsening in Reduction å¯¹åº”çš„å†…æ ¸å¦‚ä¸‹ï¼Œæˆ‘ä»¬ä¹˜ä»¥ COARSE_FACTOR æ¥è¡¨ç¤ºæ¯ä¸ªçº¿ç¨‹å—çš„è´Ÿè´£çš„æ®µçš„å¤§å°æ˜¯åŸæ¥çš„ COARSE_FACTOR å€ã€‚éƒ¨åˆ†å’Œç´¯åŠ åˆ°å±€éƒ¨å˜é‡ sum ä¸­ï¼Œå¹¶ä¸”å› ä¸ºçº¿ç¨‹æ˜¯ç‹¬ç«‹è¿è¡Œçš„ï¼Œåœ¨å¾ªç¯ä¸­ä¸ä¼šè°ƒç”¨ __syncthreads().\n#define COARSE_FACTOR 2 __global__ void CoarsenedSumReductionKernel(float* input, float* output) { __shared__ float input_s[BLOCK_DIM]; unsigned int segment = blockIdx.x * COARSE_FACTOR * blockDim.x * 2; unsigned int i = segment + threadIdx.x; unsigned int t = threadIdx.x; float sum = input[i]; for (int tile = 1; tile \u0026lt; COARSE_FACTOR; tile++) { // Partitial sum is accumulated independently sum += input[i + tile * blockDim.x]; } input_s[t] = sum; for (unsigned int stride = blockDim.x / 2; stride \u0026gt;= 1; stride /= 2) { __syncthreads(); if (t \u0026lt; stride) { input_s[t] += input_s[t + stride]; } } if (t == 0) { atomicAdd(\u0026amp;output, input_s[0]); } } ä¸‹å›¾æ¯”è¾ƒäº†ä¸¤ä¸ªåŸå§‹çº¿ç¨‹å—åœ¨æ²¡æœ‰è¿›è¡Œçº¿ç¨‹ç²—åŒ–ä¸‹è¢«ç¡¬ä»¶é¡ºåºæ‰§è¡Œæƒ…å†µï¼Œå›¾ A å½“ç¬¬ä¸€ä¸ªçº¿ç¨‹å—å®Œæˆåï¼Œç¡¬ä»¶è°ƒåº¦ç¬¬äºŒä¸ªçº¿ç¨‹å—ï¼Œåœ¨ä¸åŒçš„æ•°æ®æ®µä¸Šæ‰§è¡Œç›¸åŒçš„æ­¥éª¤ã€‚å›¾ B çš„è¿™ä¸ªçº¿ç¨‹å—å¼€å§‹éœ€è¦ä¸‰ä¸ªæ­¥éª¤ï¼Œå…¶ä¸­æ¯ä¸ªçº¿ç¨‹å¯¹å®ƒè´Ÿè´£çš„å››ä¸ªå…ƒç´ æ±‚å’Œã€‚å‰©ä¸‹çš„ä¸‰ä¸ªæ­¥éª¤æ‰§è¡Œå½’çº¦æ ‘ï¼Œæ¯ä¸ªæ­¥éª¤ä¸­æœ‰ä¸€åŠçš„çº¿ç¨‹é€€å‡ºæ´»åŠ¨çŠ¶æ€ã€‚ç›¸æ¯”å›¾ Aï¼Œå›¾ B åªéœ€è¦6ä¸ªæ­¥éª¤ (è€Œä¸æ˜¯ 8 ä¸ª)ï¼Œå…¶ä¸­ 3 ä¸ªæ­¥éª¤ (è€Œä¸æ˜¯ 2 ä¸ª) å……åˆ†åˆ©ç”¨äº†ç¡¬ä»¶ã€‚ å½“æˆ‘ä»¬ç²—åŒ–çº¿ç¨‹æ—¶ï¼Œå¹¶è¡Œå®Œæˆçš„å·¥ä½œå°±ä¼šå‡å°‘ã€‚å› æ­¤ï¼Œå¢åŠ ç²—åŒ–å› å­å°†å‡å°‘ç¡¬ä»¶æ­£åœ¨åˆ©ç”¨çš„æ•°æ®å¹¶è¡Œæ€§çš„æ•°é‡ã€‚\nComparing Parallel Reduction with and without Thread Coarsening\n","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch10/","summary":"Personal notebook 10 of Programming Massively Parallel","title":"PMPP Learning-Chapter 10 Reduction and Minimizing Divergence"},{"content":"8 Stencil åœ¨æµä½“åŠ¨åŠ›å­¦ã€çƒ­ä¼ å¯¼ã€ç‡ƒçƒ§ã€å¤©æ°”é¢„æŠ¥ã€æ°”å€™æ¨¡æ‹Ÿå’Œç”µç£å­¦ç­‰åº”ç”¨é¢†åŸŸï¼Œæ¨¡æ¿æ˜¯æ±‚è§£åå¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼æ–¹æ³•çš„åŸºç¡€ã€‚æ¨¡æ¿æ–¹æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œå°†åå¾®åˆ†æ–¹ç¨‹çš„æ±‚è§£è½¬åŒ–ä¸ºæ±‚è§£ä¸€ä¸ªå±€éƒ¨çš„çº¿æ€§æ–¹ç¨‹ç»„ï¼Œç„¶ååœ¨è¯¥å±€éƒ¨è¿›è¡Œè¿­ä»£æ±‚è§£ï¼Œæœ€åå¾—åˆ°å…¨å±€è§£ã€‚ç”±äºæ±‚è§£å¾®åˆ†é—®é¢˜æ—¶å¯¹æ•°å€¼ç²¾åº¦çš„è¦æ±‚ï¼Œæ¨¡æ¿å¤„ç†çš„æ•°æ®å¾€å¾€æ˜¯é«˜ç²¾åº¦çš„æµ®åŠ¨æ•°æ®ï¼Œå¯¹äº tiling æŠ€æœ¯æ¥è¯´ï¼Œè¿™éœ€è¦æ¶ˆè€—æ›´å¤šçš„ç‰‡ä¸Šå†…å­˜ã€‚\nBackgroud ç”¨è®¡ç®—æœºæ•°å€¼è®¡ç®—å’Œæ±‚è§£å‡½æ•°ã€æ¨¡å‹ã€å˜é‡å’Œæ–¹ç¨‹çš„ç¬¬ä¸€æ­¥æ˜¯å°†å®ƒä»¬è½¬æ¢æˆç¦»æ•£çš„è¡¨ç¤ºå½¢å¼ã€‚è¡¨ç¤ºçš„ä¿çœŸåº¦æˆ–è¿™äº›è¿‘ä¼¼æ’å€¼æŠ€æœ¯çš„å‡½æ•°å€¼çš„å‡†ç¡®æ€§å–ä¸€æ–¹é¢å†³äºç½‘æ ¼ç‚¹ä¹‹é—´çš„é—´è·:é—´è·è¶Šå°ï¼Œè¿‘ä¼¼è¶Šå‡†ç¡®ã€‚ç¦»æ•£è¡¨ç¤ºçš„ä¿çœŸåº¦è¿˜å–å†³äºæ‰€ä½¿ç”¨æ•°å­—çš„ç²¾åº¦ã€‚æœ¬ç« ä¸­å°†é‡ç‚¹å…³æ³¨è®¡ç®—æ¨¡å¼ï¼Œå…¶ä¸­æ¨¡æ¿åº”ç”¨äºæ‰€æœ‰ç›¸å…³çš„è¾“å…¥ç½‘æ ¼ç‚¹ä»¥ç”Ÿæˆæ‰€æœ‰ç½‘æ ¼ç‚¹çš„è¾“å‡ºå€¼ï¼Œè¿™å°†è¢«ç§°ä¸ºæ¨¡æ¿æ‰«æ (stencil sweep).\nOne-dimensional Stencil Example\nTwo-dimensional \u0026amp; Three-dimensional Stencil Example\n8.2 Parallel stencil: A Basic Algorithm 2D æƒ…å†µä¸‹è¾“å‡ºç½‘æ ¼çš„ tiling å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå…¶ä¸­æ¯ä¸ªçº¿ç¨‹å—è´Ÿè´£ä¸€ä¸ª 4*4 å¤§å°çš„è¾“å‡º tile. ä¸€ä¸ªåŸºæœ¬çš„ 3D stencil å†…æ ¸å‡½æ•°å¦‚ä¸‹ï¼Œå…¶ä¸­æ¯ä¸ªçº¿ç¨‹å—è´Ÿè´£è®¡ç®—ä¸€ä¸ªè¾“å‡º tile çš„å€¼ï¼Œæ¯ä¸ªçº¿ç¨‹ç”¨äºè®¡ç®—ä¸€ä¸ªå…ƒç´ ã€‚æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œ13æ¬¡æµ®ç‚¹æ“ä½œ (7 æ¬¡ä¹˜æ³•å’Œ 6 æ¬¡åŠ æ³•)ï¼Œå¹¶åŠ è½½ 7 ä¸ªè¾“å…¥å…ƒç´  (æ¯ä¸ª 4 å­—èŠ‚)ã€‚å› æ­¤ï¼Œè¿™ä¸ªå†…æ ¸çš„æµ®ç‚¹å¯¹è®¡ç®—è®¿å­˜æ¯”æ˜¯ 13 / (7*4) = 0.46 OP/B.\n2D 5-point Stencil Tiling for Output Grid\n__global__ void stencil_kernel(float* in, float* out, unsigned int N) { unsigned int i = blockIdx.z*blockDim.z+threadIdx.z; unsigned int j = blockIdx.y*blockDim.y+threadIdx.y; unsigned int k = blockIdx.x*blockDim.x+threadIdx.x; if (i \u0026gt;= 1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; N - 1 \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; N - 1) { out[i * N * N + j * N + k] = c0 * in[i * N * N + j * N + k] + c1 * in[i * N * N + j * N + k - 1] + c2 * in[i * N * N + j * N + k + 1] + c3 * in[i * N * N + (j - 1) * N + k] + c4 * in[i * N * N + (j + 1) * N + k] + c5 * in[(i - 1) * N * N + j * N + k] + c6 * in[(i + 1) * N * N + j * N + k]; } } 8.3 Shared Memory Tiling for Stencil Sweep ä¸‹å›¾å±•ç¤ºäº†äºŒç»´äº”ç‚¹æ¨¡æ¿çš„è¾“å…¥å’Œè¾“å‡º tileï¼Œå¯ä»¥å‘ç°äº”ç‚¹æ¨¡æ¿çš„è¾“å…¥ tile ä¸åŒ…æ‹¬å››ä¸ªè§’è½çš„å…ƒç´ ã€‚å› ä¸ºæ¯ä¸ªè¾“å‡ºç½‘æ ¼ç‚¹å€¼åªä½¿ç”¨è¾“å…¥ tile çš„ 5 ä¸ªå…ƒç´ ï¼Œè€Œ 3*3 å·ç§¯ä½¿ç”¨ 9 ä¸ªå…ƒç´ ã€‚è€Œ 3D æƒ…å†µä¸‹ä¸ƒç‚¹æ¨¡æ¿ç›¸å¯¹äº 3*3*3 å·ç§¯ä»å°†è¾“å…¥ç½‘æ ¼ç‚¹åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­èƒ½è·å¾—çš„æ”¶ç›Šæ›´ä½ã€‚ç”±äºä¸ºå·ç§¯åŠ è½½è¾“å…¥ tile çš„æ‰€æœ‰ç­–ç•¥éƒ½ç›´æ¥åº”ç”¨äºæ¨¡æ¿æ‰«æï¼Œä¸‹é¢ç»™å‡ºäº†ä¸€ä¸ªåŠ è½½åˆ°å…±äº«å†…å­˜ç‰ˆæœ¬çš„å†…æ ¸å‡½æ•°ï¼Œçº¿ç¨‹å—çš„å¤§å°ä¸è¾“å…¥ tile ç›¸åŒï¼Œåœ¨è®¡ç®—è¾“å‡º tile ç‚¹å€¼æ—¶æ²¡æœ‰ä½¿ç”¨éƒ¨åˆ†çº¿ç¨‹ã€‚æ¯ä¸ªè¡¨è¾¾å¼ä¸­å‡å»çš„å€¼1æ˜¯å› ä¸ºå†…æ ¸å‡è®¾ä¸€ä¸ª3Dä¸ƒç‚¹æ¨¡æ¿ï¼Œæ¯è¾¹æœ‰ä¸€ä¸ªç½‘æ ¼ç‚¹\nInput and Output Tiles for a 2D 5-point Stencil\n#define IN_TILE_DIM 16 __global__ void stencil_shared_mem_tiling_kernel(float* in, float* out, unsigned int N) { // upper left corner of input tile unsigned int i = blockIdx.z*blockDim.z+threadIdx.z - 1; unsigned int j = blockIdx.y*blockDim.y+threadIdx.y - 1; unsigned int k = blockIdx.x*blockDim.x+threadIdx.x - 1; __shared__ float in_s[IN_TILE_DIM][IN_TILE_DIM][IN_TILE_DIM]; if (i \u0026gt;= 1 \u0026amp;\u0026amp; i \u0026lt; IN_TILE_DIM \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; IN_TILE_DIM \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; IN_TILE_DIM) { in_s[threadIdx.z][threadIdx.y][threadIdx.x] = in[i * N * N + j * N + k]; } __syncthreads(); if (i \u0026gt;= 1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; N - 1 \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; N - 1) { if (threadIdx.x \u0026gt;=1 \u0026amp;\u0026amp; threadIdx.x \u0026lt; IN_TILE_DIM-1 \u0026amp;\u0026amp; threadIdx.y \u0026gt;=1 \u0026amp;\u0026amp; threadIdx.y \u0026lt; IN_TILE_DIM-1 \u0026amp;\u0026amp; threadIdx.z \u0026gt;=1 \u0026amp;\u0026amp; threadIdx.z \u0026lt; IN_TILE_DIM-1) { // 7 point template out[i * N * N + j * N + k] = c0 * in_s[threadIdx.z][threadIdx.y][threadIdx.x] + c1 * in_s[threadIdx.z][threadIdx.y][threadIdx.x - 1] + c2 * in_s[threadIdx.z][threadIdx.y][threadIdx.x + 1] + c3 * in_s[threadIdx.z][threadIdx.y - 1][threadIdx.x] + c4 * in_s[threadIdx.z][threadIdx.y + 1][threadIdx.x] + c5 * in_s[threadIdx.z - 1][threadIdx.y][threadIdx.x] + c6 * in_s[threadIdx.z + 1][threadIdx.y][threadIdx.x]; } } ç¡¬ä»¶é™åˆ¶æ¯ä¸ªå—æœ€å¤§ä¸º 1024 ï¼Œå› æ­¤ tile é€šå¸¸æ¯”è¾ƒå°ã€‚ä¸€èˆ¬ tile çš„è¾¹é•¿ä¸º8ï¼Œæ¯ä¸ªå—çš„å¤§å°ä¸º 512 ä¸ªçº¿ç¨‹ã€‚ç›¸åï¼Œå·ç§¯é€šå¸¸ç”¨äºå¤„ç†äºŒç»´å›¾åƒï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„ tile å°ºå¯¸ (32x32). ç¬¬ä¸€ä¸ªç¼ºç‚¹æ˜¯ç”±äº halo cell çš„å¼€é”€ï¼Œé‡ç”¨ç‡éšç€ tile å¤§å°çš„é™ä½è€Œé™ä½ã€‚ç¬¬äºŒä¸ªç¼ºç‚¹æ˜¯å®ƒå¯¹å†…å­˜åˆå¹¶æœ‰ä¸åˆ©å½±å“ã€‚å¯¹äºä¸€ä¸ª 8x8x8 tileï¼Œæ¯ warp çš„çº¿ç¨‹å°†è®¿é—®å…¨å±€å†…å­˜ä¸­è‡³å°‘å››è¡Œ (888*4 bytes, 32 threads, 64 bits/DRAM = 4)\n8.4 Thread Coarsening ä¸‹å›¾å‡è®¾æ¯ä¸ªè¾“å…¥ tile ç”± 6x6x6 ä¸ªç½‘æ ¼ç‚¹ç»„æˆã€‚ä¸ºäº†ä½¿è¾“å…¥ tileçš„å†…éƒ¨å¯è§ï¼Œå—çš„å‰ã€å·¦å’Œä¸Šé¢æ²¡æœ‰ç”»å‡ºã€‚å‡è®¾æ¯ä¸ªè¾“å‡º tile ç”± 4x4x4ä¸ªç½‘æ ¼ç‚¹ç»„æˆã€‚åˆ†é…ç»™å¤„ç†è¯¥ tile çš„çº¿ç¨‹å—ç”±ä¸è¾“å…¥ tile çš„ä¸€ä¸ªx-yå¹³é¢ (å³ 6x6) ç›¸åŒæ•°é‡çš„çº¿ç¨‹ç»„æˆã€‚ç¨‹åºä¸€å¼€å§‹ï¼Œæ¯ä¸ªå—éœ€è¦å°†åŒ…å«è®¡ç®—è¾“å‡ºå—å¹³é¢å€¼æ‰€éœ€çš„æ‰€æœ‰ç‚¹çš„ä¸‰ä¸ªè¾“å…¥å—å¹³é¢åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­ã€‚åœ¨æ¯æ¬¡è¿­ä»£æœŸé—´ï¼Œå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹å°†å¤„ç†è¾“å‡º tile ä¸è¿­ä»£å€¼ç›¸åŒçš„ z ç´¢å¼•å¯¹åº”çš„ x-y å¹³é¢ã€‚\nMapping of Shared Memory Array after First Iteration\n#define OUT_TILE_DIM IN_TILE_DIM - 2 __global__ void stencil_thread_coarsening_kernel(float* in, float* out, unsigned int N) { int iStart = blockIdx.z * OUT_TILE_DIM; int j = blockIdx.y * blockDim.y + threadIdx.y - 1; int k = blockIdx.x * blockDim.x + threadIdx.x - 1; __shared__ float inPrev_s[IN_TILE_DIM][IN_TILE_DIM]; __shared__ float inCurr_s[IN_TILE_DIM][IN_TILE_DIM]; __shared__ float inNext_s[IN_TILE_DIM][IN_TILE_DIM]; if (iStart \u0026gt;= 1 \u0026amp;\u0026amp; iStart \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inPrev_s[threadIdx.y][threadIdx.x] = in[(iStart - 1) * N * N + j * N + k]; } if (iStart \u0026gt;= 0 \u0026amp;\u0026amp; iStart \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0) { inCurr_s[threadIdx.y][threadIdx.x] = in[iStart * N * N + j * N + k]; } for (int i = 0; i \u0026lt; OUT_TILE_DIM; i++) { i += iStart; if (i \u0026gt;= -1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inNext_s[threadIdx.y][threadIdx.x] = in[(i + 1) * N * N + j * N + k]; } __syncthreads(); if (i \u0026gt;= 1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; N - 1 \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; N - 1 \u0026amp;\u0026amp; threadIdx.y \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.y \u0026lt; IN_TILE_DIM - 1 \u0026amp;\u0026amp; threadIdx.x \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.x \u0026lt; IN_TILE_DIM - 1) { out[i * N * N + j * N + k] = c0 * inCurr_s[threadIdx.y][threadIdx.x] + c1 * inCurr_s[threadIdx.y][threadIdx.x - 1] + c2 * inCurr_s[threadIdx.y][threadIdx.x + 1] + c3 * inCurr_s[threadIdx.y - 1][threadIdx.x] + c4 * inCurr_s[threadIdx.y + 1][threadIdx.x] + c5 * inPrev_s[threadIdx.y][threadIdx.x] + c6 * inNext_s[threadIdx.y][threadIdx.x]; } } inPrev_s[threadIdx.y][threadIdx.x] = inCurr_s[threadIdx.y][threadIdx.x]; inCurr_s[threadIdx.y][threadIdx.x] = inNext_s[threadIdx.y][threadIdx.x]; } çº¿ç¨‹ç²—åŒ–å†…æ ¸çš„ä¼˜ç‚¹æ˜¯ï¼Œå®ƒä¸è¦æ±‚è¾“å…¥ tile çš„æ‰€æœ‰å¹³é¢éƒ½å‡ºç°åœ¨å…±äº«å†…å­˜ä¸­ã€‚åœ¨ä»»æ„æ—¶åˆ»ï¼Œåªæœ‰ä¸‰å±‚è¾“å…¥ tile éœ€è¦åœ¨å…±äº«å†…å­˜ä¸­ã€‚\n8.5 Register Tiling æ ¹æ®è®¡ç®—è¿‡ç¨‹å¯ä»¥å‘ç°æ¯ä¸ª inPrev_s å’Œ inNext_s çš„å…ƒç´ ä»…ç”±ä¸€ä¸ªçº¿ç¨‹åœ¨è®¡ç®—å…·æœ‰ç›¸åŒ x-y ç´¢å¼•çš„è¾“å‡º tile ç½‘æ ¼ç‚¹æ—¶ä½¿ç”¨ã€‚åªæœ‰ inCurr_s çš„å…ƒç´ è¢«å¤šä¸ªçº¿ç¨‹è®¿é—®ï¼ŒçœŸæ­£éœ€è¦ä½äºå…±äº«å†…å­˜ä¸­ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥ä¿®æ”¹å†…æ¶µå‡½æ•°å¦‚ä¸‹ï¼Œå¯„å­˜å™¨å˜é‡ inPrev å’Œ inNext åˆ†åˆ«æ›¿æ¢å…±äº«å†…å­˜æ•°ç»„ inPrev_s å’Œ inNext_s. ä¿ç•™äº† inCurr_s ä»¥å…è®¸åœ¨çº¿ç¨‹ä¹‹é—´å…±äº« x-y å¹³é¢ç›¸é‚»ç½‘æ ¼ç‚¹å€¼ã€‚è¿™æ ·è¿™ä¸ªå†…æ ¸ä½¿ç”¨çš„å…±äº«å†…å­˜é‡å‡å°‘åˆ°åŸæ¥çš„ 1/3.\nvoid stencil_register_tiling_coarsening_kernel(float* in, float* out, unsigned int N) { int iStart = blockIdx.z * OUT_TILE_DIM; int j = blockIdx.y * blockDim.y + threadIdx.y - 1; int k = blockIdx.x * blockDim.x + threadIdx.x - 1; float inPrev; float inCurr; float inNext; __shared__ float inCurr_s[IN_TILE_DIM][IN_TILE_DIM]; if (iStart \u0026gt;= 1 \u0026amp;\u0026amp; iStart \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inPrev = in[(iStart - 1) * N * N + j * N + k]; } if (iStart \u0026gt;= 0 \u0026amp;\u0026amp; iStart \u0026lt; N \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0) { inCurr = in[iStart * N * N + j * N + k]; inCurr_s[threadIdx.y][threadIdx.x] = inCurr; } for (int i = 0; i \u0026lt; OUT_TILE_DIM; i++) { i += iStart; if (i \u0026gt;= -1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; N \u0026amp;\u0026amp; k \u0026gt;= 0 \u0026amp;\u0026amp; k \u0026lt; N) { inNext = in[(i + 1) * N * N + j * N + k]; } __syncthreads(); if (i \u0026gt;= 1 \u0026amp;\u0026amp; i \u0026lt; N - 1 \u0026amp;\u0026amp; j \u0026gt;= 1 \u0026amp;\u0026amp; j \u0026lt; N - 1 \u0026amp;\u0026amp; k \u0026gt;= 1 \u0026amp;\u0026amp; k \u0026lt; N - 1 \u0026amp;\u0026amp; threadIdx.y \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.y \u0026lt; IN_TILE_DIM - 1 \u0026amp;\u0026amp; threadIdx.x \u0026gt;= 1 \u0026amp;\u0026amp; threadIdx.x \u0026lt; IN_TILE_DIM - 1) { out[i * N * N + j * N + k] = c0 * inCurr + c1 * inCurr_s[threadIdx.y][threadIdx.x - 1] + c2 * inCurr_s[threadIdx.y][threadIdx.x + 1] + c3 * inCurr_s[threadIdx.y - 1][threadIdx.x] + c4 * inCurr_s[threadIdx.y + 1][threadIdx.x] + c5 * inPrev + c6 * inNext; } } __syncthreads(); inPrev = inCurr; inCurr = inNext; inCurr_s[threadIdx.y][threadIdx.x] = inNext; } é¦–å…ˆï¼Œè®¸å¤šå¯¹å…±äº«å†…å­˜çš„è¯»å†™ç°åœ¨è¢«è½¬ç§»åˆ°å¯„å­˜å™¨ä¸­ã€‚å…¶æ¬¡ï¼Œæ¯ä¸ªå—åªæ¶ˆè€—ä¸‰åˆ†ä¹‹ä¸€çš„å…±äº«å†…å­˜ã€‚å½“ç„¶ï¼Œè¿™æ˜¯ä»¥æ¯ä¸ªçº¿ç¨‹å¤šä½¿ç”¨ 3 ä¸ªå¯„å­˜å™¨ä¸ºä»£ä»·å®ç°çš„ã€‚éœ€è¦æ³¨æ„å…¨å±€å†…å­˜è®¿é—®çš„æ•°é‡æ²¡æœ‰æ”¹å˜ã€‚\n","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch8/","summary":"Personal notebook 8 of Programming Massively Parallel Processors.","title":"PMPP Learning-Chapter 8 Stencil"},{"content":"9 Parallel Histogram-An Introduction to Atomic Operations and Privatization æœ¬ç« ä»‹ç»å¹¶è¡Œç›´æ–¹å›¾è®¡ç®—æ¨¡å¼ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å‡ºå…ƒç´ éƒ½å¯ä»¥ç”±ä»»ä½•çº¿ç¨‹æ›´æ–°ã€‚å› æ­¤ï¼Œå½“çº¿ç¨‹æ›´æ–°è¾“å‡ºå…ƒç´ æ—¶å¿…é¡»æ³¨æ„çº¿ç¨‹ä¹‹é—´çš„åè°ƒï¼Œé¿å…ä»»ä½•å¯èƒ½ç ´åæœ€ç»ˆç»“æœçš„å¹²æ‰°ã€‚\n9.1 Background ç›´æ–¹å›¾æ˜¯æ•°æ®é›†ä¸­æ•°æ®å€¼å‡ºç°çš„æ•°é‡è®¡æ•°æˆ–ç™¾åˆ†æ¯”çš„æ˜¾ç¤ºã€‚åœ¨æœ€å¸¸è§çš„ç›´æ–¹å›¾å½¢å¼ä¸­ï¼Œé—´éš”åŒºé—´æ²¿æ°´å¹³è½´ç»˜åˆ¶ï¼Œæ¯ä¸ªé—´éš”ä¸­çš„æ•°æ®å€¼è®¡æ•°è¡¨ç¤ºä¸ºä»æ°´å¹³è½´ä¸Šå‡çš„çŸ©å½¢æˆ–æ¡å½¢çš„é«˜åº¦ã€‚ è®¸å¤šåº”ç”¨é¢†åŸŸä¾èµ–äºç›´æ–¹å›¾æ¥æ€»ç»“æ•°æ®é›†è¿›è¡Œæ•°æ®åˆ†æã€‚å…¶ä¸­ä¸€ä¸ªé¢†åŸŸå°±æ˜¯è®¡ç®—æœºè§†è§‰ã€‚å›¾åƒå­åŒºåŸŸç›´æ–¹å›¾çš„è®¡ç®—è¿‡ç¨‹æ˜¯è®¡ç®—æœºè§†è§‰ä¸­ç‰¹å¾ (å›¾åƒä¸­æ„Ÿå…´è¶£çš„æ¨¡å¼) æå–çš„é‡è¦æ–¹æ³•ã€‚\nA Histogram Representation of â€œprogramming massively parallel processorsâ€\n9.2 Atomic Operations and A Basic Histogram Kernel å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå¹¶è¡ŒåŒ–ç›´æ–¹å›¾è®¡ç®—çš„æœ€ç›´æ¥çš„æ–¹æ³•æ˜¯å¯åŠ¨æ•°æ®ä¸€æ ·å¤šçš„çº¿ç¨‹ï¼Œè®©æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ªå…ƒç´ ã€‚æ¯ä¸ªçº¿ç¨‹è¯»å–å…¶åˆ†é…çš„è¾“å…¥å…ƒç´ ï¼Œå¹¶å¢åŠ å¯¹åº”çš„éš”è®¡æ•°å™¨çš„å€¼ã€‚\nBasic Parallelization of a Histogram\nhisto æ•°ç»„ä¸­é—´éš”è®¡æ•°å™¨çš„å¢åŠ æ˜¯å¯¹å†…å­˜ä½ç½®çš„æ›´æ–°æˆ– read-modify-write æ“ä½œã€‚è¯¥æ“ä½œåŒ…æ‹¬è¯»å–å†…å­˜ä½ç½®(è¯»)ï¼Œåœ¨åŸå§‹å€¼ä¸ŠåŠ  1(ä¿®æ”¹)ï¼Œå¹¶å°†æ–°å€¼å†™å›å†…å­˜ä½ç½® (å†™)ã€‚åœ¨å®é™…è¿‡ç¨‹ä¸­ä¼šå‡ºç°è¯»-ä¿®æ”¹-å†™ç«äº‰æ¡ä»¶ (read-modify-write race condition)ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸¤ä¸ªæˆ–å¤šä¸ªåŒæ­¥æ›´æ–°æ“ä½œçš„ç»“æœä¼šæ ¹æ®æ‰€æ¶‰åŠçš„æ“ä½œçš„ç›¸å¯¹æ—¶é—´è€Œå˜åŒ–ã€‚ ä¸‹å›¾ A ä¸­çº¿ç¨‹ 1 åœ¨æ—¶é—´æ®µ 1~3 æœŸé—´å®Œæˆäº†å…¶è¯»-ä¿®æ”¹-å†™åºåˆ—çš„æ‰€æœ‰ä¸‰ä¸ªéƒ¨åˆ†ï¼Œç„¶åçº¿ç¨‹ 2 åœ¨æ—¶é—´æ®µ 4 å¼€å§‹ï¼Œæœ€åç»“æœæ­£ç¡®ã€‚åœ¨å›¾ B ä¸­ï¼Œä¸¤ä¸ªçº¿ç¨‹çš„è¯»-ä¿®æ”¹-å†™é¡ºåºé‡å ã€‚çº¿ç¨‹ 1 åœ¨æ—¶é—´æ®µ 4 æ—¶å°†æ–°å€¼å†™å…¥ histo[x]ã€‚å½“çº¿ç¨‹ 2 åœ¨æ—¶é—´æ®µ 3 è¯»å– histo[x]æ—¶ï¼Œå®ƒçš„å€¼ä»ç„¶æ˜¯ 0ï¼Œå› æ­¤æœ€åçš„å†™å…¥çš„å€¼æ˜¯ 1.\nRace Condition in Updating a histo Array Element\nåŸå­æ“ä½œ (atomic operation) çš„è¯»ã€ä¿®æ”¹å’Œå†™éƒ¨åˆ†æ„æˆä¸€ä¸ªä¸å¯åˆ†å‰²çš„å•å…ƒï¼Œå› æ­¤ç§°ä¸ºåŸå­æ“ä½œã€‚å¯¹è¯¥ä½ç½®çš„å…¶ä»–è¯»-ä¿®æ”¹-å†™åºåˆ—ä¸èƒ½ä¸å…¶åœ¨æ—¶é—´ä¸Šæœ‰é‡å ã€‚éœ€è¦æ³¨æ„åŸå­æ“ä½œåœ¨çº¿ç¨‹ä¹‹é—´ä¸å¼ºåˆ¶ä»»ä½•ç‰¹å®šçš„æ‰§è¡Œé¡ºåºï¼Œæ¯”å¦‚çº¿ç¨‹ 1 å¯ä»¥åœ¨çº¿ç¨‹ 2 ä¹‹å‰æˆ–ä¹‹åè¿è¡Œã€‚CUDAå†…æ ¸å¯ä»¥é€šè¿‡å‡½æ•°è°ƒç”¨å¯¹å†…å­˜ä½ç½®æ‰§è¡ŒåŸå­åŠ æ³•æ“ä½œ:\nint atomicAdd(int* address, int val); atomicAdd æ˜¯ä¸€ä¸ªå†…å»ºå‡½æ•° (intrinsic function)ï¼Œå®ƒè¢«ç¼–è¯‘æˆä¸€ä¸ªç¡¬ä»¶åŸå­æ“ä½œæŒ‡ä»¤ã€‚è¯¥æŒ‡ä»¤è¯»å–å…¨å±€æˆ–å…±äº«å†…å­˜ä¸­ address å‚æ•°æ‰€æŒ‡å‘çš„32ä½å­—ï¼Œå°† val åŠ ä¸Šæ—§å€¼ä¸­å¹¶å†™å…¥ç»“æœå›ç›¸åŒåœ°å€çš„å†…å­˜ä¸­ã€‚è¯¥å‡½æ•°è¿”å›åœ°å€å¤„çš„æ—§å€¼ã€‚\nIntrinsic Functions ç°ä»£å¤„ç†å™¨é€šå¸¸æä¾›ç‰¹æ®ŠæŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤è¦ä¹ˆæ‰§è¡Œå…³é”®åŠŸèƒ½ (å¦‚åŸå­æ“ä½œ)ï¼Œè¦ä¹ˆå¤§å¹…æé«˜æ€§èƒ½ (å¦‚çŸ¢é‡æŒ‡ä»¤)ã€‚è¿™äº›æŒ‡ä»¤é€šå¸¸ä½œä¸ºå†…å»ºå‡½æ•°æš´éœ²ç»™ç¨‹åºå‘˜ï¼Œä»ç¨‹åºå‘˜çš„è§’åº¦æ¥çœ‹ï¼Œè¿™äº›æ˜¯åº“å‡½æ•°ã€‚ç„¶è€Œï¼Œå®ƒä»¬è¢«ç¼–è¯‘å™¨ä»¥ä¸€ç§ç‰¹æ®Šçš„æ–¹å¼å¤„ç†ã€‚æ¯ä¸ªè¿™æ ·çš„è°ƒç”¨éƒ½è¢«ç¿»è¯‘æˆç›¸åº”çš„ç‰¹æ®ŠæŒ‡ä»¤ã€‚åœ¨æœ€ç»ˆä»£ç ä¸­æ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œåªæœ‰ä¸ç”¨æˆ·ä»£ç ä¸€è‡´çš„ç‰¹æ®ŠæŒ‡ä»¤ã€‚ __global__ void histo_kernel(char* data, unsigned int length, unsigned int* histo) { unsigned int i = threadIdx.x + blockIdx.x * blockDim.x; if (i \u0026lt; length) { int alphabet_position = data[i] - \u0026#39;a\u0026#39;; if (alphabet_position \u0026gt;= 0 \u0026amp;\u0026amp; alphabet_position \u0026lt; 26) { atomicAdd(\u0026amp;histo[alphabet_position / 4], 1); } } } 9.3 Latency and Throughput of Atomic Operations é«˜å†…å­˜è®¿é—®ååé‡çš„å…³é”®æ˜¯åŒæ—¶è¿›è¡Œè®¸å¤š DRAM è®¿é—®ã€‚ç„¶è€Œï¼Œå½“è®¸å¤šåŸå­æ“ä½œæ›´æ–°ç›¸åŒçš„å†…å­˜ä½ç½®æ—¶ï¼Œä¸€ä¸ªåé¢çº¿ç¨‹çš„è¯»-ä¿®æ”¹-å†™åºåˆ—åœ¨å‰ä¸€ä¸ªçº¿ç¨‹çš„å†™æ“ä½œç»“æŸä¹‹å‰ä¸èƒ½å¼€å§‹ï¼Œå³å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒåŒæ—¶åªèƒ½æœ‰ä¸€ä¸ªçº¿ç¨‹åœ¨åŒä¸€å†…å­˜ä½ç½®æ‰§è¡ŒåŸå­æ“ä½œã€‚æ›´æ–°è¿™äº›é—´éš”çš„å¤§é‡äº‰ç”¨æµé‡ä¼šä½¿å¾—ååé‡é™ä½ã€‚\nThe Execution of Atomic Operations at the Same Location\næé«˜åŸå­æ“ä½œååé‡çš„ä¸€ç§æ–¹æ³•æ˜¯å‡å°‘å¯¹ç«äº‰ä¸¥é‡çš„ä½ç½®çš„è®¿é—®å»¶è¿Ÿã€‚ç°ä»£ GPU å…è®¸åœ¨è¢«æ‰€æœ‰ SM å…±äº«çš„æœ€åä¸€çº§ç¼“å­˜ä¸­æ‰§è¡ŒåŸå­æ“ä½œã€‚ç”±äºå¯¹æœ€åä¸€çº§ç¼“å­˜çš„è®¿é—®æ—¶é—´æ˜¯å‡ åä¸ªå‘¨æœŸè€Œä¸æ˜¯å‡ ç™¾ä¸ªå‘¨æœŸï¼Œå› æ­¤åŸå­æ“ä½œçš„ååé‡ä¸æ—©æœŸGPUç›¸æ¯”è‡³å°‘æé«˜äº†ä¸€ä¸ªæ•°é‡çº§ã€‚\n9.4 Privatization æé«˜åŸå­æ“ä½œååé‡çš„å¦ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡å¼•å¯¼æµé‡è¿œç¦»ç«äº‰ä¸¥é‡çš„ä½ç½®ã€‚è¿™å¯ä»¥é€šè¿‡ä¸€ç§ç§°ä¸ºç§æœ‰åŒ– (privatization) çš„æŠ€æœ¯æ¥å®ç°ã€‚å…¶æ€æƒ³æ˜¯å°†é«˜åº¦ç«äº‰çš„è¾“å‡ºæ•°æ®ç»“æ„å¤åˆ¶åˆ°ç§æœ‰å‰¯æœ¬ä¸­ï¼Œä»¥ä¾¿çº¿ç¨‹çš„æ¯ä¸ªå­é›†éƒ½å¯ä»¥æ›´æ–°å…¶ç§æœ‰å‰¯æœ¬ã€‚ ä¸‹å›¾å±•ç¤ºäº†å¦‚ä½•å°†ç§æœ‰åŒ–åº”ç”¨äºç›´æ–¹å›¾ç»Ÿè®¡ã€‚æ¯ä¸ªçº¿ç¨‹å—ç”± 8 ä¸ªçº¿ç¨‹ç»„æˆï¼Œäº‰ç”¨åªä¼šåœ¨åŒä¸€å—ä¸­çš„çº¿ç¨‹ä¹‹é—´ä»¥åŠåœ¨æœ€ååˆå¹¶ç§æœ‰å‰¯æœ¬æ—¶å‘ç”Ÿï¼Œè€Œä¸æ˜¯æ›´æ–°ç›¸åŒç›´æ–¹å›¾ bin çš„æ‰€æœ‰çº¿ç¨‹ä¹‹é—´å‘ç”Ÿäº‰ç”¨ã€‚\nReduce Contention of Atomic Operations by Private Copies of Histogram\nä¸€ä¸ªç§æœ‰åŒ–ç‰ˆæœ¬çš„ä»£ç å¦‚ä¸‹ï¼Œä¸º histo æ•°ç»„åˆ†é…è¶³å¤Ÿçš„è®¾å¤‡å†…å­˜ (gridDim.x*NUM_BINS*4 bytes) æ¥ä¿å­˜ç›´æ–¹å›¾çš„æ‰€æœ‰ç§æœ‰å‰¯æœ¬ã€‚åœ¨æ‰§è¡Œç»“æŸæ—¶ï¼Œæ¯ä¸ªçº¿ç¨‹å—å°†æŠŠç§æœ‰å‰¯æœ¬ä¸­çš„å€¼æäº¤åˆ° å— 0 çš„éƒ¨åˆ†ã€‚\n#define NUM_BINS 7 // # histo bins __global__ void histo_private_kernel(char* data, unsigned int length, unsigned int* histo) { unsigned int i = threadIdx.x + blockIdx.x * blockDim.x; if (i \u0026lt; length) { int alphabet_position = data[i] - \u0026#39;a\u0026#39;; if (alphabet_position \u0026gt;= 0 \u0026amp;\u0026amp; alphabet_position \u0026lt; 26) { atomicAdd(\u0026amp;histo[blockIdx.x * 7 + alphabet_position / 4], 1); } } if (blockIdx.x \u0026gt; 0) { __syncthreads(); // for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned int binValue = histo[blockIdx * NUM_BINS + bin]; atomicAdd(\u0026amp;histo[bin], binValue); } } } åœ¨æ¯ä¸ªçº¿ç¨‹å—çš„åŸºç¡€ä¸Šåˆ›å»ºç›´æ–¹å›¾çš„ç§æœ‰å‰¯æœ¬çš„ä¸€ä¸ªå¥½å¤„æ˜¯çº¿ç¨‹å¯ä»¥åœ¨æäº¤è‡ªå·±çš„ç»Ÿè®¡ç»“æœä¹‹å‰ä½¿ç”¨ __syncthreads() æ¥ç­‰å¾…å½¼æ­¤ã€‚å¦ä¸€ä¸ªå¥½å¤„æ˜¯ï¼Œå¦‚æœç›´æ–¹å›¾ä¸­çš„ bin æ•°é‡è¶³å¤Ÿå°ï¼Œåˆ™å¯ä»¥åœ¨å…±äº«å†…å­˜ä¸­å£°æ˜ç›´æ–¹å›¾çš„ç§æœ‰å‰¯æœ¬ (æ¯ä¸ªçº¿ç¨‹å—ä¸€ä¸ª)ã€‚ä¸‹é¢ä»£ç ç›´æ–¹å›¾åœ¨å…±äº«å†…å­˜ä¸­åˆ†é…ç§æœ‰å‰¯æœ¬ histo_s æ•°ç»„ï¼Œå¹¶ç”±å—çš„çº¿ç¨‹å¹¶è¡Œåˆå§‹åŒ–ä¸º 0.\n__global__ void histo_shared_private_kernel(char* data, unsigned int length, unsigned int* histo) { // Initializing private bins __shared__ unsigned int histo_s[NUM_BINS]; for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { histo_s[bin] = 0; } __syncthreads(); // Histogram unsigned int i = threadIdx.x + blockIdx.x * blockDim.x; if (i \u0026lt; length) { int alphabet_position = data[i] - \u0026#39;a\u0026#39;; if (alphabet_position \u0026gt;= 0 \u0026amp;\u0026amp; alphabet_position \u0026lt; 26) { atomicAdd(\u0026amp;histo_s[alphabet_position / 4], 1); } } __syncthreads(); // Commit to global memory for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned binValue = histo_s[bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;histo[bin], binValue); } } } 9.5 Coarsening ç§æœ‰åŒ–çš„å¼€é”€æ˜¯éœ€è¦å°†ç§æœ‰å‰¯æœ¬æäº¤åˆ°å…¬å…±å‰¯æœ¬ã€‚æ¯ä¸ªçº¿ç¨‹å—éƒ½ä¼šæ‰§è¡Œä¸€æ¬¡æäº¤æ“ä½œï¼Œå› æ­¤ï¼Œä½¿ç”¨çš„çº¿ç¨‹å—è¶Šå¤šï¼Œè¿™ä¸ªå¼€é”€å°±è¶Šå¤§ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‡å°‘å—çš„æ•°é‡æ¥å‡å°‘ç§æœ‰å‰¯æœ¬çš„æ•°é‡ï¼Œä»è€Œå‡å°‘æäº¤åˆ°å…¬å…±å‰¯æœ¬çš„æ¬¡æ•°ï¼Œè®©æ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªè¾“å…¥å…ƒç´ ã€‚\nContiguous Partition of Input Elements\nä¸‹é¢ä»£ç æ˜¯ä¸€ä¸ªè¿ç»­åˆ†åŒº (contiguous partition) ç­–ç•¥çš„ç¤ºä¾‹ï¼Œè¾“å…¥è¢«è¿ç»­åˆ’åˆ†æˆå¤šä¸ªæ®µï¼Œæ¯ä¸ªæ®µè¢«åˆ†é…ç»™ä¸€ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹ä» tid*CFACTOR è¿­ä»£åˆ° (tid+1)*CFACTOR è¿›è¡Œæ‰€è´Ÿè´£éƒ¨åˆ†çš„ç»Ÿè®¡ã€‚\n#define CFACTOR 3 __global__ void histo_shared_private_contiguous_kernel(char* data, unsigned int length, unsigned int* histo) { { // Initializing private bins __shared__ unsigned int histo_s[NUM_BINS]; for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { histo_s[bin] = 0; } __syncthreads(); // Histogram unsigned tid = blockIdx.x * blockDim.x + threadIdx.x; for (unsigned int i = tid * CFACTOR; i \u0026lt; (tid + 1)*CFACTOR \u0026amp;\u0026amp; i \u0026lt; length; i++) { int alphabet_position = data[i] - \u0026#39;a\u0026#39;; if (alphabet_position \u0026gt;= 0 \u0026amp;\u0026amp; alphabet_position \u0026lt; 26) { atomicAdd(\u0026amp;histo_s[alphabet_position / 4], 1); } } __syncthreads(); // Commit to global memory for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned binValue = histo_s[bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;histo[bin], binValue); } } } ä¸Šè¿°åœ¨ GPU ä¸Šè¿ç»­åˆ†åŒºçš„æ€è·¯ä¼šå¯¼è‡´å†…å­˜ä¸å‹å¥½çš„è®¿é—®æ¨¡å¼ï¼Œå› ä¸º threadIdx ç›¸åŒçš„çº¿ç¨‹è®¿é—®çš„ä¸æ˜¯ä¸€å—è¿ç»­çš„å†…å­˜åŒºåŸŸã€‚å› æ­¤æˆ‘ä»¬è¦é‡‡ç”¨äº¤é”™åˆ†åŒº (interleaved partition)ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå³ä¸åŒçº¿ç¨‹è¦å¤„ç†çš„åˆ†åŒºå½¼æ­¤äº¤é”™ã€‚å®é™…åº”ç”¨ä¸­æ¯ä¸ªçº¿ç¨‹åœ¨æ¯æ¬¡è¿­ä»£ä¸­åº”è¯¥å¤„ç† 4 ä¸ª char (ä¸€ä¸ª 32 ä½å­—)ï¼Œä»¥å……åˆ†åˆ©ç”¨ç¼“å­˜å’Œ SMs ä¹‹é—´çš„äº’è¿å¸¦å®½ã€‚\nInterleaved Partition of Input Elements\nä¸‹é¢ä»£ç æ˜¯ä¸€ä¸ªäº¤é”™åˆ†åŒºçš„ç¤ºä¾‹ã€‚åœ¨å¾ªç¯çš„ç¬¬ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨å…¶å…¨å±€çº¿ç¨‹ç´¢å¼•è®¿é—®æ•°æ®æ•°ç»„:çº¿ç¨‹ 0 è®¿é—®å…ƒç´  0ï¼Œçº¿ç¨‹ 1 è®¿é—®å…ƒç´  1ï¼Œçº¿ç¨‹ 2 è®¿é—®å…ƒç´  2\u0026hellip;æ‰€æœ‰çº¿ç¨‹å…±åŒå¤„ç†è¾“å…¥çš„ç¬¬ä¸€ä¸ª blockDim.x*gridDim.x å…ƒç´ ã€‚\n__global__ void histo_shared_private_interleaved_kernel(char* data, unsigned int length, unsigned int* histo) { { // Initializing private bins __shared__ unsigned int histo_s[NUM_BINS]; for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { histo_s[bin] = 0; } __syncthreads(); // Histogram unsigned tid = blockIdx.x * blockDim.x + threadIdx.x; for (unsigned int i = tid; i \u0026lt; length; i += blockDim.x * gridDim.x) { int alphabet_position = data[i] - \u0026#39;a\u0026#39;; if (alphabet_position \u0026gt;= 0 \u0026amp;\u0026amp; alphabet_position \u0026lt; 26) { atomicAdd(\u0026amp;histo_s[alphabet_position / 4], 1); } } __syncthreads(); // Commit to global memory for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned binValue = histo_s[bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;histo[bin], binValue); } } } 9.6 Aggregation ä¸€äº›æ•°æ®é›†åœ¨å±€éƒ¨åŒºåŸŸæœ‰å¤§é‡ç›¸åŒçš„æ•°æ®å€¼ã€‚å¦‚æ­¤é«˜åº¦é›†ä¸­çš„ç›¸åŒå€¼ä¼šå¯¼è‡´ä¸¥é‡çš„äº‰ç”¨ï¼Œå¹¶é™ä½å¹¶è¡Œç›´æ–¹å›¾è®¡ç®—çš„ååé‡ã€‚ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„ä¼˜åŒ–æ˜¯ï¼Œå¦‚æœæ¯ä¸ªçº¿ç¨‹æ­£åœ¨æ›´æ–°ç›´æ–¹å›¾çš„ç›¸åŒå…ƒç´ ï¼Œåˆ™å°†è¿ç»­çš„æ›´æ–°èšåˆä¸ºå•ä¸ªæ›´æ–°ã€‚ä¸‹é¢çš„ä»£ç å±•ç¤ºäº†èšåˆçš„ç›´æ–¹å›¾è®¡ç®—ã€‚\n__global__ void histo_shared_private_interleaved_aggregated_kernel(char* data, unsigned int length, unsigned int* histo) { // Initializing private bins __shared__ unsigned int histo_s[NUM_BINS]; for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { histo_s[bin] = 0; } __syncthreads(); // Histogram unsigned int accumulator = 0; int prevBinIdx = -1; unsigned tid = blockIdx.x * blockDim.x + threadIdx.x; for (unsigned int i = tid; i \u0026lt; length; i += blockDim.x * gridDim.x) { int alphabet_position = data[i] - \u0026#39;a\u0026#39;; if (alphabet_position \u0026gt;= 0 \u0026amp;\u0026amp; alphabet_position \u0026lt; 26) { int currBinIdx = alphabet_position / 4; if (currBinIdx != prevBinIdx) { // Update previous statistics if (accumulator \u0026gt; 0) { atomicAdd(\u0026amp;histo_s[prevBinIdx], accumulator); } accumulator = 1; prevBinIdx = currBinIdx; } else { // Accumulate statistics accumulator++; } } } if (accumulator \u0026gt; 0) { // Update last bin atomicAdd(\u0026amp;histo_s[prevBinIdx], accumulator); } __syncthreads(); // Commit to global memory for (unsigned int bin = threadIdx.x; bin \u0026lt; NUM_BINS; bin += blockDim.x) { unsigned binValue = histo_s[bin]; if (binValue \u0026gt; 0) { atomicAdd(\u0026amp;histo[bin], binValue); } } } å¯ä»¥çœ‹å‡ºèšåˆå†…æ ¸éœ€è¦æ›´å¤šçš„è¯­å¥å’Œå˜é‡ã€‚æ·»åŠ çš„ if è¯­å¥å¯èƒ½ä¼šå‡ºç°æ§åˆ¶å‘æ•£ã€‚ç„¶è€Œï¼Œå¦‚æœæ²¡æœ‰äº‰ç”¨æˆ–å­˜åœ¨ä¸¥é‡çš„äº‰ç”¨ï¼Œå°±å¾ˆå°‘æœ‰æ§åˆ¶å‘æ•£ï¼Œå› ä¸ºçº¿ç¨‹è¦ä¹ˆéƒ½åœ¨å¢åŠ ç´¯åŠ å™¨å€¼ï¼Œè¦ä¹ˆéƒ½åœ¨è¿ç»­åˆ·æ–°ã€‚\n","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch9/","summary":"Personal notebook 9 of Programming Massively Parallel Processors.","title":"PMPP Learning-Chapter 9 Parallel Histogram-An Introduction to Atomic Operations and Privatization"},{"content":"7 Convolution-An Introduction to Constant Memory and Caching å·ç§¯çš„æ¯ä¸ªè¾“å‡ºæ•°æ®å…ƒç´ å¯ä»¥ç›¸äº’ç‹¬ç«‹åœ°è®¡ç®—ï¼Œè¿™æ˜¯å¹¶è¡Œè®¡ç®—çš„ç†æƒ³ç‰¹æ€§ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨å¤„ç†å…·æœ‰è¾¹ç•Œæ¡ä»¶çš„è¾“å‡ºæ•°æ®å…ƒç´ æ—¶ï¼Œæœ‰å¤§é‡çš„è¾“å…¥æ•°æ®å…±äº«ã€‚è¿™ä½¿å¾—å·ç§¯å¯ä»¥å®ç°å¤æ‚çš„ tiling æ–¹æ³•å’Œè¾“å…¥æ•°æ®åˆ†æ®µæ–¹æ³•ã€‚\n7.1 Background è¾“å…¥æ•°æ®å‘é‡ $[x_0, x_1, \\cdots, x_{n-1}]$ å’ŒåŒ…å« 2r+1 ä¸ªå…ƒç´ çš„ filter æ•°ç»„ $[f_0, f_1, \\cdots, f_{2r}]$ï¼Œ 1Då·ç§¯è®¡ç®—å…¬å¼ä¸º $$y_i=\\sum_{j=-r}^rf_{i+j}\\times x_i$$ åŒæ ·å¯¹äº n*n å¤§å°çš„äºŒç»´è¾“å…¥ï¼Œå’Œ r*r å¤§å°çš„ filterï¼Œ2D å·ç§¯è®¡ç®—å…¬å¼ä¸º $$P_{y,x}=\\sum_{j=-r_y}^{r_y}\\sum_{k=-r_x}^{r_x}f_{y+j,x+k}\\times N_{y,x}$$7.2 Parallel Convolution: a Basic Algorithm å‡è®¾äºŒç»´å·ç§¯å†…æ ¸æ¥æ”¶äº”ä¸ªå‚æ•°: è¾“å…¥æ•°ç»„ N çš„æŒ‡é’ˆ; æ»¤æ³¢å™¨ F çš„æŒ‡é’ˆ; è¾“å‡ºæ•°ç»„ P çš„æŒ‡é’ˆ; æ–¹å½¢æ»¤æ³¢å™¨çš„åŠå¾„ r; è¾“å…¥è¾“å‡ºæ•°ç»„çš„å®½åº¦; è¾“å…¥å’Œè¾“å‡ºæ•°ç»„çš„é«˜åº¦ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸€ä¸ªç®€å•çš„å¹¶è¡Œæ–¹å¼æ˜¯ç½‘æ ¼ä¸­çš„æ¯ä¸ªçº¿ç¨‹è®¡ç®—ä¸è‡ªèº«åæ ‡ç›¸åŒçš„è¾“å‡ºåƒç´ ã€‚å¯¹åº”çš„å†…æ ¸å‡½æ•°ä»£ç å¦‚ä¸‹ï¼Œæµ®ç‚¹è®¡ç®—ä¸å…¨å±€å†…å­˜è®¿é—®çš„æ¯”ä»…ä¸º 0.25 OP/B (æ¯åŠ è½½ 8 å­—èŠ‚æ‰§è¡Œ 2 æ¬¡è¿ç®—)\nParallelization and Thread Organization for 2D Convolution\n__global__ void convolution_2D_basic_kernel (float *N, float *F, float *P, int r, int width, int height) { int outCol = blockIdx.x * blockDim.x + threadIdx.x; int outRow = blockIdx.y * blockDim.y + threadIdx.y; int Pvalue = 0.0f; for (int fRow = 0; fRow \u0026lt; 2*r+1; fRow++) { for (int fCol = 0; fCol \u0026lt; 2 * r + 1; fCol++) { int inRow = outRow - r + fRow; int inCol = outCol - r + fCol; if (inRow \u0026gt; 0 \u0026amp;\u0026amp; inRow \u0026lt; height \u0026amp;\u0026amp; inCol \u0026gt; 0 \u0026amp;\u0026amp; inCol \u0026lt; width) { Pvalue += P[inRow * width + inCol] * F[fRow * r + fCol]; } } } P[outRow * width + outCol] = Pvalue; } 7.3 Constant Memory and Caching å¯ä»¥å‘ç°å·ç§¯æ ¸ F é€šå¸¸å¾ˆå°ï¼Œåœ¨æ•´ä¸ªå·ç§¯å†…æ ¸çš„æ‰§è¡Œè¿‡ç¨‹ä¸­ä¸ä¼šæ”¹å˜ï¼Œæ‰€æœ‰çº¿ç¨‹éƒ½ä»¥ç›¸åŒçš„é¡ºåºè®¿é—®å…¶å…ƒç´ ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥è€ƒè™‘å°†å…¶å­˜å‚¨åœ¨å¸¸é‡å†…å­˜é‡Œï¼Œä¹‹å‰è¯´è¿‡å®ƒå’Œå…¨å±€å†…å­˜çš„åŒºåˆ«æ˜¯çº¿ç¨‹ä¸èƒ½ä¿®æ”¹å¸¸é‡å†…å­˜å˜é‡çš„å€¼å¹¶ä¸”å¸¸é‡å†…å­˜éå¸¸å°ï¼Œç›®å‰ä¸º 64 KB. å‡è®¾å·²ç»åœ¨ä¸»æœºä»£ç é‡Œåˆ†é…å¥½ F_h çš„å†…å­˜ï¼Œå¯ä»¥é€šè¿‡ cudaMemcpyToSymbol() å°†å…¶ä»ä¸»æœºå†…å­˜ä¼ è¾“åˆ°è®¾å¤‡å¸¸é‡å†…å­˜ä¸­ã€‚å†…æ ¸å‡½æ•°ä»¥å…¨å±€å˜é‡çš„å½¢å¼è®¿é—®å¸¸é‡å†…å­˜å˜é‡ã€‚å› æ­¤ï¼Œå®ƒä»¬çš„æŒ‡é’ˆä¸éœ€è¦ä½œä¸ºå‚æ•°ä¼ é€’ç»™å†…æ ¸å‡½æ•°ã€‚\nå¦‚æœä¸»æœºä»£ç å’Œå†…æ ¸ä»£ç ä½äºä¸åŒçš„æ–‡ä»¶ä¸­ï¼Œå†…æ ¸ä»£ç æ–‡ä»¶å¿…é¡»åŒ…å«ç›¸å…³çš„å¤–éƒ¨å£°æ˜çš„å¤´æ–‡ä»¶ï¼Œä»¥ç¡®ä¿å£°æ˜å¯¹å†…æ ¸å¯è§ã€‚\nCUDA runtime çŸ¥é“å¸¸é‡å†…å­˜å˜é‡åœ¨å†…æ ¸æ‰§è¡ŒæœŸé—´ä¸ä¼šè¢«ä¿®æ”¹ï¼Œå› æ­¤ä¼šè®©ç¡¬ä»¶åœ¨å†…æ ¸æ‰§è¡ŒæœŸé—´ç›´æ¥ç¼“å­˜å¸¸é‡å†…å­˜å˜é‡ã€‚åœ¨ä¸éœ€è¦æ”¯æŒå†™çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥åœ¨å‡å°èŠ¯ç‰‡é¢ç§¯å’Œé™ä½åŠŸè€—çš„æƒ…å†µä¸‹è®¾è®¡ç”¨äºå¸¸é‡å†…å­˜å˜é‡çš„ä¸“ç”¨ç¼“å­˜ï¼Œè¢«ç§°ä¸ºå¸¸é‡ç¼“å­˜ (constant caching).\n7.4 Tiled Convolution with Halo Cells æˆ‘ä»¬å®šä¹‰è¾“å‡º tile ä¸ºæ¯ä¸ªå—å¤„ç†çš„è¾“å‡ºå…ƒç´ ï¼Œè¾“å…¥ tile ä¸ºè®¡ç®—è¾“å‡º tile ä¸­å…ƒç´ æ‰€éœ€çš„è¾“å…¥å…ƒç´ çš„é›†åˆã€‚ä¸‹å›¾ç»™å‡ºäº†ä¸€ä¸ªä¾‹å­ï¼Œå¯ä»¥çœ‹åˆ°è¾“å…¥ tile å¤§å°å’Œè¾“å‡º tile å¤§å°ä¹‹é—´çš„å·®å¼‚ä½¿ tile å·ç§¯æ ¸çš„è®¾è®¡å˜å¾—å¤æ‚ã€‚æœ‰ä¸¤ç§çº¿ç¨‹ç»„ç»‡å¯ä»¥å¤„ç†è¿™ç§å·®å¼‚ã€‚\nå¯åŠ¨ä¸è¾“å…¥ tile å…·æœ‰ç›¸åŒç»´åº¦çš„çº¿ç¨‹å—ã€‚è¿™æ ·å› ä¸ºæ¯ä¸ªçº¿ç¨‹åªéœ€è¦åŠ è½½ä¸€ä¸ªè¾“å…¥å…ƒç´ ã€‚ä½†ç”±äºè¾“å…¥ tile æ¯”å¯¹åº”çš„è¾“å‡º tile å¤§ï¼Œåœ¨è®¡ç®—è¾“å‡ºå…ƒç´ æ—¶éœ€è¦ç¦ç”¨ä¸€äº›çº¿ç¨‹ï¼Œé™ä½äº†èµ„æºåˆ©ç”¨ç‡ã€‚ å¯åŠ¨ä¸è¾“å‡º tile å…·æœ‰ç›¸åŒç»´åº¦çš„çº¿ç¨‹å—ã€‚è¿™æ ·çº¿ç¨‹éœ€è¦è¿­ä»£ä»¥ç¡®ä¿åŠ è½½æ‰€æœ‰è¾“å…¥ tile å…ƒç´ ã€‚ä½†ç®€åŒ–äº†è¾“å‡ºå…ƒç´ çš„è®¡ç®—ã€‚ Input Tile vs. Output Tile in 2D Convolution\nç¬¬ä¸€ç§çº¿ç¨‹ç»„ç»‡æ–¹å¼çš„å†…æ ¸å¦‚ä¸‹ã€‚ç°åœ¨æ¯ä¸ªå—ä¸­çš„çº¿ç¨‹å…±åŒæ‰§è¡Œ OUT_TILE_DIM^2*(2*FILTER_RADIUS+1) æ¬¡æµ®ç‚¹è¿ç®—ã€‚åˆ†é…ç»™è¾“å…¥ tile å…ƒç´ çš„æ¯ä¸ªçº¿ç¨‹åŠ è½½ä¸€ä¸ª4å­—èŠ‚çš„è¾“å…¥å€¼ã€‚å› æ­¤æ¯ä¸ªblockåŠ è½½ IN_TILE_DIM^2*4=(OUT_TILE_DIM+2*FILTER_RADIUS)^2*4\n#define IN_TILE_DIM 32 #define FILTER_RADIUS 5 #define OUT_TILE_DIM (IN_TILE_DIM - 2*(FILTER_RADIUS)) __constant__ float F_c[2 * FILTER_RADIUS + 1][FILTER_RADIUS + 1]; __global__ void convolution_tiled_2D_constant_mem_kernel_1( float* N, float* P, int width, int height) { // Upper left input tile coord int col = blockIdx.x * OUT_TILE_DIM + threadIdx.x - FILTER_RADIUS; int row = blockIdx.y * OUT_TILE_DIM + threadIdx.y - FILTER_RADIUS; // Loading input tile __shared__ float N_s[IN_TILE_DIM][IN_TILE_DIM]; if (row \u0026gt;= 0 \u0026amp;\u0026amp; row \u0026lt; height \u0026amp;\u0026amp; col \u0026gt;= 0 \u0026amp;\u0026amp; col \u0026lt; width) { N_s[threadIdx.y][threadIdx.x] = N[row * width + col]; } else { N_s[threadIdx.y][threadIdx.x] = 0.0f; } __syncthreads(); // Calculate output elements int tileCol = threadIdx.x - FILTER_RADIUS; int tileRow = threadIdx.y - FILTER_RADIUS; if (row \u0026gt;= 0 \u0026amp;\u0026amp; row \u0026lt; height \u0026amp;\u0026amp; col \u0026gt;= 0 \u0026amp;\u0026amp; col \u0026lt; width \u0026amp;\u0026amp; tileCol \u0026gt;= 0 \u0026amp;\u0026amp; tileCol \u0026lt; OUT_TILE_DIM \u0026amp;\u0026amp; tileRow \u0026gt;= 0 \u0026amp;\u0026amp; tileRow \u0026lt; OUT_TILE_DIM) { float Pvalue = 0.0f; for (int fRow = 0; fRow \u0026lt; 2 * FILTER_RADIUS + 1; fRow++) { for (int fCol = 0; fCol \u0026lt; 2 * FILTER_RADIUS + 1; fCol++) { Pvalue += F_c[fRow][fCol] * N_s[tileRow + fRow][tileCol + fCol]; } } P[row * width + col] = Pvalue; } } ç¬¬äºŒç§çº¿ç¨‹ç»„ç»‡æ–¹å¼çš„å†…æ ¸å¦‚ä¸‹ï¼Œæ¯ä¸ªçº¿ç¨‹ç°åœ¨å¯èƒ½éœ€è¦åŠ è½½å¤šä¸ªè¾“å…¥ tile çš„å…ƒç´ ã€‚\n__global__ void convolution_tiled_2D_constant_mem_kernel_2( // OUT_TILE_DIM^2 threads per block float* N, float* P, int width, int height) { // Upper left output tile coord int col = blockIdx.x * OUT_TILE_DIM + threadIdx.x; int row = blockIdx.y * OUT_TILE_DIM + threadIdx.y; // Each thread may need to load multiple elements into shared memory __shared__ float N_s[IN_TILE_DIM][IN_TILE_DIM]; for (int i = threadIdx.y; i \u0026lt; IN_TILE_DIM; i += OUT_TILE_DIM) { for (int j = threadIdx.x; j \u0026lt; IN_TILE_DIM; j += OUT_TILE_DIM) { int in_col = blockIdx.x * OUT_TILE_DIM + j - FILTER_RADIUS; int in_row = blockIdx.y * OUT_TILE_DIM + i - FILTER_RADIUS; if (in_row \u0026gt;= 0 \u0026amp;\u0026amp; in_row \u0026lt; height \u0026amp;\u0026amp; in_col \u0026gt;= 0 \u0026amp;\u0026amp; in_col \u0026lt; width) { N_s[i][j] = N[in_row * width + in_col]; } else { N_s[i][j] = 0.0f; } } } __syncthreads(); // Calculate output elements if (threadIdx.x \u0026lt; OUT_TILE_DIM \u0026amp;\u0026amp; threadIdx.y \u0026lt; OUT_TILE_DIM \u0026amp;\u0026amp; row \u0026lt; height \u0026amp;\u0026amp; col \u0026lt; width) { float Pvalue = 0.0f; for (int fRow = 0; fRow \u0026lt; 2 * FILTER_RADIUS + 1; fRow++) { for (int fCol = 0; fCol \u0026lt; 2 * FILTER_RADIUS + 1; fCol++) { Pvalue += F_c[fRow][fCol] * N_s[threadIdx.y + fRow][threadIdx.x + fCol]; } } P[row * width + col] = Pvalue; } } 7.5 Tiled Convolution Using Caches for Halo Cells å½“ä¸€ä¸ªå—éœ€è¦å®ƒçš„ halo cell æ—¶ï¼Œç”±äºç›¸é‚»å—çš„è®¿é—®ï¼Œå®ƒä»¬å·²ç»åœ¨äºŒçº§ç¼“å­˜ä¸­äº†ã€‚å› æ­¤ï¼Œå¯¹è¿™äº› halo cell çš„å†…å­˜è®¿é—®å¯ä»¥ä» L2 ç¼“å­˜æä¾›ï¼Œè€Œä¸ä¼šé€ æˆé¢å¤–çš„ DRAM æµé‡ã€‚æˆ‘ä»¬å¯ä»¥å¯¹åŸæ¥çš„ N è¿›è¡Œè¿™äº› halo cell çš„è®¿é—®ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬åŠ è½½åˆ° N_ds ä¸­ã€‚ä»£ç å¦‚ä¸‹ï¼ŒåŠ è½½ N_s å˜å¾—æ›´ç®€å•ï¼Œå› ä¸ºæ¯ä¸ªçº¿ç¨‹å¯ä»¥ç®€å•åœ°åŠ è½½ä¸å…¶åˆ†é…çš„è¾“å‡ºå…ƒç´ å…·æœ‰ç›¸åŒåæ ‡çš„è¾“å…¥å…ƒç´ ã€‚ç„¶è€Œï¼Œè®¡ç®—Pä¸ªå…ƒç´ çš„å¾ªç¯ä½“å˜å¾—æ›´åŠ å¤æ‚ã€‚å®ƒéœ€è¦æ·»åŠ æ¡ä»¶æ¥æ£€æŸ¥ helo cell å’Œ ghost cell.\n__global__ void convolution_tiled_cached_2D_shared_mem_kernel( // OUT_TILE_DIM^2 threads per block float* N, float* P, int width, int height) { int col =blockIdx.x * OUT_TILE_DIM + threadIdx.x; int row =blockIdx.y * OUT_TILE_DIM + threadIdx.y; // loading input tile __shared__ float N_s[IN_TILE_DIM][IN_TILE_DIM]; if (row \u0026lt; height \u0026amp;\u0026amp; col \u0026lt; width) { N_s[threadIdx.y][threadIdx.x] = N[row * width + col]; } else { N_s[threadIdx.y][threadIdx.x] = 0.0f; } __syncthreads(); // Calculate output elements if (col \u0026lt; width \u0026amp;\u0026amp; row \u0026lt; height) { float Pvalue = 0.0f; // turning off the threads at the edge of the block for (int fRow = 0; fRow \u0026lt; 2 * FILTER_RADIUS + 1; fRow++) { for (int fCol = 0; fCol \u0026lt; 2 * FILTER_RADIUS + 1; fCol++) { if (threadIdx.x + fCol - FILTER_RADIUS \u0026gt;= 0 \u0026amp;\u0026amp; threadIdx.x + fCol - FILTER_RADIUS \u0026lt; IN_TILE_DIM \u0026amp;\u0026amp; threadIdx.x + fRow - FILTER_RADIUS \u0026gt;= 0 \u0026amp;\u0026amp; threadIdx.x + fRow - FILTER_RADIUS \u0026lt; IN_TILE_DIM) { Pvalue += F_c[fRow][fCol] * N_s[threadIdx.y + fRow][threadIdx.x + fCol]; } else { if (row - FILTER_RADIUS + fRow \u0026gt;= 0 \u0026amp;\u0026amp; row - FILTER_RADIUS + fRow \u0026lt; height \u0026amp;\u0026amp; col - FILTER_RADIUS + fCol \u0026gt;= 0 \u0026amp;\u0026amp; col - FILTER_RADIUS + fCol \u0026lt; width) { Pvalue += F_c[fRow][fCol] * N[(row - FILTER_RADIUS + fRow) * width + (col - FILTER_RADIUS + fCol)]; } } } } N[row * width + col] = Pvalue; } } Halo Cell: å®é™…è®¡ç®—åŒºåŸŸå‘¨å›´æ·»åŠ çš„ä¸€åœˆé¢å¤–çš„å•å…ƒæ ¼ã€‚æœ¬è´¨ä¸Šæ˜¯ \u0026ldquo;è™šæ‹Ÿ\u0026rdquo; å•å…ƒæ ¼ï¼Œå­˜åœ¨äºä¸ç›´æ¥å…³æ³¨çš„åŒºåŸŸä¹‹å¤–ã€‚ Ghost Cell: å­˜å‚¨æ¥è‡ªç›¸é‚» tile çš„æ•°æ®å‰¯æœ¬ï¼Œä½¿å¾— block åœ¨æ— éœ€ç›´æ¥è®¿é—®å½¼æ­¤çš„å†…å­˜çš„æƒ…å†µä¸‹è®¿é—®ç›¸é‚»çš„å¿…è¦æ•°æ®ã€‚ ","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch7/","summary":"Personal notebook 7 of Programming Massively Parallel Processors.","title":"PMPP Learning-Chapter 7 Convolution-An Introduction to Constant Memory and Caching"},{"content":"6 Performance Considerations å¹¶è¡Œç¨‹åºçš„æ‰§è¡Œé€Ÿåº¦æ ¹æ®ç¨‹åºçš„èµ„æºéœ€æ±‚å’Œç¡¬ä»¶çš„èµ„æºçº¦æŸä¹‹é—´çš„ç›¸äº’åˆ¶çº¦ä¼šæœ‰å¾ˆå¤§çš„å˜åŒ–ã€‚ç®¡ç†å¹¶è¡Œä»£ç å’Œç¡¬ä»¶èµ„æºçº¦æŸä¹‹é—´çš„äº¤äº’å¯¹äºåœ¨å‡ ä¹æ‰€æœ‰å¹¶è¡Œç¼–ç¨‹æ¨¡å‹ä¸­å®ç°é«˜æ€§èƒ½éå¸¸é‡è¦ã€‚\n6.1 Memory Coalescing å½±å“ CUDA å†…æ ¸æ€§èƒ½æœ€é‡è¦çš„å› ç´ ä¹‹ä¸€æ˜¯è®¿é—®å…¨å±€å†…å­˜ä¸­çš„æ•°æ®ï¼Œæœ‰é™çš„å¸¦å®½å¯èƒ½æˆä¸ºç“¶é¢ˆã€‚CUDA è®¾å¤‡çš„å…¨å±€å†…å­˜æ˜¯ç”¨ DRAM å®ç°çš„ã€‚æ•°æ®å­˜å‚¨åœ¨DRAMå•å…ƒä¸­ï¼Œè®¿é—®æ—¶é—´é€šå¸¸æ˜¯çº³ç§’çº§åˆ«ï¼Œç›¸å¯¹äºäºšçº³ç§’çº§åˆ«çš„æ—¶é’Ÿå‘¨æœŸæ¥è¯´å¾ˆæ…¢ã€‚ç°ä»£ DRAM é€šè¿‡å¹¶è¡ŒåŒ–è®¾è®¡æ¥æé«˜æ•°æ®è®¿é—®é€Ÿç‡ï¼Œé€šå¸¸ç§°ä¸ºå†…å­˜è®¿é—®ååé‡ (memory access throughput).\nWhy Are DRAMs So Slow DRAM é€šè¿‡ä¸€ä¸ªä¸ª CMOS æ™¶ä½“ç®¡ (ç§°ä¸º cell) æ¥å­˜å‚¨ 0/1. å½“ç»™æ™¶ä½“ç®¡æœ€ä¸Šé¢çš„ä¸€ç«¯ (ç§°ä½œæ …æ) åŠ ä¸Šç”µå‹æˆ–æ˜¯å–æ¶ˆç”µå‹ï¼Œæ™¶ä½“ç®¡ä¸¤ç«¯å°±å¯ä»¥æµè¿‡ç”µæµã€‚cell ä¸­çš„å°ç”µå®¹æ˜¯å­˜å‚¨ä¿¡æ¯çš„å…³é”®ï¼Œå°ç”µå®¹å¯ä»¥å­˜å‚¨ç”µè·ï¼Œå½“ç”µå®¹å­˜æœ‰ç”µè·ï¼Œcell å­˜å‚¨ 1ï¼›å½“ç”µå®¹ä¸å­˜ç”µè·ï¼Œå­˜å‚¨ 0. å½“è¦è¯»å– cell çš„å­˜å‚¨å€¼ï¼Œé¦–å…ˆæ‰“å¼€æ™¶ä½“ï¼‰ï¼Œç„¶åæ ¹æ®å¯¼é€šåçš„ç”µå®¹æ˜¯å¦ä¼šè¿›è¡Œå……æ”¾ç”µä¿¡æ¯è·å¾—å­˜å‚¨å€¼ã€‚å¦‚æœ cell å­˜å‚¨ 1ï¼Œå³ç”µå®¹å­˜æœ‰ç”µè·ï¼Œé‚£ä¹ˆå½“æ‰“å¼€å¼€å…³æ—¶ç”µå®¹å°±ä¼šæ”¾ç”µï¼›åä¹‹åˆ™ä¸ä¼šã€‚ ä¸€ä¸ª cell åªèƒ½å­˜å‚¨ 1 æ¯”ç‰¹ä¿¡æ¯ï¼Œä¸ºäº†å­˜å‚¨å¤§é‡ä¿¡æ¯ï¼Œéœ€è¦æ„å»ºèµ·å¦‚å›¾æ‰€ç¤ºçš„ cell é˜µåˆ—ã€‚å¯ä»¥çœ‹åˆ°æ¯è¡Œ cell çš„æ™¶ä½“ç®¡çš„æ …æéƒ½æ˜¯è¿åœ¨ä¸€èµ·çš„ï¼Œå³éƒ½è¿åœ¨å­—çº¿ (word line) ä¸Šï¼Œè¿™æ„å‘³ç€ç»™å­—çº¿æ–½åŠ ç”µå‹ï¼Œå­—çº¿å¯¹åº”çš„ä¸€è¡Œcelléƒ½ä¼šè¢«æ‰“å¼€ã€‚å½“ä¸€è¡Œ cell è¢«æ‰“å¼€ï¼Œcell ç”µå®¹å°±ä¼šå‘ä½çº¿ (bit line) å……æ”¾ç”µï¼Œä¸€è¡Œä¸­çš„æ¯ä¸ª cell éƒ½ä¸ä¸€æ¡ä½çº¿ç›´æ¥ç›¸è¿ï¼Œè¯»å–ä½çº¿çš„ç”µå‹å˜åŒ–ï¼Œå³å¯çŸ¥é“ cell çš„å­˜å‚¨ä¿¡æ¯ã€‚\nå­—çº¿ï¼šç”¨æ¥æ§åˆ¶è¯»å–å“ªä¸€ä¸ªå­—ï¼Œä¸€ä¸ªå­—ç”± 4å­—èŠ‚ç»„æˆã€‚ä¹‹æ‰€ä»¥å«å­—çº¿ï¼Œæ˜¯å› ä¸ºç»™è¿™æ ¹çº¿é€šç”µï¼Œä¸€è¡Œ cell éƒ½ä¼šè¢«æ‰“å¼€.å¤šä¸ª cell ç»„åˆèµ·æ¥å°±æ˜¯å¤šä¸ªå­—ï¼Œå› ä¸ºè¿™æ ¹çº¿å¯ä»¥æ‰“å¼€å¤šä¸ªå­—ï¼Œæ‰€ä»¥å«å­—çº¿ ä½çº¿ï¼šåœ¨è¯»å–ä¿¡æ¯æ—¶ï¼Œæ¯ä¸€æ ¹çº¿ä¸Šçš„ç”µå‹æ³¢åŠ¨éƒ½ä»£è¡¨ä¸€ä½æ¯”ç‰¹ä¿¡æ¯ï¼Œæ‰€ä»¥å«åšä½çº¿ã€‚ cell çš„è¯»å–ä¾é å°ç”µå®¹å……æ”¾ç”µï¼Œç”µå®¹å……æ”¾ç”µå¯¼è‡´ä½çº¿äº§ç”Ÿç”µå‹æ³¢åŠ¨ï¼Œé€šè¿‡è¯»å–ä½çº¿ç”µå‹æ³¢åŠ¨å³å¯è·å–ä¿¡æ¯ã€‚å°ç”µå®¹å……æ”¾ç”µæ‰€äº§ç”Ÿçš„ç”µå‹æ³¢åŠ¨æ˜¯å¾ˆå¾®å¼±çš„ï¼Œå……æ”¾ç”µæ‰€é€ æˆçš„ç”µå‹æ³¢åŠ¨çš„æ—¶é—´ä¹Ÿæ˜¯å¾ˆçŸ­çš„ï¼Œå› æ­¤å¾ˆéš¾ç›´æ¥è¯»å–å……æ”¾ç”µä¿¡æ¯ï¼Œä¸ºæ­¤ cell é˜µåˆ—çš„è¯»å–ä½¿ç”¨åˆ°äº† sense amplifierï¼Œå³è¯»å‡ºæ”¾å¤§å™¨ã€‚è¯»å‡ºæ”¾å¤§å™¨å¯ä»¥æ•æ‰åˆ°å¾®å¼±çš„ç”µå‹æ³¢åŠ¨ï¼Œå¹¶æ ¹æ®ç”µå‹æ³¢åŠ¨çš„æƒ…å†µåœ¨æœ¬åœ°è¿˜åŸå‡º cell çš„ç”µå®¹ç”µå‹ï¼Œè€Œä¸”æ”¾å¤§å™¨å†…è¿˜æœ‰é”å­˜å™¨ï¼Œå¯ä»¥æŠŠè¿˜åŸå‡ºæ¥çš„ç”µå®¹ç”µå‹å€¼ä¿å­˜èµ·æ¥ï¼Œè¿™æ ·ä¸€æ¥ cell ä¿å­˜çš„ä¿¡æ¯å°±ä» cell ç”µå®¹è½¬ç§»åˆ°äº†æ”¾å¤§å™¨æœ¬åœ°ã€‚ æ¯æ¡ä½çº¿éƒ½è¦æ¥åˆ°ä¸€ä¸ªæ”¾å¤§å™¨ä¸­ã€‚åœ¨è¯»å– cell è¡Œå‰ï¼Œéœ€è¦æŠŠæ¯æ ¹ä½çº¿éƒ½é¢„å……ç”µ (precharge) åˆ°ç”µå®¹ç”µå‹/ä¾›ç”µç”µå‹æœ€å¤§å€¼çš„ä¸€åŠã€‚åœ¨ DRAM èŠ¯ç‰‡ä¸­ï¼Œè¯»å‡ºæ”¾å¤§å™¨æŠŠ cell é˜µåˆ—åˆ†æˆäº†ä¸¤åŠï¼Œå› ä¸ºå…¶é‡‡ç”¨çš„æ˜¯å·®åˆ†æ”¾å¤§å™¨ï¼Œéœ€è¦åŒæ—¶æ¥å…¥ä¸¤æ ¹ä½çº¿ã€‚æ”¾å¤§ä¿¡å·æ³¢åŠ¨æ—¶éœ€è¦ç”¨ä¸€ä¸ªåŸºå‡†å’Œå¾…æµ‹çº¿ä½œæ¯”è¾ƒï¼Œæ¥åˆ°æ”¾å¤§å™¨ä¸Šçš„ä¸¤æ¡ä½çº¿çš„å…¶ä¸­ä¸€æ¡å°±ä½œä¸ºåŸºå‡†ã€‚åœ¨è¯»å‡ºæ•°æ®ä¹‹åï¼Œæ ¹æ®æ”¾å¤§å™¨é”å­˜çš„å€¼ï¼ŒæŠŠå„æ¡ä½çº¿æ‹‰åˆ°ä¾›ç”µç”µå‹æˆ–æ¥åˆ°åœ°ï¼Œç„¶å cell ç”µå®¹å°±ä¼šæ ¹æ®ä½çº¿ç”µå‹è¿›è¡Œå……ç”µæˆ–æ”¾ç”µï¼Œå½“ cell ç”µå®¹å……æ”¾ç”µç»“æŸï¼Œå°±å¯ä»¥æ–­å¼€å­—çº¿ï¼Œå®£å‘Šæœ¬æ¬¡ DRAM è¯»å–ç»“æŸã€‚ ç®€å•æ¥è¯´è¯»å–ä¸€ä¸ªæ¯”ç‰¹çš„æ€»ä½“æµç¨‹æ˜¯ï¼šè·å¾—è¡Œå·ï¼Œè¯‘ç è¡Œå·ï¼Œå¼€å¯å•å…ƒè¡Œï¼Œæ”¾å¤§ä½çº¿ç”µå‹æ³¢åŠ¨å¹¶æš‚å­˜æ•°æ®åˆ°æ”¾å¤§å™¨ï¼Œè·å¾—åˆ—å·å¹¶æ ¹æ®åˆ—å·é€‰æ‹©ä¸€ä½è¿›è¡Œè¾“å‡ºï¼Œå†™å›æ•°æ®ï¼Œå…³é—­å­—çº¿ï¼Œé‡æ–°é¢„å……ç”µã€‚è€Œå†™ä¸€ä¸ªæ¯”ç‰¹çš„æ€»ä½“æµç¨‹æ˜¯ï¼šè·å¾—è¡Œå·ï¼Œè¯‘ç è¡Œå·ï¼Œå¼€å¯å•å…ƒè¡Œï¼Œæ”¾å¤§ä½çº¿ç”µå‹æ³¢åŠ¨å¹¶æš‚å­˜æ•°æ®åˆ°æ”¾å¤§å™¨ï¼Œè·å¾—åˆ—å·å¹¶è¾“å…¥å†™å…¥æ•°æ®ï¼Œæ ¹æ®åˆ—å·æŠŠå†™å…¥æ•°æ®é€åˆ°æ”¾å¤§å™¨å¹¶æ”¹å†™æš‚å­˜å€¼ï¼Œå†™å›æ•°æ®ï¼Œå…³é—­å­—çº¿ï¼Œé‡æ–°é¢„å……ç”µã€‚ å…¶ä¸­èŠ±è´¹æ—¶é—´æœ€ä¹…çš„ä¸¤é¡¹æ˜¯å¼€å¯å•å…ƒè¡Œå’Œæ”¾å¤§ç”µå‹æ³¢åŠ¨å¹¶æš‚å­˜æ•°æ®ã€‚å¼€å¯å•å…ƒè¡Œæ—¶è¡Œåœ°å€è¯‘ç å™¨éœ€è¦æ‹‰é«˜ä¸€æ¡å­—çº¿ï¼Œç„¶åç”¨è¿™ä¸€æ¡å­—çº¿æ‹‰é«˜å•å…ƒè¡Œä¸Šæ‰€æœ‰æ™¶ä½“ç®¡çš„æ …æç”µå‹ï¼Œç›¸å½“äºç»™ä¸€ä¸ªå¾ˆå¤§çš„ç”µå®¹å……ç”µï¼Œéå¸¸èŠ±è´¹æ—¶é—´ã€‚æ”¾å¤§å™¨å¤§éƒ¨åˆ†æ˜¯æ¨¡æ‹Ÿç”µè·¯ï¼Œå·¥ä½œé€Ÿåº¦ä¸å¿«ï¼Œå› æ­¤æ”¾å¤§ç”µå‹æ³¢åŠ¨å¹¶æš‚å­˜æ•°æ®ä¹Ÿå¾ˆèŠ±è´¹æ—¶é—´ã€‚ DRAM Cell Array\nç”±äºè¯»å–éå¸¸è€—æ—¶ï¼ŒDRAM æ¯æ¬¡è¯»å–æ•°æ®éƒ½ä¼šå­˜å‚¨åœ¨æ”¾å¤§å™¨æœ¬åœ°ç¼“å­˜ (row buffer / cache line). ç¼“å­˜è¡Œå†…çš„å„ä¸ªå­—åœ¨å†…å­˜ä¸Šæ˜¯ç›¸é‚»çš„ï¼Œæ¯å½“è¯»å– cell é˜µåˆ—ä¸­çš„ä¸€ä¸ªæ¯”ç‰¹ä¼šæŠŠå…¶æ‰€åœ¨ç¼“å­˜è¡Œçš„æ‰€æœ‰æ¯”ç‰¹éƒ½é€åˆ°è¾“å‡ºç¼“å­˜ï¼Œè¿™ç§è¯»å–æ–¹å¼å«åšçªå‘ (burst). å½“ warp ä¸­çš„æ‰€æœ‰çº¿ç¨‹è®¿é—®è¿ç»­çš„å…¨å±€å†…å­˜ä½ç½®æ—¶ï¼Œç¡¬ä»¶å°†æ‰€æœ‰è¿™äº›è®¿é—®åˆå¹¶ (colaesce) ä¸ºå¯¹è¿ç»­ DRAM ä½ç½®çš„è®¿é—® (å³è¡Œåœ°å€)ã€‚ æœ‰å„ç§ä¼˜åŒ–ç­–ç•¥æ¥å®ç°å†…å­˜åˆå¹¶ã€‚\né‡æ–°æ’åˆ—çº¿ç¨‹åˆ°æ•°æ®çš„æ˜ å°„ã€‚ é‡æ–°æ’åˆ—æ•°æ®æœ¬èº«çš„å¸ƒå±€ã€‚ corner turning: ä»¥åˆå¹¶çš„æ–¹å¼åœ¨å…¨å±€å†…å­˜å’Œå…±äº«å†…å­˜ä¹‹é—´ä¼ è¾“æ•°æ®ï¼Œå¹¶åœ¨å…±äº«å†…å­˜ä¸­æ‰§è¡Œä¸åˆ©çš„è®¿é—®æ¨¡å¼ã€‚å…±äº«å†…å­˜æ˜¯ç”¨SRAMæŠ€æœ¯å®ç°çš„ï¼Œä¸éœ€è¦åˆå¹¶ï¼Œå› æ­¤ä¸æ˜¯è¿ç»­çš„åœ°å€è®¿é—®å¸¦æ¥çš„å½±å“ä¸å¤§ã€‚ å†…å­˜åˆå¹¶çš„ä¸»è¦ä¼˜ç‚¹æ˜¯ï¼Œèƒ½é€šè¿‡å°†å¤šä¸ªå†…å­˜è®¿é—®åˆå¹¶ä¸ºå•ä¸ªè®¿é—®æ¥å‡å°‘å…¨å±€å†…å­˜æµé‡ã€‚ 6.2 Hiding memory latency ä¸€ä¸ª cell é˜µåˆ—ä¸€æ¬¡å¯ä»¥æä¾›ä¸€ä¸ªæ¯”ç‰¹ï¼Œé‚£ä¹ˆ 8 ä¸ª cell é˜µåˆ—å°±å¯ä»¥ä¸€æ¬¡æä¾› 8 ä¸ªæ¯”ç‰¹ï¼Œä»–ä»¬å…±äº«ä¸€ç»„è¡Œåœ°å€å’Œåˆ—åœ°å€ï¼Œè¢«ç§°ä½œä¸€ä¸ª bank. å¤„ç†å™¨åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªé€šé“ (channel). æ¯ä¸ªé€šé“éƒ½æ˜¯ä¸€ä¸ªå¸¦æœ‰æ€»çº¿çš„å†…å­˜æ§åˆ¶å™¨ï¼Œè¯¥æ€»çº¿å°†ä¸€ç»„ DRAM ç»„è¿æ¥åˆ°å¤„ç†å™¨ã€‚ å¦‚ä¸‹å›¾æ‰€ç¤ºå½“ä¸¤ä¸ª bank è¿æ¥åˆ°é€šé“æ€»çº¿æ—¶ï¼Œå½“ç¬¬ä¸€ä¸ª bank ä¸ºå¦ä¸€ä¸ªè®¿é—®æä¾›æœåŠ¡æ—¶ï¼Œå¯ä»¥åœ¨ç¬¬äºŒä¸ª bank å‘èµ·è®¿é—®ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœ cell é˜µåˆ—è®¿é—®å»¶è¿Ÿä¸æ•°æ®ä¼ è¾“æ—¶é—´ä¹‹æ¯”ä¸º Rï¼Œåˆ™å……åˆ†åˆ©ç”¨ä¿¡é“æ€»çº¿çš„æ•°æ®ä¼ è¾“å¸¦å®½è‡³å°‘éœ€è¦ R+1 ä¸ª bank ã€‚æ›´å¤šçš„ bank å‡å°‘äº†é’ˆå¯¹åŒä¸€ bank çš„å¤šä¸ªåŒæ—¶è®¿é—®çš„æ¦‚ç‡ï¼Œè¿™ç§ç°è±¡ç§°ä¸º bank å†²çª (bank conflict). ç”±äºæ¯ä¸ª bank ä¸€æ¬¡åªèƒ½è¯‘ç ä¸€è¡Œå­—çº¿ï¼Œå› æ­¤è¿™äº›å†²çªè®¿é—®çš„å•å…ƒé˜µåˆ—è®¿é—®å»¶è¿Ÿä¸èƒ½å†é‡å ã€‚æ‹¥æœ‰æ›´å¤šæ•°é‡çš„ bank ä¼šå¢åŠ è¿™äº›è®¿é—®åˆ†æ•£åˆ°å¤šä¸ª bank çš„å¯èƒ½æ€§ã€‚ç¬¬äºŒä¸ªåŸå› æ˜¯æ¯ä¸ª cell é˜µåˆ—çš„å¤§å°é™åˆ¶äº†æ¯ä¸ª bank å¯ä»¥æä¾›çš„æ¯”ç‰¹æ•°ã€‚å› æ­¤ç¬¬å››ç« æ‰€è¯´çš„æœ€å¤§åŒ–å ç”¨ç‡è¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„å¥½å¤„ï¼Œé‚£å°±æ˜¯ç¡®ä¿å‘å‡ºè¶³å¤Ÿçš„å†…å­˜è®¿é—®è¯·æ±‚æ¥éšè— DRAM è®¿é—®å»¶è¿Ÿã€‚\nBanking Improves the Utilization of Data Transfer Bandwidth of a Channel\nåˆ†å¸ƒæ–¹æ¡ˆå­˜å‚¨å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé€šå¸¸ç§°ä¸ºäº¤é”™æ•°æ®åˆ†å¸ƒ (interleaved data distribution). å¯¹äºä¸€ä¸ª 4*4 çš„çŸ©é˜µï¼Œæ¯è¾“å‡ºçŸ©é˜µçš„æ¯ä¸ªå…ƒç´ è®¡ç®—å°†å¯¹é€šé“ 0 ä¸­çš„ä¸¤ä¸ª bank ä»¥åŠé€šé“ 2 ä¸­çš„ä¸¤ä¸ª bank è¿›è¡Œåˆå¹¶è®¿é—®ã€‚\nAn Example of Interleaved Data Distribution\n6.3 Thread Coarsening ä»¥æœ€ç»†ç²’åº¦å¹¶è¡ŒåŒ–å·¥ä½œçš„ç¼ºç‚¹åœ¨äºï¼Œå¹¶è¡ŒåŒ–å·¥ä½œéœ€è¦ä»˜å‡ºä»£ä»·ï¼Œä¾‹å¦‚ä¸åŒçº¿ç¨‹å—å¯¹æ•°æ®çš„é‡å¤åŠ è½½ã€å†—ä½™å·¥ä½œã€åŒæ­¥å¼€é”€ç­‰ã€‚å¦‚æœç¡¬ä»¶æœ€ç”±äºèµ„æºä¸è¶³è€Œé¡ºåºæ‰§è¡Œï¼Œé‚£ä¹ˆè¿™ä¸ªä»£ä»·æ˜¯ä¸å¿…è¦çš„ã€‚éƒ¨åˆ†åºåˆ—åŒ–å·¥ä½œï¼Œå‡å°‘ä¸ºå¹¶è¡Œæ€§ä»˜å‡ºçš„ä»£ä»·ã€‚å› æ­¤å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ†é…å¤šä¸ªæœ€ç»†ç²’åº¦çš„å·¥ä½œæ¥è§£å†³ï¼Œé€šå¸¸è¢«ç§°ä¸ºçº¿ç¨‹ç²—åŒ– (thread coarsening). å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨ä¹‹å‰çš„ tiled çŸ©é˜µä¹˜æ³•é‡Œï¼Œç”±äºå…±äº«å†…å­˜å†…å®¹ä¸èƒ½è·¨å—å…±äº«ï¼Œæ¯ä¸ªå—å¿…é¡»åŠ è½½çŸ©é˜µ M çš„ tile å‰¯æœ¬ã€‚å› æ­¤å¯ä»¥è®©å—ä¸­çš„æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸¤ä¸ªè¾“å‡ºå…ƒç´ ã€‚è¿™æ ·ï¼Œç²—åŒ–çš„çº¿ç¨‹å—å°†åŠ è½½ M çš„ tile ä¸€æ¬¡ï¼Œå¹¶å°†å®ƒä»¬ç”¨äºè®¡ç®—ä¸ºå¤šä¸ªè¾“å‡º tile.\nThread Coarsening for Tiled Matrix Multiplication\nä¸‹é¢çš„ä»£ç å±•ç¤ºäº†çº¿ç¨‹ç²—åŒ–çš„çŸ©é˜µä¹˜æ³•å†…æ ¸å‡½æ•°ï¼Œåœ¨ width/TILE_WIDTH çš„æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä¸€ä¸ªçº¿ç¨‹è®¡ç®—åŸæ¥ COARSE_FACTOR ä¸ªçº¿ç¨‹å¯¹åº”ä½ç½®çš„è¾“å‡ºã€‚\nä½¿ç”¨çº¿ç¨‹ç²—åŒ–æ—¶è¦æ³¨æ„ï¼š\nä¸è¦åœ¨ä¸å¿…è¦çš„æ—¶å€™ä½¿ç”¨ï¼Œå½“å¹¶è¡ŒåŒ–çš„ä»£ä»·å¯ä»¥é€šè¿‡ç²—åŒ–æ¥é™ä½æ—¶ï¼Œç²—åŒ–æ˜¯æœ‰ç›Šçš„ã€‚ ä¸è¦ä½¿ç”¨è¿‡å¤šçš„ç²—åŒ–ï¼Œä»¥å…ç¡¬ä»¶èµ„æºå¾—ä¸åˆ°å……åˆ†åˆ©ç”¨ã€‚ é¿å…å°†èµ„æºæ¶ˆè€—å¢åŠ åˆ°æŸå®³å ç”¨çš„ç¨‹åº¦ã€‚æ ¹æ®å†…æ ¸çš„ä¸åŒï¼Œçº¿ç¨‹ç²—åŒ–å¯èƒ½éœ€è¦æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨æ›´å¤šçš„å¯„å­˜å™¨æˆ–æ¯ä¸ªçº¿ç¨‹å—ä½¿ç”¨æ›´å¤šçš„å…±äº«å†…å­˜ã€‚ __global__ void CoarsingMatrixMulKernel(float* M, float* N, float* P, int width) { __shared__ float Mds[TILE_WIDTH][TILE_WIDTH]; __shared__ float Nds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; // Identify the row and column of the P element to work on int row = by * TILE_WIDTH + ty; int colStart = bx * TILE_WIDTH * COARSE_FACTOR + tx; // Initialize Pvalue for all output elements float Pvalue[COARSE_FACTOR]; for (int i = 0; i \u0026lt; COARSE_FACTOR; i++) { Pvalue[i] = 0; } // Loop over the M and N tiles required to compute P element for (int ph = 0; ph \u0026lt; width/TILE_WIDTH; ph++) { // the COARSE_FACTOR tiles of N needs the same tile of M Mds[ty][tx] = M[row * width + ph * TILE_WIDTH + tx]; for (int c = 0; c \u0026lt; COARSE_FACTOR; c++) { int col = colStart + c * TILE_WIDTH; // Value to be computed in the c th tile // Collaborative loading of N tile into shared memory Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * width + col]; __syncthreads(); for (int k = 0; k \u0026lt; TILE_WIDTH; k++) { Pvalue[c] += Mds[ty][k] * Nds[k][tx]; } __syncthreads(); } for (int c = 0; c \u0026lt; COARSE_FACTOR; c++) { int col = colStart + c * TILE_WIDTH; P[row * width + col] = Pvalue[c]; } } } ","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch6/","summary":"Personal notebook 6 of Programming Massively Parallel","title":"PMPP Learning-Chapter 6 Performance Considerations"},{"content":"Compute Architecture and Scheduling æœ¬ç« ä»‹ç» GPU è®¡ç®—æ¶æ„ï¼Œå¹¶è¯´æ˜çµæ´»èµ„æºåˆ†é…ã€å—è°ƒåº¦å’Œå ç”¨çš„æ¦‚å¿µã€‚ç„¶åå°†æ·±å…¥è®¨è®ºçº¿ç¨‹è°ƒåº¦ã€å»¶è¿Ÿå®¹å¿ã€æ§åˆ¶å‘æ•£å’ŒåŒæ­¥ã€‚\n4.1 Architecture of a modern GPU ä¸‹å›¾å±•ç¤ºäº† CUDA GPU æ¶æ„ï¼Œå®ƒè¢«ç»„ç»‡æˆä¸€ä¸ªæµå¼å¤šå¤„ç†å™¨ (Streaming Multiprocessors, SMs) æ•°ç»„ã€‚æ¯ä¸ª SM éƒ½æœ‰å‡ ä¸ªå¤„ç†å•å…ƒï¼Œç§°ä¸ºæµå¤„ç†å™¨æˆ– CUDA core (ç®€ç§°ä¸º core)ï¼Œå¦‚å›¾ä¸­ SMs å†…éƒ¨çš„å°å—æ‰€ç¤ºï¼Œå®ƒä»¬å…±äº«æ§åˆ¶é€»è¾‘å’Œå†…å­˜èµ„æºã€‚\nSMs è¿˜å¸¦æœ‰ä¸åŒçš„ç‰‡ä¸Šå­˜å‚¨ç»“æ„ï¼Œç»Ÿç§°ä¸ºå†…å­˜ã€‚GPU è¿˜å¸¦æœ‰åƒå…†å­—èŠ‚çš„ç‰‡å¤–è®¾å¤‡å†…å­˜ï¼Œç§°ä¸ºå…¨å±€å†…å­˜ (global memory).\nè™½ç„¶æ—§çš„GPUä½¿ç”¨ DDR DRAMï¼Œä½†ä» NVIDIA çš„ Pascal æ¶æ„å¼€å§‹ GPU å¯èƒ½ä½¿ç”¨HBM (High-Bandwidth Memory) æˆ– HBM2ï¼Œå®ƒä»¬ç”± DRAM æ¨¡å—ç»„æˆï¼Œä¸GPUç´§å¯†é›†æˆåœ¨åŒä¸€ä¸ªå°è£…ä¸­ã€‚\nArchitecture of a CUDA-capable GPU\n4.2 Block Scheduling å½“è°ƒç”¨å†…æ ¸æ—¶ï¼ŒCUDA runtime ç³»ç»Ÿå¯åŠ¨æ‰§è¡Œå†…æ ¸ä»£ç çš„çº¿ç¨‹ç½‘æ ¼ï¼Œå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹åŒæ—¶åˆ†é…ç»™åŒä¸€ä¸ªçš„ SM. ä¸‹å›¾ä¸­æ¯ä¸ª SM åˆ†é…äº†ä¸‰ä¸ªå—ï¼Œä½†æ˜¯å—éœ€è¦å ç”¨ç¡¬ä»¶èµ„æºæ¥æ‰§è¡Œï¼Œå› æ­¤åŒæ—¶åªèƒ½å°†æœ‰é™æ•°é‡çš„å—åˆ†é…ç»™ç»™å®šçš„ SM. ä¸ºäº†ç¡®ä¿ç½‘æ ¼ä¸­çš„æ‰€æœ‰å—éƒ½å¾—åˆ°æ‰§è¡Œï¼Œruntime ç³»ç»Ÿç»´æŠ¤ä¸€ä¸ªéœ€è¦æ‰§è¡Œçš„å—åˆ—è¡¨ï¼Œå¹¶åœ¨å…ˆå‰åˆ†é…çš„å—å®Œæˆæ‰§è¡Œåå†å°†æ–°å—åˆ†é…ç»™ SMs. ä»¥å—ä¸ºåŸºæœ¬å•å…ƒå°†çº¿ç¨‹åˆ†é…ç»™ SMs ä¿è¯äº†åŒä¸€å—ä¸­çš„çº¿ç¨‹åœ¨åŒä¸€SMä¸ŠåŒæ—¶è¢«è°ƒåº¦ã€‚\nThread Block Assignment to SMs\n4.3 Synchronization and Transparent Scalability CUDA å…è®¸åŒä¸€å—ä¸­çš„çº¿ç¨‹ä½¿ç”¨ barrier åŒæ­¥å‡½æ•° __syncthreads() æ¥åè°ƒå…¶è¡ŒåŠ¨ã€‚ä¸‹å›¾å±•ç¤ºäº†å±éšœåŒæ­¥çš„æ‰§è¡Œæƒ…å†µï¼Œç®­å¤´è¡¨ç¤ºçº¿ç¨‹å„è‡ªæ‰§è¡Œè¿è¡Œçš„æ—¶é—´ã€‚å¼¯æ›²çº¿æ ‡è®°äº†æ¯ä¸ªçº¿ç¨‹å¼€å§‹æ‰§è¡Œ __syncthreads() çš„æ—¶é—´ã€‚å¼¯æ›²çº¿å³ä¾§çš„ç©ºç™½åŒºåŸŸè¡¨ç¤ºæ¯ä¸ªçº¿ç¨‹ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆæ‰€éœ€çš„æ—¶é—´ã€‚ç«–çº¿æ ‡å¿—ç€æœ€åä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œ __syncthreads() çš„æ—¶é—´ï¼Œä¹‹åæ‰€æœ‰çº¿ç¨‹éƒ½è¢«å…è®¸ç»§ç»­æ‰§è¡Œ __syncthreads() ä¹‹åçš„ä»£ç ã€‚\nä¸è¦åœ¨åˆ†æ”¯è¯­å¥ä¸­ä½¿ç”¨ __syncthreads()\næ”¾åœ¨ if è¯­å¥ä¸­æ—¶ï¼Œå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹è¦ä¹ˆå…¨æ‰§è¡ŒåŒ…å« __syncthreads() çš„è·¯å¾„ï¼Œè¦ä¹ˆéƒ½ä¸æ‰§è¡Œã€‚ if-else è¯­å¥ä¸­çš„ä¸¤ä¸ªåˆ†æ”¯éƒ½å­˜åœ¨ï¼Œå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹è¦ä¹ˆå…¨æ‰§è¡Œ if æƒ…å†µä¸‹çš„ __syncthreads() çš„è·¯å¾„ï¼Œè¦ä¹ˆå…¨æ‰§è¡Œ else ä¸‹çš„è·¯å¾„ã€‚ A Example Execution of Barrier Synchronization\nç³»ç»Ÿéœ€è¦ç¡®ä¿æ‰€æœ‰å‚ä¸ barrier åŒæ­¥çš„çº¿ç¨‹éƒ½èƒ½è®¿é—®è¶³å¤Ÿèµ„æºä»¥åˆ°è¾¾ barrier. å¦åˆ™ï¼Œé‚£äº›åˆ°è¾¾ä¸äº†çº¿ç¨‹å¯èƒ½ä¼šå¯¼è‡´æ­»é”ã€‚å› æ­¤åªæœ‰å½“ runtime ç³»ç»Ÿç¡®ä¿äº†å—ä¸­æ‰€æœ‰çº¿ç¨‹æœ‰å®Œæˆæ‰§è¡Œæ‰€éœ€çš„æ‰€æœ‰èµ„æºæ—¶ï¼Œä¸€ä¸ªå—æ‰èƒ½å¼€å§‹æ‰§è¡Œã€‚ é€šè¿‡ç¦æ­¢ä¸åŒå—ä¸­çš„çº¿ç¨‹ä¸€èµ·æ‰§è¡Œ barrier åŒæ­¥ï¼ŒCUDA runtime ç³»ç»Ÿå¯ä»¥ä»¥ä»»ä½•é¡ºåºæ‰§è¡Œå—ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨åªæœ‰å°‘é‡æ‰§è¡Œèµ„æºçš„ç³»ç»Ÿä¸­ï¼Œä¸€æ¬¡æ‰§è¡Œä¸¤ä¸ªå—ã€‚åä¹‹ï¼Œå¯ä»¥åŒæ—¶æ‰§è¡Œå¤šä¸ªå—ã€‚è¿™ç§åœ¨ä¸åŒç¡¬ä»¶ä¸Šä½¿ç”¨ä¸åŒæ•°é‡çš„æ‰§è¡Œèµ„æºæ‰§è¡Œç›¸åŒçš„ä»£ç çš„èƒ½åŠ›è¢«ç§°ä¸ºé€æ˜å¯æ‰©å±•æ€§ (transparent scalability)\nTransparent Scalability of CUDA Programs\n4.4 Warps and SIMD Hardware å½“ä¸€ä¸ªå—è¢«åˆ†é…ç»™ä¸€ä¸ª SM æ—¶ï¼Œå®ƒä¼šè¢«è¿›ä¸€æ­¥åˆ’åˆ†ä¸º 32 ä¸ªçº¿ç¨‹ä¸ºä¸€ç»„çš„å•å…ƒï¼Œç§°ä¸º warp. åœ¨ SMs ä¸­ï¼Œwarp æ˜¯çº¿ç¨‹è°ƒåº¦çš„å•ä½ã€‚ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªåˆ’åˆ†çš„ä¾‹å­ã€‚\nBlocks are Partitioned into Warps for Thread Scheduling\nç”±å¤šç»´åº¦çš„çº¿ç¨‹ç»„æˆçš„å—ï¼Œå°†è¢«æŠ•å½±åˆ°çº¿æ€§åŒ–çš„è¡Œä¸»å¸ƒå±€ä¸­æ¥åˆ’åˆ†ã€‚çº¿æ€§å¸ƒå±€æ˜¯ä»¥ (z, y, x) åæ ‡å‡åºçš„æ–¹å¼æ’åˆ—ã€‚ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªå¤§å°ä¸º 4*4 å—çš„çº¿æ€§åŒ–è§†å›¾ã€‚å‰ 4 ä¸ªçº¿ç¨‹çš„ threadIdx.y ä¸º 0ï¼Œå®ƒä»¬ä»¥ threadIdx.x å‡åºçš„æ–¹å¼æ’åˆ—ã€‚\nLinear Layout of 2D Threads\nSM æ˜¯å•æŒ‡ä»¤å¤šæ•°æ® (SIMD) æ¨¡å‹ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰çº¿ç¨‹ï¼Œwarp ä¸­çš„æ‰€æœ‰çº¿ç¨‹åŒæ—¶æ‰§è¡Œä¸€æ¡æŒ‡ä»¤ã€‚ä¸‹å›¾å±•ç¤ºäº† SM ä¸­çš„å†…æ ¸å¦‚ä½•è¢«åˆ†ç»„ä¸ºå¤„ç†å—ï¼Œå…¶ä¸­æ¯ 8 ä¸ªå†…æ ¸æ„æˆä¸€ä¸ªå¤„ç†å— (processing block) å¹¶å…±äº«ä¸€ä¸ªæŒ‡ä»¤è·å–/è°ƒåº¦å•å…ƒã€‚åŒä¸€ warp ä¸­çš„çº¿ç¨‹è¢«åˆ†é…åˆ°ç›¸åŒçš„å¤„ç†å—ï¼Œè¯¥å¤„ç†å—è·å–æŒ‡ä»¤å¹¶è®© warp ä¸­çš„æ‰€æœ‰çº¿ç¨‹å¯¹å„è‡ªè´Ÿè´£æ•°æ®çš„éƒ¨åˆ†æ‰§è¡Œè¯¥æŒ‡ä»¤ã€‚è¿™ç§è®¾è®¡å…è®¸è¾ƒå°æ¯”ä¾‹çš„ç¡¬ä»¶ä¸“æ³¨äºæ§åˆ¶ï¼Œè€Œè¾ƒå¤§æ¯”ä¾‹çš„ç¡¬ä»¶ä¸“æ³¨äºæé«˜è®¡ç®—ååé‡ã€‚\nProcessing Blocks Organization\n4.5 Control divergence å½“åŒä¸€ warp ä¸­çš„çº¿ç¨‹æ‰§è¡Œä¸åŒçš„è·¯å¾„æ—¶ï¼Œè¿™äº›çº¿ç¨‹çš„è¡Œä¸ºè¢«ç§°ä½œæ§åˆ¶å‘æ•£ (control divergence). ä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ª warp åœ¨é‡åˆ°åˆ†æ”¯è¯­å¥æ—¶çš„æ‰§è¡Œæ–¹å¼ï¼Œå³é€šè¿‡ä¸¤æ¬¡ pass (æ‰§è¡Œä»£ç çš„é˜¶æ®µ) æ¥åˆ†åˆ«æ‰§è¡Œ then-path å’Œ else-pathï¼Œæœ€ç»ˆå®ç°æ‰€æœ‰çº¿ç¨‹çš„æ±‡åˆã€‚\nPascal åŠä¹‹å‰æ¶æ„ä¸­ï¼Œwarp éœ€è¦é¡ºåºæ‰§è¡Œä¸¤ä¸ª passï¼Œä¸€ä¸ª pass æ‰§è¡Œå®Œæ‰èƒ½å¼€å§‹ä¸‹ä¸€ä¸ª passã€‚ Pass 1ï¼š çº¿ç¨‹ 0-23 æ‰§è¡Œ then-path çš„ä»£ç  Aï¼Œçº¿ç¨‹ 24-31 å¤„äº inactive çŠ¶æ€ã€‚ Pass 2ï¼š çº¿ç¨‹ 24-31 æ‰§è¡Œ else-path çš„ä»£ç  Bï¼Œçº¿ç¨‹ 0-23 å¤„äº inactive çŠ¶æ€ã€‚ Pass 3ï¼š æ‰€æœ‰çº¿ç¨‹æ±‡åˆï¼Œæ‰§è¡Œåç»­ä»£ç  Cã€‚ Volta åŠä¹‹åæ¶æ„ä¸­ï¼Œwarp å¯ä»¥åŒæ—¶æ‰§è¡Œä¸¤ä¸ª passï¼Œä¸åŒçš„çº¿ç¨‹å¯ä»¥äº¤é”™æ‰§è¡Œä¸åŒçš„ä»£ç è·¯å¾„ã€‚ Pass 1ï¼š çº¿ç¨‹ 0-23 å¼€å§‹æ‰§è¡Œ A çš„ç¬¬ä¸€ä¸ªæŒ‡ä»¤ï¼Œçº¿ç¨‹ 24-31 å¼€å§‹æ‰§è¡Œ B çš„ç¬¬ä¸€ä¸ªæŒ‡ä»¤ã€‚ Pass 2ï¼š çº¿ç¨‹ 0-23 æ‰§è¡Œ A çš„ç¬¬äºŒä¸ªæŒ‡ä»¤ï¼Œçº¿ç¨‹ 24-31 æ‰§è¡Œ B çš„ç¬¬äºŒä¸ªæŒ‡ä»¤ã€‚ \u0026hellip; Pass Nï¼š çº¿ç¨‹ 0-23 æ‰§è¡Œå®Œ A çš„æ‰€æœ‰æŒ‡ä»¤ï¼Œçº¿ç¨‹ 24-31 æ‰§è¡Œå®Œ B çš„æ‰€æœ‰æŒ‡ä»¤ã€‚ Pass N+1ï¼š æ‰€æœ‰çº¿ç¨‹æ±‡åˆï¼Œæ‰§è¡Œåç»­ä»£ç  Cã€‚ Example of a Warp Diverging at an if-else Statement\nå‘æ•£ä¹Ÿå¯èƒ½å‡ºç°åœ¨å…¶ä»–æ§åˆ¶æµä¸­ã€‚ä¸‹å›¾å±•ç¤ºäº† warp å¦‚ä½•æ‰§è¡Œå‘æ•£ for å¾ªç¯ã€‚é€šå¸¸æ¥è¯´å¦‚æœåˆ¤æ–­æ¡ä»¶åŸºäº threadIdx çš„å€¼ï¼Œé‚£ä¹ˆæ§åˆ¶è¯­å¥å¯èƒ½ä¼šå¯¼è‡´çº¿ç¨‹å‘æ•£ã€‚ç”±äºçº¿ç¨‹æ€»æ•°éœ€è¦æ˜¯çº¿ç¨‹å—å¤§å°çš„å€æ•°ï¼Œè€Œæ•°æ®å¤§å°å¯ä»¥æ˜¯ä»»æ„çš„ï¼Œå› æ­¤å…·æœ‰çº¿ç¨‹æ§åˆ¶å‘æ•£çš„æ§åˆ¶æµç¨‹å¾ˆå¸¸è§ã€‚ç”±ä»¥ä¸Šä¸¤ä¸ªä¾‹å­å¯ä»¥çœ‹å‡ºä¸èƒ½å‡è®¾ warp ä¸­çš„æ‰€æœ‰çº¿ç¨‹éƒ½å…·æœ‰ç›¸åŒçš„æ‰§è¡Œæ—¶é—´ã€‚å¦‚æœ warp ä¸­çš„æ‰€æœ‰çº¿ç¨‹éƒ½å¿…é¡»å®Œæˆæ‰§è¡Œçš„ä¸€ä¸ªé˜¶æ®µï¼Œç„¶åæ‰èƒ½ç»§ç»­å‰è¿›ï¼Œåˆ™å¿…é¡»ä½¿ç”¨ barrier åŒæ­¥æœºåˆ¶ (å¦‚ __syncwarp() )æ¥ç¡®ä¿æ­£ç¡®æ€§ã€‚\næ§åˆ¶å‘æ•£å¯¹æ€§èƒ½çš„å½±å“éšç€è¢«å¤„ç†å‘é‡å¤§å°çš„å¢åŠ è€Œå‡å°ã€‚ä¾‹å¦‚å¯¹äºé•¿åº¦ä¸º 100 çš„å‘é‡ï¼Œ4ä¸ª warp ä¸­æœ‰ 1 ä¸ªå°†ä¼šæ§åˆ¶å‘æ•£ (25%)ï¼›å¯¹äºå¤§å°ä¸º1000çš„çŸ¢é‡ï¼Œ32 ä¸ª warp ä¸­åªæœ‰ 1 ä¸ªå°†ä¼šæ§åˆ¶å‘æ•£ (3.125%).\nExample of a Warp Diverging at a for-loop\n4.6 Warp scheduling and latency tolerance å½“å°†çº¿ç¨‹åˆ†é…ç»™ SMs æ—¶ï¼Œåˆ†é…ç»™ SM çš„çº¿ç¨‹é€šå¸¸æ¯” SM ä¸­ core çš„ä¸ªæ•°è¿˜è¦å¤šï¼Œå¯¼è‡´æ¯ä¸ª SM åªèƒ½åŒæ—¶æ‰§è¡Œåˆ†é…ç»™å®ƒçš„æ‰€æœ‰çº¿ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚å½“è¦ç”± warp æ‰§è¡Œçš„æŒ‡ä»¤éœ€è¦ç­‰å¾…å…ˆå‰å¯åŠ¨çš„æ“ä½œçš„ç»“æœæ—¶ï¼Œä¸ä¼šé€‰æ‹©è¯¥ warp æ‰§è¡Œã€‚è€Œæ˜¯é€‰æ‹©æ‰§è¡Œå¦ä¸€ä¸ªä¸ç”¨ç­‰å¾…å…ˆå‰æŒ‡ä»¤ç»“æœçš„ warpã€‚è¿™ç§ç”¨å…¶ä»–çº¿ç¨‹çš„å·¥ä½œå¡«å……æŸäº›çº¿ç¨‹æ“ä½œå»¶è¿Ÿæ—¶é—´çš„æœºåˆ¶é€šå¸¸ç§°ä¸ºå»¶è¿Ÿå®¹å¿ (latency tolerance) æˆ–è€…å»¶è¿Ÿéšè— (latency hiding). è€Œé€‰æ‹©å‡†å¤‡æ‰§è¡Œçš„ warp ä¸ä¼šåœ¨æ‰§è¡Œæ—¶é—´çº¿ä¸­å¼•å…¥ä»»ä½•ç©ºé—²æˆ–æµªè´¹çš„æ—¶é—´çš„ç­–ç•¥è¢«ç§°ä¸ºé›¶å¼€é”€çº¿ç¨‹è°ƒåº¦ (zero-overhead thread scheduling). è¿™ç§å®¹å¿é•¿æ“ä½œå»¶è¿Ÿçš„èƒ½åŠ›æ˜¯ GPU ä¸åƒ CPU é‚£æ ·ä¸ºç¼“å­˜å’Œåˆ†æ”¯é¢„æµ‹æœºåˆ¶åˆ†é…é‚£ä¹ˆå¤šèŠ¯ç‰‡é¢ç§¯çš„ä¸»è¦åŸå› ï¼Œå› æ­¤å¯ä»¥æ›´ä¸“æ³¨äºæµ®ç‚¹æ•°è®¡ç®—å’Œå†…å­˜è¯»å–ã€‚\nThreads, Context-switching, and Zero-overhead Scheduling ä¹‹å‰ä»‹ç»è¿‡çº¿ç¨‹ç”±ç¨‹åºçš„ä»£ç ã€æ­£åœ¨æ‰§è¡Œçš„ä»£ç ä¸­çš„æŒ‡ä»¤ã€å˜é‡çš„å€¼å’Œæ•°æ®ç»“æ„ç»„æˆã€‚åœ¨åŸºäºå†¯Â·è¯ºä¼Šæ›¼æ¨¡å‹çš„è®¡ç®—æœºä¸­ï¼Œç¨‹åºçš„ä»£ç å­˜å‚¨åœ¨å­˜å‚¨å™¨ä¸­ã€‚PC (Program Counter) è·Ÿè¸ªæ­£åœ¨æ‰§è¡Œçš„ç¨‹åºæŒ‡ä»¤çš„åœ°å€ã€‚IR (Instruction Register) ä¿å­˜æ­£åœ¨æ‰§è¡Œçš„æŒ‡ä»¤ã€‚å¯„å­˜å™¨å’Œå†…å­˜ä¿å­˜å˜é‡å’Œæ•°æ®ç»“æ„çš„å€¼ã€‚ ç°ä»£å¤„ç†å™¨çš„è®¾è®¡å…è®¸ä¸Šä¸‹æ–‡åˆ‡æ¢ (Context-switching)ï¼Œå¤šä¸ªçº¿ç¨‹å¯ä»¥é€šè¿‡è½®æµæ‰§è¡Œçš„æ–¹å¼åˆ†æ—¶å¤ç”¨ä¸€ä¸ªå¤„ç†å™¨ã€‚é€šè¿‡ä¿å­˜å’Œæ¢å¤ PC å€¼ä»¥åŠå¯„å­˜å™¨å’Œå†…å­˜çš„å†…å®¹ï¼Œå¯ä»¥æš‚åœçº¿ç¨‹çš„æ‰§è¡Œï¼Œå¹¶åœ¨ç¨åæ­£ç¡®æ¢å¤çº¿ç¨‹çš„æ‰§è¡Œã€‚ä¸è¿‡ä¿å­˜å’Œæ¢å¤å¯„å­˜å™¨å†…å®¹å¯èƒ½ä¼šå¢åŠ å¤§é‡æ‰§è¡Œæ—¶é—´ã€‚ ä¼ ç»Ÿçš„ CPU ä»ä¸€ä¸ªçº¿ç¨‹åˆ‡æ¢åˆ°å¦ä¸€ä¸ªçº¿ç¨‹éœ€è¦å°†æ‰§è¡ŒçŠ¶æ€ (ä¾‹å¦‚è¢«åˆ‡æ¢çº¿ç¨‹çš„å¯„å­˜å™¨å†…å®¹) ä¿å­˜åˆ°å†…å­˜ä¸­ï¼Œç¨åå†ä»å†…å­˜ä¸­åŠ è½½ï¼Œè¿™æ ·ä¼šäº§ç”Ÿç©ºé—²å‘¨æœŸã€‚GPU SMs é€šè¿‡åœ¨ç¡¬ä»¶å¯„å­˜å™¨ä¸­ä¿å­˜æŒ‡å®š warp çš„æ‰€æœ‰æ‰§è¡ŒçŠ¶æ€æ¥å®ç°é›¶å¼€é”€è°ƒåº¦ï¼Œå› æ­¤ä¸éœ€è¦ä¿å­˜å’Œæ¢å¤çŠ¶æ€ã€‚ 4.7 Resource partitioning and occupancy ç»™ SM åˆ†é…å…¶æ‰€æ”¯æŒçš„æœ€å¤§ warp æ•°å¹¶ä¸æ€»æ˜¯å¯è¡Œã€‚åˆ†é…ç»™ SM çš„ warp æ•°é‡ä¸å…¶æ”¯æŒçš„ warp æ•°é‡ä¹‹æ¯”ç§°ä¸ºå ç”¨ç‡ (occupancy). ä¾‹å¦‚ï¼ŒAmpere A100 GPU æ¯ä¸ª SM æœ€å¤šæ”¯æŒ 32 ä¸ª blockï¼Œæ¯ä¸ª SM æœ€å¤šæ”¯æŒ 64 ä¸ª warp (2048 ä¸ªçº¿ç¨‹)ï¼Œæ¯ä¸ª block æœ€å¤šæ”¯æŒ 1024 ä¸ªçº¿ç¨‹ã€‚æ„å‘³ç€å—å¤§å°å¯ä»¥ä» 641024 ä¸ç­‰ï¼Œæ¯ä¸ª SM åˆ†åˆ«å¯ä»¥æœ‰ 322 ä¸ªå—ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œåˆ†é…ç»™SMçš„çº¿ç¨‹æ€»æ•°ä¸º2048ï¼Œè¿™ä½¿å ç”¨ç‡æœ€å¤§åŒ–ã€‚ SM ä¸­çš„æ‰§è¡Œèµ„æºåŒ…æ‹¬å¯„å­˜å™¨ã€å…±äº«å†…å­˜çº¿ç¨‹å—æ§½ (æ¯ä¸ª SM æœ€å¤§èƒ½è¢«åˆ†é…çš„çº¿ç¨‹å—æ•°é‡) å’Œçº¿ç¨‹æ§½ (æ¯ä¸ªçº¿ç¨‹å—æœ€å¤§èƒ½è¢«åˆ†é…çš„çº¿ç¨‹æ•°é‡)ï¼Œè¿™äº›èµ„æºåœ¨çº¿ç¨‹ä¹‹é—´åŠ¨æ€åˆ†é…ã€‚èµ„æºçš„åŠ¨æ€åˆ†é…å¯èƒ½å¯¼è‡´ä»–ä»¬ä¹‹é—´ç›¸äº’åˆ¶çº¦ï¼Œä½¿å¾—èµ„æºåˆ©ç”¨ä¸è¶³ã€‚\nç¡¬ä»¶èµ„æºæ”¯æŒçš„å½±å“ã€‚å½“æ¯ä¸ªå—æœ‰32ä¸ªçº¿ç¨‹æ—¶ã€‚Ampere A100 GPU ä¼šå°† 2048 ä¸ªçº¿ç¨‹æ§½åˆ†é…ç»™ 64 ä¸ªå—ã€‚ç„¶è€Œ Volta SM åªæ”¯æŒ 32 ä¸ªçº¿ç¨‹å—æ§½ï¼Œå¯¼è‡´å ç”¨ç‡åªæœ‰ 50%. å½“æ¯ä¸ªå—çš„æœ€å¤§çº¿ç¨‹æ•°ä¸èƒ½æ•´é™¤å—å¤§å°æ—¶ã€‚å½“å—å¤§å°ä¸º 768ï¼ŒSM å°†åªèƒ½å®¹çº³ 2 ä¸ªçº¿ç¨‹å— (1536ä¸ªçº¿ç¨‹)ï¼Œå‰©ä¸‹512ä¸ªçº¿ç¨‹æ§½æœªä½¿ç”¨ï¼Œå ç”¨ç‡ä¸º 75%. å¯„å­˜å™¨èµ„æºé™åˆ¶å¯¹å ç”¨ç‡çš„å½±å“ã€‚Ampere A100 GPU å…è®¸æ¯ä¸ª SM æœ€å¤šå æœ‰ 65,536ä¸ªå¯„å­˜å™¨ã€‚ä¸ºäº†è¾¾åˆ°æ»¡å ç”¨ç‡æ¯ä¸ªçº¿ç¨‹ä¸åº”è¯¥ä½¿ç”¨è¶…è¿‡ 32 ä¸ªå¯„å­˜å™¨ã€‚ è¿™ç§é™åˆ¶å¯¼è‡´èµ„æºä½¿ç”¨çš„è½»å¾®å¢åŠ å¯èƒ½å¯¼è‡´å¹¶è¡Œæ€§å’Œæ€§èƒ½çš„æ˜¾è‘—é™ä½ï¼Œç§°ä¸º performance cliff. ","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch4/","summary":"Personal notebook 3 of Programming Massively Parallel","title":"PMPP Learning-Chapter 4 Compute Architecture and Scheduling"},{"content":"5 Memory Architecture and Data Locality ä¹‹å‰ç« èŠ‚æ‰€å†™çš„ CUDA å†…æ ¸åªèƒ½è¾¾åˆ°åº•å±‚ç¡¬ä»¶å³°å€¼ç®—é‡Œçš„ä¸€å°éƒ¨åˆ†ã€‚å› ä¸ºå…¨å±€å†…å­˜ (é€šå¸¸ä½¿ç”¨ç‰‡å¤– DRAM å®ç°) å¾€å¾€å…·æœ‰è¾ƒé•¿çš„è®¿é—®å»¶è¿Ÿ (æ•°ç™¾ä¸ªæ—¶é’Ÿå‘¨æœŸ) å’Œæœ‰é™çš„è®¿é—®å¸¦å®½ã€‚\n5.1 Importance of Memory Access Efficiency åœ¨ä¹‹å‰çŸ©é˜µä¹˜æ³•çš„å†…æ ¸å‡½æ•°ä¸­ï¼Œæ¯æ¬¡è¿­ä»£é‡Œæ‰§è¡Œä¸€æ¬¡æµ®ç‚¹ä¹˜æ³•å’Œä¸€æ¬¡æµ®ç‚¹åŠ æ³•éœ€è¦è®¿é—®å…¨å±€å†…å­˜ä¸¤æ¬¡ã€‚å› æ­¤ï¼Œä»å…¨å±€å†…å­˜è®¿é—®çš„æµ®ç‚¹æ“ä½œæ¬¡æ•° (FLOP) ä¸å­—èŠ‚æ•° (B) çš„æ¯”ç‡ä¸º 2 FLOP-to-8 Bï¼Œå³ 0.25FLOP/B. è®¡ç®—è®¿å­˜æ¯” (compute to global memory access ratio) å®šä¹‰ä¸ºåœ¨ç¨‹åºçš„ä¸€ä¸ªåŒºåŸŸå†…å¯¹å…¨å±€å†…å­˜è®¿é—®çš„å•ä½å­—èŠ‚æ‰§è¡Œçš„ FLOPS æ•°ã€‚ è®¡ç®—è®¿å­˜æ¯”å¯¹ CUDA å†…æ ¸çš„æ€§èƒ½æœ‰é‡å¤§å½±å“ã€‚A100 GPU çš„å…¨å±€å†…å­˜å¸¦å®½å³°å€¼ä¸º 1555 GB/sï¼ŒçŸ©é˜µä¹˜æ³•å†…æ ¸è®¡ç®—è®¿å­˜æ¯”ä¸º 0.25 OP/Bï¼Œå› æ­¤å†…æ ¸å¯ä»¥æ‰§è¡Œçš„å•ç²¾åº¦ FLOPs çš„ååé‡ä¸º 389 GFLOPSï¼Œä»…ä¸º A100 GPU å³°å€¼å•ç²¾åº¦è¿ç®—ååé‡ (19,500 GFLOPS) çš„ 2%. æˆ‘ä»¬æŠŠæ‰§è¡Œé€Ÿåº¦å—å†…å­˜å¸¦å®½é™åˆ¶çš„ç¨‹åºç§°ä¸ºå†…å­˜ç“¶é¢ˆ (memory bound) ç¨‹åºã€‚\nRoofline Model Rooline æ¨¡å‹ç”¨äºè¯„ä¼°åº”ç”¨ç¨‹åºç›¸åœ¨å…¶æ‰€è¿è¡Œçš„ç¡¬ä»¶çš„é™åˆ¶ä¸Šè¾¾åˆ°çš„æ€§èƒ½ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œx è½´è¡¨ç¤ºç®—æœ¯æˆ–è®¡ç®—å¼ºåº¦ (computational intensity)ï¼Œå•ä½ä¸º FLOP/B. y è½´è¡¨ç¤ºä»¥ GFLOPS ä¸ºå•ä½çš„è®¡ç®—ååé‡ã€‚æ¨ªçº¿è¡¨ç¤ºç¡¬ä»¶å¯ä»¥æä¾›çš„å³°å€¼è®¡ç®—ååé‡ã€‚ ç¡¬ä»¶é€šå¸¸å…³æ³¨ä¸¤ä¸ªæŒ‡æ ‡:\nç®—åŠ› Ï€ï¼šä¹Ÿç§°ä¸ºè®¡ç®—å¹³å°çš„æ€§èƒ½ä¸Šé™ï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ªè®¡ç®—å¹³å°å€¾å°½å…¨åŠ›æ¯ç§’é’Ÿæ‰€èƒ½å®Œæˆçš„æµ®ç‚¹è¿ç®—æ•°ã€‚å•ä½æ˜¯ FLOP/sã€‚ å¸¦å®½ ÃŸï¼šå³è®¡ç®—å¹³å°çš„å¸¦å®½ä¸Šé™ï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ªè®¡ç®—å¹³å°å€¾å°½å…¨åŠ›æ¯ç§’æ‰€èƒ½å®Œæˆçš„å†…å­˜äº¤æ¢é‡ã€‚å•ä½æ˜¯Byte/sã€‚ ä¸¤ä¸ªæŒ‡æ ‡ç›¸é™¤å³å¯å¾—åˆ°è®¡ç®—å¹³å°çš„è®¡ç®—å¼ºåº¦ä¸Šé™ I_max = Ï€ / ÃŸï¼Œå®ƒæè¿°çš„æ˜¯åœ¨è¿™ä¸ªè®¡ç®—å¹³å°ä¸Šï¼Œå•ä½å†…å­˜äº¤æ¢æœ€å¤šç”¨æ¥è¿›è¡Œå¤šå°‘æ¬¡è®¡ç®—ã€‚ Roofline Model\nä»å›¾ä¸­å¯ä»¥çœ‹å‡ºç®—åŠ›å†³å®šâ€œå±‹é¡¶â€çš„é«˜åº¦ï¼ˆç»¿è‰²çº¿æ®µï¼‰ï¼Œå¸¦å®½å†³å®šâ€œæˆ¿æªâ€çš„æ–œç‡ï¼ˆçº¢è‰²çº¿æ®µï¼‰ã€‚\nMemory-Bound: å½“æ¨¡å‹çš„è®¡ç®—å¼ºåº¦ I å°äºç¡¬ä»¶çš„è®¡ç®—å¼ºåº¦ä¸Šé™ I_max æ—¶ï¼Œç”±äºæ­¤æ—¶æ¨¡å‹ä½äºâ€œæˆ¿æªâ€åŒºé—´ï¼Œå› æ­¤æ¨¡å‹ç†è®ºæ€§èƒ½ P çš„å¤§å°å®Œå…¨ç”±ç¡¬ä»¶çš„å¸¦å®½ä¸Šé™ ÃŸ ï¼ˆæˆ¿æªçš„æ–œç‡ï¼‰ä»¥åŠæ¨¡å‹è‡ªèº«çš„è®¡ç®—å¼ºåº¦ I æ‰€å†³å®šï¼Œå› æ­¤è¿™æ—¶å€™å°±ç§°æ¨¡å‹å¤„äº Memory-Bound çŠ¶æ€ã€‚ Compute-Bound: ä¸ç®¡æ¨¡å‹çš„è®¡ç®—å¼ºåº¦ I æœ‰å¤šå¤§ï¼Œå®ƒçš„ç†è®ºæ€§èƒ½ P æœ€å¤§åªèƒ½ç­‰äºç¡¬ä»¶çš„ç®—åŠ› Ï€ ã€‚å½“æ¨¡å‹çš„è®¡ç®—å¼ºåº¦ I å¤§äºç¡¬ä»¶çš„è®¡ç®—å¼ºåº¦ä¸Šé™ I_max æ—¶ï¼Œæ¨¡å‹åœ¨å½“å‰ç¡¬ä»¶å¤„äº Compute-Bound çŠ¶æ€ ä¸ºäº†è®©å†…æ ¸å…·æœ‰æ›´é«˜çš„æ€§èƒ½ï¼Œéœ€è¦é€šè¿‡å‡å°‘å†…æ ¸æ‰§è¡Œçš„å…¨å±€å†…å­˜è®¿é—®æ¬¡æ•°æ¥å¢åŠ è®¡ç®—è®¿å­˜æ¯”ã€‚\n5.2 CUDA memory types ä¸‹å›¾å±•ç¤ºäº† CUDA è®¾å¤‡çš„å†…å­˜ã€‚å…¨å±€å†…å­˜å’Œå¸¸é‡å†…å­˜è¿™ä¸¤ç§ç±»å‹çš„å†…å­˜éƒ½å¯ä»¥è¢«ä¸»æœºå†™å…¥ (W) å’Œè¯»å– (R) ã€‚å…¨å±€å†…å­˜ä¹Ÿå¯ä»¥è¢«è®¾å¤‡è¯»å†™ï¼Œè€Œå¸¸é‡å†…å­˜åªæ”¯æŒè®¾å¤‡å¯¹å…¶è¯»å–ã€‚ å¦ä¸€ç§ç±»å‹çš„å†…å­˜æ˜¯æœ¬åœ°å†…å­˜ï¼Œä¹Ÿå¯ä»¥è¢«è¯»å†™ã€‚æœ¬åœ°å†…å­˜å®é™…ä¸Šæ”¾åœ¨å…¨å±€å†…å­˜ä¸­ï¼Œå…·æœ‰ç›¸ä¼¼çš„è®¿é—®å»¶è¿Ÿï¼Œä½†å®ƒä¸æ˜¯è·¨çº¿ç¨‹å…±äº«çš„ã€‚æ¯ä¸ªçº¿ç¨‹éƒ½æœ‰è‡ªå·±çš„å…¨å±€å†…å­˜éƒ¨åˆ†ï¼Œå°†å…¶ç”¨ä½œè‡ªå·±çš„ç§æœ‰æœ¬åœ°å†…å­˜ï¼Œå­˜æ”¾ç§æœ‰ä½†ä¸èƒ½åœ¨å¯„å­˜å™¨ä¸­åˆ†é…çš„æ•°æ®ã€‚ å¯„å­˜å™¨ (register) å’Œå…±äº«å†…å­˜ (shared memory) æ˜¯ç‰‡ä¸Šå†…å­˜ã€‚å­˜å‚¨åœ¨è¿™äº›ç±»å‹å†…å­˜ä¸­çš„å˜é‡å¯ä»¥ä»¥é«˜åº¦å¹¶è¡Œçš„æ–¹å¼ä»¥é«˜é€Ÿè®¿é—®ã€‚å…¶ä¸­æ¯ä¸ªçº¿ç¨‹åªèƒ½è®¿é—®è‡ªå·±çš„å¯„å­˜å™¨ã€‚\nOverview of CUDA Memory Model\nä¸åŸºäºå†¯Â·è¯ºä¼Šæ›¼æ¨¡å‹çš„è®¡ç®—æœºç±»æ¯”ï¼ŒCUDA è®¾å¤‡ä¸­çš„å…¨å±€å†…å­˜å¯¹åº”äºå†…å­˜æ¡†ï¼Œå¯„å­˜å™¨å¯¹åº”äºå¯„å­˜å™¨å †ã€‚ä¸è®¿é—®å…¨å±€å†…å­˜ç›¸æ¯”ï¼Œæ¯æ¬¡è®¿é—®å¯„å­˜å™¨æ‰€æ¶‰åŠçš„æŒ‡ä»¤æ›´å°‘ã€‚å½“ç®—æœ¯æŒ‡ä»¤çš„æ“ä½œæ•°åœ¨å¯„å­˜å™¨ä¸­æ—¶ï¼Œä¸éœ€è¦é¢å¤–çš„æŒ‡ä»¤ä½¿ç®—æœ¯é€»è¾‘å•å…ƒ(ALU)å¯ä»¥ä½¿ç”¨è¯¥æ“ä½œæ•°çš„å€¼ã€‚å¦‚æœæ“ä½œæ•°å€¼åœ¨å…¨å±€å†…å­˜ä¸­ï¼Œå¤„ç†å™¨éœ€è¦æ‰§è¡Œå†…å­˜åŠ è½½æ“ä½œè®© ALU èƒ½ä½¿ç”¨æ“ä½œæ•°ã€‚å¹¶ä¸”ä»å¯„å­˜å™¨å †è®¿é—®æ‰€æ¶ˆè€—çš„èƒ½é‡è‡³å°‘æ¯”ä»å…¨å±€å†…å­˜è®¿é—®ä½ä¸€ä¸ªæ•°é‡çº§ã€‚\nMemory vs. Registers in a Modern Computer Based on the von Neumann Model\nä¸‹å›¾å±•ç¤ºäº† CUDA è®¾å¤‡ä¸­çš„å…±äº«å†…å­˜å’Œå¯„å­˜å™¨ã€‚å…±äº«å†…å­˜å®é™…ä¸Šæ˜¯ä¸€ç§æš‚å­˜å­˜å‚¨å™¨ (scratchpad memory)ï¼Œä½œä¸ºç‰‡ä¸Šå†…å­˜çš„ä¸€éƒ¨åˆ†ã€‚å½“å¤„ç†å™¨è®¿é—®å­˜å‚¨åœ¨å…±äº«å†…å­˜ä¸­çš„æ•°æ®æ—¶ï¼Œéœ€è¦æ‰§è¡Œå†…å­˜åŠ è½½æ“ä½œã€‚CUDA ä¸­å…±äº«å†…å­˜å’Œå¯„å­˜å™¨ä¹‹é—´çš„ä¸€ä¸ªé‡è¦åŒºåˆ«æ˜¯ï¼Œå­˜å‚¨åœ¨å…±äº«å†…å­˜ä¸­çš„å˜é‡å¯ä»¥è¢«å—ä¸­çš„æ‰€æœ‰çº¿ç¨‹è®¿é—®ï¼Œè€Œå¯„å­˜å™¨æ•°æ®æ˜¯çº¿ç¨‹ç§æœ‰çš„ã€‚\nShared Memory vs. Registers in a CUDA Device SM\nä¸‹è¡¨ç»™å‡ºäº†å°†ç¨‹åºå˜é‡å£°æ˜ä¸ºå„ç§å†…å­˜ç±»å‹çš„ CUDA è¯­æ³•ã€‚\næ‰€æœ‰åœ¨å†…æ ¸å’Œè®¾å¤‡å‡½æ•°ä¸­å£°æ˜çš„ automatic scalar variables éƒ½è¢«æ”¾å…¥å¯„å­˜å™¨ä¸­ã€‚ Automatic array variables å­˜å‚¨åœ¨çº¿ç¨‹çš„æœ¬åœ°å†…å­˜ä¸­ã€‚å¦‚æœæ‰€æœ‰è®¿é—®éƒ½ä½¿ç”¨å¸¸é‡ç´¢å¼•å€¼ï¼Œç¼–è¯‘å™¨å¯èƒ½å†³å®šå°†å°†å…¶å­˜å‚¨åˆ°å¯„å­˜å™¨ä¸­ã€‚ å—ä¸­çš„æ‰€æœ‰çº¿ç¨‹éƒ½çœ‹åˆ° shared variable çš„ç›¸åŒç‰ˆæœ¬ã€‚å†…æ ¸æ‰§è¡ŒæœŸé—´æ¯ä¸ªå—ä¼šåˆ›å»ºå’Œä½¿ç”¨ä¸€ä¸ªç§æœ‰ç‰ˆæœ¬ã€‚é€šå¸¸ä½¿ç”¨å…±äº«å˜é‡æ¥ä¿å­˜åœ¨å†…æ ¸æ‰§è¡Œé˜¶æ®µç»å¸¸ä½¿ç”¨å’Œé‡ç”¨çš„å…¨å±€å†…å­˜æ•°æ®éƒ¨åˆ†ã€‚ Constant variables é€šå¸¸ç”¨äºå‘æ ¸å‡½æ•°æä¾›è¾“å…¥ã€‚å†…æ ¸å‡½æ•°ä¸èƒ½ä¿®æ”¹å¸¸é‡å˜é‡çš„å€¼ã€‚ Global variables é€šå¸¸ç”¨äºå°†ä¿¡æ¯ä»ä¸€ä¸ªå†…æ ¸è°ƒç”¨ä¼ é€’åˆ°å¦ä¸€ä¸ªå†…æ ¸è°ƒç”¨ã€‚ Variable Declaration Memory Scope Lifetime Automatic variables other than arrays Register Thread Kernel AutomaticÂ array variables Local Thread Kernel __device__Â __shared__ int SharedVar; Shared Block Kernel __device__Â int GlobalVar; Global Grid Application __device__Â __constant__ int ConstantVar; Constant Grid Application åœ¨ CUDA ä¸­ï¼ŒæŒ‡é’ˆå¯ä»¥ç”¨æ¥æŒ‡å‘å…¨å±€å†…å­˜ä¸­çš„æ•°æ®å¯¹è±¡ï¼Œé€šå¸¸æœ‰ä»¥ä¸‹ä¸¤ç§æƒ…å†µä¼šä½¿ç”¨\nå¯¹è±¡ç”±ä¸»æœºå‡½æ•°åˆ†é…ï¼ŒæŒ‡å‘å¯¹è±¡çš„æŒ‡é’ˆç”±å†…å­˜åˆ†é…å‡½æ•° (å¦‚ cudaMalloc) åˆå§‹åŒ–ï¼Œä½œä¸ºå‚æ•°ä¼ é€’ç»™å†…æ ¸å‡½æ•°ã€‚ å°†åœ¨å…¨å±€å†…å­˜ä¸­å£°æ˜çš„å˜é‡çš„åœ°å€èµ‹ç»™æŒ‡é’ˆå˜é‡ã€‚ 5.3 Tiling for Reduced Memory Traffic ä¸€ç§å¸¸è§çš„ç­–ç•¥æ˜¯å°†æ•°æ®åˆ’åˆ†ä¸ºç§°ä¸º tile çš„å­é›†ï¼Œä»¥ä¾¿æ¯ä¸ª tile éƒ½é€‚åˆå…±äº«å†…å­˜ã€‚èƒ½è¿›è¡Œåˆ’åˆ†çš„ä¸€ä¸ªé‡è¦çš„æ ‡å‡†æ˜¯è¿™äº› tile ä¸Šçš„å†…æ ¸è®¡ç®—å¯ä»¥å½¼æ­¤ç‹¬ç«‹åœ°å®Œæˆã€‚ ä¸‹å›¾å±•ç¤ºäº† block(0,0) çš„å››ä¸ªçº¿ç¨‹æ‰€å®Œæˆçš„è®¡ç®—ã€‚è¿™å››ä¸ªçº¿ç¨‹è®¡ç®—P(0,0), P(0,1), P(1,0) å’Œ P(1,1). æ¯ä¸ªçº¿ç¨‹åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è®¿é—® M çš„ 4 ä¸ªå…ƒç´ å’Œ N çš„ 4 ä¸ªå…ƒç´ ï¼Œå¯ä»¥çœ‹å‡ºæœ‰æ˜æ˜¾é‡å¤çš„éƒ¨åˆ†ã€‚å°†æ¯ä¸ªå—éœ€è¦è®¿é—®çš„æ•°æ®å…ˆåŠ è½½åˆ°å…±äº«å†…å­˜ï¼Œè¿™æ ·å¯ä»¥é¿å…æ¯ä¸ªçº¿ç¨‹ä»å…¨å±€å†…å­˜é‡ŒåŠ è½½é‡å¤çš„æ•°æ®ã€‚å…¨å±€å†…å­˜æµé‡çš„å‡å°‘ä¸å—çš„ç»´åº¦æˆæ­£æ¯”ã€‚æ¯ä¸ªå—å¤§å°ä¸º Width*Width æ—¶ï¼Œå…¨å±€å†…å­˜æµé‡å°†å‡å°‘ä¸ºåŸæ¥çš„ 1/Width.\nA Small Example of Matrix Multiplication\næŒ‰ tile è¿›è¡ŒçŸ©é˜µä¹˜æ³•çš„åŸºæœ¬æ€æƒ³æ˜¯è®©çº¿ç¨‹åœ¨å„è‡ªä½¿ç”¨å…ƒç´ æ¥è¿›è¡Œå†…ç§¯è®¡ç®—ä¹‹å‰ï¼Œå°† M å’Œ N å…ƒç´ çš„å­é›†åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºæŠŠ M å’Œ N åˆ†æˆå¤§å°ä¸º 2*2 çš„å—ã€‚æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œçš„å†…ç§¯è®¡ç®—ç°åœ¨è¢«åˆ’åˆ†ä¸ºå‡ ä¸ªé˜¶æ®µã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œä¸€ä¸ªå—ä¸­çš„æ‰€æœ‰çº¿ç¨‹åä½œå°†å¯¹åº”çš„ M å’Œ N çš„ tile åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­ã€‚è¿™æ ·æ¯ä¸ªé˜¶æ®µå…³æ³¨çš„æ˜¯è¾“å…¥çŸ©é˜µå…ƒç´ çš„ä¸€ä¸ªå°å­é›†ã€‚è¿™ç§é›†ä¸­çš„è®¿é—®è¡Œä¸ºç§°ä¸ºå±€éƒ¨æ€§ (locality).\nTiling M and N to Utilize Shared Memory\n5.4 A Tiled Matrix Multiplication Kernel æŒ‰ç…§ä¸Šè¿°æ–¹æ³•ç¼–å†™çš„å†…æ ¸å‡½æ•°å¦‚ä¸‹ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œx è½´æ–¹å‘ä¸Šåæ ‡ä¸º bx å’Œ tx çš„çº¿ç¨‹åº”è¯¥è´Ÿè´£è®¡ç®— P ä¸­ç´¢å¼•ä¸º bx * tile_width + tx å…ƒç´ ã€‚ç±»ä¼¼åœ°ï¼Œy è½´æ–¹å‘ä¸Šçº¿ç¨‹è¦å¤„ç†çš„ P ä¸­ç´¢å¼•ä¸º by * tile_width + ty. å¤–å¾ªç¯çš„æ¯æ¬¡è¿­ä»£å¯¹åº”äºè®¡ç®—çš„ä¸€ä¸ªé˜¶æ®µã€‚ä¸¤æ¬¡è°ƒç”¨ __syncthreads() çš„åŸå› ä¸åŒï¼Œç¬¬ä¸€æ¬¡è¢«ç§°ä¸ºå†™åè¯» (read-after-write) ä¾èµ–å…³ç³»ï¼Œå› ä¸ºçº¿ç¨‹åœ¨å°è¯•è¯»å–æ•°æ®ä¹‹å‰å¿…é¡»ç­‰å¾…å…¶ä»–çº¿ç¨‹å°†æ•°æ®å†™å…¥æ­£ç¡®çš„ä½ç½®ã€‚ç¬¬äºŒç§è¢«ç§°ä¸ºè¯»åå†™ (write-after-read) ä¾èµ–ï¼Œå› ä¸ºçº¿ç¨‹å¿…é¡»ç­‰å¾…æ‰€æœ‰éœ€è¦å®ƒçš„çº¿ç¨‹è¯»å–æ•°æ®ï¼Œç„¶åæ‰èƒ½è¦†ç›–å®ƒã€‚\nå†™åè¯»ä¾èµ–æ˜¯ä¸€ç§çœŸæ­£ä¾èµ– (true dependence)ï¼Œå› ä¸ºè¯»çº¿ç¨‹ç¡®å®éœ€è¦å†™çº¿ç¨‹æä¾›çš„æ•°æ®ï¼Œæ‰€ä»¥å®ƒåˆ«æ— é€‰æ‹©ï¼Œåªèƒ½ç­‰å¾…ã€‚è¯»åå†™ä¾èµ–å…³ç³»æ˜¯ä¼ªä¾èµ– (false dependence) å…³ç³»ï¼Œå› ä¸ºå†™çº¿ç¨‹ä¸éœ€è¦æ¥è‡ªè¯»çº¿ç¨‹çš„ä»»ä½•æ•°æ®ã€‚è¿™ç§ä¾èµ–æ€§æ˜¯å› ä¸ºå®ƒä»¬è®¿é—®ç›¸åŒçš„å†…å­˜åœ°å€ï¼Œå¦‚æœå®ƒä»¬è®¿é—®ä¸åŒçš„åœ°å€ï¼Œåˆ™ä¸å­˜åœ¨è¿™ç§ä¾èµ–æ€§ã€‚\n__global__ void TilingMatrixMulKernel(float* M, float* N, float* P, int width) { __shared__ float Mds[TILE_WIDTH][TILE_WIDTH]; __shared__ float Nds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; // Identify the row and column of the P element to work on int Row = by * TILE_WIDTH + ty; int Col = bx * TILE_WIDTH + tx; float Pvalue = 0; // Loop over the M and N tiles required to compute the P elemrnt for (int ph = 0; ph \u0026lt; width/TILE_WIDTH; ph++) { // Collaborative loading of M and N tiles into shared memory Mds[ty][tx] = M[Row * width + ph * TILE_WIDTH + tx]; Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * width + Col]; __syncthreads(); for (int k = 0; k \u0026lt; TILE_WIDTH; k++) { Pvalue += Mds[ty][k] * Nds[k][tx]; } __syncthreads(); P[Row * width + Col] = Pvalue; } } Tiling æŠ€æœ¯å¹¶ä¸æ˜¯ GPU ä¸Šæ‰èƒ½å®ç°ã€‚CPU ä¸Šçš„ tiling ä¾èµ–ç¼“å­˜æ¥å°†é‡ç”¨çš„æ•°æ®ä¿ç•™åœ¨èŠ¯ç‰‡ä¸Šï¼Œè€Œ GPU ä¸Šçš„ tiling åˆ™ç›´æ¥åœ°ä½¿ç”¨å…±äº«å†…å­˜æ¥å­˜å‚¨ç‰‡ä¸Šæ•°æ®ã€‚CPU æ ¸å¿ƒé€šå¸¸åªè¿è¡Œä¸€ä¸ªæˆ–ä¸¤ä¸ªçº¿ç¨‹ï¼Œå› æ­¤çº¿ç¨‹å¯ä»¥ä¾èµ–äºç¼“å­˜æ¥ä¿å­˜æœ€è¿‘ä½¿ç”¨çš„æ•°æ®ã€‚ç›¸åï¼ŒGPU SM åŒæ—¶è¿è¡Œå¤šä¸ªçº¿ç¨‹ä»¥éšè—å»¶è¿Ÿï¼Œäº›çº¿ç¨‹ä¼šç«äº‰ç¼“å­˜æ§½ï¼Œä½¿å¾— GPU ç¼“å­˜ä¸å¤ªå¯é ã€‚\n5.5 Boundary Checks æˆ‘ä»¬éœ€è¦æ‰©å±• tiling çŸ©é˜µä¹˜æ³•å†…æ ¸ä½¿å…¶å¤„ç†ä»»æ„å¤§å°çš„çŸ©é˜µã€‚ä¸‹å›¾å±•ç¤ºäº† block(0,0) åœ¨ phase 1 çš„å†…å­˜è®¿é—®æ¨¡å¼ã€‚åœ¨ä¸è¿›è¡Œè¾¹ç•Œæ£€æŸ¥æ—¶ thead(0,1) è¯•å›¾è®¿é—® M(0,3) æ—¶å®é™…ä¸Šè·å¾—çš„æ˜¯ M(1,0). åŒæ ·åœ¨ Block(1,1) åœ¨ phase 0 è®¿é—®æ—¶ä¹Ÿä¼šå‡ºç°ç±»ä¼¼çš„é—®é¢˜ã€‚å› æ­¤åœ¨åŠ è½½æ‰€éœ€çš„ M å’Œ N çš„ tile æ—¶è¾¹ç•Œæ¡ä»¶ä¸ºä¸¤ä¸ªç´¢å¼•éƒ½å°äº Width: Row \u0026lt; Width \u0026amp;\u0026amp; (ph * TILE_WIDT + tx) \u0026lt; Widthï¼Œå¦åˆ™å°† 0.0f å­˜å…¥å¯¹åº”ä½ç½®ã€‚\nMemory Access of Block(0,0) in Phase 1\næ‰©å±•ä¸ºä¸€èˆ¬çš„çŸ©é˜µä¹˜æ³•å†…æ ¸æ˜¯å¾ˆå®¹æ˜“çš„ã€‚å°† Width å‚æ•°æ›¿æ¢ä¸ºä¸‰ä¸ªæ— ç¬¦å·æ•´æ•°å‚æ•°: m, k, n; å°†ç”¨äºæŒ‡ä»£ M çš„è¡Œæ•°/åˆ—æ•°å’Œ P çš„è¡Œæ•°/åˆ—æ•°çš„ Width æ›¿æ¢ä¸º m/nï¼›å°†ç”¨äºæŒ‡ä»£ M çš„åˆ—æ•°å’Œ P çš„è¡Œæ•°çš„ Width æ›¿æ¢ä¸º k. ä¿®æ”¹åä»£ç å¦‚ä¸‹\nCalculation of the Matrix Indexes in Tiled Multiplication\n__global__ void GEMMKernel(float* M, float* N, float* P, int m, int n, int k) { __shared__ float Mds[TILE_WIDTH][TILE_WIDTH]; __shared__ float Nds[TILE_WIDTH][TILE_WIDTH]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; // Identify the row and column of the P element to work on int Row = by * TILE_WIDTH + ty; int Col = bx * TILE_WIDTH + tx; float Pvalue = 0; // Loop over the M and N tiles required to compute the P element for (int ph = 0; ph \u0026lt; (k + TILE_WIDTH - 1) / TILE_WIDTH; ph++) { // Collaborative loading of M and N tiles into shared memory if (Row \u0026lt; m \u0026amp;\u0026amp; ph * TILE_WIDTH + tx \u0026lt; k) { Mds[ty][tx] = M[Row * k + ph * TILE_WIDTH + tx]; } else { Mds[ty][tx] = 0.0f; } if (ph * TILE_WIDTH + ty \u0026lt; k \u0026amp;\u0026amp; Col \u0026lt; n) { Nds[ty][tx] = N[(ph * TILE_WIDTH + ty) * n + Col]; } else { Nds[ty][tx] = 0.0f; } __syncthreads(); for (int i = 0; i \u0026lt; TILE_WIDTH; i++) { Pvalue += Mds[ty][i] * Nds[i][tx]; } __syncthreads(); } if (Row \u0026lt; m \u0026amp;\u0026amp; Col \u0026lt; n) { P[Row * n + Col] = Pvalue; } } 5.6 Impact of Memory Usage on Occupancy CUDA è®¾å¤‡æä¾›æœ‰é™çš„èµ„æºé™åˆ¶äº†å¯ä»¥åŒæ—¶åœ¨ç»™å®šç¨‹åºçš„ SM ä¸­åˆ†é…çš„çº¿ç¨‹æ•°é‡ã€‚ä¸Šé¢ä»£ç ä¸æ”¯æŒä¸»æœºä»£ç å¯¹å…±äº«å†…å­˜ä½¿ç”¨æƒ…å†µçš„ä»»ä½•åŠ¨æ€è°ƒæ•´ï¼Œå› ä¸ºå…±äº«å†…å­˜ä½¿ç”¨çš„å¤§å°æ˜¯ä¸€ä¸ªå¸¸é‡ã€‚ è§£å†³çš„æ–¹æ³•æ˜¯å…±äº«å†…å­˜å£°æ˜å‰æ·»åŠ ä¸€ä¸ª extern å…³é”®å­—ï¼Œå¹¶åœ¨å£°æ˜ä¸­çœç•¥æ•°ç»„çš„å¤§å°ã€‚å½“è°ƒç”¨å†…æ ¸æ—¶ï¼Œå¯ä»¥æ ¹æ®è®¾å¤‡æŸ¥è¯¢ç»“æœåŠ¨æ€é…ç½®æ¯ä¸ªå—è¦ä½¿ç”¨çš„å…±äº«å†…å­˜é‡ï¼Œå¹¶å°†å…¶ä½œä¸ºç¬¬ä¸‰ä¸ªæ‰§è¡Œé…ç½®å‚æ•°æä¾›ç»™å†…æ ¸è°ƒç”¨ã€‚ç„¶åå°†æ•°ç»„ä¸­æ¯ä¸ªéƒ¨åˆ†çš„å¤§å°ä½œä¸ºå‚æ•°ä¼ é€’ç»™å†…æ ¸å‡½æ•°ã€‚\nsize = ...; matrixMulKernel\u0026lt;\u0026lt;\u0026lt;dimGrid,dimBlock,size\u0026gt;\u0026gt;\u0026gt;(Mdï¼ŒNdï¼ŒPd, Widthï¼Œsize/2ï¼Œsize/2); __global__ void matrixMulKernel(float* M, float* N,float* P,int width, unsigned Mdz_sz, unsigned Nds_sz) { extern __shared__ char float Mds_Nds[]; float *Mds = (float *) Mds_Nds; float *Nds = (float*) Mds_Nds + Mds_sz; } ","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch5/","summary":"Personal notebook 5 of Programming Massively Parallel","title":"PMPP Learning-Chapter 5 Memory Architecture and Data Locality"},{"content":"3 Multidimensional Grids and Data æœ¬ç« å°†æ›´å¹¿æ³›åœ°ä»‹ç»çº¿ç¨‹æ˜¯å¦‚ä½•ç»„ç»‡çš„å’Œå¦‚ä½•ä½¿ç”¨çº¿ç¨‹å’Œå—æ¥å¤„ç†å¤šç»´æ•°ç»„ã€‚\n3.1 Multidimensional Grid Organization å†æ¬¡å¼ºè°ƒç½‘æ ¼ä¸­çš„æ‰€æœ‰çº¿ç¨‹æ‰§è¡Œç›¸åŒçš„å†…æ ¸å‡½æ•°ï¼Œå®ƒä»¬ä¾èµ–äºçº¿ç¨‹ç´¢å¼•æ¥åŒºåˆ†å½¼æ­¤ï¼Œå¹¶ç¡®å®šå„è‡ªè¦å¤„ç†çš„æ•°æ®çš„éƒ¨åˆ†ã€‚è¿™äº›çº¿ç¨‹è¢«ç»„ç»‡æˆä¸¤çº§ç»“æ„: ä¸€ä¸ªç½‘æ ¼ç”±ä¸€ä¸ªæˆ–å¤šä¸ªå—ç»„æˆï¼Œæ¯ä¸ªå—ç”±ä¸€ä¸ªæˆ–å¤šä¸ªçº¿ç¨‹ç»„æˆã€‚è°ƒç”¨å†…æ ¸å‡½æ•°æ—¶éœ€è¦æŒ‡å®šæ‰§è¡Œé…ç½®å‚æ•° gridDim å’Œ blockDimï¼ŒgridDim æ˜¯ä¸€ä¸ªä¸‰ç»´å—æ•°ç»„ï¼ŒblockDim æ˜¯ä¸€ä¸ªä¸‰ç»´çº¿ç¨‹æ•°ç»„ã€‚ä»–ä»¬çš„ç±»å‹éƒ½æ˜¯ dim3ï¼Œæ˜¯åŒ…å«ä¸‰ä¸ªå…ƒç´  x, y å’Œ z çš„æ•´æ•°å‘é‡ç±»å‹ï¼Œåˆ†åˆ«æŒ‡å®šäº†æ¯ä¸ªç»´åº¦ä¸Šçš„å—ä¸ªæ•°å’Œçº¿ç¨‹ä¸ªæ•°ã€‚ä½¿ç”¨å°‘äº 3 ä¸ªç»´åº¦æ—¶å¯ä»¥å°†æœªä½¿ç”¨çš„ç»´åº¦å¤§å°è®¾ç½®ä¸º 1ã€‚ç½‘æ ¼ä¸­çš„æ‰€æœ‰å—éƒ½å…·æœ‰ç›¸åŒçš„ç»´åº¦å’Œå¤§å°ã€‚ä¸€æ—¦ç½‘æ ¼å¯åŠ¨ï¼Œç½‘æ ¼å’Œå—çš„å°ºå¯¸å°†ä¿æŒä¸å˜ï¼Œç›´åˆ°æ•´ä¸ªç½‘æ ¼å®Œæˆæ‰§è¡Œã€‚\nå½“å‰ CUDA ç³»ç»Ÿä¸­ï¼Œæ¯ä¸ªå—çš„æ€»å¤§å°é™åˆ¶ä¸º 1024 ä¸ªçº¿ç¨‹ã€‚åªè¦çº¿ç¨‹æ€»æ•°ä¸è¶…è¿‡ 1024ï¼Œè¿™äº›çº¿ç¨‹å°±å¯ä»¥ä»¥ä»»ä½•æ–¹å¼åˆ†å¸ƒåœ¨ä¸‰ä¸ªç»´åº¦ä¸Šã€‚\nfunction_name\u0026lt;\u0026lt;\u0026lt;gridDim, blockDim\u0026gt;\u0026gt;\u0026gt;(...); ä¸€ä¸ªä¾‹å­å¦‚ä¸‹ï¼ŒdimBlockå’ŒdimGridæ˜¯ç”±ç¨‹åºå‘˜å®šä¹‰çš„ä¸»æœºä»£ç å˜é‡ã€‚\ndim3 dimGrid(32, 1, 1); dim3 dimBlock(128, 1, 1); vecAddKernel\u0026lt;\u0026lt;\u0026lt;dimGrid, dimBlock\u0026gt;\u0026gt;\u0026gt;(...); ä¸‹å›¾å±•ç¤ºäº† gridDim(2,2,1) å’Œ blockDim (4,2,2) æƒ…å†µä¸‹çº¿ç¨‹ç»„ç»‡çš„æƒ…å†µã€‚\nA Multidimensional Example of CUDA Grid Organization\n3.2 Mapping threads to multidimensional data é€‰æ‹© 1Dã€2D æˆ– 3D çš„çº¿ç¨‹ç»„ç»‡é€šå¸¸åŸºäºæ•°æ®çš„æ€§è´¨ã€‚ä¾‹å¦‚å›¾åƒæ˜¯ä¸€ä¸ªäºŒç»´åƒç´ æ•°ç»„ã€‚ä½¿ç”¨ç”± 2D å—ç»„æˆçš„ 2D ç½‘æ ¼å¯ä»¥æ–¹ä¾¿åœ°å¤„ç†å›¾åƒä¸­çš„åƒç´ ã€‚ä¸‹å›¾å±•ç¤ºäº†å¤„ç†å¤§å°ä¸º 62*76 1F1F çš„å›¾ç‰‡ P çš„ä¸€ç§ç»„ç»‡æ–¹å¼ã€‚å‡è®¾ä½¿ç”¨ 16*16 å¤§å°çš„å—ï¼Œé‚£ä¹ˆåœ¨ y æ–¹å‘ä¸Šéœ€è¦ 4 ä¸ªå—ï¼Œåœ¨ x æ–¹å‘ä¸Šéœ€è¦ 5 ä¸ªå—ã€‚æ¨ªçºµåæ ‡çš„è®¡ç®—æ–¹å¼ä¸º\nrow coordinate = blockIdx.y * blockDim.y + threadIdx.y col coordinate = blockIdx.x * blockDim.x + threadIdx.x æˆ‘ä»¬å°†æŒ‰ç»´åº¦çš„é™åº (z, y, x) è¡¨ç¤ºå¤šç»´æ•°æ®ã€‚è¿™ç§é¡ºåºä¸ gridDim å’Œ blockDim ç»´åº¦ä¸­æ•°æ®ç»´åº¦çš„é¡ºåºç›¸åï¼ï¼ï¼\nå®é™…ä¸Šï¼Œç”±äºç°ä»£è®¡ç®—æœºä¸­ä½¿ç”¨äºŒç»´å­˜å‚¨ç©ºé—´ï¼ŒC è¯­è¨€ä¸­çš„æ‰€æœ‰å¤šç»´æ•°ç»„éƒ½æ˜¯çº¿æ€§åŒ–çš„ã€‚è™½ç„¶å¯ä»¥ä½¿ç”¨å¦‚ Pin_d[j][i] è¿™æ ·çš„å¤šç»´æ•°ç»„è¯­æ³•è®¿é—®å¤šç»´æ•°ç»„çš„å…ƒç´ ï¼Œä½†ç¼–è¯‘å™¨å°†è¿™äº›è®¿é—®è½¬æ¢ä¸ºæŒ‡å‘æ•°ç»„å¼€å§‹å…ƒç´ çš„åŸºæŒ‡é’ˆï¼Œä»¥åŠä»è¿™äº›å¤šç»´ç´¢å¼•è®¡ç®—å‡ºçš„ä¸€ç»´åç§»é‡ã€‚ è‡³å°‘æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥å¯¹äºŒç»´æ•°ç»„è¿›è¡Œçº¿æ€§åŒ–ã€‚å°†åŒä¸€è¡Œ/åˆ—çš„æ‰€æœ‰å…ƒç´ æ”¾ç½®åˆ°è¿ç»­çš„ä½ç½®ã€‚ç„¶åå°†è¡Œ/åˆ—ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°æ”¾å…¥å†…å­˜ç©ºé—´ä¸­ã€‚è¿™ç§æ’åˆ—ç§°ä¸ºè¡Œ/åˆ—ä¸»åºå¸ƒå±€ (row/column-major layout). CUDA C ä½¿ç”¨è¡Œä¸»åºå¸ƒå±€ã€‚\nRow-major Layout for a 2D C Array\nä¸‹é¢å†…æ ¸ä»£ç å°†æ¯ä¸ªé¢œè‰²åƒç´ è½¬æ¢ä¸ºå¯¹åº”çš„ç°åº¦åƒç´ ã€‚æˆ‘ä»¬è®¡ç®—åæ ‡ä¸º (row, col) çš„åƒç´ å¯¹åº”çš„ 1D ç´¢å¼• row * width + col. è¿™ä¸ª 1D ç´¢å¼• grayOffset å°±æ˜¯ Pout çš„åƒç´ ç´¢å¼•ï¼Œå› ä¸ºè¾“å‡ºç°åº¦å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ éƒ½æ˜¯ 1å­—èŠ‚ (unsigned char)ã€‚æ¯ä¸ªå½©è‰²åƒç´ ç”¨ä¸‰ä¸ªå…ƒç´ (r, g, b)å­˜å‚¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º1å­—èŠ‚ã€‚å› æ­¤ rgbOffset ç»™å‡ºäº† Pin æ•°ç»„ä¸­é¢œè‰²åƒç´ çš„èµ·å§‹ä½ç½®ã€‚ä» Pin æ•°ç»„çš„ä¸‰ä¸ªè¿ç»­å­—èŠ‚ä½ç½®è¯»å–æ¯ä¸ªé€šé“å¯¹åº”çš„å€¼ï¼Œæ‰§è¡Œç°åº¦åƒç´ å€¼çš„è®¡ç®—ï¼Œå¹¶ä½¿ç”¨ grayOffset å°†è¯¥å€¼å†™å…¥ Pout æ•°ç»„ã€‚\n// we have 3 channels corresponding to RGB // The input image is encoded as unsigned characters [0, 255] __global__ void colorToGreyscaleConversion(unsigned char * Pout, unsigned char * Pin, int width, int height) { int Col = threadIdx.x + blockIdx.x * blockDim.x; int Row = threadIdx.y + blockIdx.y * blockDim.y; if (Col \u0026lt; width \u0026amp;\u0026amp; Row \u0026lt; height) { // get 1D coordinate for the grayscale image int greyOffset = Row*width + Col; // one can think of the RGB image having // CHANNEL times columns than the grayscale image int rgbOffset = greyOffset*CHANNELS; unsigned char r = Pin[rgbOffset + 0]; // red value for pixel unsigned char g = Pin[rgbOffset + 1]; // green value for pixel unsigned char b = Pin[rgbOffset + 2]; // blue value for pixel // perform the rescaling and store it // We multiply by floating point constants Pout[grayOffset] = 0.21f*r + 0.71f*g + 0.07f*b; } } 3.3 Image blur: a more complex kernel å›¾åƒæ¨¡ç³Šå‡½æ•°å°†è¾“å‡ºå›¾åƒåƒç´ çš„å€¼è®¡ç®—ä¸ºç›¸é‚»åƒç´  (åŒ…æ‹¬è¾“å…¥å›¾åƒä¸­åƒç´ ) çš„åŠ æƒå’Œã€‚ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸é‚»åƒç´ çš„å¹³å‡å€¼æ¥è®¡ç®—ç»“æœï¼Œå¯¹åº”çš„ä»£ç å¦‚ä¸‹ã€‚ä¸ colorToGrayscaleConversion ä¸­ä½¿ç”¨çš„ç­–ç•¥ç±»ä¼¼ï¼Œå¯¹æ¯ä¸ªè¾“å‡ºåƒç´ ä½¿ç”¨ 1 ä¸ªçº¿ç¨‹æ¥è®¡ç®—ã€‚colå’Œ row è¡¨ç¤ºè¾“å…¥åƒç´  patch çš„ä¸­å¿ƒåƒç´ ä½ç½®ã€‚åµŒå¥—çš„ for å¾ªç¯éå† patch ä¸­çš„æ‰€æœ‰åƒç´ ã€‚if è¯­å¥çš„ curRow \u0026lt; 0 å’Œ curCol \u0026lt; 0 æ¡ä»¶ç”¨äºè·³è¿‡æ‰§è¡Œè¶…å‡ºå›¾åƒèŒƒå›´çš„éƒ¨åˆ†ã€‚\n__global__ void blurKernel(unsigned char *in, unsigned char *out, int width, int height) { int Col = threadIdx.x + blockIdx.x * blockDim.x; int Row = threadIdx.y + blockIdx.y * blockDim.y; if (Col \u0026lt; width \u0026amp;\u0026amp; Row \u0026lt; height) { int pixVal = 0; int pixels = 0; // Get the average of the surrounding BLUR_SIZE x BLUR_SIZE box for (int blurRow = -BLUR_SIZE; blurRow \u0026lt; BLUR_SIZE + 1; blurRow++) { for (int blurCol = -BLUR_SIZE; blurCol \u0026lt; BLUR_SIZE + 1; blurCol++) { int curRow = Row + blurRow; int curCol = Col + blurCol; // If the pixel is within the image, add its value to the sum if(curRow \u0026gt; -1 \u0026amp;\u0026amp; curRow \u0026lt; height \u0026amp;\u0026amp; curCol \u0026gt; -1 \u0026amp;\u0026amp; curCol \u0026lt; width) { pixVal += in[curRow*width + curCol]; pixels++; // Keep track of the number of pixels in the avg } } } // Write our new pixel value out out[Row*width + Col] = (unsigned char)(pixVal / pixels); } } 3.4 Matrix multiplication çŸ©é˜µä¹˜æ³•æ˜¯ Basic Linear Algebra Subprograms (BLAS) çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚\nLevel 1 å½¢å¦‚ $y = \\alpha x + y$ çš„å‘é‡è¿ç®—ã€‚ Level 2 å½¢å¦‚ $y = \\alpha Ax + \\beta y$ çš„çŸ©é˜µ-å‘é‡è¿ç®—ã€‚ Level 3 å½¢å¦‚ $y = \\alpha AB + \\beta C$ çš„çŸ©é˜µ-çŸ©é˜µè¿ç®—ã€‚ ä¸ºäº†ç”¨ CUDA å®ç°çŸ©é˜µä¹˜æ³•ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–ä¸ colorToGrayscaleConversion ç›¸åŒçš„æ–¹æ³•å°†ç½‘æ ¼ä¸­çš„çº¿ç¨‹æ˜ å°„åˆ°è¾“å‡ºçŸ©é˜µ P çš„å…ƒç´ ï¼Œå³æ¯ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®— P ä¸­çš„ä¸€ä¸ªå…ƒç´ ã€‚\n// Assuming square matrices of size Width x Width __global__ void MatrixMulKernel(float* M, float* N, float* P, int Width) { // Calculate the row index of the P element and M int Row = blockIdx.y*blockDim.y+threadIdx.y; // Calculate the column index of P and N int Col = blockIdx.x*blockDim.x+threadIdx.x; if ((Row \u0026gt;= Width) || (Col \u0026gt;= Width)) return; float Pvalue = 0; // each thread computes one element of the block sub-matrix for (int k = 0; k \u0026lt; Width; ++k) Pvalue += M[Row*Width+k]*N[k*Width+Col]; P[Row*Width+Col] = Pvalue; } Matrix Multiplication by Tiling P\n","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch3/","summary":"Personal notebook 3 of Programming Massively Parallel","title":"PMPP Learning-Chapter 3 Multidimensional Grids and Data"},{"content":"2 Heterogeneous Data Parallel Computing æ•°æ®å¹¶è¡Œ (Data Parallel) æ˜¯æŒ‡åœ¨æ•°æ®é›†çš„ä¸åŒéƒ¨åˆ†ä¸Šæ‰§è¡Œçš„è®¡ç®—å·¥ä½œå¯ä»¥å½¼æ­¤ç‹¬ç«‹åœ°å®Œæˆï¼Œä»è€Œå¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„ç°è±¡ã€‚\n2.1 Data Parallel åœ¨å›¾åƒå¤„ç†ä¸­ï¼Œå°†å½©è‰²åƒç´ è½¬æ¢ä¸ºç°åº¦åªéœ€è¦è¯¥åƒç´ çš„æ•°æ®ã€‚æ¨¡ç³Šå›¾åƒå°†æ¯ä¸ªåƒç´ çš„é¢œè‰²ä¸é™„è¿‘åƒç´ çš„é¢œè‰²å¹³å‡ï¼Œåªéœ€è¦åƒç´ çš„å°é‚»åŸŸçš„æ•°æ®ã€‚å³ä½¿æ˜¯ä¸€ä¸ªçœ‹ä¼¼å…¨å±€çš„æ“ä½œï¼Œæ¯”å¦‚æ‰¾åˆ°å›¾åƒä¸­æ‰€æœ‰åƒç´ çš„å¹³å‡äº®åº¦ï¼Œä¹Ÿå¯ä»¥åˆ†è§£æˆè®¸å¤šå¯ä»¥ç‹¬ç«‹æ‰§è¡Œçš„è¾ƒå°çš„è®¡ç®—ã€‚è¿™ç§å¯¹ä¸åŒæ•°æ®å—çš„ç‹¬ç«‹è®¡ç®—æ˜¯æ•°æ®å¹¶è¡Œæ€§çš„åŸºç¡€ã€‚ ä¸ºäº†å°†å½©è‰²å›¾åƒè½¬æ¢ä¸ºç°åº¦å›¾åƒï¼Œæˆ‘ä»¬é€šè¿‡ä»¥ä¸‹åŠ æƒå’Œå…¬å¼è®¡ç®—æ¯ä¸ªåƒç´ çš„äº®åº¦å€¼L. è¿™äº›é€åƒç´ è®¡ç®—éƒ½ä¸ä¾èµ–äºå½¼æ­¤ï¼Œéƒ½å¯ä»¥ç‹¬ç«‹æ‰§è¡Œã€‚æ˜¾ç„¶ï¼Œå½©è‰²å›¾åˆ°ç°åº¦å›¾çš„è½¬æ¢å…·æœ‰å¤§é‡çš„æ•°æ®å¹¶è¡Œæ€§ã€‚ $L=0.21r+0.72g+0.07b$\nTask Parallelism vs. Data Parallelism æ•°æ®å¹¶è¡Œå¹¶ä¸æ˜¯å¹¶è¡Œç¼–ç¨‹ä¸­ä½¿ç”¨çš„å”¯ä¸€ç±»å‹çš„å¹¶è¡Œã€‚ä»»åŠ¡å¹¶è¡Œ (Task Parallelism) åœ¨å¹¶è¡Œç¼–ç¨‹ä¸­ä¹Ÿå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚ä»»åŠ¡å¹¶è¡Œæ€§é€šå¸¸é€šè¿‡åº”ç”¨ç¨‹åºçš„ä»»åŠ¡åˆ†è§£æ¥æš´éœ²ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•çš„åº”ç”¨ç¨‹åºå¯èƒ½éœ€è¦åšä¸€ä¸ªå‘é‡åŠ æ³•å’Œä¸€ä¸ªçŸ©é˜µ-å‘é‡ä¹˜æ³•ã€‚æ¯ä¸ªéƒ½æ˜¯ä¸€ä¸ªä»»åŠ¡ã€‚å¦‚æœä¸¤ä¸ªä»»åŠ¡å¯ä»¥ç‹¬ç«‹å®Œæˆï¼Œåˆ™å­˜åœ¨ä»»åŠ¡å¹¶è¡Œæ€§ã€‚I/Oå’Œæ•°æ®ä¼ è¾“ä¹Ÿæ˜¯å¸¸è§çš„ä»»åŠ¡ã€‚ Data Parallelsim in Image2Grayscale Conversion\n2.2 CUDA C Program Structure CUDA C ç”¨æœ€å°‘çš„æ–°è¯­æ³•å’Œåº“å‡½æ•°æ‰©å±•äº†æµè¡Œçš„ ANSI C è¯­è¨€ã€‚CUDA C ç¨‹åºçš„ç»“æ„åæ˜ äº†è®¡ç®—æœºä¸­ä¸»æœº (CPU) å’Œä¸€ä¸ªæˆ–å¤šä¸ªè®¾å¤‡ (GPU) çš„å…±å­˜ã€‚æ¯ä¸ª CUDA C æºæ–‡ä»¶å¯ä»¥åŒæ—¶åŒ…å«ä¸»æœº (host) ä»£ç å’Œè®¾å¤‡ (device) ä»£ç ã€‚ CUDAç¨‹åºçš„æ‰§è¡Œæµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æ‰§è¡Œä»ä¸»æœºä»£ç  (CPU ä¸²è¡Œä»£ç ) å¼€å§‹ï¼Œå½“è°ƒç”¨å†…æ ¸å‡½æ•° (kernel function) æ—¶ï¼Œä¼šåœ¨è®¾å¤‡ä¸Šå¯åŠ¨å¤§é‡çº¿ç¨‹1æ¥æ‰§è¡Œå†…æ ¸ã€‚ç”±å†…æ ¸è°ƒç”¨å¯åŠ¨çš„æ‰€æœ‰çº¿ç¨‹ç»Ÿç§°ä¸ºç½‘æ ¼ (grid)ã€‚è¿™äº›çº¿ç¨‹æ˜¯ CUDA å¹¶è¡Œæ‰§è¡Œçš„ä¸»è¦è½½ä½“ã€‚\nExecution of a CUDA Program\n2.3 A vector addition kernel ä½¿ç”¨å‘é‡åŠ æ³•æ¥å±•ç¤º CUDA C ç¨‹åºç»“æ„ã€‚ä¸‹é¢å±•ç¤ºäº†ä¸€ä¸ªç®€å•çš„ä¼ ç»Ÿ C ç¨‹åºï¼Œå®ƒç”±ä¸€ä¸ªä¸»å‡½æ•°å’Œä¸€ä¸ªå‘é‡åŠ æ³•å‡½æ•°ç»„æˆã€‚\nå½“éœ€è¦åŒºåˆ†ä¸»æœºå’Œè®¾å¤‡æ•°æ®æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šåœ¨ä¸»æœºä½¿ç”¨çš„å˜é‡ååé¢åŠ ä¸Š â€œ_hâ€ï¼Œè€Œåœ¨è®¾å¤‡ä½¿ç”¨çš„å˜é‡ååé¢åŠ ä¸Š â€œ_dâ€.\n// Compute vector sum h_C = h_A+h_B void vecAdd(float* h_A, float* h_B, float* h_C, int n) { for (int i = 0; i \u0026lt; n; i++) h_C[i] = h_A[i] + h_B[i]; } int main() { // Memory allocation for h_A, h_B, and h_C // I/O to read h_A and h_B, N elements each // â€¦ vecAdd(h_A, h_B, h_C, N); } å¹¶è¡Œæ‰§è¡Œå‘é‡åŠ æ³•çš„ä¸€ç§ç›´æ¥æ–¹æ³•æ˜¯ä¿®æ”¹ vecAdd å‡½æ•°å¹¶å°†å…¶è®¡ç®—ç§»åˆ°è®¾å¤‡ä¸Šã€‚ä¿®æ”¹åçš„ç»“æ„å¦‚ä¸‹æ‰€ç¤ºã€‚\nStructure of the Modified VecAdd\n#include \u0026lt;cuda_runtime.h\u0026gt; // â€¦ void vecAdd(float* A, float* B, float* C, int n) { int size = n* sizeof(float); float *d_A *d_B, *d_C; /* â€¦ 1. // Allocate device memory for A, B, and C // copy A and B to device memory 2. // Kernel launch code â€“ to have the device // to perform the actual vector addition 3. // copy C from the device memory // Free device vectors */ } 2.4 Device Global Memory and Data Transfer åœ¨å½“å‰çš„CUDAç³»ç»Ÿä¸­ï¼Œè®¾å¤‡é€šå¸¸æ˜¯å¸¦æœ‰è‡ªå·±çš„ DRAM çš„ç¡¬ä»¶å¡ï¼Œç§°ä¸º (è®¾å¤‡)å…¨å±€å†…å­˜ (device global memory). å¯¹äºå‘é‡åŠ æ³•å†…æ ¸ï¼Œåœ¨è°ƒç”¨å†…æ ¸ä¹‹å‰ï¼Œç¨‹åºå‘˜éœ€è¦åœ¨è®¾å¤‡å…¨å±€å†…å­˜ä¸­åˆ†é…ç©ºé—´ï¼Œå¹¶å°†æ•°æ®ä»ä¸»æœºå†…å­˜ä¼ è¾“åˆ°è®¾å¤‡å…¨å±€å†…å­˜ä¸­åˆ†é…çš„ç©ºé—´ã€‚è¿™å¯¹åº”äº 1. éƒ¨åˆ†ã€‚ç±»ä¼¼åœ°ï¼Œåœ¨è®¾å¤‡æ‰§è¡Œä¹‹åï¼Œç¨‹åºå‘˜éœ€è¦å°†ç»“æœæ•°æ®ä»è®¾å¤‡å…¨å±€å†…å­˜ä¼ è¾“å›ä¸»æœºå†…å­˜ï¼Œå¹¶é‡Šæ”¾è®¾å¤‡å…¨å±€å†…å­˜ä¸­ä¸å†éœ€è¦çš„å·²åˆ†é…ç©ºé—´ã€‚è¿™å¯¹åº”äº 3. éƒ¨åˆ†ã€‚ cudaMalloc å‡½æ•°å¯ä»¥ä»ä¸»æœºä»£ç ä¸­è°ƒç”¨ï¼Œä¸ºå¯¹è±¡åˆ†é…ä¸€å—è®¾å¤‡å…¨å±€å†…å­˜ã€‚ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æŒ‡é’ˆå˜é‡çš„åœ°å€ï¼Œè¯¥å˜é‡å°†è¢«è®¾ç½®ä¸ºæŒ‡å‘åˆ†é…çš„å¯¹è±¡ã€‚æŒ‡é’ˆå˜é‡çš„åœ°å€åº”å¼ºåˆ¶è½¬æ¢ä¸º void**ï¼Œè¿™æ ·å¯ä»¥å…è®¸ cudaMalloc å‡½æ•°å°†åˆ†é…å†…å­˜çš„åœ°å€å†™å…¥æ‰€æä¾›çš„æŒ‡é’ˆå˜é‡ä¸­ï¼Œè€Œä¸è€ƒè™‘å…¶ç±»å‹2ã€‚\ncudaError_t cudaMalloc(void** devPtr, size_t size); devPtrï¼šæŒ‡å‘æŒ‡å‘è®¾å¤‡å†…å­˜çš„æŒ‡é’ˆçš„æŒ‡é’ˆã€‚ sizeï¼šè¦åˆ†é…çš„å†…å­˜å¤§å°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚ cudaFree å‡½æ•°é€šè¿‡é‡Šæ”¾è®¾å¤‡å†…å­˜å¹¶å°†å…¶è¿”å›åˆ°å¯ç”¨å†…å­˜æ± æ¥ç®¡ç†è®¾å¤‡å†…å­˜èµ„æºã€‚å®ƒåªéœ€è¦ A_d çš„å€¼æ¥è¯†åˆ«è¦é‡Šæ”¾çš„å†…å­˜åŒºåŸŸï¼Œè€Œä¸éœ€è¦æ”¹å˜ A_d æŒ‡é’ˆæœ¬èº«çš„åœ°å€ã€‚\nåœ¨ä¸»æœºä»£ç ä¸­å¯¹è®¾å¤‡å…¨å±€å†…å­˜æŒ‡é’ˆè¿›è¡Œè§£å¼•ç”¨å¼•ç”¨å¯èƒ½å¯¼è‡´å¼‚å¸¸æˆ–å…¶ä»–ç±»å‹çš„è¿è¡Œé”™è¯¯ã€‚\ncudaMemcpy å‡½æ•°æ˜¯ CUDA ä¸­ç”¨äºåœ¨ä¸»æœºå†…å­˜å’Œè®¾å¤‡å†…å­˜ä¹‹é—´ä¼ è¾“æ•°æ®çš„æ ¸å¿ƒå‡½æ•°ã€‚å®ƒå…è®¸å°†æ•°æ®ä»ä¸»æœºå†…å­˜å¤åˆ¶åˆ°è®¾å¤‡å†…å­˜ï¼Œæˆ–ä»è®¾å¤‡å†…å­˜å¤åˆ¶åˆ°ä¸»æœºå†…å­˜ã€‚\ncudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind); dstï¼šç›®æ ‡å†…å­˜åœ°å€ï¼Œå¯ä»¥æ˜¯ä¸»æœºå†…å­˜åœ°å€æˆ–è®¾å¤‡å†…å­˜åœ°å€ã€‚ srcï¼š æºå†…å­˜åœ°å€ï¼Œå¯ä»¥æ˜¯ä¸»æœºå†…å­˜åœ°å€æˆ–è®¾å¤‡å†…å­˜åœ°å€ã€‚ countï¼š è¦å¤åˆ¶çš„æ•°æ®å¤§å°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚ kindï¼š å¤åˆ¶æ–¹å‘ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æšä¸¾å€¼ï¼š cudaMemcpyHostToDeviceï¼šä¸»æœºå†…å­˜-\u0026gt;è®¾å¤‡å†…å­˜ã€‚ cudaMemcpyDeviceToHostï¼šè®¾å¤‡å†…å­˜-\u0026gt;ä¸»æœºå†…å­˜ã€‚ cudaMemcpyDeviceToDeviceï¼šè®¾å¤‡å†…å­˜-\u0026gt;è®¾å¤‡å†…å­˜ã€‚ cudaMemcpyHostToHostï¼šä¸»æœºå†…å­˜-\u0026gt;ä¸»æœºå†…å­˜ äº†è§£å®Œè¿™äº›åï¼Œå¯ä»¥æ›´æ–°ä»£ç çš„æ¡†æ¶å¦‚ä¸‹\nChecking and Handling in CUDA CUDA API å‡½æ•°è¿”å›ä¸€ä¸ª cudaError_t ç±»å‹çš„æ ‡å¿—ï¼ŒæŒ‡ç¤ºå½“å®ƒä»¬å¤„ç†è¯·æ±‚æ—¶æ˜¯å¦å‘ç”Ÿé”™è¯¯ã€‚ åœ¨ CUDA è¿è¡Œæ—¶åº“çš„å¤´æ–‡ä»¶ cuda_runtime.h ä¸­ï¼ŒcudaError_t è¢«å®šä¹‰ä¸ºä¸€ä¸ª int ç±»å‹çš„åˆ«å\ntypedef int cudaError_t; ä¸€ä¸ªä¾‹å­å¦‚ä¸‹\n// ... float *d_a; cudaError_t err = cudaMalloc(\u0026amp;d_a, 1024 * sizeof(float)); if (err != cudaSuccess) { printf(\u0026#34;cudaMalloc failed: %s\\n\u0026#34;, cudaGetErrorString(err)); return 1; } void vecAdd(float* A, float* B, float* C, int n) { int size = n* sizeof(float); float *d_A *d_B, *d_C; cudaMalloc((void **) %d_A, size); cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice); cudaMalloc((void **) %d_B, size); cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice); cudaMalloc((void **) %d_C, size); // Kernel invocation code - to be shown later // ... cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost); // Free device memory for A, B, C cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); } 2.5 Kernel functions and threading å†…æ ¸å‡½æ•°æŒ‡æ‰€æœ‰çº¿ç¨‹åœ¨å¹¶è¡Œé˜¶æ®µæ‰§è¡Œçš„ä»£ç ï¼Œç½‘æ ¼ä¸­çš„æ‰€æœ‰çº¿ç¨‹æ‰§è¡Œç›¸åŒçš„å†…æ ¸ä»£ç ã€‚ã€‚å½“ç¨‹åºçš„ä¸»æœºä»£ç è°ƒç”¨å†…æ ¸æ—¶ï¼ŒCUDA runtime ç³»ç»Ÿå¯åŠ¨ä¸€ä¸ªçº¿ç¨‹ç½‘æ ¼ï¼Œè¿™äº›çº¿ç¨‹è¢«ç»„ç»‡æˆä¸€ä¸ªä¸¤çº§å±‚æ¬¡ç»“æ„ã€‚æ¯ä¸ªç½‘æ ¼éƒ½è¢«ç»„ç»‡ä¸ºçº¿ç¨‹å— (thread block, ç®€ç§°ä¸ºå—) æ•°ç»„ã€‚ç½‘æ ¼çš„æ‰€æœ‰å—éƒ½æ˜¯ç›¸åŒçš„å¤§å°ã€‚åœ¨è°ƒç”¨å†…æ ¸æ—¶ï¼Œæ¯ä¸ªçº¿ç¨‹å—ä¸­çš„çº¿ç¨‹æ€»æ•°ç”±ä¸»æœºä»£ç æŒ‡å®šã€‚ åŒä¸€ä¸ªå†…æ ¸å¯ä»¥åœ¨ä¸»æœºä»£ç çš„ä¸åŒéƒ¨åˆ†ç”¨ä¸åŒæ•°é‡çš„çº¿ç¨‹è°ƒç”¨ã€‚å¯¹äºç»™å®šçš„çº¿ç¨‹ç½‘æ ¼ï¼Œä¸€ä¸ªå—ä¸­çš„çº¿ç¨‹æ•°å¯ä»¥åœ¨åä¸º blockDim çš„å†…ç½®å˜é‡ä¸­è·å¾—ï¼Œå®ƒæ˜¯ä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªæ— ç¬¦å·æ•´æ•°å­—æ®µ (x, y, z) çš„ç»“æ„ä½“ã€‚ ä¸‹å›¾ç»™å‡ºäº†ä¸€ä¸ªç¤ºä¾‹ï¼Œå…¶ä¸­æ¯ä¸ªå—ç”±256ä¸ªçº¿ç¨‹ç»„æˆã€‚æ¯ä¸ªçº¿ç¨‹éƒ½ç”¨ä¸€ä¸ªç®­å¤´è¡¨ç¤ºï¼Œæ ‡æœ‰çº¿ç¨‹åœ¨å—ä¸­çš„ç´¢å¼•å·çš„æ–¹æ¡†ã€‚ç”±äºæ•°æ®æ˜¯ä¸€ç»´å‘é‡ï¼Œå› æ­¤æ¯ä¸ªçº¿ç¨‹å—è¢«ç»„ç»‡ä¸ºä¸€ç»´çº¿ç¨‹æ•°ç»„ã€‚blockDim.x çš„å€¼è¡¨ç¤ºæ¯ä¸ªå—ä¸­çš„çº¿ç¨‹æ€»æ•°ã€‚threadaIdx å˜é‡è¡¨ç¤ºæ¯ä¸ªçº¿ç¨‹åœ¨å—ä¸­çš„åæ ‡ã€‚å…¨å±€ç´¢å¼• i çš„è®¡ç®—å…¬å¼ä¸º i = blockIdx.x * blockDim.x + threadIdx.x\nè®¸å¤šç¼–ç¨‹è¯­è¨€éƒ½æœ‰å†…ç½®å˜é‡ã€‚è¿™äº›å˜é‡å…·æœ‰ç‰¹æ®Šçš„å«ä¹‰å’Œç›®çš„ã€‚è¿™äº›å˜é‡çš„å€¼é€šå¸¸ç”±è¿è¡Œæ—¶ç³»ç»Ÿé¢„å…ˆåˆå§‹åŒ–ï¼Œå¹¶ä¸”åœ¨ç¨‹åºä¸­é€šå¸¸æ˜¯åªè¯»çš„ã€‚\nHierarchical Organization in CUDA\nå‘é‡åŠ æ³•çš„æ ¸å‡½æ•°å®šä¹‰å¦‚ä¸‹ã€‚ç½‘æ ¼ä¸­çš„æ¯ä¸ªçº¿ç¨‹å¯¹åº”äºåŸå§‹å¾ªç¯çš„ä¸€æ¬¡è¿­ä»£ï¼Œè¿™è¢«ç§°ä¸ºå¾ªç¯å¹¶è¡Œ (loop parallel)ï¼Œæ„ä¸ºåŸå§‹é¡ºåºä»£ç çš„è¿­ä»£ç”±çº¿ç¨‹å¹¶è¡Œæ‰§è¡Œã€‚addVecKernel ä¸­æœ‰ä¸€ä¸ª if (i \u0026lt; n) è¯­å¥ï¼Œå› ä¸ºå¹¶éæ‰€æœ‰çš„å‘é‡é•¿åº¦éƒ½å¯ä»¥è¡¨ç¤ºä¸ºå—å¤§å°çš„å€æ•°ã€‚\n__global__ void vecAddKernel(float* A, float* B, float* C, int n) { int i = blockDim.x * blockIdx.x + threadIdx.x; if(i \u0026lt; n) C[i] = A[i] + B[i]; } CUDA C ä½¿ç”¨äº†ä¸‰ä¸ªå¯ä»¥åœ¨å‡½æ•°å£°æ˜ä¸­ä½¿ç”¨çš„é™å®šå­—ã€‚ä¸‹è¡¨å±•ç¤ºäº†è¿™äº›å…³é”®è¯çš„æ„ä¹‰ã€‚\n__host__ å°±æ˜¯åœ¨ä¸»æœºä¸Šæ‰§è¡Œçš„ä¼ ç»Ÿ C å‡½æ•°ï¼Œåªèƒ½ä»å¦ä¸€ä¸ªä¸»æœºå‡½æ•°è°ƒç”¨ã€‚ __global__ è¡¨ç¤ºè¢«å£°æ˜çš„å‡½æ•°æ˜¯ CUDA C å†…æ ¸å‡½æ•°ã€‚å†…æ ¸å‡½æ•°åœ¨è®¾å¤‡ä¸Šæ‰§è¡Œï¼Œå¹¶ä¸”å¯ä»¥ä»ä¸»æœºä¸Šè°ƒç”¨ã€‚ __device__ å‡½æ•°åœ¨ CUDA è®¾å¤‡ä¸Šæ‰§è¡Œï¼Œåªèƒ½ä»å†…æ ¸å‡½æ•°æˆ–å…¶ä»–è®¾å¤‡å‡½æ•°è°ƒç”¨ã€‚ å¯ä»¥åœ¨å‡½æ•°å£°æ˜ä¸­åŒæ—¶ä½¿ç”¨ __host__ å’Œ __device__. ç¼–è¯‘ç³»ç»Ÿä¼šä¸ºåŒä¸€ä¸ªå‡½æ•°ç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬çš„ç›®æ ‡ä»£ç ã€‚\nQualifier Keyword Callable From Executed on Executed by __host__ (default) Host Host Caller host thread __global__ Host/Device Device New grid of device thread __device__ Device Device Caller device thread 2.6 Calling kernel functions å®ç°å†…æ ¸å‡½æ•°ä¹‹åï¼Œå‰©ä¸‹çš„æ­¥éª¤æ˜¯ä»ä¸»æœºä»£ç è°ƒç”¨è¯¥å‡½æ•°æ¥å¯åŠ¨ç½‘æ ¼ã€‚å½“ä¸»æœºä»£ç è°ƒç”¨å†…æ ¸æ—¶ï¼Œå®ƒé€šè¿‡æ‰§è¡Œé…ç½®å‚æ•° (execution configuration parameters) è®¾ç½®ç½‘æ ¼å’Œçº¿ç¨‹å—å¤§å°é…ç½®å‚æ•°åœ¨åœ¨ä¼ ç»Ÿçš„Cå‡½æ•°å‚æ•°ä¹‹å‰ç”± \u0026lt;\u0026lt;\u0026lt;...\u0026gt;\u0026gt;\u0026gt; ä¹‹é—´ç»™å‡ºã€‚ç¬¬ä¸€ä¸ªé…ç½®å‚æ•°ç»™å‡ºç½‘æ ¼ä¸­çš„å—æ•°é‡ã€‚ç¬¬äºŒä¸ªå‚æ•°æŒ‡å®šæ¯ä¸ªå—ä¸­çš„çº¿ç¨‹æ•°ã€‚\nint vectAdd(float* A, float* B, float* C, int n) { // d_A, d_B, d_C allocations and copies omitted // ... // Run ceil(n/256) (or by (n + 256 - 1) / 256) blocks of 256 threads each vecAddKernel\u0026lt;\u0026lt;\u0026lt;ceil(n/256.0), 256\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, n); } ä¸‹é¢å±•ç¤ºäº† vecAdd å‡½æ•°ä¸­çš„æœ€ç»ˆä¸»æœºä»£ç ã€‚æ‰€æœ‰çš„çº¿ç¨‹å—æ“ä½œå‘é‡çš„ä¸åŒéƒ¨åˆ†ã€‚å®ƒä»¬å¯ä»¥æŒ‰ä»»æ„é¡ºåºæ‰§è¡Œã€‚\nå®é™…ä¸Šï¼Œåˆ†é…è®¾å¤‡å†…å­˜ã€ä»ä¸»æœºåˆ°è®¾å¤‡çš„è¾“å…¥æ•°æ®ä¼ è¾“ã€ä»è®¾å¤‡åˆ°ä¸»æœºçš„è¾“å‡ºæ•°æ®ä¼ è¾“ä»¥åŠé‡Šæ”¾è®¾å¤‡å†…å­˜çš„å¼€é”€å¯èƒ½ä¼šä½¿ç”Ÿæˆçš„ä»£ç æ¯”åŸå§‹é¡ºåºä»£ç æ…¢ï¼Œè¿™æ˜¯å› ä¸ºå†…æ ¸å®Œæˆçš„è®¡ç®—é‡ç›¸å¯¹äºå¤„ç†æˆ–ä¼ è¾“çš„æ•°æ®é‡æ¥è¯´å¾ˆå°ã€‚\nvoid vecAdd(float* A, float* B, float* C, int n) { int size = n * sizeof(float); float *d_A *d_B, *d_C; cudaMalloc(\u0026amp;d_A, size); cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice); cudaMalloc(\u0026amp;d_B, size); cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice); cudaMalloc(\u0026amp;d_C, size); vecAddKernel\u0026lt;\u0026lt;\u0026lt;ceil(n/256.0), 256\u0026gt;\u0026gt;\u0026gt;(d_A, d_B, d_C, n); cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost); // Free device memory for A, B, C cudaFree(d_A); cudaFree(d_B); cudaFree(d_C); } 2.7 Compilation NVCC (NVIDIA C Compiler) å¤„ç†ä¸€ä¸ªCå¤„ç†ä¸€ä¸ªCUDA Cç¨‹åºï¼Œä½¿ç”¨ CUDA å…³é”®å­—æ¥åˆ†ç¦»ä¸»æœºä»£ç å’Œè®¾å¤‡ä»£ç ã€‚\nä¸»æœºä»£ç æ˜¯å°±æ˜¯æ™®é€šçš„ANSI Cä»£ç ï¼Œä½¿ç”¨ C/C++ ç¼–è¯‘å™¨è¿›è¡Œç¼–è¯‘ï¼Œå¹¶ä½œä¸ºä¼ ç»Ÿçš„ CPU è¿›ç¨‹è¿è¡Œã€‚ è®¾å¤‡ä»£ç åŠå…¶ç›¸å…³è¾…åŠ©å‡½æ•°å’Œæ•°æ®ç»“æ„çš„CUDAå…³é”®å­—ï¼Œç”±NVCCç¼–è¯‘æˆç§°ä¸º PTX (Parallel Thread Execution) æ–‡ä»¶çš„è™šæ‹ŸäºŒè¿›åˆ¶æ–‡ä»¶, ç”± NVCC runtime ç»„ä»¶è¿›ä¸€æ­¥ç¼–è¯‘æˆç›®æ ‡æ–‡ä»¶ï¼Œå¹¶åœ¨æ”¯æŒ cuda çš„ GPU è®¾å¤‡ä¸Šæ‰§è¡Œã€‚ Overview of the Compilation Process of a CUDA C Program\nçº¿ç¨‹ç”±ç¨‹åºçš„ä»£ç ã€æ­£åœ¨æ‰§è¡Œçš„ä»£ç ä¸­çš„ä½ç½®ä»¥åŠå®ƒçš„å˜é‡å’Œæ•°æ®ç»“æ„çš„å€¼ç»„æˆã€‚\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ncudaMalloc ä¸ C è¯­è¨€ malloc å‡½æ•°çš„æ ¼å¼ä¸åŒã€‚å‰è€…æ¥å—ä¸¤ä¸ªå‚æ•°ï¼ŒæŒ‡é’ˆå˜é‡å…¶åœ°å€ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ç»™å‡ºã€‚åè€…åªæ¥å—ä¸€ä¸ªå‚æ•°æ¥æŒ‡å®šåˆ†é…å¯¹è±¡çš„å¤§å°ï¼Œè¿”å›ä¸€ä¸ªæŒ‡å‘åˆ†é…å¯¹è±¡çš„æŒ‡é’ˆã€‚\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch2/","summary":"Personal notebook 2 of Programming Massively Parallel","title":"PMPP Learning-Chapter 2 Heterogeneous Data Parallel"},{"content":"1 Introduction åŸºäºå•ä¸ªä¸­å¤®å¤„ç†å™¨ (Central Processor Unit, CPU) çš„å¾®å¤„ç†å™¨å¤–éƒ¨çœ‹èµ·æ¥æ˜¯æŒ‰é¡ºåºæ‰§è¡ŒæŒ‡ä»¤ï¼Œä¾‹å¦‚è‹±ç‰¹å°”å’Œ AMD çš„ x86 å¤„ç†å™¨ï¼Œéšç€æ—¶é’Ÿé¢‘ç‡å’Œç¡¬ä»¶èµ„æºçš„å¿«é€Ÿå¢é•¿ï¼Œåœ¨20ä¸–çºª80å¹´ä»£å’Œ90å¹´ä»£æ¨åŠ¨äº†è®¡ç®—æœºåº”ç”¨ç¨‹åºçš„æ€§èƒ½å¿«é€Ÿæé«˜å’Œæˆæœ¬é™ä½ã€‚å¯ä»¥ç»™æ¡Œé¢åº”ç”¨æä¾› GFLOPS çº§åˆ«çš„æµ®ç‚¹è¿ç®—ï¼Œç»™æ•°æ®ä¸­å¿ƒæä¾› TFLOPS çº§åˆ«çš„æµ®ç‚¹è¿ç®—ã€‚ç„¶è€Œï¼Œç”±äºèƒ½æºæ¶ˆè€—å’Œæ•£çƒ­é—®é¢˜ï¼Œè¿™ç§è¶‹åŠ¿ä»2003å¹´å¼€å§‹æ”¾ç¼“ã€‚è¿™äº›é—®é¢˜é™åˆ¶äº†æ—¶é’Ÿé¢‘ç‡çš„å¢åŠ å’Œä¿æŒæŒ‰é¡ºåºæ­¥éª¤æ‰§è¡ŒæŒ‡ä»¤çš„åŒæ—¶åœ¨å•ä¸ª CPU ä¸Šæ¯ä¸ªæ—¶é’Ÿå‘¨æœŸå†…å¯ä»¥æ‰§è¡Œçš„è¡ŒåŠ¨ã€‚ ä¹‹åå‡ ä¹æ‰€æœ‰çš„å¾®å¤„ç†å™¨ä¾›åº”å•†éƒ½è½¬å‘äº†åœ¨æ¯ä¸ªèŠ¯ç‰‡ä¸Šä½¿ç”¨å¤šä¸ªç‰©ç† CPU (ç§°ä¸ºå¤„ç†å™¨æ ¸å¿ƒ) æ¥æé«˜å¤„ç†èƒ½åŠ›ã€‚åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œä¼ ç»Ÿçš„CPUå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªå•æ ¸CPUã€‚è¿™æ ·å°±è¦æ±‚å¿…é¡»æœ‰å¤šä¸ªæŒ‡ä»¤åºåˆ—å¹¶ä¸”å¯ä»¥åŒæ—¶åœ¨è¿™äº›å¤„ç†å™¨æ ¸å¿ƒä¸Šæ‰§è¡Œ (æ— è®ºæ˜¯æ¥è‡ªç›¸åŒçš„åº”ç”¨ç¨‹åºè¿˜æ˜¯æ¥è‡ªä¸åŒçš„åº”ç”¨ç¨‹åº)ã€‚ä¸ºäº†ä½¿ä¸€ä¸ªç‰¹å®šçš„åº”ç”¨ç¨‹åºå—ç›Šäºå¤šä¸ªå¤„ç†å™¨æ ¸å¿ƒï¼Œå®ƒçš„å·¥ä½œå¿…é¡»åˆ†æˆå¤šä¸ªæŒ‡ä»¤åºåˆ—ï¼Œè¿™äº›æŒ‡ä»¤åºåˆ—å¯ä»¥åŒæ—¶åœ¨è¿™äº›å¤„ç†å™¨æ ¸å¿ƒä¸Šæ‰§è¡Œã€‚è¿™ç§ä»å•ä¸ªCPUæŒ‰é¡ºåºæ‰§è¡ŒæŒ‡ä»¤åˆ°å¤šä¸ªå†…æ ¸å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŒ‡ä»¤åºåˆ—çš„è½¬å˜é€ å°±äº†å¹¶è¡Œè®¡ç®—çš„éœ€æ±‚ã€‚\n1.1 Heterogeneous parallel computing åŠå¯¼ä½“è¡Œä¸šç¡®å®šäº†è®¾è®¡å¾®å¤„ç†å™¨çš„ä¸¤æ¡ä¸»è¦è·¯çº¿\nMulticore Trajectory: å¯»æ±‚åœ¨è½¬å˜åˆ°å¤šä¸ªæ ¸æ—¶ä¿æŒé¡ºåºç¨‹åºçš„æ‰§è¡Œé€Ÿåº¦ã€‚ Many-thread Trajectory: æ›´å¤šåœ°å…³æ³¨å¹¶è¡Œåº”ç”¨ç¨‹åºçš„æ‰§è¡Œååé‡ã€‚ è‡ª2003å¹´ä»¥æ¥ï¼Œå¤šçº¿ç¨‹å¤„ç†å™¨å°¤å…¶æ˜¯ GPUï¼Œä¸€ç›´åœ¨æµ®ç‚¹è®¡ç®—æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚å¤šæ ¸å’Œå¤šçº¿ç¨‹ä¹‹é—´åœ¨å³°å€¼æ€§èƒ½ä¸Šçš„å¦‚æ­¤å¤§çš„å·®è·ä¿ƒä½¿è®¸å¤šåº”ç”¨ç¨‹åºå¼€å‘äººå‘˜å°†å…¶è½¯ä»¶çš„è®¡ç®—å¯†é›†å‹éƒ¨åˆ†è½¬ç§»åˆ°gpuä¸Šæ‰§è¡Œã€‚\n64-bit double-precision 32-bit single-precision Tesla A100 GPU 9.7 TFLOPS 156 TFLOPS Intel 24-core Processor 0.33 TLOPS 0.66 TLOPS CPU çš„è®¾è®¡ä¸ºé¢å‘å»¶è¿Ÿçš„ (latency-oriented) è®¾è®¡ã€‚é’ˆå¯¹é¡ºåºä»£ç æ€§èƒ½è¿›è¡Œäº†ä¼˜åŒ–ã€‚è®¡ç®—å•å…ƒå’Œæ“ä½œæ•°ä¼ è¾“é€»è¾‘çš„è®¾è®¡æ˜¯ä¸ºäº†æœ€å°åŒ–è®¡ç®—çš„æœ‰æ•ˆå»¶è¿Ÿï¼Œä»£ä»·æ˜¯å¢åŠ èŠ¯ç‰‡é¢ç§¯å’Œå•ä½åŠŸç‡çš„ä½¿ç”¨ã€‚é‡‡ç”¨å¤æ‚çš„åˆ†æ”¯é¢„æµ‹é€»è¾‘å’Œæ‰§è¡Œæ§åˆ¶é€»è¾‘æ¥å‡å°‘æ¡ä»¶åˆ†æ”¯æŒ‡ä»¤çš„å»¶è¿Ÿä½¿å¾—æ¯ä¸ªçº¿ç¨‹çš„æ‰§è¡Œå»¶è¿Ÿé™ä½ã€‚ç„¶è€Œï¼Œä½å»¶è¿Ÿè®¡ç®—å•å…ƒã€å¤æ‚çš„æ“ä½œæ•°ä¼ é€’é€»è¾‘ã€å¤§ç¼“å­˜å­˜å‚¨å™¨å’Œæ§åˆ¶é€»è¾‘æ¶ˆè€—äº†èŠ¯ç‰‡é¢ç§¯å’ŒåŠŸç‡ï¼Œå¦åˆ™å¯ä»¥ç”¨æ¥æä¾›æ›´å¤šçš„ç®—æœ¯æ‰§è¡Œå•å…ƒå’Œå†…å­˜è®¿é—®é€šé“ã€‚ GPU çš„è®¾è®¡æ˜¯é¢å‘ååé‡ (throught-put oriented)çš„è®¾è®¡ã€‚å¯»æ±‚åœ¨æœ‰é™çš„èŠ¯ç‰‡é¢ç§¯å’ŒåŠŸè€—é¢„ç®—ä¸‹æœ€å¤§åŒ–æµ®ç‚¹è®¡ç®—å’Œå†…å­˜è®¿é—®ååé‡ã€‚è®¸å¤šå›¾å½¢åº”ç”¨ç¨‹åºçš„é€Ÿåº¦å—åˆ°æ•°æ®ä»å†…å­˜ç³»ç»Ÿä¼ è¾“åˆ°å¤„ç†å™¨çš„é€Ÿç‡çš„é™åˆ¶ï¼Œå¿…é¡»èƒ½å¤Ÿå°†å¤§é‡æ•°æ®åŠ è½½å’Œå­˜å‚¨åˆ° DRAM ä¸­çš„å›¾å½¢å¸§ç¼“å†²åŒºã€‚ æ¸¸æˆåº”ç”¨ç¨‹åºæ™®éæ¥å—çš„å®½æ¾å†…å­˜æ¨¡å‹(å„ç§ç³»ç»Ÿè½¯ä»¶ï¼Œåº”ç”¨ç¨‹åºå’ŒI/Oè®¾å¤‡æœŸæœ›å…¶å†…å­˜è®¿é—®å·¥ä½œçš„æ–¹å¼)ä¹Ÿä½¿ GPU æ›´å®¹æ˜“æ”¯æŒè®¿é—®å†…å­˜çš„å¤§è§„æ¨¡å¹¶è¡Œæ€§ã€‚é€šç”¨å¤„ç†å™¨å¿…é¡»æ»¡è¶³é—ç•™æ“ä½œç³»ç»Ÿã€åº”ç”¨ç¨‹åºå’ŒI/Oè®¾å¤‡çš„è¦æ±‚ï¼Œè¿™äº›è¦æ±‚å¯¹æ”¯æŒå¹¶è¡Œå†…å­˜è®¿é—®æå‡ºäº†æ›´å¤šæŒ‘æˆ˜ï¼Œä»è€Œä½¿æé«˜å†…å­˜è®¿é—®çš„ååé‡ (é€šå¸¸ç§°ä¸ºå†…å­˜å¸¦å®½ memory bandwidth) å˜å¾—æ›´åŠ å›°éš¾ã€‚ å°±åŠŸè€—å’ŒèŠ¯ç‰‡é¢ç§¯è€Œè¨€ï¼Œå‡å°‘å»¶è¿Ÿæ¯”å¢åŠ ååé‡è¦æ˜‚è´µå¾—å¤š1ã€‚å› æ­¤ï¼ŒGPU çš„ä¸»æµè§£å†³æ–¹æ¡ˆæ˜¯é’ˆå¯¹å¤§é‡çº¿ç¨‹çš„æ‰§è¡Œååé‡è¿›è¡Œä¼˜åŒ–ï¼Œè€Œä¸æ˜¯å‡å°‘å•ä¸ªçº¿ç¨‹çš„å»¶è¿Ÿã€‚è¿™ç§è®¾è®¡æ–¹æ³•å…è®¸åˆ†çº§å­˜å‚¨å±‚æ¬¡å’Œè®¡ç®—å…·æœ‰è¾ƒé•¿çš„å»¶è¿Ÿï¼Œä»è€ŒèŠ‚çœäº†èŠ¯ç‰‡é¢ç§¯å’ŒåŠŸè€—ã€‚\nCPU and GPU Design Philosophies\n1.2 Why More Speed or Parallelism åŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„æ·±åº¦å­¦ä¹ æ˜¯é€šè¿‡å¤§å¹…æé«˜è®¡ç®—ååé‡è€Œå®ç°çš„æ–°åº”ç”¨ã€‚è™½ç„¶è‡ª 20 ä¸–çºª 70 å¹´ä»£ä»¥æ¥ï¼Œç¥ç»ç½‘ç»œå¾—åˆ°äº†ç§¯æçš„å…³æ³¨ï¼Œä½†ç”±äºéœ€è¦å¤ªå¤šçš„æ ‡è®°æ•°æ®å’Œå¤ªå¤šçš„è®¡ç®—æ¥è®­ç»ƒè¿™äº›ç½‘ç»œï¼Œå®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­ä¸€ç›´æ•ˆæœä¸ä½³ã€‚äº’è”ç½‘çš„å…´èµ·æä¾›äº†å¤§é‡æœ‰æ ‡ç­¾çš„å›¾ç‰‡ï¼Œè€Œ GPU çš„å…´èµ·åˆ™å¸¦æ¥äº†è®¡ç®—ååé‡çš„æ¿€å¢ã€‚å› æ­¤ï¼Œè‡ª2012å¹´ä»¥æ¥ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„åº”ç”¨åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å¾—åˆ°äº†å¿«é€Ÿçš„é‡‡ç”¨ã€‚è¿™ç§é‡‡ç”¨å½»åº•æ”¹å˜äº†è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ï¼Œå¹¶å¼•å‘äº†è‡ªåŠ¨é©¾é©¶æ±½è½¦å’Œå®¶åº­è¾…åŠ©è®¾å¤‡çš„å¿«é€Ÿå‘å±•ã€‚\n1.3 Speeding up real applications å¹¶è¡Œè®¡ç®—ç³»ç»Ÿç›¸å¯¹äºä¸²è¡Œè®¡ç®—ç³»ç»Ÿæ‰€èƒ½å®ç°çš„åŠ é€Ÿçš„ä¸€ä¸ªé‡è¦å› ç´ æ˜¯å¯ä»¥å¹¶è¡ŒåŒ–çš„åº”ç”¨ç¨‹åºéƒ¨åˆ†ï¼Œå¦ä¸€ä¸ªé‡è¦å› ç´ æ˜¯ä»å†…å­˜è®¿é—®æ•°æ®å’Œå‘å†…å­˜å†™å…¥æ•°æ®çš„é€Ÿåº¦æœ‰å¤šå¿«ã€‚ä¸‹å›¾å±•ç¤ºäº†é¡ºåºå’Œå¹¶è¡Œåº”ç”¨ç¨‹åºéƒ¨åˆ†çš„è¦†ç›–ç‡ã€‚é¡ºåºéƒ¨åˆ†å’Œä¼ ç»Ÿçš„(å•æ ¸)CPUè¦†ç›–éƒ¨åˆ†ç›¸äº’é‡å ã€‚ä»¥å‰çš„GPGPUæŠ€æœ¯å¯¹æ•°æ®å¹¶è¡Œéƒ¨åˆ†çš„è¦†ç›–éå¸¸æœ‰é™ï¼Œå› ä¸ºå®ƒä»…é™äºå¯ä»¥è¡¨ç¤ºä¸ºç»˜åˆ¶åƒç´ çš„è®¡ç®—ã€‚éšœç¢æ˜¯æŒ‡éš¾ä»¥æ‰©å±•å•æ ¸cpuä»¥è¦†ç›–æ›´å¤šæ•°æ®å¹¶è¡Œéƒ¨åˆ†çš„åŠŸç‡é™åˆ¶ã€‚\nCoverage of Application Portions\n1.4 Challenges in parallel programming è®¾è®¡å…·æœ‰ä¸é¡ºåºç®—æ³•ç›¸åŒçš„ç®—æ³•(è®¡ç®—)å¤æ‚åº¦çš„å¹¶è¡Œç®—æ³•å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ è®¸å¤šåº”ç”¨ç¨‹åºçš„æ‰§è¡Œé€Ÿåº¦å—åˆ°å†…å­˜è®¿é—®å»¶è¿Ÿå’Œ/æˆ–ååé‡çš„é™åˆ¶ã€‚ ä¸é¡ºåºç¨‹åºç›¸æ¯”ï¼Œå¹¶è¡Œç¨‹åºçš„æ‰§è¡Œé€Ÿåº¦é€šå¸¸å¯¹è¾“å…¥æ•°æ®ç‰¹å¾æ›´ä¸ºæ•æ„Ÿã€‚ æœ‰äº›åº”ç”¨ç¨‹åºå¯ä»¥å¹¶è¡ŒåŒ–ï¼Œè€Œä¸éœ€è¦è·¨ä¸åŒçº¿ç¨‹çš„åä½œ (embarrassingly parallel)ã€‚å…¶ä»–åº”ç”¨ç¨‹åºéœ€è¦ä½¿ç”¨åŒæ­¥æ“ä½œ (synchronization operations) ä½¿å¾—çº¿ç¨‹èƒ½ç›¸äº’åä½œã€‚ ä¾‹å¦‚ï¼Œå¯ä»¥é€šè¿‡å°†è®¡ç®—å•å…ƒçš„æ•°é‡ç¿»å€æ¥ä½¿ååé‡ç¿»å€ï¼Œä½†ä»£ä»·æ˜¯èŠ¯ç‰‡é¢ç§¯å’ŒåŠŸè€—ç¿»å€ã€‚ç„¶è€Œï¼Œå°†ç®—æœ¯å»¶è¿Ÿå‡å°‘ä¸€åŠå¯èƒ½éœ€è¦ç”µæµç¿»å€ï¼Œä»£ä»·æ˜¯ä½¿ç”¨çš„èŠ¯ç‰‡é¢ç§¯å¢åŠ ä¸€å€ä»¥ä¸Šï¼ŒåŠŸè€—å˜ä¸ºå››å€ã€‚\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/courselearning/pmpp/pmpp-ch1/","summary":"Personal notebook 1 of Programming Massively Parallel","title":"PMPP Learning-Chapter 1 Introduction"},{"content":"Pattern Match and Rewriting ä¸‹é¢ä»£ç ä¸­ MyModule åŒ…å«ä¸€ä¸ªå¸¦æœ‰ä¸¤ä¸ªé«˜çº§ç®—å­ relax.opmultiply å’Œ relax.op.add çš„ relax å‡½æ•°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°è¿™ä¸¤ä¸ªç®—å­ï¼Œå¹¶å°†å…¶æ›¿æ¢ä¸ºå¯¹ relax.ewise_fma ç®—å­çš„è°ƒç”¨ã€‚\n@tvm.script.ir_module class MyModule: @R.function def main(x: R.Tensor((3, 4), \u0026#34;float32\u0026#34;), y: R.Tensor((3, 4), \u0026#34;float32\u0026#34;)): # type: ignore with R.dataflow(): cls = MyModule lv0 = relax.op.multiply(x, y) gv0 = relax.op.add(lv0, y) R.output(gv0) return gv0 æ¯ä¸ª IRModule éƒ½åŒ…å«ä¸€ç»„å‡½æ•°ï¼Œå‡½æ•°ä½“ç”±ä¸€ç»„ç§°ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰çš„æ•°æ®ç»“æ„ç»„æˆã€‚ {% fold info @Abstract Syntax Tree %} æŠ½è±¡è¯­æ³•æ ‘ï¼ˆAbstract Syntax Treeï¼ŒASTï¼‰æ˜¯ä¸€ç§å¹¿æ³›ç”¨äºç¼–ç¨‹è¯­è¨€å¤„ç†çš„æ ‘çŠ¶æ•°æ®ç»“æ„ã€‚å®ƒæ˜¯ä¸€ç§å¯¹æºä»£ç è¯­æ³•ç»“æ„çš„æŠ½è±¡è¡¨ç¤ºï¼Œå»æ‰äº†ç¼–ç¨‹è¯­è¨€çš„å…·ä½“è¯­æ³•ç»†èŠ‚ï¼Œä½†ä¿ç•™äº†ä»£ç çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚ AST æ˜¯ä¸€æ£µæ ‘çŠ¶ç»“æ„ï¼Œå…¶èŠ‚ç‚¹è¡¨ç¤ºæºä»£ç ä¸­çš„è¯­æ³•ç»“æ„ã€‚ä¾‹å¦‚ï¼Œå˜é‡å£°æ˜ã€æ“ä½œç¬¦ã€å‡½æ•°è°ƒç”¨ã€æ§åˆ¶ç»“æ„ï¼ˆå¦‚æ¡ä»¶è¯­å¥ã€å¾ªç¯ï¼‰ç­‰ã€‚æ¯ä¸ªèŠ‚ç‚¹åŒ…å«ä¸ç›¸åº”è¯­æ³•ç»“æ„ç›¸å…³çš„ä¿¡æ¯ï¼Œå¦‚æ“ä½œç¬¦çš„ç±»å‹ã€å˜é‡çš„åç§°ã€å¸¸é‡çš„å€¼ç­‰ã€‚\na = b + 1 è¿™ä¸ªä»£ç å¯ä»¥è½¬æ¢ä¸ºå¦‚ä¸‹å½¢å¼çš„ ASTï¼š\nAssignment â”œâ”€â”€ Identifier (a) â””â”€â”€ BinaryOperation â”œâ”€â”€ Identifier (b) â””â”€â”€ Constant (1) {% endfold %} æ¯ä¸ªå‡½æ•°éƒ½ç”±ä¸€ä¸ª relax.expr.Function èŠ‚ç‚¹è¡¨ç¤ºã€‚\nrelax_func = MyModule[\u0026#34;main\u0026#34;] type(relax_func) # \u0026lt;class \u0026#39;tvm.relax.expr.Function\u0026#39;\u0026gt; è¯¥å‡½æ•°åŒ…å«ä¸€ç³»åˆ—å‚æ•°\nprint(relax_func.params) # [x, y] è¯¥å‡½æ•°åŒ…å«ä¸€ä¸ªè¿”å›å€¼è¡¨è¾¾å¼ï¼Œå’Œå‡½æ•°ä¸­çš„ä¸€ç»„ binding blocks.\nfunc_body = relax_func.body print(type(func_body)) # \u0026lt;class \u0026#39;tvm.relax.expr.SeqExpr\u0026#39;\u0026gt; å‡½æ•°ä¸»ä½“ SeqExpr åŒ…å«ä¸€ç³»åˆ— binding.\nprint(relax_func.body.blocks) \u0026#39;\u0026#39;\u0026#39; [x: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;) y: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;) with R.dataflow(): lv0: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;) = R.multiply(x, y) gv0: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;) = R.add(lv0, y) R.output(gv0)] \u0026#39;\u0026#39;\u0026#39; åœ¨ DataflowBlock ä¸­,æˆ‘ä»¬å¯ä»¥è®¿é—®å„ä¸ª binding ,åŒ…æ‹¬ value å’Œ var.\ndataflow_block = func_body.blocks[0] print(type(dataflow_block)) # \u0026lt;class \u0026#39;tvm.relax.expr.DataflowBlock\u0026#39;\u0026gt; binding = dataflow_block.bindings[0] print(type(binding)) # \u0026lt;class \u0026#39;tvm.relax.expr.VarBinding\u0026#39;\u0026gt; print(binding.var) # LHS of binding: lv0 print(binding.value) # # LHS of binding: R.multiply(x, y) Relax Function Data Structure\næ”¹å†™ç¨‹åºå¯ä»¥é€šè¿‡é€’å½’éå† MyModule çš„ AST ï¼Œå¹¶ç”Ÿæˆè½¬æ¢åçš„ AST æ¥å®ç°ã€‚ä½†æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢å¤–çš„å·¥å…·æ”¯æŒæ¥ç®€åŒ–æµç¨‹ã€‚ä¸‹é¢çš„ä»£ç éµå¾ªä¸€ç§ç§°ä¸º visitor pattern çš„è®¾è®¡æ¨¡å¼ï¼Œå…è®¸æˆ‘ä»¬è®¿é—®æ¯ä¸ª AST èŠ‚ç‚¹å¹¶å°†å®ƒä»¬é‡å†™ä¸ºè½¬æ¢åçš„ç‰ˆæœ¬ã€‚ä¸»è¦ç›®çš„æ˜¯å°†å½¢å¦‚ a * b + c çš„è¡¨è¾¾å¼è½¬æ¢ä¸º ewise_fma(a, b, c) çš„å½¢å¼ã€‚\nEwiseFMARewriter ç»§æ‰¿è‡ª relax.PyExprMutatorï¼Œè¿™æ˜¯ TVM ä¸­çš„ä¸€ä¸ªåŸºç±»ï¼Œç”¨äºéå†å’Œä¿®æ”¹è¡¨è¾¾å¼æ ‘ä¸­çš„èŠ‚ç‚¹ã€‚visit_call_ æ–¹æ³•è¢«é‡è½½æ¥å¤„ç† relax.Call èŠ‚ç‚¹ï¼Œè¢«é‡è½½æ¥å¤„ç† relax.Call èŠ‚ç‚¹ã€‚\nå¦‚æœå½“å‰èŠ‚ç‚¹ä¸æ˜¯åŠ æ³•æ“ä½œï¼Œç›´æ¥è¿”å›è¯¥èŠ‚ç‚¹ï¼Œè¡¨ç¤ºå¯¹è¯¥èŠ‚ç‚¹ä¸è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚å¦‚æœåŠ æ³•çš„ç¬¬ä¸€ä¸ªæ“ä½œæ•°ä¸æ˜¯ä¹˜æ³•æ“ä½œï¼Œæˆ–è€…ç¬¬ä¸€ä¸ªæ“ä½œæ•°çš„ç»‘å®šå€¼ä¸æ˜¯ä¸€ä¸ª relax.Call èŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›è¯¥åŠ æ³•èŠ‚ç‚¹ã€‚å¦‚æœåŒ¹é…æˆåŠŸï¼Œæ„é€ ä¸€ä¸ªæ–°çš„ ewise_fma æ“ä½œèŠ‚ç‚¹ï¼Œå°†ä¹˜æ³•çš„ä¸¤ä¸ªæ“ä½œæ•°å’ŒåŠ æ³•çš„ç¬¬äºŒä¸ªæ“ä½œæ•°ä½œä¸ºå‚æ•°ä¼ å…¥ã€‚\n@relax.expr_functor.mutator class EwiseFMARewriter(relax.PyExprMutator): def visit_call_(self, op: relax.Call): # Reloaded call = self.visit_expr_post_order(op) add_op = tvm.ir.Op.get(\u0026#34;relax.add\u0026#34;) multiply_op = tvm.ir.Op.get(\u0026#34;relax.multiply\u0026#34;) ewise_fma_op = tvm.ir.Op.get(\u0026#34;relax.ewise_fma\u0026#34;) if call.op != add_op: return call value = self.lookup_binding(call.args[0]) if not isinstance(value, relax.Call) or value.op != multiply_op: return call fma_call = relax.Call( ewise_fma_op, [value.args[0], value.args[1], call.args[1]], None, None ) return fma_call updated_fn = EwiseFMARewriter().visit_expr(MyModule[\u0026#34;main\u0026#34;]) updated_fn.show() #----------------------------- @R.function def main(x: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;), y: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;): with R.dataflow(): lv0: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;) = R.multiply(x, y) gv0: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;) = R.ewise_fma(x, y, y) R.output(gv0) return gv0 ä½¿ç”¨ remove_all_unused æ¥åˆ é™¤ä»£ç ä¸­æ²¡æœ‰ç”¨åˆ°çš„ DataflowBlocks å’Œ VarBindings.\nrelax.analysis.remove_all_unused(updated_fn).show() #------------------------------------------- @R.function def main(x: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;), y: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;): with R.dataflow(): gv0: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;) = R.ewise_fma(x, y, y) R.output(gv0) return gv0 Fuse Linear and ReLU ä¸‹é¢åœ¨ç«¯åˆ°ç«¯æ¨¡å‹ä¸Šè¿›è¡Œè®¡ç®—å›¾çš„æ”¹å†™ã€‚é‡‡ç”¨çš„è¿˜æ˜¯ä¹‹å‰ä½¿ç”¨çš„ FashionMNIST MLP æ¨¡å‹ã€‚ä¸ºäº†ç®€åŒ–è¿‡ç¨‹ï¼Œç›´æ¥ä½¿ç”¨é«˜çº§è¿ç®—ç¬¦æ„å»ºæ¨¡å‹ã€‚\nimport pickle as pkl mlp_params = pkl.load(open(\u0026#34;fasionmnist_mlp_params.pkl\u0026#34;, \u0026#34;rb\u0026#34;)) def create_model(): bb = relax.BlockBuilder() x = relax.Var(\u0026#34;x\u0026#34;, relax.TensorStructInfo((1, 784), \u0026#34;float32\u0026#34;)) w0 = relax.const(mlp_params[\u0026#34;w0\u0026#34;], \u0026#34;float32\u0026#34;) b0 = relax.const(mlp_params[\u0026#34;b0\u0026#34;], \u0026#34;float32\u0026#34;) w1 = relax.const(mlp_params[\u0026#34;w1\u0026#34;], \u0026#34;float32\u0026#34;) b1 = relax.const(mlp_params[\u0026#34;b1\u0026#34;], \u0026#34;float32\u0026#34;) with bb.function(\u0026#34;main\u0026#34;, [x]): with bb.dataflow(): lv0 = bb.emit(relax.op.matmul(x, relax.op.permute_dims(w0))) lv1 = bb.emit(relax.op.add(lv0, b0)) lv2 = bb.emit(relax.op.nn.relu(lv1)) lv3 = bb.emit(relax.op.matmul(lv2, relax.op.permute_dims(w1))) lv4 = bb.emit(relax.op.add(lv3, b1)) gv = bb.emit_output(lv4) bb.emit_func_output(gv) return bb.get() MLPModel = create_model() MLPModel.show() #------------------------------- @I.ir_module class Module: @R.function def main(x: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): with R.dataflow(): lv: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(metadata[\u0026#34;relax.expr.Constant\u0026#34;][0], axes=None) lv1: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.matmul(x, lv, out_dtype=\u0026#34;void\u0026#34;) lv2: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.add(lv1, metadata[\u0026#34;relax.expr.Constant\u0026#34;][1]) lv3: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.nn.relu(lv2) lv4: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(metadata[\u0026#34;relax.expr.Constant\u0026#34;][2], axes=None) lv5: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.matmul(lv3, lv4, out_dtype=\u0026#34;void\u0026#34;) lv6: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.add(lv5, metadata[\u0026#34;relax.expr.Constant\u0026#34;][3]) gv: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = lv6 R.output(gv) return gv æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯¹ matmul å’Œ add è¿›è¡Œç®—å­èåˆã€‚å…·ä½“å®ç°æ­¥éª¤ä¸ FMA ç›¸ä¼¼ï¼š\nè¯†åˆ« matmul å’Œ add ç®—å­ã€‚ ç”Ÿæˆå¦ä¸€ä¸ªè°ƒç”¨ matmul å’Œ add ç®—å­çš„å­å‡½æ•°ã€‚ å°† matmul å’Œ add æ›¿æ¢ä¸ºèåˆåçš„å­å‡½æ•°ã€‚ ä¸‹é¢ä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º DenseAddFusor çš„ç±»ï¼Œç”¨äºåœ¨ TVM çš„ Relax æ¡†æ¶ä¸­å°†ç‰¹å®šçš„çŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•æ“ä½œæ¨¡å¼èåˆæˆä¸€ä¸ªé«˜æ•ˆçš„åŸè¯­å‡½æ•°ã€‚\ntransform æ–¹æ³•éå†æ¨¡å—ä¸­çš„æ¯ä¸ªå‡½æ•°ã€‚å¦‚æœå‡½æ•°å·²ç»è¢«æ ‡è®°ä¸º primitiveï¼ˆå³å·²ç»è¢«èåˆè¿‡ï¼‰ï¼Œåˆ™è·³è¿‡ã€‚å¯¹æ¯ä¸ªå‡½æ•°åº”ç”¨ visit_expr ä»¥è¿›è¡Œæ¨¡å¼åŒ¹é…å’Œæ½œåœ¨çš„èåˆæ“ä½œï¼Œç„¶ååˆ é™¤æœªä½¿ç”¨çš„å˜é‡ï¼Œå¹¶æ›´æ–°å‡½æ•°ã€‚æœ€åï¼Œè¿”å›æ›´æ–°åçš„ IRModuleã€‚ visit_call_ æ–¹æ³•ç”¨äºè®¿é—® relax.Call èŠ‚ç‚¹ï¼ˆè¡¨ç¤ºæ“ä½œç¬¦è°ƒç”¨ï¼‰ã€‚å®ƒé¦–å…ˆé€’å½’å¤„ç†å­è¡¨è¾¾å¼ï¼Œç„¶åå°è¯•åŒ¹é…ç‰¹å®šæ¨¡å¼ã€‚match_call æ˜¯ä¸€ä¸ªå†…éƒ¨å‡½æ•°ï¼Œç”¨äºæ£€æŸ¥æŸä¸ªèŠ‚ç‚¹æ˜¯å¦æ˜¯ç‰¹å®šæ“ä½œç¬¦çš„è°ƒç”¨ã€‚å¦‚æœå½“å‰èŠ‚ç‚¹ä¸æ˜¯ add æ“ä½œï¼Œæˆ–è€… add æ“ä½œçš„ç¬¬ä¸€ä¸ªå‚æ•°ä¸æ˜¯ matmulï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰æ“ä½œï¼Œåˆ™ç›´æ¥è¿”å›å½“å‰èŠ‚ç‚¹ï¼Œä¸è¿›è¡Œä¿®æ”¹ã€‚å¦‚æœåŒ¹é…æˆåŠŸï¼Œåˆ™æå– matmul çš„ä¸¤ä¸ªæ“ä½œæ•° x å’Œ w ä»¥åŠ add çš„ç¬¬äºŒä¸ªæ“ä½œæ•° bï¼Œå‡†å¤‡è¿›è¡Œèåˆã€‚ é€šè¿‡ relax.BlockBuilderå®šä¹‰ä¸€ä¸ªåä¸º fused_dense_addXæ–°çš„èåˆå‡½æ•°ï¼Œå…¶ä¸­ X æ˜¯ä¸€ä¸ªé€’å¢çš„è®¡æ•°å™¨ã€‚è¯¥å‡½æ•°æ¥æ”¶ xã€wã€b ä½œä¸ºå‚æ•°ï¼Œé¦–å…ˆè¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œç„¶åå°†ç»“æœä¸ b ç›¸åŠ ï¼Œæœ€ç»ˆè¾“å‡ºç»“æœã€‚ ç»™æ–°ç”Ÿæˆçš„èåˆå‡½æ•°æ·»åŠ ä¸€ä¸ªå±æ€§ Primitiveï¼Œæ ‡è®°ä¸ºå·²ç»èåˆçš„åŸè¯­å‡½æ•°ã€‚é€šè¿‡ builder_ æ›´æ–°å…¨å±€æ¨¡å—ï¼Œå°†èåˆå‡½æ•°æ·»åŠ åˆ°æ¨¡å—ä¸­ (GlobalVar ç”¨äºæŒ‡ä»£å­˜å‚¨åœ¨ IRModule ä¸­çš„å…¨å±€å‡½æ•°)ã€‚è¿”å›ä¸€ä¸ªæ–°çš„ relax.Call èŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹è°ƒç”¨ç”Ÿæˆçš„èåˆå‡½æ•°ï¼Œå¹¶ä¼ é€’åŸå§‹çš„è¾“å…¥å‚æ•° xã€wã€bã€‚ VisitExpr TVM ä¸­çš„ VisitExpr æµç¨‹æ˜¯ä¸€ç§é€’å½’éå† IR èŠ‚ç‚¹çš„æœºåˆ¶,å®ƒæ˜¯å®ç°å„ç§ IR è½¬æ¢å’Œä¼˜åŒ–çš„åŸºç¡€ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹:\né¦–å…ˆåˆ›å»ºä¸€ä¸ª ExprVisitor æˆ– ExprMutator çš„å­ç±»å®ä¾‹,è¿™ä¸ªå­ç±»ä¼šå®ç°å„ç§å…·ä½“çš„è®¿é—®é€»è¾‘ã€‚ è°ƒç”¨ visit_expr æ–¹æ³•,ä¼ å…¥æ ¹ IR èŠ‚ç‚¹ã€‚è¿™ä¸ªæ–¹æ³•ä¼šè§¦å‘æ•´ä¸ªéå†è¿‡ç¨‹çš„å¯åŠ¨ã€‚ visit_expr æ–¹æ³•ä¼šé¦–å…ˆè°ƒç”¨ visit_expr_post_order æ–¹æ³•,è¿™ä¸ªæ–¹æ³•ä¼šä»¥æ·±åº¦ä¼˜å…ˆçš„æ–¹å¼éå†æ‰€æœ‰å­èŠ‚ç‚¹ã€‚ å¯¹äºæ¯ä¸ªå­èŠ‚ç‚¹,visit_expr_post_order ä¼šæ ¹æ®èŠ‚ç‚¹çš„å…·ä½“ç±»å‹,è°ƒç”¨ç›¸åº”çš„ visit_XXX_ æ–¹æ³•ã€‚è¿™äº› visit_XXX_ æ–¹æ³•æ˜¯ç”±è®¿é—®å™¨å­ç±»å®ç°çš„,åŒ…å«äº†å…·ä½“çš„è®¿é—®é€»è¾‘ã€‚ åœ¨ visit_XXX_ æ–¹æ³•ä¸­,å¦‚æœé‡åˆ°å­èŠ‚ç‚¹,ä¼šé€’å½’è°ƒç”¨ visit_expr_post_order æ–¹æ³•ç»§ç»­éå†ã€‚ å½“éå†å®Œæ•´ä¸ª IR æ ‘å,visit_expr æ–¹æ³•ä¼šè¿”å›æœ€ç»ˆçš„ç»“æœ,å³ç»è¿‡è½¬æ¢å’Œä¿®æ”¹çš„ IR èŠ‚ç‚¹ã€‚ @relax.expr_functor.mutator class DenseAddFusor(relax.PyExprMutator): def __init__(self, mod: IRModule) -\u0026gt; None: super().__init__(mod) self.mod_ = mod # cache pre-defined ops self.add_op = tvm.ir.Op.get(\u0026#34;relax.add\u0026#34;) self.dense_op = tvm.ir.Op.get(\u0026#34;relax.matmul\u0026#34;) self.counter = 0 def transform(self) -\u0026gt; IRModule: for global_var, func in self.mod_.functions_items(): if not isinstance(func, relax.Function): continue # avoid already fused primitive function if \u0026#34;Primitive\u0026#34; in func.attrs.keys() and func.attrs[\u0026#34;primitive\u0026#34;] != 0: continue updated_fn = self.visit_expr(func) updated_fn = relax.analysis.remove_all_unused(updated_fn) self.builder_.update_func(global_var, updated_fn) return self.builder_.get() def visit_call_(self, op: relax.Call): call = self.visit_expr_post_order(op) def match_call(node, op): if not isinstance(node, relax.Call): return False return node.op == op # pattern match dense =\u0026gt; add if not match_call(call, self.add_op): return call value = self.lookup_binding(call.args[0]) if value is None: return call if not match_call(value, self.dense_op): return call x = value.args[0] w = value.args[1] b = call.args[1] # construct a new fused primitive function param_x = relax.Var(\u0026#34;x\u0026#34;, relax.TensorStructInfo(x.struct_info.shape, x.struct_info.dtype)) param_w = relax.Var(\u0026#34;w\u0026#34;, relax.TensorStructInfo(w.struct_info.shape, w.struct_info.dtype)) param_b = relax.Var(\u0026#34;b\u0026#34;, relax.TensorStructInfo(b.struct_info.shape, b.struct_info.dtype)) bb = relax.BlockBuilder() fn_name = \u0026#34;fused_dense_add%d\u0026#34; % (self.counter) self.counter += 1 with bb.function(fn_name, [param_x, param_w, param_b]): with bb.dataflow(): lv0 = bb.emit(relax.op.matmul(param_x, param_w)) gv0 = bb.emit_output(relax.op.add(lv0, param_b)) bb.emit_func_output(gv0) # add primitive attribute to the fused functions fused_fn = bb.get()[fn_name].with_attr(\u0026#34;Primitive\u0026#34;, 1) global_var = self.builder_.add_func(fused_fn, fn_name) # construct call into the fused function return relax.Call(global_var, [x, w, b], None, None) @tvm.ir.transform.module_pass(opt_level=2, name=\u0026#34;DenseAddFuse\u0026#34;) class FuseDenseAddPass: \u0026#39;\u0026#39;\u0026#39;The wrapper for the LowerTensorIR pass.\u0026#39;\u0026#39;\u0026#39; def transform_module(self, mod, ctx): return DenseAddFusor(mod).transform() MLPFused = FuseDenseAddPass()(MLPModel) MLPFused.show() èåˆåçš„ MLPFused å¯¹åº”çš„ TensorIR å¦‚ä¸‹\nTVM æ¡†æ¶ä¸­ä½¿ç”¨ module_pass æ¥ç®¡ç†å„ç§ä¼˜åŒ–æ“ä½œã€‚è¿™ç§æœºåˆ¶å…è®¸å°†ä¸åŒçš„ä¼˜åŒ–æ“ä½œï¼ˆå¦‚å›¾ä¼˜åŒ–ã€ä»£ç ç”Ÿæˆã€ç®—å­èåˆç­‰ï¼‰ç»„ç»‡æˆä¸€ä¸ªæµæ°´çº¿ï¼ˆpipelineï¼‰ï¼ŒæŒ‰é¡ºåºå¯¹æ¨¡å—è¿›è¡Œå¤„ç†ã€‚å°† DenseAddFusor å°è£…ä¸ºä¸€ä¸ª module_passï¼Œä½¿å¾—å®ƒèƒ½å¤Ÿè½»æ¾é›†æˆåˆ° TVM çš„ Pass æµæ°´çº¿ä¸­ï¼Œä¸å…¶ä»– Pass ä¸€èµ·å·¥ä½œï¼Œä»è€Œä¿è¯ä¼˜åŒ–è¿‡ç¨‹çš„æ•´ä½“æ€§å’Œä¸€è‡´æ€§ã€‚\n@I.ir_module class Module: @R.function def fused_dense_add0(x: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;), w: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;), b: R.Tensor((128,), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;): R.func_attr({\u0026#34;Primitive\u0026#34;: 1}) with R.dataflow(): lv: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.matmul(x, w, out_dtype=\u0026#34;void\u0026#34;) gv: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.add(lv, b) R.output(gv) return gv @R.function def fused_dense_add1(x: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;), w: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;), b: R.Tensor((10,), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): R.func_attr({\u0026#34;Primitive\u0026#34;: 1}) with R.dataflow(): lv: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.matmul(x, w, out_dtype=\u0026#34;void\u0026#34;) gv: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.add(lv, b) R.output(gv) return gv @R.function def main(x: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(metadata[\u0026#34;relax.expr.Constant\u0026#34;][0], axes=None) lv2: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = cls.fused_dense_add0(x, lv, metadata[\u0026#34;relax.expr.Constant\u0026#34;][1]) lv3: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.nn.relu(lv2) lv4: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(metadata[\u0026#34;relax.expr.Constant\u0026#34;][2], axes=None) lv6: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = cls.fused_dense_add1(lv3, lv4, metadata[\u0026#34;relax.expr.Constant\u0026#34;][3]) gv: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = lv6 R.output(gv) return gv ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªå‰ç¼€ä¸º fuse_matmul_add çš„å­å‡½æ•°ã€‚ è¿™äº›å­å‡½æ•°åŒ…å«æœ‰èåˆåç®—å­çš„è®¡ç®—ä¿¡æ¯ã€‚ è¿™ç§é‡å†™çš„æ›¿ä»£æ–¹æ³•æ˜¯ç®€å•åœ°ä¸ºèåˆç®—å­åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„åŸè¯­ç®—å­ï¼ˆå¦‚ewise_fmaï¼‰ã€‚ ä½†æ˜¯ï¼Œå½“æˆ‘ä»¬å°è¯•èåˆæ›´å¤šç®—å­æ—¶ï¼Œå¯èƒ½å­˜åœ¨æŒ‡æ•°çº§æ•°é‡çš„ç»„åˆã€‚ å°†èåˆæ“ä½œåˆ†ç»„åœ¨ä¸€èµ·çš„å­å‡½æ•°ä¸ºåç»­çš„ pass ä¿ç•™äº†åŸå§‹ä¿¡æ¯ï¼Œè¿›è€Œä¾¿äºåˆ†æï¼Œæ— éœ€ä¸ºæ¯ä¸ªèåˆ pattern å¼•å…¥ä¸“ç”¨çš„é«˜çº§ç®—å­ã€‚\nMap to TensorIR Calls ä¸ºäº†è¿›ä¸€æ­¥è¿›è¡Œåº•å±‚ä¼˜åŒ–å’Œä»£ç ç”Ÿæˆï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº›é«˜çº§åŸè¯­è¿ç®—è½¬æ¢ä¸ºç›¸åº”çš„ TensorIR å‡½æ•°ã€‚ä¸‹é¢ä»£ç ä¸»è¦åŠŸèƒ½æ˜¯å°† Relax è¡¨è¾¾å¼æ ‘ä¸­çš„é«˜å±‚æ¬¡ç®—å­ï¼ˆ matmulã€addã€reluï¼‰è½¬æ¢ä¸ºå¯¹åº”çš„ TensorIR è¡¨ç¤ºï¼Œä»è€Œä½¿å¾—è¿™äº›ç®—å­èƒ½å¤Ÿæ˜ å°„åˆ°åº•å±‚çš„å¼ é‡æ“ä½œï¼ˆtensor operationsï¼‰ã€‚è¿™ç§è½¬æ¢ä½¿å¾—ç¼–è¯‘å™¨å¯ä»¥ç”Ÿæˆæ›´æ¥è¿‘ç¡¬ä»¶çš„é«˜æ•ˆä»£ç ï¼Œå¹¶ä¸ºåç»­çš„ä»£ç ä¼˜åŒ–å’Œç”Ÿæˆåšå¥½å‡†å¤‡ã€‚\nè°ƒç”¨ transform æ–¹æ³•ä¼šéå† mod_ ä¸­çš„æ‰€æœ‰å‡½æ•°: å¯¹äºæ¯ä¸ªå‡½æ•°,é¦–å…ˆè°ƒç”¨ visit_expr æ–¹æ³•,è¿™ä¼šè§¦å‘ VisitExpr æµç¨‹ visit_expr æ–¹æ³•ä¼šè°ƒç”¨ visit_expr_post_orderæ–¹æ³•è¿›è¡Œæ·±åº¦ä¼˜å…ˆéå† åœ¨éå†è¿‡ç¨‹ä¸­å¯¹äºæ¯ä¸ª relax.Call èŠ‚ç‚¹,ä¼šè°ƒç”¨ visit_call_ æ–¹æ³• visit_call_ æ–¹æ³•ä¼šæ£€æŸ¥ op_map å­—å…¸,å¦‚æœå½“å‰æ“ä½œåœ¨å­—å…¸ä¸­,åˆ™è°ƒç”¨å¯¹åº”çš„è½¬æ¢å‡½æ•°( map_dense, map_add, map_relu) è¿™äº›è½¬æ¢å‡½æ•°ä¼šä½¿ç”¨ bb.call_te æ–¹æ³•,å°† Relax IR æ“ä½œè½¬æ¢ä¸º TensorIR æ“ä½œ åœ¨ transform æ–¹æ³•çš„æœ€å,ä¼šè°ƒç”¨ builder_.get() æ–¹æ³•,è¿”å›è½¬æ¢åçš„æ–° IR æ¨¡å—ã€‚ æœ€å LowerToTensorIRPass ç±»å°† LowerToTensorIR è½¬æ¢å™¨åŒ…è£…æˆä¸€ä¸ªå¯æ³¨å†Œåˆ° TVM ä¼˜åŒ– pipeline çš„ pass. module_pass çš„ opt_level å‚æ•°å†³å®šäº†ä¼˜åŒ– pass åœ¨ä¼˜åŒ– pipeline ä¸­çš„æ‰§è¡Œé¡ºåºã€‚ TVM çš„ä¼˜åŒ– pipeline æ˜¯ç”±å¤šä¸ª module_pass ç»„æˆçš„,æ¯ä¸ª module_pass éƒ½æœ‰ä¸€ä¸ª opt_level å±æ€§æ¥æŒ‡å®šå®ƒçš„ä¼˜åŒ–çº§åˆ«ã€‚\nå½“ TVM è¿›è¡Œä¼˜åŒ–æ—¶,å®ƒä¼šæŒ‰ç…§ opt_level ä»ä½åˆ°é«˜çš„é¡ºåºä¾æ¬¡åº”ç”¨å„ä¸ª module_pass. opt_level=0 çš„ pass ä¼šé¦–å…ˆè¢«æ‰§è¡Œã€‚è¿™äº› pass é€šå¸¸ä¼šæ‰§è¡Œä¸€äº›åŸºç¡€çš„ã€å¿…è¦çš„è½¬æ¢,ä¸ºåç»­çš„ä¼˜åŒ–å¥ å®šåŸºç¡€ã€‚ éšåä¼šæ‰§è¡Œ opt_level=1 çš„ pass,è¿™äº› pass å¯èƒ½ä¼šæ‰§è¡Œä¸€äº›æ›´å¤æ‚çš„ä¼˜åŒ–,æ¯”å¦‚å¾ªç¯ä¼˜åŒ–ã€å†…å­˜è®¿é—®ä¼˜åŒ–ç­‰ã€‚ä¾æ­¤ç±»æ¨,opt_level è¶Šé«˜çš„ pass ä¼šåœ¨ä¼˜åŒ– pipeline çš„åæœŸæ‰§è¡Œ,å®ƒä»¬æ‰§è¡Œçš„ä¼˜åŒ–é€šå¸¸ä¹Ÿè¶Šå¤æ‚å’Œæ·±å…¥ã€‚\né€šè¿‡åˆç†åœ°è®¾ç½® opt_level,å¼€å‘è€…å¯ä»¥æ§åˆ¶å„ä¸ªä¼˜åŒ– pass çš„æ‰§è¡Œé¡ºåº,ä»è€Œæ„å»ºå‡ºé’ˆå¯¹æ€§å¼ºã€æ€§èƒ½ä¼˜ç§€çš„ä¼˜åŒ– pipeline ã€‚è¿™ç§çµæ´»çš„ä¼˜åŒ–ç®¡ç†æœºåˆ¶æ˜¯ TVM çš„ä¸€å¤§ç‰¹ç‚¹ã€‚\nå¯¹äº LowerToTensorIRPass,å®ƒçš„ opt_level è¢«è®¾ç½®ä¸º 0, è¯´æ˜å®ƒæ˜¯ä¸€ä¸ªåŸºç¡€çš„ pass, ä¸»è¦ç”¨äºå°†é«˜çº§çš„ Relax IR æ“ä½œè½¬æ¢ä¸ºåº•å±‚çš„ TensorIR æ“ä½œã€‚\n@relax.expr_functor.mutator class LowerToTensorIR(relax.PyExprMutator): def __init__(self, mod: IRModule, op_map: dict) -\u0026gt; None: super().__init__(mod) self.mod_ = mod self.op_map = { tvm.ir.Op.get(k): v for k, v in op_map.items() } def visit_call_(self, op: relax.Call): call = self.visit_expr_post_order(op) if call.op in self.op_map: return self.op_map[call.op](self.builder_, call) return call def transform(self) -\u0026gt; IRModule: for global_val, func in self.mod_.functions_items(): if not isinstance(func, relax.Function): continue updated_fn = self.visit_expr(func) self.builder_.update_func(global_val, updated_fn) return self.builder_.get() def map_dense(bb, call): x, w = call.args return bb.call_te(topi.nn.matmul, x, w) def map_add(bb, call): a, b = call.args return bb.call_te(topi.add, a, b) def map_relu(bb, call): return bb.call_te(topi.nn.relu, call.args[0]) op_map = { \u0026#34;relax.matmul\u0026#34;: map_dense, \u0026#34;relax.add\u0026#34;: map_add, \u0026#34;relax.nn.relu\u0026#34;: map_relu } @tvm.ir.transform.module_pass(opt_level=0, name=\u0026#34;LowerToTensorIR\u0026#34;) class LowerToTensorIRPass: \u0026#39;\u0026#39;\u0026#39;The wrapper for the LowerTensorIR pass.\u0026#39;\u0026#39;\u0026#39; def transform_module(self, mod, ctx): return LowerToTensorIR(mod, op_map).transform() MLPModelTIR = LowerToTensorIRPass()(MLPFused) MLPModelTIR.show() èåˆåçš„ TensorIR å¦‚ä¸‹\n@I.ir_module class Module: @T.prim_func(private=True) def add(lv: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;), b: T.Buffer((T.int64(128),), \u0026#34;float32\u0026#34;), T_add: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for ax0, ax1 in T.grid(T.int64(1), T.int64(128)): with T.block(\u0026#34;T_add\u0026#34;): v_ax0, v_ax1 = T.axis.remap(\u0026#34;SS\u0026#34;, [ax0, ax1]) T.reads(lv[v_ax0, v_ax1], b[v_ax1]) T.writes(T_add[v_ax0, v_ax1]) T_add[v_ax0, v_ax1] = lv[v_ax0, v_ax1] + b[v_ax1] @T.prim_func(private=True) def add1(lv: T.Buffer((T.int64(1), T.int64(10)), \u0026#34;float32\u0026#34;), b: T.Buffer((T.int64(10),), \u0026#34;float32\u0026#34;), T_add: T.Buffer((T.int64(1), T.int64(10)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for ax0, ax1 in T.grid(T.int64(1), T.int64(10)): with T.block(\u0026#34;T_add\u0026#34;): v_ax0, v_ax1 = T.axis.remap(\u0026#34;SS\u0026#34;, [ax0, ax1]) T.reads(lv[v_ax0, v_ax1], b[v_ax1]) T.writes(T_add[v_ax0, v_ax1]) T_add[v_ax0, v_ax1] = lv[v_ax0, v_ax1] + b[v_ax1] @T.prim_func(private=True) def matmul(x: T.Buffer((T.int64(1), T.int64(784)), \u0026#34;float32\u0026#34;), w: T.Buffer((T.int64(784), T.int64(128)), \u0026#34;float32\u0026#34;), T_matmul_NN: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;layout_free_buffers\u0026#34;: [1], \u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i0, i1, k in T.grid(T.int64(1), T.int64(128), T.int64(784)): with T.block(\u0026#34;T_matmul_NN\u0026#34;): v_i0, v_i1, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, i1, k]) T.reads(x[v_i0, v_k], w[v_k, v_i1]) T.writes(T_matmul_NN[v_i0, v_i1]) with T.init(): T_matmul_NN[v_i0, v_i1] = T.float32(0.0) T_matmul_NN[v_i0, v_i1] = T_matmul_NN[v_i0, v_i1] + x[v_i0, v_k] * w[v_k, v_i1] @T.prim_func(private=True) def matmul1(x: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;), w: T.Buffer((T.int64(128), T.int64(10)), \u0026#34;float32\u0026#34;), T_matmul_NN: T.Buffer((T.int64(1), T.int64(10)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;layout_free_buffers\u0026#34;: [1], \u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i0, i1, k in T.grid(T.int64(1), T.int64(10), T.int64(128)): with T.block(\u0026#34;T_matmul_NN\u0026#34;): v_i0, v_i1, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, i1, k]) T.reads(x[v_i0, v_k], w[v_k, v_i1]) T.writes(T_matmul_NN[v_i0, v_i1]) with T.init(): T_matmul_NN[v_i0, v_i1] = T.float32(0.0) T_matmul_NN[v_i0, v_i1] = T_matmul_NN[v_i0, v_i1] + x[v_i0, v_k] * w[v_k, v_i1] @T.prim_func(private=True) def relu(lv2: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i0, i1 in T.grid(T.int64(1), T.int64(128)): with T.block(\u0026#34;compute\u0026#34;): v_i0, v_i1 = T.axis.remap(\u0026#34;SS\u0026#34;, [i0, i1]) T.reads(lv2[v_i0, v_i1]) T.writes(compute[v_i0, v_i1]) compute[v_i0, v_i1] = T.max(lv2[v_i0, v_i1], T.float32(0.0)) @R.function def fused_dense_add0(x: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;), w: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;), b: R.Tensor((128,), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;): R.func_attr({\u0026#34;Primitive\u0026#34;: 1}) cls = Module with R.dataflow(): lv = R.call_tir(cls.matmul, (x, w), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) gv = R.call_tir(cls.add, (lv, b), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) R.output(gv) return gv @R.function def fused_dense_add1(x: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;), w: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;), b: R.Tensor((10,), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): R.func_attr({\u0026#34;Primitive\u0026#34;: 1}) cls = Module with R.dataflow(): lv = R.call_tir(cls.matmul1, (x, w), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) gv = R.call_tir(cls.add1, (lv, b), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(gv) return gv @R.function def main(x: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(metadata[\u0026#34;relax.expr.Constant\u0026#34;][0], axes=None) lv2: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = cls.fused_dense_add0(x, lv, metadata[\u0026#34;relax.expr.Constant\u0026#34;][1]) lv3 = R.call_tir(cls.relu, (lv2,), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv4: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(metadata[\u0026#34;relax.expr.Constant\u0026#34;][2], axes=None) lv6: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = cls.fused_dense_add1(lv3, lv4, metadata[\u0026#34;relax.expr.Constant\u0026#34;][3]) gv: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = lv6 R.output(gv) return gv åœ¨ä¸Šé¢çš„ IRModule ä¸­ fused_matmul_add0 å’Œ fused_matmul_add1 ä»ç„¶æ˜¯ relax å‡½æ•°ï¼Œå®ƒä»¬è°ƒç”¨ç›¸åº”çš„ TensorIR matmul å’Œ add å‡½æ•°ã€‚ æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬å˜æˆä¸€ä¸ªå•ä¸€çš„ TensorIR å‡½æ•°ã€‚\nMLPModelFinal = relax.transform.FuseTIR()(MLPModelTIR) MLPModelFinal.show() #----------------------- @I.ir_module class Module: @T.prim_func(private=True) def fused_dense_add0(x: T.Buffer((T.int64(1), T.int64(784)), \u0026#34;float32\u0026#34;), w: T.Buffer((T.int64(784), T.int64(128)), \u0026#34;float32\u0026#34;), b: T.Buffer((T.int64(128),), \u0026#34;float32\u0026#34;), T_add_intermediate: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): T_matmul_NN_intermediate = T.alloc_buffer((T.int64(1), T.int64(128))) for i0, i1, k in T.grid(T.int64(1), T.int64(128), T.int64(784)): with T.block(\u0026#34;T_matmul_NN\u0026#34;): v_i0, v_i1, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, i1, k]) T.reads(x[v_i0, v_k], w[v_k, v_i1]) T.writes(T_matmul_NN_intermediate[v_i0, v_i1]) with T.init(): T_matmul_NN_intermediate[v_i0, v_i1] = T.float32(0.0) T_matmul_NN_intermediate[v_i0, v_i1] = T_matmul_NN_intermediate[v_i0, v_i1] + x[v_i0, v_k] * w[v_k, v_i1] for ax0, ax1 in T.grid(T.int64(1), T.int64(128)): with T.block(\u0026#34;T_add\u0026#34;): v_ax0, v_ax1 = T.axis.remap(\u0026#34;SS\u0026#34;, [ax0, ax1]) T.reads(T_matmul_NN_intermediate[v_ax0, v_ax1], b[v_ax1]) T.writes(T_add_intermediate[v_ax0, v_ax1]) T_add_intermediate[v_ax0, v_ax1] = T_matmul_NN_intermediate[v_ax0, v_ax1] + b[v_ax1] @T.prim_func(private=True) def fused_dense_add1(x: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;), w: T.Buffer((T.int64(128), T.int64(10)), \u0026#34;float32\u0026#34;), b: T.Buffer((T.int64(10),), \u0026#34;float32\u0026#34;), T_add_intermediate: T.Buffer((T.int64(1), T.int64(10)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): T_matmul_NN_intermediate = T.alloc_buffer((T.int64(1), T.int64(10))) for i0, i1, k in T.grid(T.int64(1), T.int64(10), T.int64(128)): with T.block(\u0026#34;T_matmul_NN\u0026#34;): v_i0, v_i1, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, i1, k]) T.reads(x[v_i0, v_k], w[v_k, v_i1]) T.writes(T_matmul_NN_intermediate[v_i0, v_i1]) with T.init(): T_matmul_NN_intermediate[v_i0, v_i1] = T.float32(0.0) T_matmul_NN_intermediate[v_i0, v_i1] = T_matmul_NN_intermediate[v_i0, v_i1] + x[v_i0, v_k] * w[v_k, v_i1] for ax0, ax1 in T.grid(T.int64(1), T.int64(10)): with T.block(\u0026#34;T_add\u0026#34;): v_ax0, v_ax1 = T.axis.remap(\u0026#34;SS\u0026#34;, [ax0, ax1]) T.reads(T_matmul_NN_intermediate[v_ax0, v_ax1], b[v_ax1]) T.writes(T_add_intermediate[v_ax0, v_ax1]) T_add_intermediate[v_ax0, v_ax1] = T_matmul_NN_intermediate[v_ax0, v_ax1] + b[v_ax1] @T.prim_func(private=True) def relu(lv2: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i0, i1 in T.grid(T.int64(1), T.int64(128)): with T.block(\u0026#34;compute\u0026#34;): v_i0, v_i1 = T.axis.remap(\u0026#34;SS\u0026#34;, [i0, i1]) T.reads(lv2[v_i0, v_i1]) T.writes(compute[v_i0, v_i1]) compute[v_i0, v_i1] = T.max(lv2[v_i0, v_i1], T.float32(0.0)) @R.function def main(x: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(metadata[\u0026#34;relax.expr.Constant\u0026#34;][0], axes=None) lv2 = R.call_tir(cls.fused_dense_add0, (x, lv, metadata[\u0026#34;relax.expr.Constant\u0026#34;][1]), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv3 = R.call_tir(cls.relu, (lv2,), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv4: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(metadata[\u0026#34;relax.expr.Constant\u0026#34;][2], axes=None) gv = R.call_tir(cls.fused_dense_add1, (lv3, lv4, metadata[\u0026#34;relax.expr.Constant\u0026#34;][3]), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(gv) return gv ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch8/","summary":"Personal notebook 7.","title":"TVM Learning (10)-Computational Graph Optimization"},{"content":"Key Elements of Specialized Code ä¸‹é¢ç”¨ low-level numpy å†™çš„ python ä»£ç å±•ç¤ºäº†ä¸€ç³»åˆ—åœ¨ä¸“ç”¨ç¡¬ä»¶åç«¯å¯èƒ½ä½¿ç”¨åˆ°çš„æ“ä½œã€‚\ndef accel_fill_zero(C): C[:] = 0 def accel_tmm_add(C, A, B): C[:] += A @ B.T def accel_dma_copy(reg, dram): reg[:] = dram[:] æˆ‘ä»¬å‡è®¾åŸºç¡€çš„è¿ç®—å•å…ƒå¯ä»¥è¿›è¡Œ 16x16çš„çŸ©é˜µä¹˜æ³• (accel_tmm_add)ï¼Œæ¥æ”¶2ä¸ªå¯„å­˜å™¨é‡Œçš„ RHS è¾“å…¥å’Œè¡¨ç¤ºç´¯åŠ ä¸­é—´ç»“æœçš„ LHS è¾“å…¥ï¼Œæ•°æ®æ‹·è´ä½¿ç”¨çš„æ˜¯ä¸“ç”¨å‡½æ•° (accel_dma_copy).\n# The basis unit of computation is a 16*16*16 matrix multiplication def lnumpy_tmm(A: np.ndarray, B: np.ndarray, C: np.ndarray): # a special accumulator memory C_accumulator = np.empty((16, 16), dtype=\u0026#34;float32\u0026#34;) A_reg = np.empty((16, 16), dtype=\u0026#34;float32\u0026#34;) B_reg = np.empty((16, 16), dtype=\u0026#34;float32\u0026#34;) for i in range(64): for j in range(64): accel_fill_zero(C_accumulator) for k in range(64): accel_dma_copy(A_reg[:], A[i*16 : (i+1)*16, k*16 : (k+1)*16]) accel_dma_copy(B_reg[:], B[j*16 : (j+1)*16, k*16 : (k+1)*16]) accel_tmm_add(C_accumulator, A_reg, B_reg) accel_dma_copy(C[i*16 : (i+1)*16, j*16 : (j+1)*16], C_accumulator) A Block with Tensorized Computation ä¸“ç”¨åŠ é€Ÿå™¨ä»£ç çš„ç»“æ„å¹¶éä»¥æ ‡é‡è®¡ç®—ä¸ºå•ä½ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œæˆ‘ä»¬è¿è¡Œçš„å¤§å¤šæ•° TensorIR ä»£ç éƒ½åŒ…å«ä¸€ä¸ª blockï¼Œç”¨äºè®¡ç®—è¾“å‡ºå¼ é‡ä¸­çš„å•ä¸ªå…ƒç´ ã€‚è®¸å¤šä¸“ç”¨åŠ é€Ÿå™¨åœ¨å¼ é‡åŒºåŸŸå†…è¿›è¡Œè®¡ç®—ã€‚TensorIRä¸­çš„ block å¯ä»¥å¸®åŠ©æˆ‘ä»¬å°†è¿™äº›ç›¸å…³è®¡ç®—åˆ†ç»„ã€‚\n@tvm.script.ir_module class MatmulBlockModule: @T.prim_func def main(A: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], B: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], C: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;]) -\u0026gt; None: T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) for i0, j0, k0 in T.grid(64, 64, 64): with T.block(\u0026#34;tmm-16x16\u0026#34;): vi0, vj0, vk0 = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, j0, k0]) with T.init(): for i1, j1 in T.grid(16, 16): with T.block(\u0026#34;tmm_init\u0026#34;): vi1, vj1 = T.axis.remap(\u0026#34;SS\u0026#34;, [i1, j1]) C[vi0 * 16 + vi1, vj0 * 16 + vj1] = T.float32(0) for i1, j1, k1 in T.grid(16, 16, 16): with T.block(\u0026#34;tmm\u0026#34;): vi1, vj1, vk1 = T.axis.remap(\u0026#34;SSR\u0026#34;, [i1, j1, k1]) C[vi0 * 16 + vi1, vj0 * 16 + vj1] += A[vi0 * 16 + vi1, vk0 * 16 + vk1] * B[vj0 * 16 + vj1, vk0 * 16 + vk1] è°ƒç”¨ MatmulBlockModule.show() åæ˜¾ç¤ºçš„ TensorIRå¦‚ä¸‹\nT.reads(C[vi0 * 16 + vi1, vj0 * 16 + vj1], A[vi0 * 16 + vi1, vk0 * 16 + vk1], B[vj0 * 16 + vj1, vk0 * 16 + vk1]) T.writes(C[vi0 * 16 + vi1, vj0 * 16 + vj1]) è¯¥ä»£ç ä» A å’Œ B çš„ 16x16 åŒºåŸŸè¯»å–æ•°æ®ï¼Œå¹¶å†™å…¥ C çš„ 16x16 åŒºåŸŸã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå—çš„å†…å®¹åŒ…å«å­åŒºåŸŸè®¡ç®—çš„å…·ä½“å®ç°çš„è¿›ä¸€æ­¥ç»†èŠ‚ã€‚æˆ‘ä»¬ç§°è¿™ç§åŒºå—ä¸ºå¼ é‡åŒºå—ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«è·¨è¶Šå¼ é‡å­åŒºåŸŸçš„è®¡ç®—ã€‚\n@I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), B: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), C: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i0, j0, k0 in T.grid(64, 64, 64): with T.block(\u0026#34;tmm-16x16\u0026#34;): vi0, vj0, vk0 = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, j0, k0]) T.reads(A[vi0 * 16:vi0 * 16 + 16, vk0 * 16:vk0 * 16 + 16], B[vj0 * 16:vj0 * 16 + 16, vk0 * 16:vk0 * 16 + 16]) T.writes(C[vi0 * 16:vi0 * 16 + 16, vj0 * 16:vj0 * 16 + 16]) with T.init(): for i1, j1 in T.grid(16, 16): with T.block(\u0026#34;tmm_init\u0026#34;): vi1, vj1 = T.axis.remap(\u0026#34;SS\u0026#34;, [i1, j1]) T.reads() T.writes(C[vi0 * 16 + vi1, vj0 * 16 + vj1]) C[vi0 * 16 + vi1, vj0 * 16 + vj1] = T.float32(0.0) for i1, j1, k1 in T.grid(16, 16, 16): with T.block(\u0026#34;tmm\u0026#34;): vi1, vj1, vk1 = T.axis.remap(\u0026#34;SSR\u0026#34;, [i1, j1, k1]) T.reads(C[vi0 * 16 + vi1, vj0 * 16 + vj1], A[vi0 * 16 + vi1, vk0 * 16 + vk1], B[vj0 * 16 + vj1, vk0 * 16 + vk1]) T.writes(C[vi0 * 16 + vi1, vj0 * 16 + vj1]) C[vi0 * 16 + vi1, vj0 * 16 + vj1] = C[vi0 * 16 + vi1, vj0 * 16 + vj1] + A[vi0 * 16 + vi1, vk0 * 16 + vk1] * B[vj0 * 16 + vj1, vk0 * 16 + vk1] Transforming Loops Around Tensorized Block æˆ‘ä»¬å¯ä»¥å¯¹å¼ é‡è®¡ç®—å—çš„å¾ªç¯è¿›è¡Œå˜æ¢ï¼Œè¿™äº›å¾ªç¯å˜æ¢å¯ä»¥é‡æ–°ç»„ç»‡è®¡ç®—è¯¥å—çš„è¿­ä»£æ–¹å¼ï¼Œå¾—åˆ°ä¸åŒçš„å¼ é‡ç¨‹åºã€‚\nsch = tvm.tir.Schedule(MatmulBlockModule) block_mm = sch.get_block(\u0026#34;tmm-16x16\u0026#34;) i, j, k = sch.get_loops(block_mm) i0, i1 = sch.split(i, [None, 4]) sch.reorder(i0, j, i1, k) sch.mod.show() #------------------------------------ @I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), B: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), C: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i0_0, j0, i0_1, k0 in T.grid(16, 64, 4, 64): with T.block(\u0026#34;tmm-16x16\u0026#34;): vi0 = T.axis.spatial(64, i0_0 * 4 + i0_1) vj0, vk0 = T.axis.remap(\u0026#34;SR\u0026#34;, [j0, k0]) T.reads(A[vi0 * 16:vi0 * 16 + 16, vk0 * 16:vk0 * 16 + 16], B[vj0 * 16:vj0 * 16 + 16, vk0 * 16:vk0 * 16 + 16]) T.writes(C[vi0 * 16:vi0 * 16 + 16, vj0 * 16:vj0 * 16 + 16]) with T.init(): for i1, j1 in T.grid(16, 16): with T.block(\u0026#34;tmm_init\u0026#34;): vi1, vj1 = T.axis.remap(\u0026#34;SS\u0026#34;, [i1, j1]) T.reads() T.writes(C[vi0 * 16 + vi1, vj0 * 16 + vj1]) C[vi0 * 16 + vi1, vj0 * 16 + vj1] = T.float32(0.0) for i1, j1, k1 in T.grid(16, 16, 16): with T.block(\u0026#34;tmm\u0026#34;): vi1, vj1, vk1 = T.axis.remap(\u0026#34;SSR\u0026#34;, [i1, j1, k1]) T.reads(C[vi0 * 16 + vi1, vj0 * 16 + vj1], A[vi0 * 16 + vi1, vk0 * 16 + vk1], B[vj0 * 16 + vj1, vk0 * 16 + vk1]) T.writes(C[vi0 * 16 + vi1, vj0 * 16 + vj1]) C[vi0 * 16 + vi1, vj0 * 16 + vj1] = C[vi0 * 16 + vi1, vj0 * 16 + vj1] + A[vi0 * 16 + vi1, vk0 * 16 + vk1] * B[vj0 * 16 + vj1, vk0 * 16 + vk1] Blockization \u0026ndash; Creating Tensorized Blocks TensorIR æä¾›äº†ä¸€ç§å˜æ¢åŸè¯­ blockize æ¥å°†å¾ªç¯çš„å­åŒºåŸŸç»„åˆåœ¨ä¸€èµ·ä»¥å½¢æˆå¼ é‡åŒ–çš„è®¡ç®— block. ä¾‹å¦‚æˆ‘ä»¬å¯ä»¥å°†ä¸‹é¢2ä¸ªçš„ 1024x1024 çŸ©é˜µä¹˜æ³•åˆ†è§£æˆå¾ˆå¤šä¸ª 16x16 çš„çŸ©é˜µä¹˜æ³•ã€‚\n@tvm.script.ir_module class MatmulModule: @T.prim_func def main( A: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], B: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], C: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], ) -\u0026gt; None: T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) for i, j, k in T.grid(1024, 1024, 1024): with T.block(\u0026#34;matmul\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): C[vi, vj] = T.float32(0) C[vi, vj] += A[vi, vk] * B[vj, vk] sch = tvm.tir.Schedule(MatmulModule) i, j, k = sch.get_loops(\u0026#34;matmul\u0026#34;) i, ii = sch.split(i, factors=[None, 16]) j, ji = sch.split(j, factors=[None, 16]) k, ki = sch.split(k, factors=[None, 16]) sch.reorder(i, j, k, ii, ji, ki) sch.mod.show() #------------------------------------- @I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), B: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), C: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i_0, j_0, k_0, i_1, j_1, k_1 in T.grid(64, 64, 64, 16, 16, 16): with T.block(\u0026#34;matmul\u0026#34;): vi = T.axis.spatial(1024, i_0 * 16 + i_1) vj = T.axis.spatial(1024, j_0 * 16 + j_1) vk = T.axis.reduce(1024, k_0 * 16 + k_1) T.reads(A[vi, vk], B[vj, vk]) T.writes(C[vi, vj]) with T.init(): C[vi, vj] = T.float32(0.0) C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vj, vk] blockize æ˜¯ç”¨æ¥å°†ä¸€ä¸ªæˆ–å¤šä¸ªå—(block)æˆ–ä¸€ä¸ªç‰¹å®šå¾ªç¯çš„å­æ ‘åˆå¹¶æˆä¸€ä¸ªæ–°çš„å—ã€‚å¦‚æœ target æ˜¯ä¸€ä¸ªå¾ªç¯çš„æ ¹èŠ‚ç‚¹,åˆ™ä¼šå°†è¯¥å¾ªç¯ä¸‹çš„æ‰€æœ‰å—åˆå¹¶æˆä¸€ä¸ªæ–°å—ï¼Œå¦‚æœ target æ˜¯ä¸€ä¸ªå—çš„åˆ—è¡¨,åˆ™ä¼šå°†è¿™äº›å—åˆå¹¶æˆä¸€ä¸ªæ–°å—ã€‚ç„¶åå°†æ–°å—è¿”å›\nå‚æ•°è¯´æ˜ :\ntarget: éœ€è¦è¢«åˆå¹¶çš„å—æˆ–å¾ªç¯çš„æ ¹èŠ‚ç‚¹ã€‚å¯ä»¥æ˜¯ LoopRV ç±»å‹(è¡¨ç¤ºä¸€ä¸ªå¾ªç¯)æˆ– List[BlockRV] ç±»å‹(è¡¨ç¤ºå¤šä¸ªå—)ã€‚ preserve_unit_iters: ä¸€ä¸ªå¸ƒå°”å€¼,è¡¨ç¤ºæ˜¯å¦ä¿ç•™å—ç»‘å®šä¸­çš„å•å…ƒè¿­ä»£å™¨ã€‚ é™åˆ¶æ¡ä»¶ :\nblockize è¦æ±‚ç»™å®šçš„å¾ªç¯ä¸‹åªæœ‰ä¸€ä¸ªå—,ä¸”è¯¥å—çš„ç»‘å®šå¿…é¡»èƒ½å¤Ÿè¢«è¯¥å¾ªç¯çš„å­ç©ºé—´æ•´é™¤ã€‚ è°ƒç”¨ blockize åçš„ TensorIR å¦‚ä¸‹\nblock_mm = sch.blockize(ii) sch.mod.show() #------------------------------------- @I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), B: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), C: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i_0, j_0, k_0 in T.grid(64, 64, 64): with T.block(\u0026#34;matmul_o\u0026#34;): vi_o, vj_o, vk_o = T.axis.remap(\u0026#34;SSR\u0026#34;, [i_0, j_0, k_0]) T.reads(A[vi_o * 16:vi_o * 16 + 16, vk_o * 16:vk_o * 16 + 16], B[vj_o * 16:vj_o * 16 + 16, vk_o * 16:vk_o * 16 + 16]) T.writes(C[vi_o * 16:vi_o * 16 + 16, vj_o * 16:vj_o * 16 + 16]) with T.init(): for i_1, j_1 in T.grid(16, 16): with T.block(\u0026#34;matmul_init\u0026#34;): vi_i_init, vj_i_init = T.axis.remap(\u0026#34;SS\u0026#34;, [i_1, j_1]) T.reads() T.writes(C[vi_o * 16 + vi_i_init, vj_o * 16 + vj_i_init]) C[vi_o * 16 + vi_i_init, vj_o * 16 + vj_i_init] = T.float32(0.0) for i_1, j_1, k_1 in T.grid(16, 16, 16): with T.block(\u0026#34;matmul\u0026#34;): vi_i, vj_i, vk_i = T.axis.remap(\u0026#34;SSR\u0026#34;, [i_1, j_1, k_1]) T.reads(C[vi_o * 16 + vi_i, vj_o * 16 + vj_i], A[vi_o * 16 + vi_i, vk_o * 16 + vk_i], B[vj_o * 16 + vj_i, vk_o * 16 + vk_i]) T.writes(C[vi_o * 16 + vi_i, vj_o * 16 + vj_i]) C[vi_o * 16 + vi_i, vj_o * 16 + vj_i] = C[vi_o * 16 + vi_i, vj_o * 16 + vj_i] + A[vi_o * 16 + vi_i, vk_o * 16 + vk_i] * B[vj_o * 16 + vj_i, vk_o * 16 + vk_i] Transforming TensorIR to Introduce Special Memory Scope æ­£å¦‚åœ¨ low-level NumPy ä»£ç ä¸­æåˆ°çš„ï¼Œåº•å±‚ TensorIR çš„ä¸€ä¸ªå…³é”®è¦ç´ æ˜¯åŠ é€Ÿè¿‡ç¨‹ä¸­ä½¿ç”¨çš„ç‰¹æ®Šå†…å­˜èŒƒå›´ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ cache_read å’Œ write æ¥åˆ›å»ºä¸­é—´å†…å­˜é˜¶æ®µã€‚\nstorage_scope åœ¨è¿™é‡ŒæŒ‡çš„æ˜¯å†…å­˜å­˜å‚¨èŒƒå›´æˆ–å­˜å‚¨å±‚æ¬¡ã€‚å¸¸è§çš„å­˜å‚¨èŒƒå›´åŒ…æ‹¬:\nglobal: è¡¨ç¤ºæ•°æ®å­˜å‚¨åœ¨å…¨å±€å†…å­˜ä¸­ã€‚è¿™æ˜¯æœ€é«˜å±‚æ¬¡çš„å†…å­˜èŒƒå›´ã€‚ shared: è¡¨ç¤ºæ•°æ®å­˜å‚¨åœ¨GPUçš„å…±äº«å†…å­˜ä¸­ã€‚ local: è¡¨ç¤ºæ•°æ®å­˜å‚¨åœ¨CPUæˆ–GPUçš„å¯„å­˜å™¨ä¸­ã€‚è¿™æ˜¯æœ€åº•å±‚çš„å†…å­˜èŒƒå›´ã€‚ global.A_reg è¡¨ç¤ºæ•°æ®å°†è¢«ç¼“å­˜åˆ°ä¸€ä¸ªåä¸º A_reg çš„å…¨å±€å†…å­˜ç¼“å­˜ä¸­ã€‚\nStorage Scope\nA_reg = sch.cache_read(block_mm, 0, storage_scope=\u0026#34;global.A_reg\u0026#34;) B_reg = sch.cache_read(block_mm, 1, storage_scope=\u0026#34;global.B_reg\u0026#34;) sch.compute_at(A_reg, k) sch.compute_at(B_reg, k) write_back_block = sch.cache_write(block_mm, 0, storage_scope=\u0026#34;global.accumulator\u0026#34;) sch.reverse_compute_at(write_back_block, j) sch.mod.show() #----------------------------------- @I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), B: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), C: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): A_global_A_reg = T.alloc_buffer((1024, 1024), scope=\u0026#34;global.A_reg\u0026#34;) B_global_B_reg = T.alloc_buffer((1024, 1024), scope=\u0026#34;global.B_reg\u0026#34;) C_global_accumulator = T.alloc_buffer((1024, 1024), scope=\u0026#34;global.accumulator\u0026#34;) for i_0, j_0 in T.grid(64, 64): for k_0 in range(64): for ax0, ax1 in T.grid(16, 16): with T.block(\u0026#34;A_global.A_reg\u0026#34;): v0 = T.axis.spatial(1024, i_0 * 16 + ax0) v1 = T.axis.spatial(1024, k_0 * 16 + ax1) T.reads(A[v0, v1]) T.writes(A_global_A_reg[v0, v1]) A_global_A_reg[v0, v1] = A[v0, v1] for ax0, ax1 in T.grid(16, 16): with T.block(\u0026#34;B_global.B_reg\u0026#34;): v0 = T.axis.spatial(1024, j_0 * 16 + ax0) v1 = T.axis.spatial(1024, k_0 * 16 + ax1) T.reads(B[v0, v1]) T.writes(B_global_B_reg[v0, v1]) B_global_B_reg[v0, v1] = B[v0, v1] with T.block(\u0026#34;matmul_o\u0026#34;): vi_o, vj_o, vk_o = T.axis.remap(\u0026#34;SSR\u0026#34;, [i_0, j_0, k_0]) T.reads(A_global_A_reg[vi_o * 16:vi_o * 16 + 16, vk_o * 16:vk_o * 16 + 16], B_global_B_reg[vj_o * 16:vj_o * 16 + 16, vk_o * 16:vk_o * 16 + 16]) T.writes(C_global_accumulator[vi_o * 16:vi_o * 16 + 16, vj_o * 16:vj_o * 16 + 16]) with T.init(): for i_1, j_1 in T.grid(16, 16): with T.block(\u0026#34;matmul_init\u0026#34;): vi_i_init, vj_i_init = T.axis.remap(\u0026#34;SS\u0026#34;, [i_1, j_1]) T.reads() T.writes(C_global_accumulator[vi_o * 16 + vi_i_init, vj_o * 16 + vj_i_init]) C_global_accumulator[vi_o * 16 + vi_i_init, vj_o * 16 + vj_i_init] = T.float32(0.0) for i_1, j_1, k_1 in T.grid(16, 16, 16): with T.block(\u0026#34;matmul\u0026#34;): vi_i, vj_i, vk_i = T.axis.remap(\u0026#34;SSR\u0026#34;, [i_1, j_1, k_1]) T.reads(C_global_accumulator[vi_o * 16 + vi_i, vj_o * 16 + vj_i], A_global_A_reg[vi_o * 16 + vi_i, vk_o * 16 + vk_i], B_global_B_reg[vj_o * 16 + vj_i, vk_o * 16 + vk_i]) T.writes(C_global_accumulator[vi_o * 16 + vi_i, vj_o * 16 + vj_i]) C_global_accumulator[vi_o * 16 + vi_i, vj_o * 16 + vj_i] = C_global_accumulator[vi_o * 16 + vi_i, vj_o * 16 + vj_i] + A_global_A_reg[vi_o * 16 + vi_i, vk_o * 16 + vk_i] * B_global_B_reg[vj_o * 16 + vj_i, vk_o * 16 + vk_i] for ax0, ax1 in T.grid(16, 16): with T.block(\u0026#34;C_global.accumulator\u0026#34;): v0 = T.axis.spatial(1024, i_0 * 16 + ax0) v1 = T.axis.spatial(1024, j_0 * 16 + ax1) T.reads(C_global_accumulator[v0, v1]) T.writes(C[v0, v1]) C[v0, v1] = C_global_accumulator[v0, v1] Tensorization ç°åœ¨æˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ç»„æ˜ å°„åˆ° TensorIR ä¸­ç›¸åº”è®¡ç®—é˜¶æ®µçš„å—ã€‚å‰©ä¸‹çš„æ­¥éª¤æ˜¯æ˜ å°„éƒ¨åˆ†å¼ é‡å—ï¼Œä»¥ä½¿ç”¨æ˜ å°„åˆ°ç¡¬ä»¶åŠ é€ŸæŒ‡ä»¤çš„ç‰¹å®šå®ç°ã€‚è¿™ä¸€æ˜ å°„è¿‡ç¨‹ç§°ä¸ºå¼ é‡åŒ–ã€‚ä¸ºäº†å®ç°å¼ é‡åŒ–ï¼Œæˆ‘ä»¬é¦–å…ˆæ³¨å†Œä¸€ä¸ª TensorIntrinï¼Œå…¶ä¸­åŒ…å«è®¡ç®—å’Œå®ç°çš„æè¿°ã€‚\n@T.prim_func def tmm16_desc(a: T.handle, b: T.handle, c: T.handle) -\u0026gt; None: A = T.match_buffer(a, (16, 16), \u0026#34;float32\u0026#34;, offset_factor=16, scope=\u0026#34;global.A_reg\u0026#34;) B = T.match_buffer(b, (16, 16), \u0026#34;float32\u0026#34;, offset_factor=16, scope=\u0026#34;global.B_reg\u0026#34;) C = T.match_buffer(c, (16, 16), \u0026#34;float32\u0026#34;, offset_factor=16, scope=\u0026#34;global.accumulator\u0026#34;) with T.block(\u0026#34;root\u0026#34;): T.reads(C[0:16, 0:16], A[0:16, 0:16], B[0:16, 0:16]) T.writes(C[0:16, 0:16]) for i, j, k in T.grid(16, 16, 16): with T.block(\u0026#34;\u0026#34;): vii, vjj, vkk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) C[vii, vjj] = C[vii, vjj] + A[vii, vkk] * B[vjj, vkk] @T.prim_func def tmm16_impl(a: T.handle, b: T.handle, c: T.handle) -\u0026gt; None: A = T.match_buffer(a, (16, 16), \u0026#34;float32\u0026#34;, offset_factor=16, scope=\u0026#34;global.A_reg\u0026#34;) B = T.match_buffer(b, (16, 16), \u0026#34;float32\u0026#34;, offset_factor=16, scope=\u0026#34;global.B_reg\u0026#34;) C = T.match_buffer(c, (16, 16), \u0026#34;float32\u0026#34;, offset_factor=16, scope=\u0026#34;global.accumulator\u0026#34;) sa = T.int32(16)#T.var(\u0026#34;int32\u0026#34;) sb = T.int32(16)#T.var(\u0026#34;int32\u0026#34;) sc = T.int32(16)#T.var(\u0026#34;int32\u0026#34;) with T.block(\u0026#34;root\u0026#34;): T.reads(C[0:16, 0:16], A[0:16, 0:16], B[0:16, 0:16]) T.writes(C[0:16, 0:16]) T.evaluate( T.call_extern(\u0026#34;float32\u0026#34;, \u0026#34;tmm16\u0026#34;, C.access_ptr(\u0026#34;w\u0026#34;), A.access_ptr(\u0026#34;r\u0026#34;), B.access_ptr(\u0026#34;r\u0026#34;), sa, sb, sc) ) tvm.tir.TensorIntrin.register(\u0026#34;tmm16\u0026#34;, tmm16_desc, tmm16_impl) é¦–å…ˆæˆ‘ä»¬ç”¨ decompose_reduction å°† C_global_accumulator çš„åˆå§‹åŒ–å’Œæ›´æ–°éƒ¨åˆ†åˆ†å¼€æˆ T.block(\u0026quot;matmul_init\u0026quot;) å’Œ T.block(\u0026quot;matmul_o_update\u0026quot;)\nsch.decompose_reduction(block_mm, k) sch.mod.show() #--------------------------------------------- @I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), B: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), C: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): A_global_A_reg = T.alloc_buffer((1024, 1024), scope=\u0026#34;global.A_reg\u0026#34;) B_global_B_reg = T.alloc_buffer((1024, 1024), scope=\u0026#34;global.B_reg\u0026#34;) C_global_accumulator = T.alloc_buffer((1024, 1024), scope=\u0026#34;global.accumulator\u0026#34;) for i_0, j_0 in T.grid(64, 64): with T.block(\u0026#34;matmul_o_init\u0026#34;): vi_o, vj_o = T.axis.remap(\u0026#34;SS\u0026#34;, [i_0, j_0]) T.reads() T.writes(C_global_accumulator[vi_o * 16:vi_o * 16 + 16, vj_o * 16:vj_o * 16 + 16]) for i_1, j_1 in T.grid(16, 16): with T.block(\u0026#34;matmul_init\u0026#34;): vi_i_init, vj_i_init = T.axis.remap(\u0026#34;SS\u0026#34;, [i_1, j_1]) T.reads() T.writes(C_global_accumulator[vi_o * 16 + vi_i_init, vj_o * 16 + vj_i_init]) C_global_accumulator[vi_o * 16 + vi_i_init, vj_o * 16 + vj_i_init] = T.float32(0.0) for k_0 in range(64): for ax0, ax1 in T.grid(16, 16): with T.block(\u0026#34;A_global.A_reg\u0026#34;): v0 = T.axis.spatial(1024, i_0 * 16 + ax0) v1 = T.axis.spatial(1024, k_0 * 16 + ax1) T.reads(A[v0, v1]) T.writes(A_global_A_reg[v0, v1]) A_global_A_reg[v0, v1] = A[v0, v1] for ax0, ax1 in T.grid(16, 16): with T.block(\u0026#34;B_global.B_reg\u0026#34;): v0 = T.axis.spatial(1024, j_0 * 16 + ax0) v1 = T.axis.spatial(1024, k_0 * 16 + ax1) T.reads(B[v0, v1]) T.writes(B_global_B_reg[v0, v1]) B_global_B_reg[v0, v1] = B[v0, v1] with T.block(\u0026#34;matmul_o_update\u0026#34;): vi_o, vj_o, vk_o = T.axis.remap(\u0026#34;SSR\u0026#34;, [i_0, j_0, k_0]) T.reads(C_global_accumulator[vi_o * 16:vi_o * 16 + 16, vj_o * 16:vj_o * 16 + 16], A_global_A_reg[vi_o * 16:vi_o * 16 + 16, vk_o * 16:vk_o * 16 + 16], B_global_B_reg[vj_o * 16:vj_o * 16 + 16, vk_o * 16:vk_o * 16 + 16]) T.writes(C_global_accumulator[vi_o * 16:vi_o * 16 + 16, vj_o * 16:vj_o * 16 + 16]) for i_1, j_1, k_1 in T.grid(16, 16, 16): with T.block(\u0026#34;matmul\u0026#34;): vi_i, vj_i, vk_i = T.axis.remap(\u0026#34;SSR\u0026#34;, [i_1, j_1, k_1]) T.reads(C_global_accumulator[vi_o * 16 + vi_i, vj_o * 16 + vj_i], A_global_A_reg[vi_o * 16 + vi_i, vk_o * 16 + vk_i], B_global_B_reg[vj_o * 16 + vj_i, vk_o * 16 + vk_i]) T.writes(C_global_accumulator[vi_o * 16 + vi_i, vj_o * 16 + vj_i]) C_global_accumulator[vi_o * 16 + vi_i, vj_o * 16 + vj_i] = C_global_accumulator[vi_o * 16 + vi_i, vj_o * 16 + vj_i] + A_global_A_reg[vi_o * 16 + vi_i, vk_o * 16 + vk_i] * B_global_B_reg[vj_o * 16 + vj_i, vk_o * 16 + vk_i] for ax0, ax1 in T.grid(16, 16): with T.block(\u0026#34;C_global.accumulator\u0026#34;): v0 = T.axis.spatial(1024, i_0 * 16 + ax0) v1 = T.axis.spatial(1024, j_0 * 16 + ax1) T.reads(C_global_accumulator[v0, v1]) T.writes(C[v0, v1]) C[v0, v1] = C_global_accumulator[v0, v1] ç„¶åæˆ‘ä»¬è°ƒç”¨ tensorizeï¼Œå°† block_mmï¼ˆå¯¹åº”äº matmul_o_update block ï¼‰æ˜ å°„åˆ° tmm16_impl. è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ T.call_extern æ¥è°ƒç”¨ç¯å¢ƒä¸­çš„å¤–éƒ¨å‡½æ•°ã€‚ ä¸‹æ¸¸ç¼–è¯‘æ­¥éª¤å¯ä»¥è½»æ¾åœ°å°†å®ç°æ˜ å°„åˆ°å®ç°æ“ä½œçš„æŒ‡ä»¤ã€‚æˆ–è€…æˆ‘ä»¬å¯ä»¥å°† tmm16 æ˜ å°„åˆ°å®ç°è¿™ç§å¼ é‡åŒ–è®¡ç®—çš„å¾®å†…æ ¸ã€‚ ä»¥ä¸‹ä»£ç æ˜¾ç¤ºäº†å¦‚ä½•é€šè¿‡å¤–éƒ¨ C++ ä»£ç æ‰§è¡Œæ­¤æ“ä½œã€‚\nå…·ä½“å®ç°æ­¥éª¤å¦‚ä¸‹:\nå®šä¹‰ C++ é£æ ¼çš„ tmm16 å‡½æ•°: è¿™ä¸ªå‡½æ•°å®ç°äº†ä¸€ä¸ª 16x16 çŸ©é˜µä¹˜æ³•çš„è®¡ç®—é€»è¾‘ã€‚å®ƒæ¥å—ä¸‰ä¸ªè¾“å…¥å¼ é‡ aaã€bb å’Œ ccï¼Œä»¥åŠå¯¹åº”çš„æ­¥é•¿ stride_aã€stride_b å’Œ stride_cã€‚å‡½æ•°ä½¿ç”¨ä¸‰é‡å¾ªç¯æ‰§è¡ŒçŸ©é˜µä¹˜æ³•çš„è®¡ç®—,å°†ç»“æœç´¯åŠ åˆ° cc å¼ é‡ä¸­ã€‚ ä½¿ç”¨ TVM çš„ clang æ¨¡å—å°† C++ ä»£ç ç¼–è¯‘ä¸º LLVM IR ä»£ç : é¦–å…ˆåˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½• temp ç”¨äºå­˜å‚¨ç”Ÿæˆçš„ LLVM IR æ–‡ä»¶ã€‚ç„¶åè°ƒç”¨ clang.create_llvm() å‡½æ•°,ä¼ å…¥ C++ ä»£ç å­—ç¬¦ä¸² cc_codeã€‚create_llvm() å‡½æ•°ä¼šå°† C++ ä»£ç ç¼–è¯‘ä¸º LLVM IR ä»£ç ,å¹¶ä¿å­˜åˆ° ll_path æŒ‡å®šçš„æ–‡ä»¶ä¸­ã€‚æœ€åè¿”å›ç”Ÿæˆçš„ LLVM IR ä»£ç ã€‚ def tmm_kernel(): cc_code = \u0026#39;\u0026#39;\u0026#39; extern \u0026#34;C\u0026#34; int tmm16(float *cc, float *aa, float *bb, int stride_a, int stride_b, int stride_c) { for (int i = 0; i \u0026lt; 16; i++) { for (int j = 0; i \u0026lt; 16; j++) { for (int k = 0; k \u0026lt; 16; k++) { cc[i * stride_c + j] += aa[i * stride_a + k] * bb[j * stride_b + k]; } } } return 0; } \u0026#39;\u0026#39;\u0026#39; from tvm.contrib import utils, clang temp = utils.tempdir() ll_path = temp.relpath(\u0026#34;temp.ll\u0026#34;) # Create LLVM ir from c source code ll_code = clang.create_llvm(cc_code, output=ll_path) return ll_code è°ƒç”¨ sch.tensorize(block_mm, \u0026quot;tmm16\u0026quot;)æŠ¥é”™ï¼ŒåŸå› æœªçŸ¥ã€‚\nå‘ç”Ÿå¼‚å¸¸: TVMError TVMError: invalid unordered_map\u0026lt;K, T\u0026gt; key File \u0026#34;C:\\Users\\17725\\Desktop\\Machine Learning Compilation\\chapter7.py\u0026#34;, line 186, in \u0026lt;module\u0026gt; sch.tensorize(block_mm, \u0026#34;tmm16\u0026#34;) tvm._ffi.base.TVMError: TVMError: invalid unordered_map\u0026lt;K, T\u0026gt; key ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch7/","summary":"Personal notebook 7.","title":"TVM Learning (9)-GPU and Hardware Acceleration, Part 2"},{"content":"GPU Architecture å…¸å‹çš„ GPU åŒ…å«ä¸€ç³»åˆ—æµå¤šå¤„ç†å™¨ (Stream Multi-processor, SM)ï¼Œæ¯ä¸ªå¤šå¤„ç†å™¨éƒ½æœ‰è®¸å¤šå†…æ ¸ (core). GPU å…·æœ‰é«˜åº¦å¹¶è¡Œæ€§ï¼Œå¯ä»¥åŒæ—¶æ‰§è¡Œå¤šé¡¹ä»»åŠ¡ã€‚\nGPU Architecture\nè¦å¯¹ GPU è¿›è¡Œç¼–ç¨‹ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ç»„çº¿ç¨‹å— (thread blocks)ï¼Œæ¯ä¸ª thread æ˜ å°„åˆ°å•ä¸ªæ ¸å¿ƒï¼Œè€Œ block æ˜ å°„åˆ°æµå¼å¤šå¤„ç†å™¨ (SM)ã€‚\nGPU Programming\næˆ‘ä»¬ä»¥ä¸¤ä¸ªé•¿åº¦ä¸º1024çš„å‘é‡åŠ æ³• C=A+Bä¸ºä¾‹ï¼Œæˆ‘ä»¬å…ˆæŠŠå¤–å¾ªç¯ split æˆä¸¤éƒ¨åˆ†\n@tvm.script.ir_module class MyModuleVecAdd: @T.prim_func def main(A: T.Buffer[(1024, ), \u0026#34;float32\u0026#34;], B: T.Buffer[(1024, ), \u0026#34;float32\u0026#34;], C: T.Buffer[(1024, ), \u0026#34;float32\u0026#34;]) -\u0026gt; None: T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) for i in T.grid(1024): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.remap(\u0026#34;S\u0026#34;, [i]) C[vi] = A[vi] + B[vi] sch = tvm.tir.Schedule(MyModuleVecAdd) block_C = sch.get_block(\u0026#34;C\u0026#34;) i, = sch.get_loops(block=block_C) i0, i1 = sch.split(i, [None, 128]) sch.mod.show() å¾—åˆ°çš„ TensorIR å¦‚ä¸‹\n@I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024,), \u0026#34;float32\u0026#34;), B: T.Buffer((1024,), \u0026#34;float32\u0026#34;), C: T.Buffer((1024,), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i_0, i_1 in T.grid(8, 128): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(1024, i_0 * 128 + i_1) T.reads(A[vi], B[vi]) T.writes(C[vi]) C[vi] = A[vi] + B[vi] Build and Run the TensorIR Function on GPU ä¸€ä¸ªCUDAç¨‹åºçš„è®¡ç®—è¢«ç»„ç»‡æˆä¸‰å±‚æ¬¡ï¼šç½‘æ ¼ï¼ˆGridï¼‰ã€çº¿ç¨‹å—ï¼ˆBlockï¼‰å’Œçº¿ç¨‹ï¼ˆThreadï¼‰ã€‚ç½‘æ ¼æ˜¯ä¸€ä¸ªäºŒç»´çš„æ•°ç»„ï¼ŒåŒ…å«å¤šä¸ªçº¿ç¨‹å—ã€‚æ¯ä¸ªçº¿ç¨‹å—ä¹Ÿæ˜¯ä¸€ä¸ªäºŒç»´çš„æ•°ç»„ï¼ŒåŒ…å«å¤šä¸ªçº¿ç¨‹ã€‚æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œç›¸åŒçš„ä»£ç ï¼Œä½†æ˜¯åœ¨æ‰§è¡Œæ—¶å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ•°æ®ã€‚æ¯ä¸ªçº¿ç¨‹ç”±ä¸¤ä¸ªç´¢å¼•è¿›è¡Œè¡¨ç¤º threadIdx.xå’Œ blockIdx.x. åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæœ‰å¤šç»´çº¿ç¨‹ç´¢å¼•ï¼Œä½†è¿™é‡Œæˆ‘ä»¬ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œå°†å®ƒä»¬å›ºå®šä¸ºä¸€ç»´è¡¨ç¤ºã€‚\nGPU Thread Block\nsch.bind(i0, \u0026quot;blockIdx.x\u0026quot;) å°† i0 å¾ªç¯ç»‘å®šåˆ° GPU çš„ block ç´¢å¼•ï¼Œä»¥ä¾¿å°†è®¡ç®—åˆ†å‘åˆ°ä¸åŒçš„ GPU block ä¸Šã€‚ sch.bind(i1, \u0026quot;threadIdx.x\u0026quot;) å°† i1 å¾ªç¯ç»‘å®šåˆ° GPU çš„ thread ç´¢å¼•ï¼Œä»¥ä¾¿å°†è®¡ç®—åˆ†å‘åˆ°æ¯ä¸ª block å†…çš„ä¸åŒçš„ GPU thread ä¸Šã€‚ å¯ä»¥çœ‹åˆ°å¾ªç¯å˜é‡å˜æˆäº† T.thead_binding\nsch.bind(i0, \u0026#34;blockIdx.x\u0026#34;) sch.bind(i1, \u0026#34;threadIdx.x\u0026#34;) sch.mod.show() #-------------------------------- @I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024,), \u0026#34;float32\u0026#34;), B: T.Buffer((1024,), \u0026#34;float32\u0026#34;), C: T.Buffer((1024,), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i_0 in T.thread_binding(8, thread=\u0026#34;blockIdx.x\u0026#34;): for i_1 in T.thread_binding(128, thread=\u0026#34;threadIdx.x\u0026#34;): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(1024, i_0 * 128 + i_1) T.reads(A[vi], B[vi]) T.writes(C[vi]) C[vi] = A[vi] + B[vi] ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨GPUä¸Šæ„å»ºå¹¶æµ‹è¯•ç¨‹åºçš„æ­£ç¡®æ€§\nrt_mod = tvm.build(sch.mod, target=\u0026#34;cuda\u0026#34;) A_np = np.random.uniform(size=(1024,)).astype(\u0026#34;float32\u0026#34;) B_np = np.random.uniform(size=(1024,)).astype(\u0026#34;float32\u0026#34;) A_nd = tvm.nd.array(A_np, tvm.cuda(0)) B_nd = tvm.nd.array(B_np, tvm.cuda(0)) C_nd = tvm.nd.array(np.zeros((1024,), dtype=\u0026#34;float32\u0026#34;), tvm.cuda(0)) rt_mod[\u0026#34;main\u0026#34;](A_nd, B_nd, C_nd) np.testing.assert_allclose(C_nd.numpy(), A_np + B_np) Window Sum Example æ»‘åŠ¨çª—å£æ±‚å’Œå¯ä»¥è¢«è§†ä¸ºæƒé‡ä¸º [1,1,1]çš„å·ç§¯ï¼Œå¯¹è¾“å…¥è¿›è¡Œæ»‘åŠ¨å¹¶å°†ä¸‰ä¸ªç›¸é‚»å€¼ç›¸åŠ ã€‚\nWindow Sum\nè·Ÿä¸Šä¸€èŠ‚ä¸€æ ·æˆ‘ä»¬å°†å¾ªç¯splitåæŠŠå¤–å¾ªç¯å’Œå†…å¾ªç¯åˆ†åˆ«bindåˆ°blockå’Œthreadä¸Š\n@tvm.script.ir_module class MyModuleWindowSum: @T.prim_func def main(A: T.Buffer[(1027, ), \u0026#34;float32\u0026#34;], B: T.Buffer[(1024, ), \u0026#34;float32\u0026#34;]) -\u0026gt; None: T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) for i in T.grid(1024): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.remap(\u0026#34;S\u0026#34;, [i]) B[vi] = A[vi] + A[vi + 1] + A[vi + 2] sch = tvm.tir.Schedule(MyModuleWindowSum) nthread = 128 block_C = sch.get_block(\u0026#34;C\u0026#34;) i, = sch.get_loops(block=block_C) i0, i1 = sch.split(i, [None, nthread]) sch.bind(i0, \u0026#34;blockIdx.x\u0026#34;) sch.bind(i1, \u0026#34;threadIdx.x\u0026#34;) å¯¹åº”çš„TensorIRå¦‚ä¸‹\n@I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1027,), \u0026#34;float32\u0026#34;), B: T.Buffer((1024,), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i_0 in T.thread_binding(8, thread=\u0026#34;blockIdx.x\u0026#34;): for i_1 in T.thread_binding(128, thread=\u0026#34;threadIdx.x\u0026#34;): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(1024, i_0 * 128 + i_1) T.reads(A[vi:vi + 3]) T.writes(B[vi]) B[vi] = A[vi] + A[vi + 1] + A[vi + 2] Cache in Shared Memory æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨çª—å£æ»‘åŠ¨çš„è¿‡ç¨‹ä¸­æœ‰ä¸€éƒ¨åˆ†æ•°æ®æ˜¯é‡å¤çš„ã€‚æ¯ä¸ª block åŒ…å«æ‰€æœ‰çº¿ç¨‹éƒ½å¯ä»¥åœ¨å—å†…è®¿é—®çš„å…±äº«å†…å­˜ (shared memory)ï¼Œä¸ºäº†é¿å…é‡å¤ä» global memory åŠ è½½ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠéƒ¨åˆ†æ•°æ®ç¼“å­˜åˆ°å…±äº«å†…å­˜ä¸Š\nB[vi] = A[vi] + A[vi + 1] + A[vi + 2] è¿™ä¸€è¡Œä»£ç ä¼šé‡å¤è¯»å– A ç¼“å†²åŒºä¸­çš„æ•°æ®ã€‚ sch.cache_read(block_C, read_buffer_index=0, storage_scope=\u0026quot;shared\u0026quot;) åˆ›å»ºäº†ä¸€ä¸ªåä¸º A_shared çš„å…±äº«å†…å­˜ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨ A ç¼“å†²åŒºä¸­çš„ä¸€éƒ¨åˆ†æ•°æ®ã€‚ block_C æŒ‡ç¤ºç¼“å­˜ä¸ C block ç›¸å…³è”ã€‚ read_buffer_index=0 æŒ‡ç¤ºç¼“å­˜ A ç¼“å†²åŒºï¼Œå› ä¸º A æ˜¯ C block ä¸­çš„ç¬¬ä¸€ä¸ªè¯»å–ç¼“å†²åŒºã€‚ storage_scope=\u0026quot;shared\u0026quot; æŒ‡ç¤ºç¼“å­˜ä½¿ç”¨å…±äº«å†…å­˜ã€‚ sch.compute_at(A_shared, i1) å°† A_shared çš„è®¡ç®—ä½ç½®è®¾ç½®ä¸º i1 å¾ªç¯ï¼Œè¿™æ„å‘³ç€ A_shared å°†åœ¨æ¯ä¸ª thread ä¸­è¢«è®¡ç®—ã€‚ sch = tvm.tir.Schedule(MyModuleWindowSum) nthread = 128 block_C = sch.get_block(\u0026#34;C\u0026#34;) i, = sch.get_loops(block=block_C) i0, i1 = sch.split(i, [None, nthread]) sch.bind(i0, \u0026#34;blockIdx.x\u0026#34;) sch.bind(i1, \u0026#34;threadIdx.x\u0026#34;) A_shared = sch.cache_read(block_C, read_buffer_index=0, storage_scope=\u0026#34;shared\u0026#34;) sch.compute_at(A_shared, i1) sch.mod.show() å˜æ¢åçš„TensorIRå¦‚ä¸‹ï¼Œä¸»è¦è¿›è¡Œäº†\nå…±äº«å†…å­˜åˆ†é…ï¼š åœ¨æ¯ä¸ª GPU block çš„å…±äº«å†…å­˜ä¸­åˆ†é…äº†ä¸€ä¸ªå¤§å°ä¸º (1027,) çš„ç¼“å†²åŒº A_sharedã€‚\nA_shared = T.alloc_buffer((1027,), scope=\u0026#34;shared\u0026#34;) æ·»åŠ äº†ä¸€ä¸ªæ–°çš„ block A_sharedï¼Œå¾ªç¯éå†æ¯ä¸ª threadå¹¶å°† A ç¼“å†²åŒºä¸­çš„æ•°æ®ç¼“å­˜åˆ° A_shared ä¸­ï¼š\nfor i_0 in T.thread_binding(8, thread=\u0026#34;blockIdx.x\u0026#34;): for i_1 in T.thread_binding(128, thread=\u0026#34;threadIdx.x\u0026#34;): for ax0 in range(130): with T.block(\u0026#34;A_shared\u0026#34;): v0 = T.axis.spatial(1027, i_0 * 128 + ax0) T.reads(A[v0]) T.writes(A_shared[v0]) A_shared[v0] = A[v0] ç æ›´æ–°äº† C block ä¸­çš„è®¡ç®—ï¼Œä½¿å…¶ä» A_shared ä¸­è¯»å–æ•°æ®ï¼š\nwith T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(1024, i_0 * 128 + i_1) T.reads(A_shared[vi:vi + 3]) T.writes(B[vi]) B[vi] = A_shared[vi] + A_shared[vi + 1] + A_shared[vi + 2] rane(130) çš„å‡ºç°æ˜¯å› ä¸ºéœ€è¦å°† A ç¼“å†²åŒºä¸­çš„æ•°æ®ç¼“å­˜åˆ°å…±äº«å†…å­˜ A_shared ä¸­ã€‚æ¯ä¸ª GPU block å¤„ç†çš„æ•°æ®èŒƒå›´æ˜¯ 128 ä¸ªå…ƒç´ ï¼Œå¯¹åº”äº i1 å¾ªç¯çš„èŒƒå›´ã€‚ç”±äºçª—å£æ±‚å’Œæ“ä½œéœ€è¦è®¿é—® A ç¼“å†²åŒºä¸­å½“å‰å…ƒç´ çš„ä¸‰ä¸ªç›¸é‚»å…ƒç´ ï¼Œå› æ­¤æ¯ä¸ª thread éœ€è¦è®¿é—® 128 + 2 = 130 ä¸ªå…ƒç´ ã€‚ä¸ºäº†ç¡®ä¿æ¯ä¸ª thread éƒ½èƒ½è®¿é—®åˆ°æ‰€éœ€çš„æ•°æ®ï¼Œéœ€è¦å°† A ç¼“å†²åŒºä¸­ 130 ä¸ªå…ƒç´ ç¼“å­˜åˆ° A_shared ä¸­ã€‚\n@I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1027,), \u0026#34;float32\u0026#34;), B: T.Buffer((1024,), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): A_shared = T.alloc_buffer((1027,), scope=\u0026#34;shared\u0026#34;) for i_0 in T.thread_binding(8, thread=\u0026#34;blockIdx.x\u0026#34;): for i_1 in T.thread_binding(128, thread=\u0026#34;threadIdx.x\u0026#34;): for ax0 in range(130): with T.block(\u0026#34;A_shared\u0026#34;): v0 = T.axis.spatial(1027, i_0 * 128 + ax0) T.reads(A[v0]) T.writes(A_shared[v0]) A_shared[v0] = A[v0] with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(1024, i_0 * 128 + i_1) T.reads(A_shared[vi:vi + 3]) T.writes(B[vi]) B[vi] = A_shared[vi] + A_shared[vi + 1] + A_shared[vi + 2] Get CUDA Source æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç›¸åº”çš„åº•å±‚ä»£ç ï¼ˆCUDA ï¼‰\nrt_mod = tvm.build(sch.mod, target=\u0026#34;cuda\u0026#34;) print(rt_mod.imported_modules[0].get_source()) ç”Ÿæˆçš„ä»£ç åŒ…å«ä¸¤éƒ¨åˆ†ï¼š\nåœ¨ä¸»æœº (CPU) ä¸Šçš„è°ƒç”¨ GPU ç¨‹åºçš„éƒ¨åˆ†ï¼› ç›¸åº”è®¡ç®—çš„ CUDA å†…æ ¸ã€‚ #if (((__CUDACC_VER_MAJOR__ == 11) \u0026amp;\u0026amp; (__CUDACC_VER_MINOR__ \u0026gt;= 4)) || \\ (__CUDACC_VER_MAJOR__ \u0026gt; 11)) #define TVM_ENABLE_L2_PREFETCH 1 #else #define TVM_ENABLE_L2_PREFETCH 0 #endif #ifdef _WIN32 using uint = unsigned int; using uchar = unsigned char; using ushort = unsigned short; using int64_t = long long; using uint64_t = unsigned long long; #else #define uint unsigned int #define uchar unsigned char #define ushort unsigned short #define int64_t long long #define uint64_t unsigned long long #endif extern \u0026#34;C\u0026#34; __global__ void __launch_bounds__(128) main_kernel(float* __restrict__ A, float* __restrict__ B); extern \u0026#34;C\u0026#34; __global__ void __launch_bounds__(128) main_kernel(float* __restrict__ A, float* __restrict__ B) { __shared__ float A_shared[130]; for (int ax0 = 0; ax0 \u0026lt; 130; ++ax0) { A_shared[ax0] = A[((((int)blockIdx.x) * 128) + ax0)]; } __syncthreads(); B[((((int)blockIdx.x) * 128) + ((int)threadIdx.x))] = ((A_shared[((int)threadIdx.x)] + A_shared[(((int)threadIdx.x) + 1)]) + A_shared[(((int)threadIdx.x) + 2)]); } Matrix Multiplication ä¸‹é¢æˆ‘ä»¬å¯¹åŸå§‹çš„ 1024*1024çš„çŸ©é˜µä¹˜æ³•è¿›è¡Œä¼˜åŒ–\n@tvm.script.ir_module class MyModuleMatmul: @T.prim_func def main(A: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], B: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], C: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;]) -\u0026gt; None: T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) for i, j, k in T.grid(1024, 1024, 1024): with T.block(\u0026#34;C\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): C[vi, vj] = 0.0 C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vk, vj] Local Blocking ä¸‹é¢çš„blocking å‡½æ•°ä½¿ç”¨äº†ä¸€ç§ç§°ä¸º å±€éƒ¨é˜»å¡ çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå°†çŸ©é˜µä¹˜æ³•çš„è®¡ç®—åˆ†è§£æˆæ›´å°çš„å—ï¼Œå¹¶ä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜æ¥æé«˜æ€§èƒ½ã€‚\nLocal Blocking\nå°†ä¸‰ä¸ªå¾ªç¯ iã€j å’Œ k åˆ†åˆ«æ‹†åˆ†æˆå¤šä¸ªå¾ªç¯ï¼Œä¾‹å¦‚å°† i æ‹†åˆ†æˆ i0ã€i1 å’Œ i2ï¼Œåˆ†åˆ«å¯¹åº”äº block ç´¢å¼•ã€thread ç´¢å¼•å’Œå±€éƒ¨å¾ªç¯ç´¢å¼•ã€‚ k1è¡¨ç¤ºçŸ©é˜µè®¡ç®—è¢«æ‹†åˆ†æˆå¤šå°‘ä¸ªå°å—ï¼Œk0å†³å®šäº†æ¯ä¸ªçº¿ç¨‹éœ€è¦è¿›è¡Œå¤šå°‘æ¬¡ç´¯åŠ æ“ä½œã€‚è°ƒæ•´å¾ªç¯çš„é¡ºåºï¼Œä»¥ä¾¿åœ¨æ¯ä¸ª thread ä¸­è®¡ç®— k0 å¾ªç¯çš„æ‰€æœ‰è¿­ä»£ï¼Œä»è€Œåˆ©ç”¨å…±äº«å†…å­˜ç¼“å­˜ã€‚ ä½¿ç”¨ cache_write å‡½æ•°åˆ›å»ºä¸€ä¸ªåä¸º C_local çš„å…±äº«å†…å­˜ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨ C çŸ©é˜µçš„ä¸­é—´ç»“æœã€‚ ä½¿ç”¨ reverse_compute_at å‡½æ•°å°† C_local çš„è®¡ç®—ä½ç½®è®¾ç½®ä¸º j1 å¾ªç¯ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ª thread ä¸­è®¡ç®— C_local çš„æ‰€æœ‰è¿­ä»£ï¼Œä»è€Œåˆ©ç”¨å…±äº«å†…å­˜ç¼“å­˜ã€‚ å°† i0 å’Œ j0 ç»‘å®šåˆ° GPU çš„ blockIdx.y å’Œ blockIdx.x çº¿ç¨‹ç´¢å¼•ï¼Œå°† i1 å’Œ j1 ç»‘å®šåˆ° GPU çš„ threadIdx.y å’Œ threadIdx.x çº¿ç¨‹ç´¢å¼•ã€‚ ä½¿ç”¨ unroll å‡½æ•°å±•å¼€ k1 å¾ªç¯ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ª thread ä¸­å±•å¼€è®¡ç®—ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚ ä½¿ç”¨ decompose_reduction å‡½æ•°åˆ†è§£ k0 å¾ªç¯ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ª thread ä¸­è®¡ç®— k0 å¾ªç¯çš„æ‰€æœ‰è¿­ä»£ï¼Œä»è€Œåˆ©ç”¨å…±äº«å†…å­˜ç¼“å­˜ã€‚ def blocking(sch: tvm.tir.Schedule, tile_local_y, tile_local_x, tile_block_y, tile_block_x, tile_k): block_C = sch.get_block(\u0026#34;C\u0026#34;) C_local = sch.cache_write(block_C, 0, \u0026#34;local\u0026#34;) i, j, k = sch.get_loops(block=block_C) i0, i1, i2 = sch.split(loop=i, factors=[None, tile_block_y, tile_local_y]) j0, j1, j2 = sch.split(loop=j, factors=[None, tile_block_x, tile_local_x]) k0, k1 = sch.split(loop=k, factors=[None, tile_k]) sch.unroll(k1) sch.reorder(i0, j0, i1, j1, k0, k1, i2, j2) sch.reverse_compute_at(C_local, j1) sch.bind(i0, \u0026#34;blockIdx.y\u0026#34;) sch.bind(j0, \u0026#34;blockIdx.x\u0026#34;) sch.bind(i1, \u0026#34;threadIdx.y\u0026#34;) sch.bind(j1, \u0026#34;threadIdx.x\u0026#34;) sch.decompose_reduction(block_C, k0) return sch è¿›è¡Œ Local Blocking åçš„TensorIRå¦‚ä¸‹\nsch = tvm.tir.Schedule(MyModuleMatmul) sch = blocking(sch, 8, 8, 8, 8, 4) sch.mod.show() #--------------------------------------- @I.ir_module class Module: @T.prim_func def main(A: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), B: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;), C: T.Buffer((1024, 1024), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): C_local = T.alloc_buffer((1024, 1024), scope=\u0026#34;local\u0026#34;) for i_0 in T.thread_binding(16, thread=\u0026#34;blockIdx.y\u0026#34;): for j_0 in T.thread_binding(16, thread=\u0026#34;blockIdx.x\u0026#34;): for i_1 in T.thread_binding(8, thread=\u0026#34;threadIdx.y\u0026#34;): for j_1 in T.thread_binding(8, thread=\u0026#34;threadIdx.x\u0026#34;): for i_2_init, j_2_init in T.grid(8, 8): with T.block(\u0026#34;C_init\u0026#34;): vi = T.axis.spatial(1024, i_0 * 64 + i_1 * 8 + i_2_init) vj = T.axis.spatial(1024, j_0 * 64 + j_1 * 8 + j_2_init) T.reads() T.writes(C_local[vi, vj]) C_local[vi, vj] = T.float32(0.0) for k_0 in range(256): for k_1 in T.unroll(4): for i_2, j_2 in T.grid(8, 8): with T.block(\u0026#34;C_update\u0026#34;): vi = T.axis.spatial(1024, i_0 * 64 + i_1 * 8 + i_2) vj = T.axis.spatial(1024, j_0 * 64 + j_1 * 8 + j_2) vk = T.axis.reduce(1024, k_0 * 4 + k_1) T.reads(C_local[vi, vj], A[vi, vk], B[vk, vj]) T.writes(C_local[vi, vj]) C_local[vi, vj] = C_local[vi, vj] + A[vi, vk] * B[vk, vj] for ax0, ax1 in T.grid(8, 8): with T.block(\u0026#34;C_local\u0026#34;): v0 = T.axis.spatial(1024, i_0 * 64 + i_1 * 8 + ax0) v1 = T.axis.spatial(1024, j_0 * 64 + j_1 * 8 + ax1) T.reads(C_local[v0, v1]) T.writes(C[v0, v1]) C[v0, v1] = C_local[v0, v1] Shared Memory Blocking ä¸Šé¢çš„æ–¹æ³•æ²¡æœ‰è€ƒè™‘ç›¸é‚» thread ä½äºåŒä¸€ä¸ª block ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬éœ€è¦çš„æ•°æ®åŠ è½½åˆ° shared memory ä¸­ã€‚\nShared Memory Blocking\ncache_read_and_coop_fetch å‡½æ•°è´Ÿè´£å°† A å’Œ B çŸ©é˜µä¸­çš„æ•°æ®åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­ã€‚é¦–å…ˆä½¿ç”¨ cache_read åˆ›å»ºä¸€ä¸ªå…±äº«å†…å­˜ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨ A æˆ– B çŸ©é˜µçš„æ•°æ®ã€‚ç„¶åä½¿ç”¨ compute_at å°†ç¼“å­˜çš„è®¡ç®—ä½ç½®è®¾ç½®ä¸º k0 å¾ªç¯ï¼Œåœ¨æ¯ä¸ªçº¿ç¨‹ä¸­è®¡ç®—ç¼“å­˜çš„æ‰€æœ‰è¿­ä»£ã€‚æœ€åä½¿ç”¨ split å’Œ vectorize å‡½æ•°å¯¹ k0 å¾ªç¯è¿›è¡Œå‘é‡åŒ–ï¼Œæé«˜åŠ è½½æ•°æ®çš„æ•ˆç‡ã€‚\ndef cache_read_and_coop_fetch(sch: tvm.tir.Schedule, block, nthread, read_idx, read_loc): read_cache = sch.cache_read(block=block, read_buffer_index=read_idx, storage_scope=\u0026#34;shared\u0026#34;) sch.compute_at(block=read_cache, loop=read_loc) # vertorized cooperative fetch inner0, inner1 = sch.get_loops(block=read_cache)[-2:] inner = sch.fuse(inner0, inner1) _, tx, vec = sch.split(loop=inner, factors=[None, nthread, 4]) sch.vectorize(vec) sch.bind(tx, \u0026#34;threadIdx.x\u0026#34;) å…¶ä½™çš„æ“ä½œå’Œ Local Blocking ä¸€è‡´\ndef blocking_with_shared(sch: tvm.tir.Schedule, tile_local_y, tile_local_x, tile_block_y, tile_block_x, tile_k): block_C = sch.get_block(\u0026#34;C\u0026#34;) C_local = sch.cache_write(block_C, 0, \u0026#34;local\u0026#34;) i, j, k = sch.get_loops(block=block_C) i0, i1, i2 = sch.split(loop=i, factors=[None, tile_block_y, tile_local_y]) j0, j1, j2 = sch.split(loop=j, factors=[None, tile_block_x, tile_local_x]) k0, k1 = sch.split(loop=k, factors=[None, tile_k]) sch.reorder(i0, j0, i1, j1, k0, k1, i2, j2) sch.reverse_compute_at(C_local, j1) sch.bind(i0, \u0026#34;blockIdx.y\u0026#34;) sch.bind(j0, \u0026#34;blockIdx.x\u0026#34;) tx = sch.fuse(i1, j1) sch.bind(tx, \u0026#34;threadIdx.x\u0026#34;) nthread = tile_block_y * tile_block_x cache_read_and_coop_fetch(sch, block_C, nthread, 0, k0) cache_read_and_coop_fetch(sch, block_C, nthread, 1, k0) sch.decompose_reduction(block_C, k0) return sch ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch6/","summary":"Personal notebook 6.","title":"TVM Learning (8)-GPU and Hardware Acceleration, Part 1"},{"content":"Model Preparation æˆ‘ä»¬é‡‡ç”¨Pytorchæ¡†æ¶å…ˆå®šä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ¥å—ä¸€æ‰¹å›¾åƒä¸ºè¾“å…¥ï¼Œç„¶åå¯¹å®ƒä»¬ä¾æ¬¡ä½œç”¨å·ç§¯å±‚ï¼Œæ¿€æ´»å±‚ï¼Œæ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚ï¼Œå¾—åˆ°åˆ†ç±»ç»“æœã€‚å¹¶ä»è®­ç»ƒå¥½çš„æ¨¡å‹é‡ŒåŠ è½½æƒé‡ï¼Œè¾“å…¥å›¾åƒæ¥è‡ªFashionMNISTæ•°æ®é›†ï¼Œshapeä¸º(1, 28, 28)ï¼Œæˆ‘ä»¬è®¾ç½®batch size=4.\n# Load the weight map from file. weight_map = pkl.load(open(\u0026#34;fasionmnist_mlp_assignment_params.pkl\u0026#34;, \u0026#34;rb\u0026#34;)) class_names = [\u0026#39;T-shirt/top\u0026#39;, \u0026#39;Trouser\u0026#39;, \u0026#39;Pullover\u0026#39;, \u0026#39;Dress\u0026#39;, \u0026#39;Coat\u0026#39;, \u0026#39;Sandal\u0026#39;, \u0026#39;Shirt\u0026#39;, \u0026#39;Sneaker\u0026#39;, \u0026#39;Bag\u0026#39;, \u0026#39;Ankle boot\u0026#39;] def pytorch_model(): list = [] list.append(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), bias=True)) list.append(nn.ReLU()) list.append(nn.MaxPool2d(kernel_size=(2, 2))) list.append(nn.Flatten()) list.append(nn.Linear(in_features=5408, out_features=100, bias=True)) list.append(nn.ReLU()) list.append(nn.Linear(in_features=100, out_features=10, bias=True)) list.append(nn.Softmax(dim=1)) model = nn.Sequential(*list).cuda() name_map = { \u0026#34;0.weight\u0026#34;: \u0026#34;conv2d_weight\u0026#34;, \u0026#34;0.bias\u0026#34;: \u0026#34;conv2d_bias\u0026#34;, \u0026#34;4.weight\u0026#34;: \u0026#34;linear0_weight\u0026#34;, \u0026#34;4.bias\u0026#34;: \u0026#34;linear0_bias\u0026#34;, \u0026#34;6.weight\u0026#34;: \u0026#34;linear1_weight\u0026#34;, \u0026#34;6.bias\u0026#34;: \u0026#34;linear1_bias\u0026#34;, } for name, param in model.named_parameters(): param.data = torch.from_numpy(weight_map[name_map[name]]).cuda() return model Ingest Model From Pytorch ä¹‹å‰æˆ‘ä»¬éƒ½æ˜¯æ‰‹å†™T.prim_funcæ¥å®ç°ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚ï¼Œè¿™æ ·å¾ˆå®¹æ˜“å‡ºé”™å¹¶ä¸”ä¸æ˜“äºè°ƒè¯•ã€‚TVMæä¾›äº† relax.BlockBuilderç±»å¯ä»¥ä»å¤´å¼€å§‹ä¸€æ­¥æ­¥æ„é€ ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªåä¸º emit_teçš„APIï¼Œå®ƒå¯ä»¥å°†ä¸€ä¸ªå¼ é‡è¡¨è¾¾å¼çš„ç®—å­æè¿°è½¬å˜æˆä¸€ä¸ªå¯¹åº”TensorIRå‡½æ•°çš„ call_tiræ“ä½œã€‚\nåœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œä¸ºäº†æ„å»ºä¸€ä¸ªæ‰§è¡Œå•ä¸ªReLUç®—å­çš„Relaxå‡½æ•°ï¼Œåœ¨ emit_te_exampleä¸­æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ª BlockBuilderå®ä¾‹ bbã€‚åŒæ ·å®šä¹‰äº†ä¸€ä¸ª 128x128å¤§å°çš„å¼ é‡å˜é‡ xï¼Œå®ƒå°†ä½œä¸ºReLUæ“ä½œçš„è¾“å…¥ï¼ˆåŒæ—¶ä¹Ÿæ˜¯Relaxå‡½æ•°çš„è¾“å…¥ï¼‰ã€‚\nåœ¨è¿™ä¹‹åï¼Œæˆ‘ä»¬ç”¨ with bb.function(name, [*input]) APIæ„å»ºä¸€ä¸ªä»¥ xä¸ºè¾“å…¥çš„Relaxå‡½æ•° mainã€‚ç„¶åæˆ‘ä»¬æ„å»ºä¸€ä¸ªdataflow blockã€‚åœ¨è¿™ä¸ªdataflow blocké‡Œï¼Œæˆ‘ä»¬é¦–å…ˆç”¨ emit_teç”Ÿæˆä¸€ä¸ªè°ƒç”¨ReLUç®—å­çš„ call_tirã€‚ emit_teä¼šåœ¨IRModuleä¸­ç”Ÿæˆä¸€ä¸ªåå­—ä¸º reluçš„TensorIRå‡½æ•°ï¼Œç„¶ååœ¨dataflow blockä¸­ç”Ÿæˆ call_tir(relu, (x,), (128, 128), dtype=\u0026quot;float32\u0026quot;)æ“ä½œã€‚call_tirä¹‹åæ˜¯å‡½æ•°è¿”å›ã€‚åœ¨è¿™ä¸€æ„é€ ä¹‹åï¼ŒBlockBuilderå®ä¾‹ bbåŒ…å«æ„å»ºå®Œçš„IRModuleï¼Œå¯ä»¥é€šè¿‡ bb.get()å¾—åˆ°ã€‚\nemit_te çš„ä½œç”¨æ˜¯å°†ä¸€ä¸ª TVM å¼ é‡è¡¨è¾¾å¼ï¼ˆTEï¼‰å‡½æ•°è½¬æ¢ä¸º Relax ä¸­çš„è°ƒç”¨èŠ‚ç‚¹ï¼ˆCall Nodeï¼‰ã€‚å®ƒå…è®¸ä½ åœ¨ Relax ä¸­ä½¿ç”¨ TE å‡½æ•°æ¥è¿›è¡Œè®¡ç®—ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„ TVM Script ä»£ç ã€‚è¯¥å‡½æ•°é¦–å…ˆå°† Relax è¡¨è¾¾å¼çš„å‚æ•°è½¬æ¢ä¸º TE å¼ é‡ã€‚ç„¶åï¼Œå®ƒè°ƒç”¨ TE å‡½æ•°ï¼Œå¹¶å°†è½¬æ¢åçš„ TE å¼ é‡ä½œä¸ºå‚æ•°ä¼ é€’ç»™å®ƒã€‚TE å‡½æ•°æ‰§è¡Œè®¡ç®—å¹¶è¿”å›ä¸€ä¸ª TE å¼ é‡æˆ– TE å¼ é‡åˆ—è¡¨ã€‚è¯¥å‡½æ•°å°†è¿”å›çš„ TE å¼ é‡è½¬æ¢ä¸º Relax ä¸­çš„ Call Node. æœ€åï¼Œå®ƒä½¿ç”¨ self.emit æ–¹æ³•å°†è°ƒç”¨èŠ‚ç‚¹æ·»åŠ åˆ° Relax BlockBuilder ä¸­ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ–°çš„ Relax å˜é‡ï¼Œè¯¥å˜é‡ç»‘å®šåˆ° Call Node.\nå‡½æ•°å‚æ•°ï¼š\nfunc: ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡ï¼Œå®ƒä»£è¡¨ä¸€ä¸ª TE å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å— Relax å¼ é‡ä½œä¸ºå‚æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ª TE å¼ é‡æˆ– TE å¼ é‡åˆ—è¡¨ã€‚ *args: funcè¾“å…¥çš„ä½ç½®å‚æ•° (relax Tensor)ã€‚ **kwargs: funcè¾“å…¥çš„çš„å…³é”®å­—å‚æ•° (relax Tensor)ã€‚ name_hint: å¯é€‰å‚æ•°ï¼Œç”¨äºæŒ‡å®šç”Ÿæˆçš„ PrimFunc çš„åç§°ã€‚ def relu(A): B = te.compute(shape=(128, 128), fcompute=lambda i, j: te.max(A[i, j], 0), name=\u0026#34;B\u0026#34;) return B def emit_te_example(): # relax.BlockBuilder can construct e2e models # step by step in an IRModule that starts empty. bb =relax.BlockBuilder() # relax.DynTensorType is the type assigned to tensors with a known dtype and unknown shape. x = relax.Var(\u0026#34;x\u0026#34;, relax.TensorStructInfo((128, 128), \u0026#34;float32\u0026#34;)) with bb.function(\u0026#34;main\u0026#34;, [x]): # construct a Relax function main with x as input with bb.dataflow(): # Emit a call node according to the te function # which should return a te tensor or a list of te tensors. lv0 = bb.emit_te(relu, x) gv = bb.emit_output(lv0) # mark the dataflow output bb.emit_func_output(gv) # mark the function output return bb.get() # return the constructed IRModule å¯ä»¥çœ‹åˆ°é€šè¿‡BlockBuilderç”Ÿæˆçš„IRModuleåŒ…å«äº†ReLUçš„TensorIRå®ç°å’Œä¸€ä¸ªå«æœ‰è°ƒç”¨ReLUå®ç°çš„ call_tirçš„Relaxå‡½æ•°\n@I.ir_module class Module: @T.prim_func(private=True) def relu(x: T.Buffer((T.int64(128), T.int64(128)), \u0026#34;float32\u0026#34;), B: T.Buffer((T.int64(128), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i, j in T.grid(T.int64(128), T.int64(128)): with T.block(\u0026#34;B\u0026#34;): v_i, v_j = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(x[v_i, v_j]) T.writes(B[v_i, v_j]) B[v_i, v_j] = T.max(x[v_i, v_j], T.float32(0.0)) @R.function def main(x: R.Tensor((128, 128), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((128, 128), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv = R.call_tir(cls.relu, (x,), out_sinfo=R.Tensor((128, 128), dtype=\u0026#34;float32\u0026#34;)) gv: R.Tensor((128, 128), dtype=\u0026#34;float32\u0026#34;) = lv R.output(gv) return gv Construct IRModule Equals to Pytorch æˆ‘ä»¬å¯ä»¥ç”¨ BlockBuilderå’Œ emit_teæ¥åˆ›å»ºä¸€ä¸ªå’Œä¹‹å‰å®šä¹‰çš„PyTorchæ¨¡å‹ç­‰ä»·çš„IRModuleã€‚é¦–å…ˆæˆ‘ä»¬è¦å®ç°è¿™äº›ç®—å­çš„å¼ é‡è¡¨è¾¾å¼è¿ç®—å‡½æ•°ã€‚\nåœ¨åŠ ä¸Šbiasçš„æ—¶å€™è¦å’Œreductionæ“ä½œåˆ†å¼€è¿›è¡Œï¼Œå³ä¸èƒ½åœ¨ä¸€ä¸ªte.computeé‡Œé¢è¿›è¡Œ te.sum+bias[...]çš„æ“ä½œï¼Œå¦åˆ™ä¼šæŠ¥é”™\nTVMError Traceback (most recent call last): File \u0026#34;D:\\Work\\tvm\\tvm0.18\\tvm\\src\\te\\operation\\compute_op.cc\u0026#34;, line 566 InternalError: Check failed: (0 == level_) is false: Reductions are only allowed at the top level of compute. Please create another tensor for further composition. def my_conv2d(X, K, B): # No padding, stride = 1 N, CI, H, W = X.shape CO, _, KH, KW = K.shape k = te.reduce_axis((0, CI), name=\u0026#34;k\u0026#34;) r = te.reduce_axis((0, KH), name=\u0026#34;r\u0026#34;) s = te.reduce_axis((0, KW), name=\u0026#34;s\u0026#34;) OH = (H - KH) + 1 OW = (W - KW) + 1 conv2d_te = te.compute(shape=(N, CO, OH, OW), fcompute=lambda n, co, oh, ow: te.sum( X[n, k, oh + r, ow + s] * K[co, k, r, s], axis=[k, r, s]), name=\u0026#34;conv2d\u0026#34;) out = te.compute(shape=(N, CO, OH, OW), fcompute=lambda n, co, oh, ow: conv2d_te[n, co, oh, ow] + B[0, co, 0, 0]) return out def my_relu(X): return te.compute(shape=X.shape, fcompute=lambda *i: te.max(X(*i), 0)) def my_maxpool2d(X, S): N, C, H, W = X.shape i = te.reduce_axis((0, S), name=\u0026#34;i\u0026#34;) j = te.reduce_axis((0, S), name=\u0026#34;j\u0026#34;) maxpool2d_te = te.compute(shape=(N, C, H//2, W//2), fcompute=lambda n, co, oh, ow: te.max( X[n, co, oh*S+i, ow*S+j], axis=[i, j]), name=\u0026#34;maxpool2d\u0026#34;) return maxpool2d_te def my_flatten(X): N, C, H, W = X.shape flatten_te = te.compute(shape=(N, C*H*W), fcompute=lambda n, i: X[n, i//(H*W), i//(W)%(H), i%(W)]) return flatten_te def my_linear(X, W, B=None): FO, FI = W.shape N, _ = X.shape fi = te.reduce_axis((0, FI), name=\u0026#34;FI\u0026#34;) linear_te = te.compute(shape=(N, FO), fcompute=lambda i, j: te.sum( X[i, fi] * W[j, fi], axis=fi)) if B is not None: out = te.compute(shape=(N, FO), fcompute=lambda i, j: B[0, j] + linear_te[i, j]) else: out = linear_te return out def my_softmax(X): N, C = X.shape c = te.reduce_axis((0, C), name=\u0026#34;c\u0026#34;) max_val = te.compute(shape=(N, ), fcompute=lambda i: te.max(X[i, c], axis=c)) exp_te = te.compute(shape=(N, C), fcompute=lambda i, j: te.exp(X[i, j] - max_val[i])) sum_exp_te = te.compute(shape=(N, ), fcompute=lambda i: te.sum(exp_te[i, c], axis=c)) softmax_te = te.compute(shape=(N, C), fcompute=lambda i, j: exp_te[i, j] / sum_exp_te[i]) return softmax_te ç„¶åæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨ BlockBuilderæ„å»ºIRModule\ndef create_model_via_emit_te(): batch_size = 4 input_shape = (batch_size, 1, 28, 28) # BCHW bb = relax.BlockBuilder() x = relax.Var(\u0026#34;x\u0026#34;, relax.TensorStructInfo(input_shape, \u0026#34;float32\u0026#34;)) conv2d_weight = relax.const(weight_map[\u0026#34;conv2d_weight\u0026#34;], \u0026#34;float32\u0026#34;) conv2d_bias = relax.const(weight_map[\u0026#34;conv2d_bias\u0026#34;].reshape(1, 32, 1, 1), \u0026#34;float32\u0026#34;) linear0_weight = relax.const(weight_map[\u0026#34;linear0_weight\u0026#34;], \u0026#34;float32\u0026#34;) linear0_bias = relax.const(weight_map[\u0026#34;linear0_bias\u0026#34;].reshape(1, 100), \u0026#34;float32\u0026#34;) linear1_weight = relax.const(weight_map[\u0026#34;linear1_weight\u0026#34;], \u0026#34;float32\u0026#34;) linear1_bias = relax.const(weight_map[\u0026#34;linear1_bias\u0026#34;].reshape(1, 10), \u0026#34;float32\u0026#34;) # Build the model using BlockBuilder with bb.function(\u0026#34;main\u0026#34;, [x]): with bb.dataflow(): gv_conv = bb.emit_te(my_conv2d, x, conv2d_weight, conv2d_bias) gv_relu1 = bb.emit_te(my_relu, gv_conv) gv_pool = bb.emit_te(my_maxpool2d, gv_relu1, 2) gv_flatten = bb.emit_te(my_flatten, gv_pool) gv_dense1 = bb.emit_te(my_linear, gv_flatten, linear0_weight, linear0_bias) gv_relu2 = bb.emit_te(my_relu, gv_dense1) gv_dense2 = bb.emit_te(my_linear, gv_relu2, linear1_weight, linear1_bias) gv_softmax = bb.emit_te(my_softmax, gv_dense2) out = bb.emit_output(gv_softmax) bb.emit_func_output(out) return bb.get() å¾—åˆ°çš„IRModuleçš„TensorIRå¦‚ä¸‹\nmod = create_model_via_emit_te() exec = relax.build(mod, \u0026#34;llvm\u0026#34;) dev = tvm.cpu() vm = relax.VirtualMachine(exec, dev) print(mod.script()) mod.script @I.ir_module class Module: @T.prim_func(private=True) def my_conv2d(x: T.Buffer((T.int64(4), T.int64(1), T.int64(28), T.int64(28)), \u0026#34;float32\u0026#34;), B: T.Buffer((T.int64(32), T.int64(1), T.int64(3), T.int64(3)), \u0026#34;float32\u0026#34;), C: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(4), T.int64(32), T.int64(26), T.int64(26)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): conv2d = T.alloc_buffer((T.int64(4), T.int64(32), T.int64(26), T.int64(26))) for n, co, oh, ow, k, r, s in T.grid(T.int64(4), T.int64(32), T.int64(26), T.int64(26), T.int64(1), T.int64(3), T.int64(3)): with T.block(\u0026#34;conv2d\u0026#34;): v_n, v_co, v_oh, v_ow, v_k, v_r, v_s = T.axis.remap(\u0026#34;SSSSRRR\u0026#34;, [n, co, oh, ow, k, r, s]) T.reads(x[v_n, v_k, v_oh + v_r, v_ow + v_s], B[v_co, v_k, v_r, v_s]) T.writes(conv2d[v_n, v_co, v_oh, v_ow]) with T.init(): conv2d[v_n, v_co, v_oh, v_ow] = T.float32(0.0) conv2d[v_n, v_co, v_oh, v_ow] = conv2d[v_n, v_co, v_oh, v_ow] + x[v_n, v_k, v_oh + v_r, v_ow + v_s] * B[v_co, v_k, v_r, v_s] for n, co, oh, ow in T.grid(T.int64(4), T.int64(32), T.int64(26), T.int64(26)): with T.block(\u0026#34;compute\u0026#34;): v_n, v_co, v_oh, v_ow = T.axis.remap(\u0026#34;SSSS\u0026#34;, [n, co, oh, ow]) T.reads(conv2d[v_n, v_co, v_oh, v_ow], C[T.int64(0), v_co, T.int64(0), T.int64(0)]) T.writes(compute[v_n, v_co, v_oh, v_ow]) compute[v_n, v_co, v_oh, v_ow] = conv2d[v_n, v_co, v_oh, v_ow] + C[T.int64(0), v_co, T.int64(0), T.int64(0)] @T.prim_func(private=True) def my_flatten(lv2: T.Buffer((T.int64(4), T.int64(32), T.int64(13), T.int64(13)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(4), T.int64(5408)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for n, i in T.grid(T.int64(4), T.int64(5408)): with T.block(\u0026#34;compute\u0026#34;): v_n, v_i = T.axis.remap(\u0026#34;SS\u0026#34;, [n, i]) T.reads(lv2[v_n, v_i // T.int64(169), v_i % T.int64(169) // T.int64(13), v_i % T.int64(13)]) T.writes(compute[v_n, v_i]) compute[v_n, v_i] = lv2[v_n, v_i // T.int64(169), v_i % T.int64(169) // T.int64(13), v_i % T.int64(13)] @T.prim_func(private=True) def my_linear(lv3: T.Buffer((T.int64(4), T.int64(5408)), \u0026#34;float32\u0026#34;), B: T.Buffer((T.int64(100), T.int64(5408)), \u0026#34;float32\u0026#34;), C: T.Buffer((T.int64(1), T.int64(100)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(4), T.int64(100)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): compute_1 = T.alloc_buffer((T.int64(4), T.int64(100))) for i, j, FI in T.grid(T.int64(4), T.int64(100), T.int64(5408)): with T.block(\u0026#34;compute\u0026#34;): v_i, v_j, v_FI = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, FI]) T.reads(lv3[v_i, v_FI], B[v_j, v_FI]) T.writes(compute_1[v_i, v_j]) with T.init(): compute_1[v_i, v_j] = T.float32(0.0) compute_1[v_i, v_j] = compute_1[v_i, v_j] + lv3[v_i, v_FI] * B[v_j, v_FI] for i, j in T.grid(T.int64(4), T.int64(100)): with T.block(\u0026#34;compute_1\u0026#34;): v_i, v_j = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(C[T.int64(0), v_j], compute_1[v_i, v_j]) T.writes(compute[v_i, v_j]) compute[v_i, v_j] = C[T.int64(0), v_j] + compute_1[v_i, v_j] @T.prim_func(private=True) def my_linear1(lv5: T.Buffer((T.int64(4), T.int64(100)), \u0026#34;float32\u0026#34;), B: T.Buffer((T.int64(10), T.int64(100)), \u0026#34;float32\u0026#34;), C: T.Buffer((T.int64(1), T.int64(10)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(4), T.int64(10)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): compute_1 = T.alloc_buffer((T.int64(4), T.int64(10))) for i, j, FI in T.grid(T.int64(4), T.int64(10), T.int64(100)): with T.block(\u0026#34;compute\u0026#34;): v_i, v_j, v_FI = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, FI]) T.reads(lv5[v_i, v_FI], B[v_j, v_FI]) T.writes(compute_1[v_i, v_j]) with T.init(): compute_1[v_i, v_j] = T.float32(0.0) compute_1[v_i, v_j] = compute_1[v_i, v_j] + lv5[v_i, v_FI] * B[v_j, v_FI] for i, j in T.grid(T.int64(4), T.int64(10)): with T.block(\u0026#34;compute_1\u0026#34;): v_i, v_j = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(C[T.int64(0), v_j], compute_1[v_i, v_j]) T.writes(compute[v_i, v_j]) compute[v_i, v_j] = C[T.int64(0), v_j] + compute_1[v_i, v_j] @T.prim_func(private=True) def my_maxpool2d(lv1: T.Buffer((T.int64(4), T.int64(32), T.int64(26), T.int64(26)), \u0026#34;float32\u0026#34;), maxpool2d: T.Buffer((T.int64(4), T.int64(32), T.int64(13), T.int64(13)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for n, co, oh, ow, i, j in T.grid(T.int64(4), T.int64(32), T.int64(13), T.int64(13), T.int64(2), T.int64(2)): with T.block(\u0026#34;maxpool2d\u0026#34;): v_n, v_co, v_oh, v_ow, v_i, v_j = T.axis.remap(\u0026#34;SSSSRR\u0026#34;, [n, co, oh, ow, i, j]) T.reads(lv1[v_n, v_co, v_oh * T.int64(2) + v_i, v_ow * T.int64(2) + v_j]) T.writes(maxpool2d[v_n, v_co, v_oh, v_ow]) with T.init(): maxpool2d[v_n, v_co, v_oh, v_ow] = T.float32(-340282346638528859811704183484516925440.0) maxpool2d[v_n, v_co, v_oh, v_ow] = T.max(maxpool2d[v_n, v_co, v_oh, v_ow], lv1[v_n, v_co, v_oh * T.int64(2) + v_i, v_ow * T.int64(2) + v_j]) @T.prim_func(private=True) def my_relu(lv: T.Buffer((T.int64(4), T.int64(32), T.int64(26), T.int64(26)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(4), T.int64(32), T.int64(26), T.int64(26)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i0, i1, i2, i3 in T.grid(T.int64(4), T.int64(32), T.int64(26), T.int64(26)): with T.block(\u0026#34;compute\u0026#34;): v_i0, v_i1, v_i2, v_i3 = T.axis.remap(\u0026#34;SSSS\u0026#34;, [i0, i1, i2, i3]) T.reads(lv[v_i0, v_i1, v_i2, v_i3]) T.writes(compute[v_i0, v_i1, v_i2, v_i3]) compute[v_i0, v_i1, v_i2, v_i3] = T.max(lv[v_i0, v_i1, v_i2, v_i3], T.float32(0.0)) @T.prim_func(private=True) def my_relu1(lv4: T.Buffer((T.int64(4), T.int64(100)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(4), T.int64(100)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i0, i1 in T.grid(T.int64(4), T.int64(100)): with T.block(\u0026#34;compute\u0026#34;): v_i0, v_i1 = T.axis.remap(\u0026#34;SS\u0026#34;, [i0, i1]) T.reads(lv4[v_i0, v_i1]) T.writes(compute[v_i0, v_i1]) compute[v_i0, v_i1] = T.max(lv4[v_i0, v_i1], T.float32(0.0)) @T.prim_func(private=True) def my_softmax(lv6: T.Buffer((T.int64(4), T.int64(10)), \u0026#34;float32\u0026#34;), compute: T.Buffer((T.int64(4), T.int64(10)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): compute_1 = T.alloc_buffer((T.int64(4),)) compute_2 = T.alloc_buffer((T.int64(4), T.int64(10))) compute_3 = T.alloc_buffer((T.int64(4),)) for i, c in T.grid(T.int64(4), T.int64(10)): with T.block(\u0026#34;compute\u0026#34;): v_i, v_c = T.axis.remap(\u0026#34;SR\u0026#34;, [i, c]) T.reads(lv6[v_i, v_c]) T.writes(compute_1[v_i]) with T.init(): compute_1[v_i] = T.float32(-340282346638528859811704183484516925440.0) compute_1[v_i] = T.max(compute_1[v_i], lv6[v_i, v_c]) for i, j in T.grid(T.int64(4), T.int64(10)): with T.block(\u0026#34;compute_1\u0026#34;): v_i, v_j = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(lv6[v_i, v_j], compute_1[v_i]) T.writes(compute_2[v_i, v_j]) compute_2[v_i, v_j] = T.exp(lv6[v_i, v_j] - compute_1[v_i]) for i, c in T.grid(T.int64(4), T.int64(10)): with T.block(\u0026#34;compute_2\u0026#34;): v_i, v_c = T.axis.remap(\u0026#34;SR\u0026#34;, [i, c]) T.reads(compute_2[v_i, v_c]) T.writes(compute_3[v_i]) with T.init(): compute_3[v_i] = T.float32(0.0) compute_3[v_i] = compute_3[v_i] + compute_2[v_i, v_c] for i, j in T.grid(T.int64(4), T.int64(10)): with T.block(\u0026#34;compute_3\u0026#34;): v_i, v_j = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(compute_2[v_i, v_j], compute_3[v_i]) T.writes(compute[v_i, v_j]) compute[v_i, v_j] = compute_2[v_i, v_j] / compute_3[v_i] @R.function def main(x: R.Tensor((4, 1, 28, 28), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((4, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv = R.call_tir(cls.my_conv2d, (x, metadata[\u0026#34;relax.expr.Constant\u0026#34;][0], metadata[\u0026#34;relax.expr.Constant\u0026#34;][1]), out_sinfo=R.Tensor((4, 32, 26, 26), dtype=\u0026#34;float32\u0026#34;)) lv1 = R.call_tir(cls.my_relu, (lv,), out_sinfo=R.Tensor((4, 32, 26, 26), dtype=\u0026#34;float32\u0026#34;)) lv2 = R.call_tir(cls.my_maxpool2d, (lv1,), out_sinfo=R.Tensor((4, 32, 13, 13), dtype=\u0026#34;float32\u0026#34;)) lv3 = R.call_tir(cls.my_flatten, (lv2,), out_sinfo=R.Tensor((4, 5408), dtype=\u0026#34;float32\u0026#34;)) lv4 = R.call_tir(cls.my_linear, (lv3, metadata[\u0026#34;relax.expr.Constant\u0026#34;][2], metadata[\u0026#34;relax.expr.Constant\u0026#34;][3]), out_sinfo=R.Tensor((4, 100), dtype=\u0026#34;float32\u0026#34;)) lv5 = R.call_tir(cls.my_relu1, (lv4,), out_sinfo=R.Tensor((4, 100), dtype=\u0026#34;float32\u0026#34;)) lv6 = R.call_tir(cls.my_linear1, (lv5, metadata[\u0026#34;relax.expr.Constant\u0026#34;][4], metadata[\u0026#34;relax.expr.Constant\u0026#34;][5]), out_sinfo=R.Tensor((4, 10), dtype=\u0026#34;float32\u0026#34;)) lv7 = R.call_tir(cls.my_softmax, (lv6,), out_sinfo=R.Tensor((4, 10), dtype=\u0026#34;float32\u0026#34;)) gv: R.Tensor((4, 10), dtype=\u0026#34;float32\u0026#34;) = lv7 R.output(gv) return gv æˆ‘ä»¬å¯ä»¥ä¸Pytorchæ¨¡å‹çš„æ‰§è¡Œç»“æœè¿›è¡Œæ¯”è¾ƒæ¥éªŒè¯æ­£ç¡®æ€§ã€‚\ndef build_mod(mod): exec = relax.vm.build(mod, \u0026#34;llvm\u0026#34;) dev = tvm.cpu() vm = relax.VirtualMachine(exec, dev) return vm def check_equivalence(mod, torch_model, test_loader): torch_model.eval() with torch.no_grad(): rt_mod = build_mod(mod) for data, label in test_loader: data, label = data.cpu(), label.cpu() output_from_pytorch = torch_model(data).numpy() output_from_relax = rt_mod[\u0026#34;main\u0026#34;](tvm.nd.array(data, tvm.cpu())).numpy() tvm.testing.assert_allclose(output_from_pytorch, output_from_relax, rtol=1e-4) test_data = torchvision.datasets.FashionMNIST( \u0026#34;./data\u0026#34;, download=True, train=False, transform=transforms.Compose([transforms.ToTensor()]) ) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False) mod = create_model_via_emit_te() torch_model = pytorch_model() check_equivalence(mod, torch_model, test_loader) ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch5/","summary":"Personal notebook 5.","title":"TVM Learning (6)-Exercise of End to End Model Execution"},{"content":"Transform a Primitive Tensor Function ä¹‹å‰å·²ç»è®²è¿‡å¦‚ä½•é€šè¿‡ tir.Scheduleå¯¹T.prim_funcè¿›è¡Œå˜æ¢ï¼Œä»ä»¥çŸ©é˜µä¹˜æ³•ä¸ºä¾‹\n@tvm.script.ir_module class MyModule: @T.prim_func def main(A: T.Buffer((128, 128), \u0026#34;float32\u0026#34;), # type: ignore B: T.Buffer((128, 128), \u0026#34;float32\u0026#34;), # type: ignore C: T.Buffer((128, 128), \u0026#34;float32\u0026#34;)): # type: ignore T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) for i, j, k in T.grid(128, 128, 128): with T.block(\u0026#34;C\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): C[vi, vj] = 0.0 C[vi, vj] += A[vi, vk] * B[vk, vj] å¯¹å…¶è¿›è¡Œ split, reorderå’Œ decompose_reductionå˜æ¢å¾—åˆ°çš„TensorIRå¦‚ä¸‹ã€‚\né€šè¿‡ä»¥ä¸Šå˜æ¢åï¼ŒçŸ©é˜µä¹˜æ³•çš„æ‰§è¡Œæ—¶é—´å‡å°‘æ˜¯ç”±äºï¼š\nå¾ªç¯æ‹†åˆ† (sch.split) ï¼š å°† jå¾ªç¯æ‹†åˆ†æˆäº†ä¸¤ä¸ªå¾ªç¯ï¼šj_0å’Œ j_1ï¼Œå…¶ä¸­ j_1çš„å› å­ä¸º4ï¼ˆå†…å±‚å¾ªç¯ï¼‰ã€‚ æé«˜æ•°æ®çš„å±€éƒ¨æ€§ï¼Œå› ä¸ºè¾ƒå°çš„æ•°æ®å—ä¼šåœ¨æ›´çŸ­çš„æ—¶é—´å†…è¢«é¢‘ç¹è®¿é—®ï¼Œä»è€Œæ›´å¥½åœ°åˆ©ç”¨ç¼“å­˜ã€‚ å¾ªç¯é‡æ’ (sch.reorder) ï¼š å°†å¾ªç¯çš„é¡ºåºè°ƒæ•´ä¸º i, j_0, k, j_1ï¼Œæ„å‘³ç€å¤–å±‚å¾ªç¯å…ˆéå† iå’Œ j_0ï¼Œå†…å±‚å¾ªç¯å†éå† kå’Œ j_1ã€‚ ä¼˜å…ˆè€ƒè™‘äº†æ•°æ®åœ¨å¯„å­˜å™¨æˆ–ç¼“å­˜ä¸­çš„é‡ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å†…å±‚å¾ªç¯æ“ä½œæœŸé—´ AçŸ©é˜µä¸­çš„å…ƒç´ ã€‚ åˆ†è§£å½’çº¦ (sch.decompose_reduction) ï¼š å°†å¯¹ kçš„å½’çº¦æ“ä½œåˆ†è§£ä¸ºåˆå§‹åŒ–é˜¶æ®µå’Œæ›´æ–°é˜¶æ®µï¼Œæœ‰åŠ©äºå°†è®¡ç®—çš„ä¸¤ä¸ªé˜¶æ®µï¼ˆå³è®¾ç½®åˆå§‹å€¼å’Œå®é™…å½’çº¦ï¼‰åˆ†å¼€ã€‚ æé«˜å¹¶è¡ŒåŒ–çš„æœºä¼šï¼Œå¹¶ä¸”å…è®¸æ›´å¥½åœ°åˆ©ç”¨å‘é‡åŒ–æŒ‡ä»¤æˆ–å…¶ä»–ç¡¬ä»¶ä¼˜åŒ–ã€‚ def schedule_mm(sch: tvm.tir.Schedule, jfactor=4): block_C = sch.get_block(\u0026#34;C\u0026#34;, \u0026#34;main\u0026#34;) i, j, k = sch.get_loops(block=block_C) j_0, j_1 = sch.split(loop=j, factors=[None, jfactor]) sch.reorder(i, j_0, k, j_1) sch.decompose_reduction(block_C, k) return sch sch = tvm.tir.Schedule(MyModule) sch = schedule_mm(sch) sch.mod.show() #----------------------------------- @I.ir_module class Module: @T.prim_func def main(A: T.Buffer((128, 128), \u0026#34;float32\u0026#34;), B: T.Buffer((128, 128), \u0026#34;float32\u0026#34;), C: T.Buffer((128, 128), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i, j_0 in T.grid(128, 32): for j_1_init in range(4): with T.block(\u0026#34;C_init\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j_0 * 4 + j_1_init) T.reads() T.writes(C[vi, vj]) C[vi, vj] = T.float32(0.0) for k, j_1 in T.grid(128, 4): with T.block(\u0026#34;C_update\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j_0 * 4 + j_1) vk = T.axis.reduce(128, k) T.reads(C[vi, vj], A[vi, vk], B[vk, vj]) T.writes(C[vi, vj]) C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vk, vj] æˆ‘ä»¬å¯ä»¥æ¯”è¾ƒå˜æ¢å‰åçš„è®¡ç®—ç”¨æ—¶\na_np = np.random.rand(128, 128).astype(dtype) b_np = np.random.rand(128, 128).astype(dtype) c_mm = a_np @ b_np a_nd = tvm.nd.array(a_np) b_nd = tvm.nd.array(b_np) c_nd = tvm.nd.empty((128, 128), dtype=\u0026#34;float32\u0026#34;) # Before transformation lib = tvm.build(MyModule, target= \u0026#34;llvm\u0026#34;) f_timer_before = lib.time_evaluator(\u0026#34;main\u0026#34;, tvm.cpu()) print(\u0026#34;Time cost of MyModule: %.3f ms\u0026#34; % (f_timer_before(a_nd, b_nd, c_nd).mean * 1000)) #Time cost of MyModule: 1.365 ms # After transformation lib = tvm.build(sch.mod, target=\u0026#34;llvm\u0026#34;) f_timer_after = lib.time_evaluator(\u0026#34;main\u0026#34;, tvm.cpu()) print(\u0026#34;Time cost of MyModule=\u0026gt;schedule_mm: %.3f ms\u0026#34; % (f_timer_after(a_nd, b_nd, c_nd).mean * 1000)) # Time cost of MyModule=\u0026gt;schedule_mm: 1.041 ms Transformation Trace é™¤äº† sch.modå­—æ®µï¼Œtir.Scheduleè¿˜æä¾›äº†ä¸€ä¸ªè·Ÿè¸ªå­—æ®µ sch.traceï¼Œç”¨äºæ˜¾ç¤ºå˜æ¢IRModuleçš„æ­¥éª¤ã€‚\nprint(sch.trace) #------------------------------------------- def apply_trace(sch: tir.Schedule) -\u0026gt; None: b0 = sch.get_block(name=\u0026#34;C\u0026#34;, func_name=\u0026#34;main\u0026#34;) l1, l2, l3 = sch.get_loops(block=b0) l4, l5 = sch.split(loop=l2, factors=[None, 4], preserve_unit_iters=True, disable_predication=False) sch.reorder(l1, l4, l3, l5) b6 = sch.decompose_reduction(block=b0, loop=l3) Stochastic Schedule Transformation åœ¨ä¹‹å‰çš„å˜æ¢ä¸­ï¼Œæˆ‘ä»¬éƒ½æ˜¯æŒ‡å®šè¿™äº›å‡½æ•°çš„è¾“å…¥å‚æ•°ã€‚å®é™…æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥éšæœºæ€§ï¼Œæ ¹æ®ä¸åŒå˜æ¢çš„è¾“å…¥å‚æ•°å¾—å‡ºçš„æ‰§è¡Œæ—¶é—´æ¥é€‰æ‹©æ€§èƒ½æœ€å¥½çš„ä¸€ä¸ªã€‚\nsample_perfect_tileå‡½æ•°å¯ä»¥è®¡ç®—ä»»åŠ¡ä¸­çš„ç‰¹å®šå¾ªç¯é‡‡æ ·æœ€ä¼˜çš„åˆ‡åˆ†ç­–ç•¥ã€‚\nè¾“å…¥å‚æ•°ï¼š\nloopï¼šè¦åˆ‡åˆ†çš„å¾ªç¯ã€‚ nï¼šè¦åˆ‡åˆ†æˆå‡ ä»½ã€‚ max_innermost_factorï¼šå…è®¸åœ¨æœ€å†…å±‚å¾ªç¯ä¸­é‡‡æ ·çš„æœ€å¤§åˆ‡åˆ†å¤§å°ã€‚æ­¤å‚æ•°æœ‰åŠ©äºæ§åˆ¶å¹³é“ºçš„ç²’åº¦ã€‚ decisionï¼šä¸€ä¸ªå¯é€‰çš„æ•´æ•°åˆ—è¡¨ï¼Œè¡¨ç¤ºé¢„å…ˆç¡®å®šçš„åˆ‡åˆ†å†³ç­–ã€‚å¦‚æœæä¾›ï¼Œå‡½æ•°å°†ä½¿ç”¨æ­¤å†³ç­–è€Œä¸æ˜¯é‡‡æ ·ã€‚ ä¸‹é¢å‡½æ•° stochastic_schedule_mmå’Œ schedule_mmå”¯ä¸€çš„åŒºåˆ«æ˜¯æŒ‡å®š j_factorsé‡‡ç”¨çš„æ˜¯éšæœºçš„ç­–ç•¥ã€‚\ndef stochastic_schedule_mm(sch: tvm.tir.Schedule): block_C = sch.get_block(\u0026#34;C\u0026#34;, \u0026#34;main\u0026#34;) i, j, k = sch.get_loops(block=block_C) j_factors = sch.sample_perfect_tile(loop=j, n=2) # tvm.tir.expr.Var j_0, j_1 = sch.split(loop=j, factors=j_factors) sch.reorder(i, j_0, k, j_1) sch.decompose_reduction(block_C, k) return sch å¯ä»¥å‘ç°ï¼Œå®ƒæ˜¯å¯¹åŸæ¥çš„ç¡®å®šæ€§å˜æ¢çš„æ³›åŒ–ç‰ˆæœ¬ï¼Œåªæ˜¯å¤šäº†ä¸¤ä¸ªå…ƒç´ ï¼š\næ¥è‡ª sample_perfect_tile çš„éšæœºå˜é‡ï¼Œä»¥åŠæˆ‘ä»¬åœ¨ç¤ºä¾‹ä¸­æ²¡æœ‰æ¶‰åŠçš„å…¶ä»–é‡‡æ ·æ“ä½œã€‚ æ ¹æ®éšæœºå˜é‡é‡‡å–è¡ŒåŠ¨çš„ scheduleæ“ä½œã€‚ j_factors ä¸­çš„å…ƒç´ ä¸æ˜¯æ•´æ•°ã€‚ç›¸å®ƒä»¬æ˜¯ç¬¦å·å˜é‡ï¼ŒæŒ‡çš„æ˜¯æ­£åœ¨é‡‡æ ·çš„éšæœºå˜é‡ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™äº›å˜é‡ä¼ é€’ç»™è½¬æ¢ APIï¼Œä»¥æŒ‡å®šfactors. è°ƒç”¨ stochastic_schedule_mmåçš„traceå¦‚ä¸‹\nsch = tvm.tir.Schedule(MyModule) sch = stochastic_schedule_mm(sch) print(sch.trace) #------------------------------------------------------ def apply_trace(sch: tir.Schedule) -\u0026gt; None: b0 = sch.get_block(name=\u0026#34;C\u0026#34;, func_name=\u0026#34;main\u0026#34;) l1, l2, l3 = sch.get_loops(block=b0) v4, v5 = sch.sample_perfect_tile(loop=l2, n=2, max_innermost_factor=16, decision=[64, 2]) l6, l7 = sch.split(loop=l2, factors=[v4, v5], preserve_unit_iters=True, disable_predication=False) sch.reorder(l1, l6, l3, l7) b8 = sch.decompose_reduction(block=b0, loop=l3) Search Over Stochastic Transformations stochastic_schedule_mmå®é™…ä¸Šä¼šæ ¹æ®æ¯ä¸ªé‡‡æ ·æ­¥éª¤çš„å®é™…å†³å®šï¼Œåˆ›å»ºä¸€ä¸ªç¨‹åºçš„æœç´¢ç©ºé—´ã€‚\nTransformation Search Space\næˆ‘ä»¬éœ€è¦ä¸€ç§æœç´¢ç®—æ³•èƒ½æ‰¾åˆ°æ€§èƒ½æœ€å¥½çš„å˜æ¢ã€‚ä¸‹é¢çš„å‡½æ•°ä½¿ç”¨æœ€ç›´æ¥çš„æœç´¢ç®—æ³•\u0026ndash;éšæœºæœç´¢ã€‚å®ƒå°è¯•é‡å¤è¿è¡Œ stochastic_schedule_mmï¼Œå¾—åˆ°ä¸€ä¸ªè½¬æ¢åçš„IR moduleï¼Œè¿è¡Œbenchmarkï¼Œç„¶åå°†æ€§èƒ½æœ€å¥½çš„IR moduleè®°å½•ä¸‹æ¥ã€‚\ndef random_search(mod: tvm.IRModule, num_trails=5): best_result = None best_sch = False for i in range(num_trails): sch = stochastic_schedule_mm(tvm.tir.Schedule(mod)) lib = tvm.build(sch.mod, target=\u0026#34;llvm\u0026#34;) f_timer_after = lib.time_evaluator(\u0026#34;main\u0026#34;, tvm.cpu()) result = f_timer_after(a_nd, b_nd, c_nd).mean print(\u0026#34;=====Attempt %d, time-cost: %.3f ms====\u0026#34; % (i, result * 1000)) print(sch.trace) # book keep the best result so far if best_result is None or result \u0026lt; best_result: best_result = result best_sch = sch return best_sch å®é™…æƒ…å†µä¸‹ä¼šä½¿ç”¨æ›´é«˜çº§çš„ç®—æ³•ã€‚è¿˜éœ€è¦æä¾›é¢å¤–çš„å·¥å…·ï¼Œä¾‹å¦‚åœ¨è¿œç¨‹è®¾å¤‡ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ç­‰ã€‚TVM çš„ meta_schedule API æä¾›äº†è¿™äº›åŠŸèƒ½ã€‚\nmeta_scheduleæ˜¯ä¸€ä¸ªå‘½åç©ºé—´ï¼Œç”¨äºæ”¯æŒåœ¨å¯èƒ½çš„å˜æ¢ç©ºé—´ä¸­è¿›è¡Œæœç´¢ã€‚\nè·¨å¤šä¸ªè¿›ç¨‹çš„å¹¶è¡ŒåŸºå‡†æµ‹è¯•ã€‚ ä½¿ç”¨ cost modelï¼Œé¿å…æ¯æ¬¡éƒ½è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ åœ¨ trace ä¸Šè¿›è¡Œè¿›åŒ–æœç´¢ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½éšæœºå–æ ·ã€‚ tune_tir API ä»ä½¿ç”¨éšæœºå˜æ¢æ¥æŒ‡å®šå¥½ç¨‹åºçš„æœç´¢ç©ºé—´å¹¶åœ¨æœç´¢ç©ºé—´å†…æ‰¾åˆ°ä¼˜åŒ–çš„æ–¹æ¡ˆã€‚\ndatabase = ms.tune_tir( mod=MyModule, target=\u0026#34;llvm --num-cores=1\u0026#34;, max_trials_global=64, num_trials_per_iter=64, space=ms.space_generator.ScheduleFn(stochastic_schedule_mm), work_dir=\u0026#34;./tune_tmp\u0026#34;, task_name=\u0026#34;main\u0026#34; ) sch_tuned = ms.tir_integration.compile_tir(database, MyModule, target=\u0026#34;llvm --num-cores=1\u0026#34;) print(sch_tuned.trace) clang error on Windows ä¸çŸ¥é“ä¸ºä½•Windowsä¸Šè¿è¡Œclangä¼šå‡ºé”™\nLocalRunner: An exception occurred Traceback (most recent call last): File \u0026#34;D:\\Work\\Anaconda\\envs\\tvm-build\\lib\\site-packages\\tvm-0.18.dev0-py3.9-win-amd64.egg\\tvm\\exec\\popen_worker.py\u0026#34;, line 87, in main result = fn(*args, **kwargs) File \u0026#34;D:\\Work\\Anaconda\\envs\\tvm-build\\lib\\site-packages\\tvm-0.18.dev0-py3.9-win-amd64.egg\\tvm\\meta_schedule\\runner\\local_runner.py\u0026#34;, line 148, in _worker_func rt_mod = tvm.runtime.load_module(artifact_path) File \u0026#34;D:\\Work\\Anaconda\\envs\\tvm-build\\lib\\site-packages\\tvm-0.18.dev0-py3.9-win-amd64.egg\\tvm\\runtime\\module.py\u0026#34;, line 696, in load_module _cc.create_shared(path + \u0026#34;.so\u0026#34;, files) File \u0026#34;D:\\Work\\Anaconda\\envs\\tvm-build\\lib\\site-packages\\tvm-0.18.dev0-py3.9-win-amd64.egg\\tvm\\contrib\\cc.py\u0026#34;, line 96, in create_shared _windows_compile(output, objects, options, cwd, ccache_env) File \u0026#34;D:\\Work\\Anaconda\\envs\\tvm-build\\lib\\site-packages\\tvm-0.18.dev0-py3.9-win-amd64.egg\\tvm\\contrib\\cc.py\u0026#34;, line 415, in _windows_compile raise RuntimeError(msg) RuntimeError: Compilation error: clang -O2 -shared -o C:\\Users\\17725\\AppData\\Local\\Temp\\tmp96lbzaxg\\tvm_tmp_mod.tar.so C:\\Users\\17725\\AppData\\Local\\Temp\\tmp96lbzaxg\\tvm_tmp_mod\\lib0.o ld.lld: error: undefined symbol: _fltused \u0026gt;\u0026gt;\u0026gt; referenced by C:\\Users\\17725\\AppData\\Local\\Temp\\tmp96lbzaxg\\tvm_tmp_mod\\lib0.o clang: error: linker command failed with exit code 1 (use -v to see invocation) ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch4/","summary":"Personal notebook 4.","title":"TVM Learning (5)-Automatic Program Optimization"},{"content":"E2E Model Integration æˆ‘ä»¬ä»¥ä¸‹å›¾ä¸­çš„ MLP ç½‘ç»œä¸ºä¾‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œå¹¶ä¸”çœç•¥äº†æœ€åçš„ Softmax å±‚ã€‚\nMLP Model\nåˆ©ç”¨é«˜çº§Numpyçš„å®ç°å¦‚ä¸‹\ndef numpy_mlp(data, w0, b0, w1, b1): lv0 = data @ w0.T + b0 lv1 = np.maximum(lv0, 0) lv2 = lv1 @ w1.T + b1 return lv2 ä¸ºäº†æ–¹ä¾¿è¯´æ˜åº•å±‚è®¡ç®—è¿‡ç¨‹ï¼Œç”¨ Low-level Numpy è¿›è¡Œé‡å†™åå¦‚ä¸‹\ndef lnumpy_linear0(X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray): Y = np.empty((1, 128), dtype=\u0026#34;float32\u0026#34;) for i in range(1): for j in range(128): for k in range(784): if k == 0: Y[i, j] = 0 Y[i, j] = Y[i, j] + X[i, k] * W[j, k] for i in range(1): for j in range(128): Z[i, j] = Y[i, j] + B[j] def lnumpy_relu0(X: np.ndarray, Y: np.ndarray): for i in range(1): for j in range(128): Y[i, j] = np.maximum(X[i, j], 0) def lnumpy_linear1(X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray): Y = np.empty((1, 10), dtype=\u0026#34;float32\u0026#34;) for i in range(1): for j in range(10): for k in range(128): if k == 0: Y[i, j] = 0 Y[i, j] = Y[i, j] + X[i, k] * W[j, k] for i in range(1): for j in range(10): Z[i, j] = Y[i, j] + B[j] def lnumpy_mlp(data, w0, b0, w1, b1): lv0 = np.empty((1, 128), dtype=\u0026#34;float32\u0026#34;) lnumpy_linear0(data, w0, b0, lv0) lv1 = np.empty((1, 128), dtype=\u0026#34;float32\u0026#34;) lnumpy_relu0(lv0, lv1) out = np.empty((1, 10), dtype=\u0026#34;float32\u0026#34;) lnumpy_linear1(lv1, w1, b1, out) return out Constructing an E2E IRModule in TVMScript åŒæ ·å¯ä»¥ç”¨ TVMScript æ„å»ºè¿™ä¸ªç½‘ç»œçš„ IRModuleï¼Œåªä¸è¿‡è¿™æ¬¡é™¤äº†è¦ç”¨ Primitive Tensor Function (@T.prim_function) è¿˜è¦ç”¨ Relax Function (@R.function) æ¥æŠ½è±¡ç¥ç»ç½‘ç»œçš„è®¡ç®—è¿‡ç¨‹ã€‚\n@tvm.script.ir_module class MyModule: @T.prim_func def relu0(X: T.Buffer((1, 128), \u0026#34;float32\u0026#34;), Y: T.Buffer((1, 128), \u0026#34;float32\u0026#34;)): # function attr dict T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;relu0\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) for i, j in T.grid(1, 128): with T.block(\u0026#34;Y\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) Y[vi, vj] = T.max(X[vi, vj], T.float32(0)) @T.prim_func def linear0(X: T.Buffer((1, 784), \u0026#34;float32\u0026#34;), W: T.Buffer((128, 784), \u0026#34;float32\u0026#34;), B: T.Buffer((128,), \u0026#34;float32\u0026#34;), Z: T.Buffer((1, 128), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;linear0\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) Y = T.alloc_buffer((1, 128), \u0026#34;float32\u0026#34;) for i, j, k in T.grid(1, 128, 784): with T.block(\u0026#34;Y\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): Y[vi, vj] = T.float32(0) Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk] for i, j in T.grid(1, 128): with T.block(\u0026#34;Z\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) Z[vi, vj] = Y[vi, vj] + B[vj] @T.prim_func def linear1(X: T.Buffer((1, 128), \u0026#34;float32\u0026#34;), W: T.Buffer((10, 128), \u0026#34;float32\u0026#34;), B: T.Buffer((10,), \u0026#34;float32\u0026#34;), Z: T.Buffer((1, 10), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;linear1\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) Y = T.alloc_buffer((1, 10), \u0026#34;float32\u0026#34;) for i, j, k in T.grid(1, 10, 128): with T.block(\u0026#34;Y\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): Y[vi, vj] = T.float32(0) Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk] for i, j in T.grid(1, 10): with T.block(\u0026#34;Z\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) Z[vi, vj] = Y[vi, vj] + B[vj] @R.function def main(x: R.Tensor((1, 784), \u0026#34;float32\u0026#34;), w0: R.Tensor((128, 784), \u0026#34;float32\u0026#34;), b0: R.Tensor((128,), \u0026#34;float32\u0026#34;), w1: R.Tensor((10, 128), \u0026#34;float32\u0026#34;), b1: R.Tensor((10,), \u0026#34;float32\u0026#34;)): with R.dataflow(): cls = MyModule lv0 = R.call_tir(cls.linear0, (x, w0, b0), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv1 = R.call_tir(cls.relu0, (lv0,), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) out = R.call_tir(cls.linear1, (lv1, w1, b1), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(out) return out Computational Graph View è¯¥ç½‘ç»œçš„è®¡ç®—å›¾å¦‚ä¸‹ï¼Œè®¡ç®—å›¾é€šå¸¸å…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼š\næ¡†çš„æ¯ä¸ªè¾“å…¥è¾¹å¯¹åº”äºæ“ä½œçš„è¾“å…¥ï¼› æ¯ä¸ªå‡ºè¾¹å¯¹åº”äºæ“ä½œçš„è¾“å‡ºï¼› å¯ä»¥ä»»æ„è°ƒæ•´æ“ä½œçš„é¡ºåºï¼Œåªè¦ä¿è¯è¾¹çš„æ‹“æ‰‘æ’åºï¼ˆTopological Orderï¼‰æ²¡æœ‰æ”¹å˜ã€‚ Topological Order æ‹“æ‰‘æ’åºæ˜¯é’ˆå¯¹æœ‰å‘æ— ç¯å›¾ (DAG) çš„ä¸€ç§æ’åºç®—æ³•ï¼Œå®ƒå°†å›¾ä¸­çš„èŠ‚ç‚¹æ’æˆä¸€ä¸ªçº¿æ€§åºåˆ—ï¼Œæ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š\nå¯¹äºå›¾ä¸­çš„ä»»æ„ä¸€æ¡è¾¹ (u, v)ï¼ŒèŠ‚ç‚¹ u åœ¨æ’åºä¸­éƒ½å‡ºç°åœ¨èŠ‚ç‚¹ v ä¹‹å‰ã€‚ Example DAG\nè¿›è¡Œæ‹“æ‰‘æ’åºè¾ƒå¸¸ç”¨çš„æ–¹æ³•ï¼š\nä» DAG å›¾ä¸­é€‰æ‹©ä¸€ä¸ª æ²¡æœ‰å‰é©±ï¼ˆå³å…¥åº¦ä¸º0ï¼‰çš„é¡¶ç‚¹å¹¶è¾“å‡ºã€‚ ä»å›¾ä¸­åˆ é™¤è¯¥é¡¶ç‚¹å’Œæ‰€æœ‰ä»¥å®ƒä¸ºèµ·ç‚¹çš„æœ‰å‘è¾¹ã€‚ é‡å¤ 1 å’Œ 2 ç›´åˆ°å½“å‰çš„ DAG å›¾ä¸ºç©ºæˆ– å½“å‰å›¾ä¸­ä¸å­˜åœ¨æ— å‰é©±çš„é¡¶ç‚¹ä¸ºæ­¢ ã€‚åä¸€ç§æƒ…å†µè¯´æ˜æœ‰å‘å›¾ä¸­å¿…ç„¶å­˜åœ¨ç¯ã€‚ Topological Sort Algorithm\nComputational Graph View\nR.call_tir R.call_tir æ­£å¦‚åå­—ä¸€æ ·è°ƒç”¨ä¸€ä¸ª T.prim_func å¹¶è¿”å›è®¡ç®—ç»“æœã€‚å®ƒçš„è¡Œä¸ºç”¨Numpyè¡¨ç¤ºå¦‚ä¸‹ï¼Œå…ˆæ ¹æ® shapeå’Œ dtypeå¼€è¾Ÿè¾“å‡ºæ•°æ®çš„å†…å­˜ç©ºé—´ï¼Œç„¶åè°ƒç”¨å‡½æ•°ï¼Œæœ€åè¿”å›è¾“å‡ºç»“æœã€‚R.call_tirå‡½æ•°çš„è¾“å…¥æ˜¯è¿™ç§å½¢å¼çš„åŸå› æ˜¯ T.prim_funcå‡½æ•°çš„è¾“å…¥éœ€è¦æˆ‘ä»¬å…ˆä¸ºè¾“å‡ºç»“æœå¼€è¾Ÿå†…å­˜ï¼Œç§°ä¸º ç›®æ ‡ä¼ é€’ (destination passing) ã€‚\ndef lnumpy_call_tir(prim_func, inputs, shape, dtype): res = np.empty(shape, dtype=dtype) prim_func(*inputs, res) return res ä¸ºäº†è®©ç¨‹åºæ‰§è¡Œå…·æœ‰è®¡ç®—å›¾çš„æ€§è´¨ï¼Œæˆ‘ä»¬é‡‡ç”¨è¿™ç§æ–¹å¼è¿›è¡Œè°ƒç”¨\ndef lnumpy_mlp_with_call_tir(data, w0, b0, w1, b1): lv0 = lnumpy_call_tir(lnumpy_linear0, (data, w0, b0), (1, 128), dtype=\u0026#34;float32\u0026#34;) lv1 = lnumpy_call_tir(lnumpy_relu0, (lv0, ), (1, 128), dtype=\u0026#34;float32\u0026#34;) out = lnumpy_call_tir(lnumpy_linear1, (lv1, w1, b1), (1, 10), dtype=\u0026#34;float32\u0026#34;) return out Dataflow Block ç†æƒ³æƒ…å†µä¸‹ï¼Œè®¡ç®—å›¾ä¸­çš„æ“ä½œåº”ä¸º side-effect freeï¼Œå³ä¸€ä¸ªå‡½æ•°åªä»å…¶è¾“å…¥ä¸­è¯»å–å¹¶é€šè¿‡å…¶è¾“å‡ºè¿”å›ç»“æœï¼Œä¸ä¼šæ”¹å˜ç¨‹åºçš„å…¶ä»–éƒ¨åˆ†ï¼ˆä¾‹å¦‚é€’å¢å…¨å±€è®¡æ•°å™¨ï¼‰ã€‚å¦‚æœè¦å¼•å…¥åŒ…å« side-effect çš„æ“ä½œï¼Œå°±éœ€è¦å®šä¹‰å¤šä¸ªdataflow blockï¼Œåœ¨ä»–ä»¬ä¹‹å¤–æˆ–è€…ä¹‹é—´è¿›è¡Œæ“ä½œã€‚\n@R.function def main(x: Tensor((1, 784), \u0026#34;float32\u0026#34;), w0: Tensor((128, 784), \u0026#34;float32\u0026#34;), b0: Tensor((128,), \u0026#34;float32\u0026#34;), w1: Tensor((10, 128), \u0026#34;float32\u0026#34;), b1: Tensor((10,), \u0026#34;float32\u0026#34;)): with R.dataflow(): lv0 = R.call_tir(cls.linear0, (x, w0, b0), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) gv0 = R.call_tir(cls.relu0, (lv0,), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) R.output(gv0) gv1 = R.alloc_tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) # side-effect operation with R.dataflow(): out = R.call_tir(cls.linear1, (gv0, gv1, b0), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) R.output(out) return out Build and Run the Model è¯¥ç½‘ç»œå¯¹åº”çš„TensorIRå¦‚ä¸‹\n@I.ir_module class Module: @T.prim_func def linear0( X: T.Buffer((1, 784), \u0026#34;float32\u0026#34;), W: T.Buffer((128, 784), \u0026#34;float32\u0026#34;), B: T.Buffer((128,), \u0026#34;float32\u0026#34;), Z: T.Buffer((1, 128), \u0026#34;float32\u0026#34;), ): T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;linear0\u0026#34;, \u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): Y = T.alloc_buffer((1, 128)) for i, j, k in T.grid(1, 128, 784): with T.block(\u0026#34;Y\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) T.reads(X[vi, vk], W[vj, vk]) T.writes(Y[vi, vj]) with T.init(): Y[vi, vj] = T.float32(0) Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk] for i, j in T.grid(1, 128): with T.block(\u0026#34;Z\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(Y[vi, vj], B[vj]) T.writes(Z[vi, vj]) Z[vi, vj] = Y[vi, vj] + B[vj] @T.prim_func def linear1( X: T.Buffer((1, 128), \u0026#34;float32\u0026#34;), W: T.Buffer((10, 128), \u0026#34;float32\u0026#34;), B: T.Buffer((10,), \u0026#34;float32\u0026#34;), Z: T.Buffer((1, 10), \u0026#34;float32\u0026#34;), ): T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;linear1\u0026#34;, \u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): Y = T.alloc_buffer((1, 10)) for i, j, k in T.grid(1, 10, 128): with T.block(\u0026#34;Y\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) T.reads(X[vi, vk], W[vj, vk]) T.writes(Y[vi, vj]) with T.init(): Y[vi, vj] = T.float32(0) Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk] for i, j in T.grid(1, 10): with T.block(\u0026#34;Z\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(Y[vi, vj], B[vj]) T.writes(Z[vi, vj]) Z[vi, vj] = Y[vi, vj] + B[vj] @T.prim_func def relu0(X: T.Buffer((1, 128), \u0026#34;float32\u0026#34;), Y: T.Buffer((1, 128), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;relu0\u0026#34;, \u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i, j in T.grid(1, 128): with T.block(\u0026#34;Y\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(X[vi, vj]) T.writes(Y[vi, vj]) Y[vi, vj] = T.max(X[vi, vj], T.float32(0)) @R.function def main( x: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;), w0: R.Tensor((128, 784), dtype=\u0026#34;float32\u0026#34;), b0: R.Tensor((128,), dtype=\u0026#34;float32\u0026#34;), w1: R.Tensor((10, 128), dtype=\u0026#34;float32\u0026#34;), b1: R.Tensor((10,), dtype=\u0026#34;float32\u0026#34;), ) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv0 = R.call_tir(cls.linear0, (x, w0, b0), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv1 = R.call_tir(cls.relu0, (lv0,), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) out = R.call_tir(cls.linear1, (lv1, w1, b1), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(out) return out æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢æ–¹å¼æ¥æ„é€  virtual machine. relax.buildè¿”å›ä¸€ä¸ª tvm.relax.Executableå¯¹è±¡ï¼Œç„¶åå°±å¯ä»¥åœ¨æŒ‡å®šçš„ç¡¬ä»¶ä¸Šåˆ›å»ºvirtual machine æ¥æ‰§è¡Œè®¡ç®—å›¾ã€‚\nex = relax.build(MyModule, target=\u0026#34;llvm\u0026#34;) vm = relax.VirtualMachine(ex, tvm.cpu()) nd_res = vm[\u0026#34;main\u0026#34;](data_nd, nd_params[\u0026#34;w0\u0026#34;], nd_params[\u0026#34;b0\u0026#34;], nd_params[\u0026#34;w1\u0026#34;], nd_params[\u0026#34;b1\u0026#34;]) Integrate Existing Libraries in the Environment é™¤äº†ç”¨ T.prim_funcæ„é€ RelaxIRï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä»ç°æœ‰çš„æ·±åº¦å­¦ä¹ åº“çš„å‡½æ•°æ¥æ„é€ ã€‚\nè¿™æ˜¯é€šè¿‡ R.call_dps_packedæ¥å®Œæˆçš„ï¼Œå®ƒç”¨äºè°ƒç”¨ä¸€ä¸ªç›®æ ‡ä¼ é€’é£æ ¼ (Destination-Passing Style) çš„æ‰“åŒ…å‡½æ•° (Packed Function)ï¼Œå¹¶è¿”å›è¾“å‡ºç»“æœã€‚\nç›®æ ‡ä¼ é€’é£æ ¼ (Destination-Passing Style): ç›®æ ‡ä¼ é€’é£æ ¼æ˜¯ä¸€ç§å‡½æ•°è°ƒç”¨æ–¹å¼ï¼Œå…¶ä¸­å‡½æ•°çš„è¾“å‡ºå‚æ•°ä½œä¸ºå‡½æ•°å‚æ•°ä¼ é€’ç»™å‡½æ•°ã€‚ æ‰“åŒ…å‡½æ•° (Packed Function): æ‰“åŒ…å‡½æ•°æ˜¯ä¸€ç§å‡½æ•°ï¼Œå…¶è¾“å…¥å‚æ•°å’Œè¾“å‡ºå‚æ•°éƒ½è¢«æ‰“åŒ…æˆä¸€ä¸ªç»“æ„ä½“ã€‚ çº¯å‡½æ•° (Pure Function): çº¯å‡½æ•°æ˜¯æŒ‡ä¸äº§ç”Ÿå‰¯ä½œç”¨çš„å‡½æ•°ï¼Œå³å‡½æ•°çš„æ‰§è¡Œç»“æœåªä¾èµ–äºè¾“å…¥å‚æ•°ï¼Œå¹¶ä¸”ä¸ä¼šä¿®æ”¹ä»»ä½•å…¨å±€çŠ¶æ€ã€‚\nç¤ºä¾‹ï¼š\nR.call_dps_packed(\u0026#34;env.linear\u0026#34;, (x, w0, b0), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) å‡½æ•°å‚æ•°ï¼š\nfunc: å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–è¡¨è¾¾å¼ï¼Œè¡¨ç¤ºç›®æ ‡ä¼ é€’é£æ ¼çš„å‡½æ•°ã€‚å¦‚æœ func æ˜¯å­—ç¬¦ä¸²ï¼Œå®ƒå°†è¢«è½¬æ¢ä¸º ExternFunc å¯¹è±¡ã€‚ args: è¡¨è¾¾å¼ï¼Œè¡¨ç¤ºè¾“å…¥å‚æ•°ã€‚å¦‚æœ args æ˜¯å•ä¸ªè¡¨è¾¾å¼ï¼Œå®ƒå°†è¢«åŒ…è£…æˆä¸€ä¸ª RxTuple å¯¹è±¡ã€‚ out_sinfo: å¯ä»¥æ˜¯ TensorStructInfo å¯¹è±¡æˆ– TensorStructInfo å¯¹è±¡åˆ—è¡¨ï¼Œè¡¨ç¤º call_dps_packed å‡½æ•°è¾“å‡ºçš„ç»“æ„ä¿¡æ¯ã€‚æ¯ä¸ª TensorStructInfo å¯¹è±¡è¡¨ç¤ºä¸€ä¸ªè¿”å›çš„å¼ é‡çš„ç»“æ„ä¿¡æ¯ã€‚ å‡½æ•°è¿”å›å€¼ï¼š\nret: Call å¯¹è±¡ï¼Œè¡¨ç¤º call_dps_packed æ“ä½œç¬¦çš„è°ƒç”¨èŠ‚ç‚¹ã€‚ Registering Runtime Function ä¸ºäº†èƒ½å¤Ÿæ‰§è¡Œè°ƒç”¨å¤–éƒ¨å‡½æ•°çš„ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦æ³¨å†Œç›¸åº”çš„å‡½æ•°ã€‚ä¸‹é¢è¿™æ®µä»£ç æ³¨å†Œäº†ä¸¤ä¸ªè‡ªå®šä¹‰å‡½æ•°ï¼Œåˆ†åˆ«ç”¨äºå®ç°çº¿æ€§å±‚å’Œ ReLU æ¿€æ´»å‡½æ•°ã€‚\n@tvm.register_func(\u0026quot;env.linear\u0026quot;, override=True): ä½¿ç”¨ @tvm.register_func è£…é¥°å™¨å°† torch_linear å‡½æ•°æ³¨å†Œä¸ºåä¸º \u0026quot;env.linear\u0026quot; çš„ TVM å‡½æ•°ã€‚ override=True è¡¨ç¤ºå¦‚æœå·²ç»å­˜åœ¨åŒåå‡½æ•°ï¼Œåˆ™è¦†ç›–å®ƒã€‚ torch_linear(x: tvm.nd.NDArray, w: tvm.nd.NDArray, b: tvm.nd.NDArray, out: tvm.nd.NDArray): è¯¥å‡½æ•°æ¥å—å››ä¸ªå‚æ•°ï¼š x: è¾“å…¥å¼ é‡ã€‚ w: æƒé‡å¼ é‡ã€‚ b: åç½®å¼ é‡ã€‚ out: è¾“å‡ºå¼ é‡ã€‚ å‡½æ•°å†…éƒ¨ï¼š ä½¿ç”¨ torch.from_dlpack å°† TVM çš„ NDArray å¯¹è±¡è½¬æ¢ä¸º PyTorch çš„ Tensor å¯¹è±¡ã€‚ ä½¿ç”¨ PyTorch çš„ torch.mm å‡½æ•°è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå°† x å’Œ w çš„è½¬ç½®ç›¸ä¹˜ï¼Œå¹¶å°†ç»“æœå†™å…¥ outã€‚ ä½¿ç”¨ PyTorch çš„ torch.add å‡½æ•°å°† b åŠ åˆ° out ä¸Šã€‚ @tvm.register_func(\u0026quot;env.relu\u0026quot;, override=True): ä½¿ç”¨ @tvm.register_func è£…é¥°å™¨å°† lnumpy_relu å‡½æ•°æ³¨å†Œä¸ºåä¸º \u0026quot;env.relu\u0026quot; çš„ TVM å‡½æ•°ã€‚ override=True è¡¨ç¤ºå¦‚æœå·²ç»å­˜åœ¨åŒåå‡½æ•°ï¼Œåˆ™è¦†ç›–å®ƒã€‚ lnumpy_relu(x: tvm.nd.NDArray, out: tvm.nd.NDArray): è¯¥å‡½æ•°æ¥å—ä¸¤ä¸ªå‚æ•°ï¼š x: è¾“å…¥å¼ é‡ã€‚ out: è¾“å‡ºå¼ é‡ã€‚ å‡½æ•°å†…éƒ¨ï¼š ä½¿ç”¨ torch.from_dlpack å°† TVM çš„ NDArray å¯¹è±¡è½¬æ¢ä¸º PyTorch çš„ Tensor å¯¹è±¡ã€‚ ä½¿ç”¨ PyTorch çš„ torch.maximum å‡½æ•°è®¡ç®— x å’Œ 0 ä¹‹é—´çš„æœ€å¤§å€¼ï¼Œå¹¶å°†ç»“æœå†™å…¥ outã€‚ @tvm.register_func(\u0026#34;env.linear\u0026#34;, override=True) def torch_linear(x: tvm.nd.NDArray, w: tvm.nd.NDArray, b: tvm.nd.NDArray, out: tvm.nd.NDArray): x_torch = torch.from_dlpack(x) w_torch = torch.from_dlpack(w) b_torch = torch.from_dlpack(b) out_torch = torch.from_dlpack(out) torch.mm(x_torch, w_torch.T, out=out_torch) torch.add(out_torch, b_torch, out=out_torch) @tvm.register_func(\u0026#34;env.relu\u0026#34;, override=True) def lnumpy_relu(x: tvm.nd.NDArray, out: tvm.nd.NDArray): x_torch = torch.from_dlpack(x) out_torch = torch.from_dlpack(out) torch.maximum(x_torch, torch.Tensor([0.0]), out=out_torch) ç„¶åæˆ‘ä»¬å°±å¯ä»¥åˆ›å»ºIRModuleå¹¶é€šè¿‡ä¸Šä¸€èŠ‚æ‰€è¯´æ–¹æ³•å» build and run.\n@tvm.script.ir_module class MyModuleWithExternCall: @R.function def main(x: R.Tensor((1, 784), \u0026#34;float32\u0026#34;), w0: R.Tensor((128, 784), \u0026#34;float32\u0026#34;), b0: R.Tensor((128,), \u0026#34;float32\u0026#34;), w1: R.Tensor((10, 128), \u0026#34;float32\u0026#34;), b1: R.Tensor((10,), \u0026#34;float32\u0026#34;)): # block 0 with R.dataflow(): lv0 = R.call_dps_packed(\u0026#34;env.linear\u0026#34;, (x, w0, b0), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv1 = R.call_dps_packed(\u0026#34;env.relu\u0026#34;, (lv0,), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) out = R.call_dps_packed(\u0026#34;env.linear\u0026#34;, (lv1, w1, b1), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(out) return out ex = relax.build(MyModuleWithExternCall, target=\u0026#34;llvm\u0026#34;) vm = relax.VirtualMachine(ex, tvm.cpu()) Mixing TensorIR Code and Libraries æˆ‘ä»¬å¯ä»¥æ··åˆä½¿ç”¨T.prim_funcå’Œ æ³¨å†Œçš„ runtime å‡½æ•°æ¥åˆ›å»º RelaxIR. ä»¥ä¸‹ä»£ç å±•ç¤ºäº†ä¸€ä¸ªä¾‹å­ï¼Œå…¶ä¸­ linear0 ä»åœ¨ TensorIR ä¸­å®ç°ï¼Œè€Œå…¶ä»–å‡½æ•°åˆ™è¢«é‡å®šå‘åˆ°åº“å‡½æ•°ä¸­ã€‚\n@tvm.script.ir_module class MyModuleMixture: @T.prim_func def linear0(X: T.Buffer((1, 784), \u0026#34;float32\u0026#34;), W: T.Buffer((128, 784), \u0026#34;float32\u0026#34;), B: T.Buffer((128,), \u0026#34;float32\u0026#34;), Z: T.Buffer((1, 128), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;linear0\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) Y = T.alloc_buffer((1, 128), \u0026#34;float32\u0026#34;) for i, j, k in T.grid(1, 128, 784): with T.block(\u0026#34;Y\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) with T.init(): Y[vi, vj] = T.float32(0) Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk] for i, j in T.grid(1, 128): with T.block(\u0026#34;Z\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) Z[vi, vj] = Y[vi, vj] + B[vj] @R.function def main(x: R.Tensor((1, 784), \u0026#34;float32\u0026#34;), w0: R.Tensor((128, 784), \u0026#34;float32\u0026#34;), b0: R.Tensor((128,), \u0026#34;float32\u0026#34;), w1: R.Tensor((10, 128), \u0026#34;float32\u0026#34;), b1: R.Tensor((10,), \u0026#34;float32\u0026#34;)): with R.dataflow(): cls = MyModuleMixture lv0 = R.call_tir(cls.linear0, (x, w0, b0), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv1 = R.call_dps_packed(\u0026#34;env.relu\u0026#34;, (lv0,), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) out = R.call_dps_packed(\u0026#34;env.linear\u0026#34;, (lv1, w1, b1), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(out) return out Bind Parameters to IRModule ä¹‹å‰éƒ½æ˜¯é€šè¿‡æ˜¾ç¤ºä¼ é€’å‚æ•°ç»™ vm[\u0026quot;main\u0026quot;]å‡½æ•°æ¥è°ƒç”¨ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†å‚æ•°å½“ä½œå¸¸ç†Ÿä¸IRModuleè¿›è¡Œç»‘å®šã€‚\nmetadata[\u0026quot;relax.expr.Constant\u0026quot;]å¯¹åº”çš„æ˜¯å­˜å‚¨å¸¸é‡çš„éšå¼å­—å…¸ï¼ˆè™½ç„¶æ²¡æœ‰æ˜¾ç¤ºåœ¨è„šæœ¬ä¸­ï¼Œä½†ä»æ˜¯ IRModule çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚æ„å»ºäº†è½¬æ¢åçš„ IRModuleï¼Œç°åœ¨åªéœ€è¾“å…¥æ•°æ®å°±å¯ä»¥è°ƒç”¨å‡½æ•°ã€‚\nMyModuleWithParams = relax.transform.BindParams(\u0026#34;main\u0026#34;, nd_params)(MyModuleMixture) MyModuleWithParams.show() #------------------------------------- @I.ir_module class Module: @T.prim_func def linear0(X: T.Buffer((1, 784), \u0026#34;float32\u0026#34;), W: T.Buffer((128, 784), \u0026#34;float32\u0026#34;), B: T.Buffer((128,), \u0026#34;float32\u0026#34;), Z: T.Buffer((1, 128), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): Y = T.alloc_buffer((1, 128)) for i, j, k in T.grid(1, 128, 784): with T.block(\u0026#34;Y\u0026#34;): vi, vj, vk = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) T.reads(X[vi, vk], W[vj, vk]) T.writes(Y[vi, vj]) with T.init(): Y[vi, vj] = T.float32(0.0) Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk] for i, j in T.grid(1, 128): with T.block(\u0026#34;Z\u0026#34;): vi, vj = T.axis.remap(\u0026#34;SS\u0026#34;, [i, j]) T.reads(Y[vi, vj], B[vj]) T.writes(Z[vi, vj]) Z[vi, vj] = Y[vi, vj] + B[vj] @R.function def main(x: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv0 = R.call_tir(cls.linear0, (x, metadata[\u0026#34;relax.expr.Constant\u0026#34;][0], metadata[\u0026#34;relax.expr.Constant\u0026#34;][1]), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv1 = R.call_dps_packed(\u0026#34;env.relu\u0026#34;, (lv0,), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) out = R.call_dps_packed(\u0026#34;env.linear\u0026#34;, (lv1, metadata[\u0026#34;relax.expr.Constant\u0026#34;][2], metadata[\u0026#34;relax.expr.Constant\u0026#34;][3]), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) R.output(out) return out ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch3/","summary":"Personal notebook 3.","title":"TVM Learning (4)-End to End Model Execution"},{"content":"Primitive Tensor Function æœºå™¨å­¦ä¹ ç¼–è¯‘çš„è¿‡ç¨‹å¯ä»¥è¢«çœ‹ä½œå¼ é‡å‡½æ•°ä¹‹é—´çš„å˜æ¢ã€‚ä¸€ä¸ªå…¸å‹çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ‰§è¡ŒåŒ…å«è®¸å¤šæ­¥å°†è¾“å…¥å¼ é‡ä¹‹é—´è½¬åŒ–ä¸ºæœ€ç»ˆé¢„æµ‹çš„è®¡ç®—æ­¥éª¤ï¼Œå…¶ä¸­çš„æ¯ä¸€æ­¥éƒ½è¢«ç§°ä¸ºå…ƒå¼ é‡å‡½æ•° (Primitive Tensor Function) Primitive Tensor Function\né€šå¸¸æ¥è¯´ï¼Œä¸€ä¸ªå…¸å‹çš„å…ƒå¼ é‡å‡½æ•°å®ç°çš„æŠ½è±¡åŒ…å«äº†ä»¥ä¸‹æˆåˆ†ï¼šå­˜å‚¨æ•°æ®çš„å¤šç»´æ•°ç»„ï¼Œé©±åŠ¨å¼ é‡è®¡ç®—çš„å¾ªç¯åµŒå¥—ä»¥åŠè®¡ç®—éƒ¨åˆ†æœ¬èº«çš„è¯­å¥ã€‚ä¸‹å›¾ä»¥ä¸Šä¸€ç¯‡ä¸­çš„å‘é‡åŠ æ³•ä¸ºä¾‹å­è¿›è¡Œäº†åˆ†è§£ã€‚ Tensor Function Elements\næˆ‘ä»¬ç§°è¿™ç±»æŠ½è±¡ä¸ºå¼ é‡ç¨‹åºæŠ½è±¡(Tensor Program Abstraction). å¼ é‡ç¨‹åºæŠ½è±¡çš„ä¸€ä¸ªé‡è¦æ€§è´¨æ˜¯ï¼Œä»–ä»¬èƒ½å¤Ÿè¢«ä¸€ç³»åˆ—æœ‰æ•ˆçš„ç¨‹åºå˜æ¢æ‰€æ”¹å˜ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ä¸€ç»„å˜æ¢æ“ä½œï¼ˆå¦‚å¾ªç¯æ‹†åˆ†ã€å¹¶è¡Œå’Œå‘é‡åŒ–ï¼‰å°†ä¸‹å›¾å·¦ä¾§çš„ä¸€ä¸ªåˆå§‹å¾ªç¯ç¨‹åºå˜æ¢ä¸ºå³ä¾§çš„ç¨‹åºã€‚ Tensor Function Transforms\nLearning one Tensor Program Abstraction \u0026ndash; TensorIR æˆ‘ä»¬å¯¹äºç¥ç»ç½‘ç»œçš„ä¸€ä¸ªåŸºæœ¬çš„ Linear+ReLU å±‚å¯ä»¥ç”¨ä»¥ä¸‹çš„æ•°å­¦å…¬å¼è¡¨ç¤º\n$Y_{ij} = \\sum_k A_{ik} B_{kj}$ $C_{ij} = \\mathbb{ReLU}(Y_{ij}) = \\mathbb{max}(Y_{ij}, 0)$ å…¶Numpyå®ç°å¦‚ä¸‹ï¼Œä¸‹é¢çš„ä»£ç ç›´æ¥è°ƒç”¨äº†Numpyçš„é«˜çº§APIï¼Œçœ‹èµ·æ¥éå¸¸ç®€æ´ã€‚\ndtype = \u0026#34;float32\u0026#34; a_np = np.random.rand(128, 128).astype(dtype) b_np = np.random.rand(128, 128).astype(dtype) # a @ b is equivalent to np.matmul(a, b) c_mm_relu = np.maximum(a_np @ b_np, 0) æˆ‘ä»¬å¯ä»¥å°†ä¸Šè¿°ç¨‹åºæ”¹å†™æˆLow-level Numpyï¼Œæ„å‘³ç€å¯¹äºå¤æ‚çš„è®¡ç®—æˆ‘ä»¬ä½¿ç”¨å¾ªç¯è¿›è¡Œè¡¨ç¤ºï¼Œå¹¶ä¸”å†™å‡ºå¼€è¾Ÿæ•°ç»„ç©ºé—´çš„è¿‡ç¨‹ã€‚\ndef lnumpy_mm_relu(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((128, 128), dtype=\u0026#34;float32\u0026#34;) for i in range(128): for j in range(128): for k in range(128): if k == 0: Y[i, j] = 0 Y[i, j] = Y[i, j] + A[i, k] * B[k, j] for i in range(128): for j in range(128): C[i, j] = max(Y[i, j], 0) è¯¥å‡½æ•°æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\nçŸ©é˜µä¹˜æ³•ï¼š å°†ä¸¤ä¸ªçŸ©é˜µ A å’Œ B ç›¸ä¹˜ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ Y ä¸­ã€‚ ReLU æ¿€æ´»ï¼š å°† ReLU æ¿€æ´»å‡½æ•°åº”ç”¨äº Y çš„å…ƒç´ ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨ C ä¸­ã€‚ å¯ä»¥ç”¨ä»¥ä¸‹ä»£ç æ¥æ£€æŸ¥ä¸Šè¿°å®ç°çš„æ­£ç¡®æ€§ï¼š\nc_np = np.empty((128, 128), dtype=dtype) lnumpy_mm_relu(a_np, b_np, c_np) np.testing.assert_allclose(c_mm_relu, c_np, rtol=1e-5) ç¤ºä¾‹ numpy ä»£ç åŒ…å«äº†å®é™…è¿‡ç¨‹ä¸­å®ç°è¿™äº›è®¡ç®—æ—¶å¯èƒ½ä¼šç”¨åˆ°çš„æ‰€æœ‰å…ƒç´ ï¼Œç”¨Numpyå‡½æ•°å†…éƒ¨å·¥ä½œæœºåˆ¶ (Under the Hood) å®ç°äº†MM-ReLUã€‚\nå¼€è¾Ÿå¤šç»´æ•°ç»„ç©ºé—´ã€‚ å¾ªç¯éå†æ•°ç»„çš„ç»´åº¦ã€‚ è®¡ç®—åœ¨å¾ªç¯å†…æ‰§è¡Œã€‚ æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨ä¸Šä¸€èŠ‚çš„TensorIRæ¥å®ç°ï¼ŒTVMScript æ˜¯åµŒå…¥åœ¨ Python AST ä¸­çš„é¢†åŸŸç‰¹å®šè¯­è¨€çš„ Dialect, å®ƒæœ¬è´¨ä¸Šæ˜¯ Python çš„ä¸€ä¸ªå­é›†ï¼Œä½†æ·»åŠ äº†ä¸€äº›ç‰¹å®šäº TVM çš„æ‰©å±•ï¼Œä¾‹å¦‚ç”¨äºæè¿°è®¡ç®—å›¾çš„ç‰¹æ®Šè¯­æ³•å’Œè¯­ä¹‰ã€‚\nDialect é€šå¸¸æŒ‡ä¸€ç§è¯­è¨€çš„å˜ä½“æˆ–å­é›†ï¼Œå®ƒä¸åŸå§‹è¯­è¨€å…±äº«å¤§éƒ¨åˆ†è¯­æ³•å’Œè¯­ä¹‰ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›ç‹¬ç‰¹çš„ç‰¹å¾ã€‚ æŠ½è±¡è¯­æ³•æ ‘ (AST) æ˜¯æºä»£ç çš„æ ‘çŠ¶è¡¨ç¤ºå½¢å¼ã€‚å®ƒå°†ä»£ç çš„ç»“æ„ä»¥ä¸€ç§å±‚æ¬¡åŒ–çš„æ–¹å¼å‘ˆç°ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä»£ç ä¸­çš„ä¸€ä¸ªè¯­æ³•å…ƒç´ ï¼Œä¾‹å¦‚å˜é‡ã€è¿ç®—ç¬¦ã€å‡½æ•°è°ƒç”¨ç­‰ã€‚\n@tvm.script.ir_module class MyModule: @T.prim_func def mm_relu(A: T.Buffer[(128, 128), \u0026#34;float32\u0026#34;], B: T.Buffer[(128, 128), \u0026#34;float32\u0026#34;], C: T.Buffer[(128, 128), \u0026#34;float32\u0026#34;]): T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;mm_relu\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) Y = T.alloc_buffer((128, 128), dtype=\u0026#34;float32\u0026#34;) for i, j, k in T.grid(128, 128, 128): with T.block(\u0026#34;Y\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j) vk = T.axis.reduce(128, k) with T.init(): Y[vi, vj] = T.float32(0) Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj] for i, j in T.grid(128, 128): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j) C[vi, vj] = T.max(Y[vi, vj], T.float32(0)) ä¸Šè¿° TensorIR ç¨‹åºçš„ä¸€ä¸ªç¤ºä¾‹å®ä¾‹æ¶µç›–äº†å¤§éƒ¨åˆ†å†…å®¹ï¼ŒåŒ…æ‹¬\nå‚æ•°å’Œä¸­é—´ä¸´æ—¶å†…å­˜ä¸­çš„ç¼“å†²åŒºå£°æ˜ã€‚ For å¾ªç¯è¿­ä»£ã€‚ Block å’Œ Block Axiså±æ€§ã€‚ Transformation TVM çš„ tvm.tir.Schedule æä¾›äº†ä¸€ç³»åˆ—ç”¨äºè°ƒåº¦å’Œä¼˜åŒ–è®¡ç®—å›¾çš„å˜æ¢å‡½æ•°ã€‚è¿™äº›å‡½æ•°å…è®¸ç”¨æˆ·çµæ´»åœ°è°ƒæ•´è®¡ç®—é¡ºåºã€å†…å­˜è®¿é—®æ¨¡å¼å’Œå¹¶è¡ŒåŒ–ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚\næˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹å‡½æ•°è·å¾—è®¡ç®—å—å’Œå…¶å¯¹åº”çš„å¾ªç¯\nblock_Y = sch.get_block(\u0026#34;Y\u0026#34;, func_name=\u0026#34;mm_relu\u0026#34;) i, j, k = sch.get_loops(block_Y) æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ splitå‡½æ•°å°†ä¸€ä¸ªå¾ªç¯æ‹†æˆå¤šä¸ªå¾ªç¯ï¼Œç”¨ reorderå‡½æ•°äº¤æ¢å¾ªç¯çš„é¡ºåºï¼Œç”¨ reverse_compute_at å‡½æ•°ç§»åŠ¨è®¡ç®—å—æ‰€åœ¨çš„å¾ªç¯ï¼Œç”¨ decompose_reductionå‡½æ•°å°†åˆå§‹åŒ–å’Œå½’çº¦æ“ä½œåˆ†å¼€ã€‚\nj0, j1 = sch.split(j, factors=[None, 4]) sch.reorder(j0, k, j1) block_C = sch.get_block(\u0026#34;C\u0026#34;, \u0026#34;mm_relu\u0026#34;) sch.reverse_compute_at(block_C, j0) block_Y = sch.get_block(\u0026#34;Y\u0026#34;, \u0026#34;mm_relu\u0026#34;) sch.decompose_reduction(block_Y, k) sch.mod.show() # Output @tvm.script.ir_module class Module: @T.prim_func def mm_relu(A: T.Buffer[(128, 128), \u0026#34;float32\u0026#34;], B: T.Buffer[(128, 128), \u0026#34;float32\u0026#34;], C: T.Buffer[(128, 128), \u0026#34;float32\u0026#34;]) -\u0026gt; None: # function attr dict T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;mm_relu\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) # body # with T.block(\u0026#34;root\u0026#34;) Y = T.alloc_buffer([128, 128], dtype=\u0026#34;float32\u0026#34;) for i, j_0 in T.grid(128, 32): for j_1_init in T.serial(4): with T.block(\u0026#34;Y_init\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j_0 * 4 + j_1_init) T.reads() T.writes(Y[vi, vj]) Y[vi, vj] = T.float32(0) for k, j_1 in T.grid(128, 4): with T.block(\u0026#34;Y_update\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j_0 * 4 + j_1) vk = T.axis.reduce(128, k) T.reads(Y[vi, vj], A[vi, vk], B[vk, vj]) T.writes(Y[vi, vj]) Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj] for ax0 in T.serial(4): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(128, i) vj = T.axis.spatial(128, j_0 * 4 + ax0) T.reads(Y[vi, vj]) T.writes(C[vi, vj]) C[vi, vj] = T.max(Y[vi, vj], T.float32(0)) å¯¹åº”çš„ Low-level Numpy å‡½æ•°å¦‚ä¸‹\ndef lnumpy_mm_relu_v3(A: np.ndarray, B: np.ndarray, C: np.ndarray): Y = np.empty((128, 128), dtype=\u0026#34;float32\u0026#34;) for i in range(128): for j0 in range(32): # Y_init for j1 in range(4): j = j0 * 4 + j1 Y[i, j] = 0 # Y_update for k in range(128): for j1 in range(4): j = j0 * 4 + j1 Y[i, j] = Y[i, j] + A[i, k] * B[k, j] # C for j1 in range(4): j = j0 * 4 + j1 C[i, j] = max(Y[i, j], 0) Why Do Loop Influence the Exec Time CPU Architecture CPU å¸¦æœ‰å¤šçº§ç¼“å­˜ï¼Œéœ€è¦å…ˆå°†æ•°æ®æå–åˆ°ç¼“å­˜ä¸­ï¼Œç„¶å CPU æ‰èƒ½è®¿é—®å®ƒã€‚è€Œä¸”è®¿é—®å·²ç»åœ¨ç¼“å­˜ä¸­çš„æ•°æ®è¦å¿«å¾—å¤šã€‚CPU é‡‡ç”¨çš„ä¸€ç§ç­–ç•¥æ˜¯è·å–å½¼æ­¤æ›´æ¥è¿‘çš„æ•°æ®ã€‚ å½“æˆ‘ä»¬è¯»å–å†…å­˜ä¸­çš„ä¸€ä¸ªå…ƒç´ æ—¶ï¼Œå®ƒä¼šå°è¯•å°†é™„è¿‘çš„å…ƒç´ ï¼ˆCache Lineï¼‰è·å–åˆ°ç¼“å­˜ä¸­ï¼Œå½“è¯»å–ä¸‹ä¸€ä¸ªå…ƒç´ æ—¶å®ƒå·²ç»åœ¨ç¼“å­˜ä¸­ã€‚ å› æ­¤ï¼Œå…·æœ‰è¿ç»­å†…å­˜è®¿é—®çš„ä»£ç é€šå¸¸æ¯”éšæœºè®¿é—®å†…å­˜ä¸åŒéƒ¨åˆ†çš„ä»£ç æ›´å¿«ã€‚\nLoop Order j1 è¿™ä¸€è¿­ä»£äº§ç”Ÿäº†å¯¹ B å…ƒç´ çš„è¿ç»­è®¿é—®ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ„å‘³ç€åœ¨ j1=0 å’Œ j1=1 æ—¶æˆ‘ä»¬è¯»å–çš„å€¼å½¼æ­¤ç›¸é‚»ã€‚è¿™å¯ä»¥è®©æˆ‘ä»¬æ‹¥æœ‰æ›´å¥½çš„ç¼“å­˜è®¿é—®è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ C çš„è®¡ç®—æ›´æ¥è¿‘ Yï¼Œä»è€Œå®ç°æ›´å¥½çš„ç¼“å­˜è¡Œä¸ºã€‚\nWays to Create and Interact with TensorIR Create TensorIR via TVMScript åˆ›å»º TensorIR å‡½æ•°çš„ç¬¬ä¸€ç§æ–¹æ³•æ˜¯ç›´æ¥åœ¨ TVMScript ä¸­ç¼–å†™å‡½æ•°ï¼Œå®ƒä¹Ÿæ˜¯ä¸€ç§åœ¨å˜æ¢è¿‡ç¨‹ä¸­æ£€æŸ¥å¼ é‡å‡½æ•°çš„æœ‰ç”¨æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥æ‰“å°å‡º TVMScriptï¼Œè¿›è¡Œä¸€äº›æ‰‹åŠ¨ç¼–è¾‘ï¼Œç„¶åå°†å…¶åé¦ˆç»™ MLC æµç¨‹ä»¥è°ƒè¯•å’Œå°è¯•å¯èƒ½çš„ï¼ˆæ‰‹åŠ¨ï¼‰å˜æ¢ï¼Œç„¶åå°†å˜æ¢åçš„ç¨‹åºé‡æ–°åº”ç”¨åˆ° MLC æµç¨‹ä¸­ã€‚\nGenerate TensorIR code using Tensor Expression å¼ é‡è¡¨è¾¾å¼ (TE) æ˜¯ä¸€ç§ç‰¹å®šé¢†åŸŸçš„è¯­è¨€ï¼Œå®ƒé€šè¿‡ API ä¹‹ç±»çš„è¡¨è¾¾å¼æè¿°ä¸€ç³»åˆ—è®¡ç®—ã€‚MM-ReLU å¯ä»¥é€šè¿‡ä»¥ä¸‹ç¨‹åºå®Œæˆ\nfrom tvm import te A = te.placeholder((128, 128), \u0026#34;float32\u0026#34;, name=\u0026#34;A\u0026#34;) B = te.placeholder((128, 128), \u0026#34;float32\u0026#34;, name=\u0026#34;B\u0026#34;) k = te.reduce_axis((0, 128), \u0026#34;k\u0026#34;) Y = te.compute((128, 128), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\u0026#34;Y\u0026#34;) C = te.compute((128, 128), lambda i, j: te.max(Y[i, j], 0), name=\u0026#34;C\u0026#34;) ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch2/","summary":"Personal notebook 2.","title":"TVM Learning (2)-Tensor Program Abstraction Case"},{"content":"My notebook of MLC: https://mlc.ai/summer22-zh\nConstructing Tensor Program by TVMScript åœ¨æœºå™¨å­¦ä¹ ç¼–è¯‘ (Machine Learning Compilation) ä¸­ï¼ŒTensor Program æŒ‡çš„æ˜¯ä¸€ç§è¡¨ç¤ºæœºå™¨å­¦ä¹ æ¨¡å‹è®¡ç®—è¿‡ç¨‹çš„ç¨‹åºï¼Œå®ƒä»¥å¼ é‡ (Tensor) ä¸ºåŸºæœ¬æ•°æ®å•å…ƒï¼Œå¹¶ä½¿ç”¨å¼ é‡æ“ä½œæ¥æè¿°æ¨¡å‹çš„è®¡ç®—æ­¥éª¤ã€‚\nVector-Add Example ä¸‹é¢è¿™æ®µä»£ç ä½¿ç”¨ TVM çš„ script æ¨¡å—å®šä¹‰äº†ä¸€ä¸ªåä¸º MyModule çš„æ¨¡å—ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªåä¸º main çš„è®¡ç®—å‡½æ•°ã€‚\nè¯¥å‡½æ•°å®ç°äº†ç®€å•çš„å‘é‡åŠ æ³• (vector add) æ“ä½œ, ä¸¤ä¸ªè¾“å…¥å‘é‡ A å’Œ B ç›¸åŠ ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åˆ°è¾“å‡ºå‘é‡ C ä¸­ã€‚\nimport tvm from tvm.ir.module import IRModule from tvm.script import tir as T import numpy as np @tvm.script.ir_module class MyModule: @T.prim_func def main(A: T.Buffer[128, \u0026#34;float32\u0026#34;], B: T.Buffer[128, \u0026#34;float32\u0026#34;], C: T.Buffer[128, \u0026#34;float32\u0026#34;]): # extra annotations for the function T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) for i in range(128): with T.block(\u0026#34;C\u0026#34;): # declare a data parallel iterator on spatial domain vi = T.axis.spatial(128, i) C[vi] = A[vi] + B[vi] 1. æ¨¡å—å®šä¹‰:\n@tvm.script.ir_module class MyModule: # ... @tvm.script.ir_module: ç”¨äºå°† MyModule ç±»å®šä¹‰ä¸ºä¸€ä¸ª TVM çš„ IRModule å¯¹è±¡ã€‚IRModule æ˜¯ TVM ä¸­ç”¨äºè¡¨ç¤ºè®¡ç®—å›¾ (Computation Graph) çš„æ ‡å‡†æ•°æ®ç»“æ„ã€‚ class MyModule:: å®šä¹‰ä¸€ä¸ªåä¸º MyModule çš„ç±»ï¼Œè¯¥ç±»å°†åŒ…å«è®¡ç®—å‡½æ•°ã€‚ Decorator åœ¨ Python ä¸­ï¼Œè£…é¥°å™¨ (Decorator) æ˜¯ä¸€ç§ç‰¹æ®Šçš„å‡½æ•°ï¼Œå®ƒå¯ä»¥ç”¨æ¥ä¿®æ”¹å…¶ä»–å‡½æ•°çš„è¡Œä¸ºï¼Œè€Œæ— éœ€ç›´æ¥ä¿®æ”¹è¢«è£…é¥°çš„å‡½æ•°ä»£ç ã€‚\ndef decorator_function(func): def wrapper(*args, **kwargs): # åœ¨è°ƒç”¨è¢«è£…é¥°çš„å‡½æ•°ä¹‹å‰æ‰§è¡Œçš„æ“ä½œ result = func(*args, **kwargs) # åœ¨è°ƒç”¨è¢«è£…é¥°çš„å‡½æ•°ä¹‹åæ‰§è¡Œçš„æ“ä½œ return result return wrapper @decorator_function def my_function(x, y): # è¢«è£…é¥°çš„å‡½æ•° return x + y decorator_function: è£…é¥°å™¨å‡½æ•°ï¼Œå®ƒæ¥æ”¶è¢«è£…é¥°çš„å‡½æ•°ä½œä¸ºå‚æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…è£…å‡½æ•°ã€‚ wrapper: åŒ…è£…å‡½æ•°ï¼Œå®ƒåœ¨è°ƒç”¨è¢«è£…é¥°çš„å‡½æ•°ä¹‹å‰å’Œä¹‹åæ‰§è¡Œä¸€äº›æ“ä½œã€‚ @decorator_function: è£…é¥°å™¨è¯­æ³•ï¼Œå°† decorator_function åº”ç”¨åˆ° my_function ä¸Šã€‚ è£…é¥°å™¨çš„å·¥ä½œåŸç†:\nå½“ Python é‡åˆ° @decorator_function è¯­æ³•æ—¶ï¼Œå®ƒä¼šå°† my_function ä½œä¸ºå‚æ•°ä¼ é€’ç»™ decorator_functionã€‚ decorator_function æ‰§è¡Œï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…è£…å‡½æ•° wrapperã€‚ wrapper å‡½æ•°å°†æ›¿æ¢ my_function çš„åŸå§‹å®šä¹‰ã€‚ å½“è°ƒç”¨ my_function æ—¶ï¼Œå®é™…ä¸Šæ˜¯åœ¨è°ƒç”¨ wrapper å‡½æ•°ã€‚ 2. è®¡ç®—å‡½æ•°å®šä¹‰:\n@T.prim_func def main(A: T.Buffer[128, \u0026#34;float32\u0026#34;], B: T.Buffer[128, \u0026#34;float32\u0026#34;], C: T.Buffer[128, \u0026#34;float32\u0026#34;]): # ... @T.prim_func: è¿™æ˜¯ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºå°† main å‡½æ•°å®šä¹‰ä¸ºä¸€ä¸ª TVM çš„ prim_func å¯¹è±¡ã€‚prim_func æ˜¯ TVM ä¸­ç”¨äºè¡¨ç¤ºåº•å±‚è®¡ç®—å‡½æ•°çš„æ ‡å‡†æ•°æ®ç»“æ„ã€‚ def main(...): å®šä¹‰ä¸€ä¸ªåä¸º main çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—ä¸‰ä¸ªå‚æ•°ï¼š A: ä¸€ä¸ªé•¿åº¦ä¸º 128 çš„ float32 ç±»å‹ Bufferï¼Œè¡¨ç¤ºç¬¬ä¸€ä¸ªè¾“å…¥å‘é‡ã€‚ B: ä¸€ä¸ªé•¿åº¦ä¸º 128 çš„ float32 ç±»å‹ Bufferï¼Œè¡¨ç¤ºç¬¬äºŒä¸ªè¾“å…¥å‘é‡ã€‚ C: ä¸€ä¸ªé•¿åº¦ä¸º 128 çš„ float32 ç±»å‹ Bufferï¼Œç”¨äºå­˜å‚¨è®¡ç®—ç»“æœã€‚ 3. å‡½æ•°å±æ€§:\nT.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) T.func_attr({\u0026quot;global_symbol\u0026quot;: \u0026quot;main\u0026quot;, \u0026quot;tir.noalias\u0026quot;: True})ï¼š è®¾ç½®å‡½æ•°çš„å±æ€§ã€‚ global_symbol: è®¾ç½®å‡½æ•°çš„å…¨å±€ç¬¦å·åç§°ä¸º mainã€‚ tir.noalias: è®¾ç½®å‡½æ•°çš„åˆ«åå±æ€§ä¸º Trueï¼Œè¡¨ç¤ºå‡½æ•°ä¸ä¼šä¿®æ”¹è¾“å…¥ç¼“å†²åŒºã€‚ 4. è®¡ç®—å¾ªç¯:\nfor i inrange(128): with T.block(\u0026#34;C\u0026#34;): # ... T.block å°†è®¡ç®—å›¾åˆ†è§£æˆå¤šä¸ªç‹¬ç«‹çš„è®¡ç®—å—ï¼Œæ¯ä¸ªå—å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„è®¡ç®—ä»»åŠ¡ï¼Œå¯ä»¥åŒ…å«å¤šä¸ªè¿­ä»£å™¨ï¼Œè¿™äº›è¿­ä»£å™¨å…±åŒå®šä¹‰äº†è®¡ç®—å—çš„è®¡ç®—èŒƒå›´ã€‚\nfor i in range(128): å®šä¹‰ä¸€ä¸ªå¾ªç¯ï¼Œè¿­ä»£ 128 æ¬¡ï¼Œç”¨äºå¤„ç†æ¯ä¸ªå‘é‡å…ƒç´ ã€‚ with T.block(\u0026quot;C\u0026quot;): å®šä¹‰ä¸€ä¸ªåä¸º C çš„è®¡ç®—å—ï¼Œè¯¥å—åŒ…å«å¾ªç¯çš„è®¡ç®—é€»è¾‘ã€‚ 5. è¿­ä»£å™¨å®šä¹‰:\nvi = T.axis.spatial(128, i) vi = T.axis.spatial(128, i): å®šä¹‰ä¸€ä¸ªåä¸º vi çš„ç©ºé—´è¿­ä»£å™¨ï¼Œå®ƒéå† 128 ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ çš„ç´¢å¼•ç”± i ç¡®å®šã€‚ ä¸€èˆ¬æ¥è¯´ï¼Œç©ºé—´è¿­ä»£å™¨çš„è®¿é—®é¡ºåºå¯¹æœ€åç»“æœä¸äº§ç”Ÿå½±å“ã€‚\n6. è®¡ç®—æ“ä½œ:\nC[vi] = A[vi] + B[vi] C[vi] = A[vi] + B[vi]ï¼š å°† A å’Œ B ä¸­å¯¹åº”å…ƒç´ ç›¸åŠ ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åˆ° C ä¸­ã€‚ æˆ‘ä»¬å¯ä»¥é€šè¿‡ MyModule.show() æ¥æ˜¾ç¤ºæ„å»ºçš„IRModule.\n@tvm.script.ir_module class Module: @T.prim_func def main(A: T.Buffer[128, \u0026#34;float32\u0026#34;], B: T.Buffer[128, \u0026#34;float32\u0026#34;], C: T.Buffer[128, \u0026#34;float32\u0026#34;]) -\u0026gt; None: # function attr dict T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) # body # with T.block(\u0026#34;root\u0026#34;) for i in T.serial(128): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(128, i) T.reads(A[vi], B[vi]) T.writes(C[vi]) C[vi] = A[vi] + B[vi] Build and Run æˆ‘ä»¬å¯ä»¥é€šè¿‡ tvm.buildå‡½æ•°å°†ä¸€ä¸ªIRModuleè½¬å˜æˆå¯ä»¥è¿è¡Œçš„å‡½æ•°ï¼Œé€šè¿‡å®šä¹‰çš„å‡½æ•°åå¯ä»¥è·å–æƒ³è¦çš„å‡½æ•°ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸‰ä¸ª NDArray æ•°ç»„æ¥è°ƒç”¨å‡½æ•°ã€‚\nrt_mod = tvm.build(MyModule, target=\u0026#34;llvm\u0026#34;) func = rt_mod[\u0026#34;main\u0026#34;] a = tvm.nd.array(np.arange(128, dtype=\u0026#34;float32\u0026#34;)) b = tvm.nd.array(np.ones(128, dtype=\u0026#34;float32\u0026#34;)) c = tvm.nd.empty((128,), dtype=\u0026#34;float32\u0026#34;) func(a, b, c) tvm.build å‡½æ•°çš„å‚æ•°:\nfunc: è¦ç¼–è¯‘çš„è®¡ç®—å›¾ï¼Œå¯ä»¥æ˜¯ tvm.script.ir_module å¯¹è±¡ã€tvm.relay.Function å¯¹è±¡æˆ–å…¶ä»–æ”¯æŒçš„è®¡ç®—å›¾ç±»å‹ã€‚ target: ç›®æ ‡å¹³å°ï¼Œä¾‹å¦‚ï¼Œllvm -mcpu=core-avx2ã€cudaã€opencl ç­‰ã€‚ name: ç¼–è¯‘åçš„æ¨¡å—åç§°ã€‚ Transform the Tensor Program åœ¨ TVM ä¸­ï¼Œtvm.tir.Schedule æ˜¯ä¸€ä¸ªç”¨äºå¯¹è®¡ç®—å›¾è¿›è¡Œæ‰‹åŠ¨ä¼˜åŒ–çš„å·¥å…·ã€‚å®ƒå…è®¸å¯¹è®¡ç®—å›¾ä¸­çš„å¾ªç¯ã€å—å’Œæ“ä½œè¿›è¡Œé‡æ’åºã€èåˆã€å¹¶è¡ŒåŒ–ç­‰æ“ä½œï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚\nä¸‹é¢è¿™æ®µä»£ç åšäº†ä»¥ä¸‹ä¼˜åŒ–ï¼š\nå¾ªç¯åˆ‡åˆ†: å°†å¾ªç¯ i åˆ‡åˆ†æˆä¸‰ä¸ªå¾ªç¯ï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨å†…å­˜å±€éƒ¨æ€§ï¼Œä¾‹å¦‚ï¼Œå°† i_1 å’Œ i_2 çš„å¤§å°è®¾ç½®ä¸º 4ï¼Œå¯ä»¥å°†æ•°æ®åŠ è½½åˆ°ç¼“å­˜ä¸­ï¼Œå‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°ã€‚ å¾ªç¯é‡æ’åº: æŒ‰ç…§ i_0ã€i_2 å’Œ i_1 è¿™ä¸ªé¡ºåºæ‰§è¡Œã€‚ å¹¶è¡ŒåŒ–: å°† i_0 å¹¶è¡ŒåŒ–ï¼Œå¯ä»¥åˆ©ç”¨å¤šæ ¸ CPU æˆ– GPU çš„è®¡ç®—èƒ½åŠ›ï¼Œæé«˜è®¡ç®—é€Ÿåº¦ sch = tvm.tir.Schedule(MyModule) # Get block by its name block_c = sch.get_block(\u0026#34;C\u0026#34;) # Get loops surronding the block (i,) = sch.get_loops(block_c) # Tile the loop nesting. i_0, i_1, i_2 = sch.split(i, factors=[None, 4, 4]) # Reorder the loop. sch.reorder(i_0, i_2, i_1) sch.parallel(i_0) sch.mod.show() ä¼˜åŒ–åçš„è®¡ç®—å›¾å¦‚ä¸‹\n@tvm.script.ir_module class Module: @T.prim_func def main(A: T.Buffer[128, \u0026#34;float32\u0026#34;], B: T.Buffer[128, \u0026#34;float32\u0026#34;], C: T.Buffer[128, \u0026#34;float32\u0026#34;]) -\u0026gt; None: # function attr dict T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) # body # with T.block(\u0026#34;root\u0026#34;) for i_0 in T.parallel(8): for i_2, i_1 in T.grid(4, 4): with T.block(\u0026#34;C\u0026#34;): vi = T.axis.spatial(128, i_0 * 16 + i_1 * 4 + i_2) T.reads(A[vi], B[vi]) T.writes(C[vi]) C[vi] = A[vi] + B[vi] Constructing Tensor Program by Tensor Expression Tensor Expression æŒ‡çš„æ˜¯ä¸€ç§ç”¨äºæè¿°å¼ é‡è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼ã€‚\nConstruct Vector-Add by TE æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ¥åˆ›å»ºå’Œ ä¸Šä¸€èŠ‚ ä¸€æ ·çš„IRModule.\n# namespace for tensor expression utility from tvm import te # declare the computation using the expression API A = te.placeholder((128, ), name=\u0026#34;A\u0026#34;) B = te.placeholder((128, ), name=\u0026#34;B\u0026#34;) C = te.compute((128,), lambda i: A[i] + B[i], name=\u0026#34;C\u0026#34;) # create a function with the specified list of arguments. func = te.create_prim_func([A, B, C]) # mark that the function name is main func = func.with_attr(\u0026#34;global_symbol\u0026#34;, \u0026#34;main\u0026#34;) ir_mod_from_te = IRModule({\u0026#34;main\u0026#34;: func}) ir_mod_from_te.show() å®šä¹‰å¼ é‡:\nA = te.placeholder((128,), name=\u0026#34;A\u0026#34;) B = te.placeholder((128,), name=\u0026#34;B\u0026#34;) è¿™ä¸¤è¡Œä»£ç å®šä¹‰äº†ä¸¤ä¸ªåä¸º A å’Œ B çš„å¼ é‡ï¼Œå®ƒä»¬éƒ½æ˜¯ä¸€ç»´å¼ é‡ï¼Œå¤§å°ä¸º 128ã€‚te.placeholder å‡½æ•°ç”¨äºåˆ›å»ºå ä½ç¬¦å¼ é‡ï¼Œå®ƒä»£è¡¨è¾“å…¥æ•°æ®ã€‚\nå®šä¹‰è®¡ç®—:\nC = te.compute((128,), lambda i: A[i] + B[i], name=\u0026#34;C\u0026#34;) è¿™è¡Œä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º C çš„å¼ é‡ï¼Œå®ƒè¡¨ç¤º A å’Œ B çš„å…ƒç´ ç›¸åŠ çš„ç»“æœã€‚te.compute å‡½æ•°ç”¨äºå®šä¹‰å¼ é‡è®¡ç®—ï¼Œå®ƒæ¥å—ä¸¤ä¸ªå‚æ•°ï¼š\nç¬¬ä¸€ä¸ªå‚æ•° shapeæ˜¯å¼ é‡çš„å½¢çŠ¶ï¼Œè¿™é‡Œä¸º (128,)ã€‚ ç¬¬äºŒä¸ªå‚ fcomputeæ•°æ˜¯ä¸€ä¸ª lambda å‡½æ•°ï¼Œå®ƒå®šä¹‰äº†æ¯ä¸ªå…ƒç´ çš„è®¡ç®—æ–¹å¼ï¼Œè¿™é‡Œä¸º A[i] + B[i]ï¼Œè¡¨ç¤º C çš„ç¬¬ i ä¸ªå…ƒç´ ç­‰äº A çš„ç¬¬ i ä¸ªå…ƒç´ åŠ ä¸Š B çš„ç¬¬ i ä¸ªå…ƒç´ ã€‚ åˆ›å»º PrimFunc:\nfunc = te.create_prim_func([A, B, C]) è¿™è¡Œä»£ç ä½¿ç”¨ te.create_prim_func å‡½æ•°åˆ›å»ºäº†ä¸€ä¸ª PrimFunc å¯¹è±¡ï¼Œå®ƒä»£è¡¨ä¸€ä¸ª TVM çš„åŸºæœ¬è®¡ç®—å‡½æ•°ã€‚te.create_prim_func å‡½æ•°æ¥å—ä¸€ä¸ªå‚æ•°ï¼Œå³å‡½æ•°çš„è¾“å…¥å‚æ•°åˆ—è¡¨ï¼Œè¿™é‡Œä¸º [A, B, C]\nè®¾ç½®å‡½æ•°åç§°:\nfunc = func.with_attr(\u0026#34;global_symbol\u0026#34;, \u0026#34;main\u0026#34;) è¿™è¡Œä»£ç å°†å‡½æ•°çš„åç§°è®¾ç½®ä¸º mainï¼Œwith_attr å‡½æ•°ç”¨äºè®¾ç½®å‡½æ•°çš„å±æ€§ã€‚\nåˆ›å»º IRModule:\nir_mod_from_te = IRModule({\u0026#34;main\u0026#34;: func}) è¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ª IRModule å¯¹è±¡ï¼Œå®ƒåŒ…å«äº† func å‡½æ•°ï¼Œå¹¶å°†è¯¥å‡½æ•°å­˜å‚¨åœ¨ IRModule çš„ main å­—æ®µä¸­ã€‚\nTransforming a matrix multiplication program ä¸‹é¢ä»£ç å±•ç¤ºäº†ä¸¤ä¸ª $1024 \\times 1024$ çŸ©é˜µç›¸ä¹˜çš„IRModuleåˆ›å»ºæµç¨‹ã€‚\nM = 1024 K = 1024 N = 1024 # The default tensor type in tvm dtype = \u0026#34;float32\u0026#34; target = \u0026#34;llvm\u0026#34; dev = tvm.device(target, 0) # Algorithm k = te.reduce_axis((0, K), \u0026#34;k\u0026#34;) A = te.placeholder((M, K), name=\u0026#34;A\u0026#34;) B = te.placeholder((K, N), name=\u0026#34;B\u0026#34;) C = te.compute((M, N), lambda m, n: te.sum(A[m, k] * B[k, n], axis=k), name=\u0026#34;C\u0026#34;) # Default schedule func = te.create_prim_func([A, B, C]) func = func.with_attr(\u0026#34;global_symbol\u0026#34;, \u0026#34;main\u0026#34;) ir_module = IRModule({\u0026#34;main\u0026#34;: func}) ir_module.show() func = tvm.build(ir_module, target=\u0026#34;llvm\u0026#34;) # The module for CPU backends. a = tvm.nd.array(np.random.rand(M, K).astype(dtype), dev) b = tvm.nd.array(np.random.rand(K, N).astype(dtype), dev) c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev) func(a, b, c) # Create evaluation function evaluator = func.time_evaluator(func.entry_name, dev, number=1) print(\u0026#34;Baseline: %f\u0026#34; % evaluator(a, b, c).mean) time_evaluator æ˜¯ IRModule ç”¨äºè¯„ä¼°è®¡ç®—å›¾æ‰§è¡Œæ—¶é—´çš„æ–¹æ³•ã€‚å®ƒå¯ä»¥å¸®åŠ©æµ‹é‡ä¸åŒç¡¬ä»¶å¹³å°ä¸Šä¸åŒè®¡ç®—å›¾çš„æ€§èƒ½ï¼Œå¹¶è¿›è¡Œä¼˜åŒ–ã€‚\ntime evaluator IRModule.time_evaluator(func, args, number=1, repeat=1, min_repeat_ms=0, f_type=0) å‚æ•°è§£é‡Š:\nfunc: è¦è¯„ä¼°çš„è®¡ç®—å›¾å‡½æ•°ã€‚ args: è®¡ç®—å›¾å‡½æ•°çš„è¾“å…¥å‚æ•°ï¼Œå¯ä»¥æ˜¯å¼ é‡æˆ–å…¶ä»–æ•°æ®ç»“æ„ã€‚ number: æ¯æ¬¡è¿è¡Œè®¡ç®—å›¾çš„æ¬¡æ•°ï¼Œé»˜è®¤å€¼ä¸º 1ã€‚ repeat: é‡å¤è¿è¡Œè®¡ç®—å›¾çš„æ¬¡æ•°ï¼Œé»˜è®¤å€¼ä¸º 1ã€‚ min_repeat_ms: æœ€å°è¿è¡Œæ—¶é—´ï¼Œå•ä½ä¸ºæ¯«ç§’ã€‚å¦‚æœè®¡ç®—å›¾è¿è¡Œæ—¶é—´å°äº min_repeat_msï¼Œåˆ™ä¼šç»§ç»­è¿è¡Œç›´åˆ°è¾¾åˆ° min_repeat_msã€‚é»˜è®¤å€¼ä¸º 0ã€‚ f_type: è¿è¡Œæ¨¡å¼ï¼Œå¯ä»¥æ˜¯ 0ï¼ˆé»˜è®¤å€¼ï¼‰ã€1 æˆ– 2ã€‚ 0ï¼šæ­£å¸¸è¿è¡Œæ¨¡å¼ã€‚ 1ï¼šä»…æ‰§è¡Œç¼–è¯‘ï¼Œä¸è¿è¡Œè®¡ç®—å›¾ã€‚ 2ï¼šä»…æ‰§è¡Œè¿è¡Œï¼Œä¸ç¼–è¯‘è®¡ç®—å›¾ã€‚ func.time_evaluator çš„è¿”å›å€¼:\nfunc.time_evaluator è¿”å›ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°å¯ä»¥ç”¨æ¥æ‰§è¡Œè¯„ä¼°å¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸ã€‚\næ€§èƒ½æŒ‡æ ‡:\nmean: å¹³å‡è¿è¡Œæ—¶é—´ï¼Œå•ä½ä¸ºæ¯«ç§’ã€‚ median: ä¸­ä½æ•°è¿è¡Œæ—¶é—´ï¼Œå•ä½ä¸ºæ¯«ç§’ã€‚ min: æœ€å°è¿è¡Œæ—¶é—´ï¼Œå•ä½ä¸ºæ¯«ç§’ã€‚ max: æœ€å¤§è¿è¡Œæ—¶é—´ï¼Œå•ä½ä¸ºæ¯«ç§’ã€‚ std: æ ‡å‡†å·®ï¼Œå•ä½ä¸ºæ¯«ç§’ã€‚ ä»£ç çš„å¤§éƒ¨åˆ†æµç¨‹ç›¸åŒï¼Œæˆ‘ä»¬æ¥çœ‹è®¡ç®—éƒ¨åˆ†ã€‚\nå®šä¹‰çº¦ç®€è½´ (Reduce axis):\nk = te.reduce_axis((0, K), \u0026#34;k\u0026#34;) è¿™è¡Œä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º k çš„çº¦ç®€è½´ï¼Œè¡¨ç¤ºåœ¨çŸ©é˜µä¹˜æ³•æ“ä½œä¸­è¿›è¡Œæ±‚å’Œçš„ç»´åº¦ï¼ŒèŒƒå›´ä¸º (0, K)\nå®šä¹‰è¾“å…¥çŸ©é˜µ (Placeholders):\nA = te.placeholder((M, K), name=\u0026#34;A\u0026#34;)\rB = te.placeholder((K, N), name=\u0026#34;B\u0026#34;) è¿™ä¸¤è¡Œä»£ç å®šä¹‰äº†ä¸¤ä¸ªåä¸º A å’Œ B çš„è¾“å…¥çŸ©é˜µï¼Œå®ƒä»¬åˆ†åˆ«ä»£è¡¨çŸ©é˜µä¹˜æ³•çš„ä¸¤ä¸ªè¾“å…¥çŸ©é˜µã€‚A çš„å½¢çŠ¶ä¸º (M, K)ï¼ŒB çš„å½¢çŠ¶ä¸º (K, N)\nå®šä¹‰è¾“å‡ºçŸ©é˜µ (Compute):\nC = te.compute((M, N), lambda m, n: te.sum(A[m, k] * B[k, n], axis=k), name=\u0026#34;C\u0026#34;) è¿™è¡Œä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º C çš„è¾“å‡ºçŸ©é˜µï¼Œå®ƒè¡¨ç¤ºçŸ©é˜µä¹˜æ³•çš„ç»“æœã€‚C çš„å½¢çŠ¶ä¸º (M, N)ï¼Œé‡‡ç”¨ te.sumè®¡ç®—ç»“æœã€‚\nte.sum te.sum(expr, axis=None, keepdims=False, where=None) å‚æ•°è§£é‡Š:\nexpr: è¦è¿›è¡Œæ±‚å’Œçš„è¡¨è¾¾å¼ï¼Œå¯ä»¥æ˜¯å¼ é‡ã€æ ‡é‡æˆ–å…¶ä»–è¡¨è¾¾å¼ã€‚ axis: è¦è¿›è¡Œæ±‚å’Œçš„è½´ï¼Œå¯ä»¥æ˜¯æ•´æ•°ã€å…ƒç»„æˆ–åˆ—è¡¨ã€‚å¦‚æœ axis ä¸º Noneï¼Œåˆ™å¯¹æ‰€æœ‰è½´è¿›è¡Œæ±‚å’Œã€‚ keepdims: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä¿ç•™æ±‚å’Œåçš„ç»´åº¦ã€‚å¦‚æœä¸º Trueï¼Œåˆ™ä¿ç•™æ±‚å’Œåçš„ç»´åº¦ï¼Œå¹¶å°†å…¶å¤§å°è®¾ç½®ä¸º 1ã€‚å¦‚æœä¸º Falseï¼Œåˆ™åˆ é™¤æ±‚å’Œåçš„ç»´åº¦ã€‚ where: å¸ƒå°”å€¼å¼ é‡ï¼Œè¡¨ç¤ºè¦è¿›è¡Œæ±‚å’Œçš„å…ƒç´ ã€‚å¦‚æœ where ä¸º Noneï¼Œåˆ™å¯¹æ‰€æœ‰å…ƒç´ è¿›è¡Œæ±‚å’Œã€‚ åˆ›å»ºçš„IRModuleå¦‚ä¸‹æ‰€ç¤ºã€‚\n@tvm.script.ir_module class Module: @T.prim_func def main(A: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], B: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], C: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;]) -\u0026gt; None: # function attr dict T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) # body # with T.block(\u0026#34;root\u0026#34;) for i0, i1, i2 in T.grid(1024, 1024, 1024): with T.block(\u0026#34;C\u0026#34;): m, n, k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, i1, i2]) T.reads(A[m, k], B[k, n]) T.writes(C[m, n]) with T.init(): C[m, n] = T.float32(0) C[m, n] = C[m, n] + A[m, k] * B[k, n] æˆ‘ä»¬å¯ä»¥å°†å¾ªç¯æ‹†åˆ†æˆå¤–å±‚å¾ªç¯å’Œå†…å±‚å¾ªç¯å¯ä»¥æé«˜æ•°æ®å±€éƒ¨æ€§ã€‚å†…å±‚å¾ªç¯è®¿é—®çš„æ•°æ®æ›´æ¥è¿‘ï¼Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨ç¼“å­˜ã€‚ä¸‹é¢ä»£ç çš„ block_size å‚æ•°æ§åˆ¶äº†å†…å±‚å¾ªç¯çš„å¤§å°ï¼Œé€‰æ‹©åˆé€‚çš„å—å¤§å°å¯ä»¥æœ€å¤§ç¨‹åº¦åœ°åˆ©ç”¨ç¼“å­˜ã€‚\nsch = tvm.tir.Schedule(ir_module) block_c = sch.get_block(\u0026#34;C\u0026#34;) # Get loops surronding the block (y, x, k) = sch.get_loops(block_c) block_size = 32 yo, yi = sch.split(y, [None, block_size]) xo, xi = sch.split(x, [None, block_size]) sch.reorder(yo, xo, k, yi, xi) sch.mod.show() func = tvm.build(sch.mod, target=\u0026#34;llvm\u0026#34;) # The module for CPU backends. c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev) func(a, b, c) evaluator = func.time_evaluator(func.entry_name, dev, number=1) print(\u0026#34;after transformation: %f\u0026#34; % evaluator(a, b, c).mean) åˆ›å»ºçš„IRModuleå¦‚ä¸‹æ‰€ç¤ºã€‚å®é™…ä¸­æˆ‘ä»¬ä¼šæµ‹è¯•å¾ˆå¤šä¸åŒ block_sizeå¯¹åº”çš„æ‰§è¡Œæ—¶é—´æ¥é€‰æ‹©æœ€åˆé€‚çš„ã€‚\n@tvm.script.ir_module class Module: @T.prim_func def main(A: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], B: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;], C: T.Buffer[(1024, 1024), \u0026#34;float32\u0026#34;]) -\u0026gt; None: # function attr dict T.func_attr({\u0026#34;global_symbol\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;tir.noalias\u0026#34;: True}) # body # with T.block(\u0026#34;root\u0026#34;) for i0_0, i1_0, i2, i0_1, i1_1 in T.grid(32, 32, 1024, 32, 32): with T.block(\u0026#34;C\u0026#34;): m = T.axis.spatial(1024, i0_0 * 32 + i0_1) n = T.axis.spatial(1024, i1_0 * 32 + i1_1) k = T.axis.reduce(1024, i2) T.reads(A[m, k], B[k, n]) T.writes(C[m, n]) with T.init(): C[m, n] = T.float32(0) C[m, n] = C[m, n] + A[m, k] * B[k, n] ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch1/","summary":"Personal notebook 1.","title":"TVM Learning (1)-Tensor Program Abstraction in Action"},{"content":"IRModule: The key concept in TVM Unity IRModule æ˜¯å¼ é‡å‡½æ•°çš„é›†åˆï¼Œä»£è¡¨æˆ‘ä»¬éœ€è¦åœ¨æ¨¡å‹ä¸­æ‰§è¡Œçš„è®¡ç®—å­é›†ã€‚ä¾‹å¦‚ï¼Œåœ¨ MLC-LLM ä¸­ï¼Œå®ƒå¯ä»¥æ˜¯ä¸€ä¸ª Transformer æ¨¡å—ã€‚ æœºå™¨å­¦ä¹ ç¼–è¯‘æ¡†æ¶ä¸­çš„ IRModule å°±åƒæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­çš„å¼ é‡ï¼Œæ˜¯ä¸€åˆ‡çš„åŸºç¡€ã€‚åœ¨æ•´ä¸ªç¼–è¯‘æµç¨‹ä¸­ï¼Œæ¨¡å‹å°†ä»¥ IRModule çš„å½¢å¼å¯¼å…¥ï¼Œç„¶åä»¥ IRModule åˆ° IRModule çš„æ–¹å¼è¿›è¡Œè½¬æ¢å’Œä¼˜åŒ–ï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥åœ¨ä»»ä½•æ”¯æŒçš„å¹³å°ä¸Šå°† IRModule è½¬åŒ–ä¸ºå¯è¿è¡Œçš„æ¨¡å—ã€‚IRModule å¯ä»¥ç”¨ python æ–¹å¼è®¿é—®ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ python AST çš„å½¢å¼æ˜¾ç¤ºå®ƒï¼Œä»¥ä¾¿æ£€æŸ¥ã€è°ƒæ•´å’Œè°ƒè¯•ã€‚unity çš„ä¸»è¦è®¾è®¡ç›®æ ‡ä¹‹ä¸€æ˜¯å®ç°å•ä¸€æŠ½è±¡ï¼Œå°†æ‰€æœ‰ä¸»è¦å…ƒç´ å°è£…åœ¨åŒä¸€æ¨¡å—ä¸­ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±èƒ½åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæœ‰æœºçš„å¢é‡è½¬æ¢ã€‚\nTVM Unity.png\nTVMScript æ˜¯ IRModule çš„ python AST æ ¼å¼ï¼Œç”¨äºåœ¨æ•´å¥—è½¬æ¢è¿‡ç¨‹ä¸­æ£€æŸ¥ IRModules å¹¶ä¸ä¹‹äº¤äº’ã€‚ä¸ IRModule çš„äº¤äº’éƒ½å¯ä»¥ä½¿ç”¨ TVMScript åœ¨ python ä¸­è¿›è¡Œã€‚ç”¨æˆ·å°† TVMScript è§£æä¸º IRModule å†…éƒ¨ç»“æ„ï¼Œä½¿ç”¨ python API æ“ä½œ IRModuleï¼Œå¹¶å°† IRModule æ‰“å°ä¸º TVMScript æ ¼å¼ã€‚\nTVMScript Examples ç”¨ Pytorch æ¡†æ¶å®ç°çŸ©é˜µä¹˜æ³•ä¸€èˆ¬è°ƒç”¨ torch.matmul æˆ–è€…ä½¿ç”¨ @ ç®—å­ã€‚\nimport torch a = torch.randn((3, 4)) b = torch.randn((4, 5)) print(torch.matmul(a, b)) \u0026#39;\u0026#39;\u0026#39; tensor([[ 2.5387, 2.2756, -2.2032, 2.5928, -3.6539], [ 2.0151, 0.0628, -0.8041, -1.6947, 0.2884], [-0.8118, -0.0453, 0.0742, -1.2028, 1.3722]]) \u0026#39;\u0026#39;\u0026#39; åœ¨ Relax ä¸­å¯ä»¥ç”¨ IRModule å®ç°ç›¸åŒçš„åŠŸèƒ½ã€‚\nfrom tvm.script import ir as I from tvm.script import relax as R @I.ir_module class Module: @R.function def main(A: R.Tensor((3, 4), dtype=\u0026#34;float32\u0026#34;), B: R.Tensor((4, 5), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((3, 5), dtype=\u0026#34;float32\u0026#34;): with R.dataflow(): lv: R.Tensor((3, 5), dtype=\u0026#34;float32\u0026#34;) = R.matmul(A, B, out_dtype=\u0026#34;void\u0026#34;) R.output(lv) return lv é€šè¿‡ä¸Šè¿° TVMScript åˆ›å»ºçš„ IRModule æ˜¯ä¸€ä¸ªå®Œå…¨å›¾çº§åˆ«çš„æŠ½è±¡ï¼ŒåªåŒ…å«ä¸€ä¸ª R.function (Relax å‡½æ•°ï¼š IRModule ä¸­è®¡ç®—å›¾çš„è¡¨ç¤ºå½¢å¼) ä¸Šè¿°ç¤ºä¾‹åŒ…å« Relax å‡½æ•°ä¸­çš„ä¸¤ä¸ªé‡è¦æ¦‚å¿µï¼šé«˜çº§ Relax ç®—å­å’Œæ•°æ®æµå—ã€‚\nRelax å‡½æ•°åŒ…å«é«˜çº§ Relax ç®—å­ R.matmulï¼Œå®ƒæè¿°è®¡ç®—å›¾ä¸­çš„èŠ‚ç‚¹ï¼Œä¸åŒ…å«å…¶åº•å±‚å®ç°çš„ä¿¡æ¯ã€‚ä¸€ä¸ªé«˜çº§ Relax ç®—å­å¯ä»¥æ˜ å°„åˆ°ä¸åŒçš„åº•å±‚å®ç°ï¼ŒTVM Unity çš„ç¼–è¯‘æµç¨‹ä¼šç”Ÿæˆæ€§èƒ½è‰¯å¥½çš„å®ç°ã€‚ R.dataflow() æ˜¯æ•°æ®æµå—çš„ä¸€ä¸ªé‡è¦ä½œç”¨åŸŸæ³¨è§£ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æ•°æ®æµå—å†…ï¼Œæ‰€æœ‰æ“ä½œéƒ½å¿…é¡»æ˜¯ side-effect free. è€Œåœ¨æ•°æ®æµå—ä¹‹å¤–ï¼Œæ“ä½œå¯èƒ½åŒ…å«å‰¯ä½œç”¨ã€‚ A more complex TVMScript example: 2-layer MLP ä¸‹é¢æˆ‘ä»¬ä»¥ä¸€ä¸ªæ›´å¤æ‚çš„ä¸¤å±‚ MLP ä¸ºä¾‹ï¼Œæ¨¡å‹ç»“æ„å¦‚ä¸‹ã€‚\n2-layer MLP\nå…¶å¯¹åº”çš„ Pytoch å®ç°å¦‚ä¸‹\nclass MLP(torch.nn.Module): def __init__(self, *args, **kwargs) -\u0026gt; None: super(MLP, self).__init__(*args, **kwargs) self.linear1 = torch.nn.Linear(784, 128) self.linear2 = torch.nn.Linear(128, 10) def forward(self, x): x = self.linear1(x) x = torch.nn.functional.relu(x) x = self.linear2(x) return x å¯¹åº”çš„ IRModule çš„ TVMScript è¡¨ç¤ºå¦‚ä¸‹\n@I.ir_module class Module: @R.function def main(inp_0: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;), weight1: R.Tensor((128, 784), dtype=\u0026#34;float32\u0026#34;), bias1: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;), weight2: R.Tensor((10, 128), dtype=\u0026#34;float32\u0026#34;), bias2: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): with R.dataflow(): lv: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(weight1, axes=None) lv_1: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.matmul(inp_0, lv, out_dtype=\u0026#34;void\u0026#34;) lv1: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.add(lv_1, bias1) lv2: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.nn.relu(lv1) lv4: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(weight2, axes=None) lv3: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.matmul(lv2, lv4, out_dtype=\u0026#34;void\u0026#34;) lv4_1: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.add(lv3, bias2) R.output(lv4_1) return lv4_1 ä¸Šè¿° Relax å‡½æ•°åªåŒ…å«é«˜çº§ Relax ç®—å­ã€‚åœ¨ pytorch ä¸­ï¼Œtorch.nn.Linear è®¡ç®— $y = xW^T + b$ åœ¨ relax ä¸­ï¼Œè½¬ç½®ç”± permute_dims å®ç°ï¼Œå…¶æ¬¡æ˜¯ çŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•åˆ†åˆ«ç”± R.matmul å’Œ R.add å®ç°ã€‚\nCompilation Flow in TVM Unity å°†æ¨¡å‹å¯¼å…¥ IRModule. å¯¹äºé™æ€æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ pytorch dynamo å°† pytorch ç¨‹åºè·Ÿè¸ªä¸º fx å›¾ï¼Œç„¶åè½¬æ¢ä¸º IRModuleã€‚ç„¶è€Œï¼ŒLLM é€šå¸¸æ˜¯åŠ¨æ€çš„ï¼Œå› ä¸ºåºåˆ—é•¿åº¦å’Œ kv cache é•¿åº¦éƒ½æ˜¯å¯å˜çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ç›´æ¥åœ¨ IRModule ä¸­å»ºç«‹æ¨¡å‹ã€‚ç¬¬ä¸€æ­¥å¯ä»¥æŠ½è±¡ä¸º LLM -\u0026gt; IRModule è½¬æ¢ã€‚ ä¼˜åŒ–æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿç¼–è¯‘å™¨ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ IRModule ä¸Šåº”ç”¨ pass (IRModule åˆ° IRModule çš„å˜æ¢ï¼Œæ”¹å˜è®¡ç®—ä½†ä¿ç•™äº†åŸå§‹ IRModule çš„è¯­ä¹‰)ã€‚åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŠ é€Ÿæ¨¡å‹è®¡ç®—ã€‚åœ¨æ¶ˆè´¹ç±»è®¾å¤‡ä¸Šä»¥é€‚å½“é€Ÿåº¦è¿è¡Œ LLM çš„å¤§å¤šæ•°å…³é”®æŠ€æœ¯ï¼Œå¦‚é‡åŒ–ã€ç®—å­èåˆå’Œå¼ é‡å‡½æ•°è°ƒåº¦ï¼Œéƒ½æ˜¯åœ¨è¿™ä¸€æ­¥å®ç°çš„ã€‚ åœ¨è®¾å¤‡ä¸Šéƒ¨ç½² IRModuleã€‚å¯¹äºæ¯ä¸ª IRM æ¨¡å—ï¼Œæˆ‘ä»¬éƒ½èƒ½å°†å…¶è½¬åŒ–ä¸ºå¯è¿è¡Œæ¨¡å—ï¼Œå¹¶åœ¨ tvm è¿è¡Œæ—¶æ”¯æŒçš„ä»»ä½•å¹³å°ä¸Šè¿è¡Œã€‚IRModule ä¸Šçš„æ¯ä¸ªå‡½æ•°éƒ½å°†æˆä¸ºç¯å¢ƒä¸­çš„æœ¬åœ°å¯è¿è¡Œå‡½æ•°ã€‚ ä»¥ä¸‹æ˜¯ 2 å±‚ MLP æ¨¡å‹çš„ç¼–è¯‘æµç¨‹\nfrom tvm import relax import tvm from tvm.ir.module import IRModule mod = MLPModule def optimize_and_deploy(mod: IRModule): # step 2. Optimization # Use default graph optimization pipeline mod = relax.pipeline.get_pipeline()(mod) # Use default tensor function scheduling with tvm.target.Target(\u0026#34;cuda\u0026#34;): mod = tvm.tir.transform.DefaultGPUSchedule()(mod) # Step 3. Deploy to GPU ex = relax.build(mod, \u0026#34;cuda\u0026#34;) vm = relax.VirtualMachine(ex, tvm.cuda()) # test correctness import numpy as np input_np = np.random.rand(1, 784).astype(\u0026#34;float32\u0026#34;) weight1_np = np.random.rand(128, 784).astype(\u0026#34;float32\u0026#34;) bias1_np = np.random.rand(1, 128).astype(\u0026#34;float32\u0026#34;) weight2_np = np.random.rand(10, 128).astype(\u0026#34;float32\u0026#34;) bias2_np = np.random.rand(1, 10).astype(\u0026#34;float32\u0026#34;) tvm_nd_arrays = [tvm.nd.array(np_array, device=tvm.cuda()) for np_array in [input_np, weight1_np, bias1_np, weight2_np, bias2_np]] # call into the runnable function converted from IRModule nd_res = vm[\u0026#34;main\u0026#34;](*tvm_nd_arrays) numpy_res = (input_np @ weight1_np.T + bias1_np) @ weight2_np.T + bias2_np np.testing.assert_allclose(numpy_res, nd_res.numpy(), rtol=1e-5) optimize_and_deploy(mod) Build IRModule in Pytorch Style æ„å»º IRModule æœ€ç›´æ¥çš„æ–¹æ³•æ˜¯æ‰‹åŠ¨ç¼–å†™ TVMScriptã€‚è¿™ç§æ–¹æ³•é€‚ç”¨äºå°å‹æ¨¡å‹ï¼Œä½† LLM çš„ IRModule éå¸¸åºå¤§å’Œå¤æ‚ï¼Œæ‰‹å·¥ç¼–å†™å¹¶ä¸ç°å®ã€‚TVM Unity æä¾›äº†å¦ä¸€ä¸ªç±» nn.Moduleï¼Œå¯ä»¥åƒ pytorch æ¨¡å—ä¸€æ ·è½»æ¾æ„å»º IRModule. ç”¨ Pytorch æ‰‹åŠ¨ç¼–å†™çš„ä¸€ä¸ª Linear å±‚å¦‚ä¸‹\nclass TorchLinear(torch.nn.Module): def __init__(self, in_features, out_features, bias=True): super().__init__() self.in_features = in_features self.out_features = out_features self.weight = torch.nn.Parameter(torch.randn(out_features, in_features)) if bias: self.bias = torch.nn.Parameter(torch.randn(out_features)) else: bias = None def forward(self, x): return x @ self.weight.T + self.bias åœ¨ Relax ä¸­çš„å®ç°å¦‚ä¸‹\nfrom tvm.relax.testing import nn class RelaxLinear(nn.Module): def __init__(self, in_features, out_features, dtype: str, bias=True) -\u0026gt; None: super(RelaxLinear, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = nn.Parameter((out_features, in_features), dtype, name=\u0026#34;linear_weight\u0026#34;) if bias: self.bias = nn.Parameter((1, out_features), dtype, name=\u0026#34;linear_bias\u0026#34;) else: self.bias = None def forward(self, x: relax.Expr) -\u0026gt; relax.Var: return nn.emit(relax.op.linear(x, self.weight, self.bias)) ä¸ Pytorch çš„ç»“æ„éå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯å‰å‘å‡½æ•°å®é™…ä¸Šå¹¶ä¸æ‰§è¡Œè®¡ç®—ã€‚å®ƒä½¿ç”¨ä½œä¸ºè¾“å…¥ä¼ é€’çš„å ä½ç¬¦è·Ÿè¸ªç®—å­çš„è®¡ç®—å›¾ã€‚ nn.emit(relax.op.linear(input, self.weight, self.bias)) è¡¨ç¤ºåœ¨æ„å»ºçš„ IRModule ä¸­æ·»åŠ é«˜çº§ linear ç®—å­ã€‚ é€šè¿‡å †å  1 ä¸ªçº¿æ€§å±‚ã€1 ä¸ª relu å±‚å’Œ 1 ä¸ªçº¿æ€§å±‚ï¼Œå°±å¯ä»¥æ„å»ºä¾‹å­ä¸­çš„ MLP.\nclass RelaxMLP(nn.Module): def __init__(self, in_features, hidden_dims, out_features, dtype=\u0026#34;float32\u0026#34;) -\u0026gt; None: super(RelaxMLP, self).__init__() self.linear1 = RelaxLinear(in_features, hidden_dims, dtype) self.lienar2 = RelaxLinear(hidden_dims, out_features, dtype) def forward(self, x: relax.Expr) -\u0026gt; relax.Var: hidden = self.linear1(x) hidden = nn.emit(relax.op.nn.relu(hidden)) out = self.lienar2(hidden) return out ç›´æ¥è°ƒç”¨ nn.Module çš„å‰å‘å‡½æ•°å°±å¯ä»¥ä»£æ›¿åŸå…ˆåœ¨ with bb.dataflow(): ä¸‹çš„æ“ä½œï¼Œå°† nn.Module æ„å»ºæˆ IRModule çš„æ­¥éª¤å¦‚ä¸‹\ndef build_relax(mod: nn.Module): # relax.BlockBuilder can construct end-to-end models step by step in an IRModule that starts empty bb = relax.BlockBuilder() # relax nn.Module model = mod(784, 128, 10) # create a function called \u0026#34;main\u0026#34; in the IRModule with bb.function(\u0026#34;main\u0026#34;): # define input placeholder to the relax nn.Module input = nn.Placeholder((1, 784), dtype=\u0026#34;float32\u0026#34;, name=\u0026#34;input\u0026#34;) # build dataflow block with bb.dataflow(): # call forward function logits = model(input) # The params of the constructed IRModule params = [input] + model.parameters() # return value of the dataflow block gv = bb.emit_output(logits) # return value and params of the Relax function bb.emit_func_output(gv, params) return bb.get() build_relax(RelaxMLP).show() #------------------------------ @I.ir_module class Module: @R.function def main(input: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;), linear_weight: R.Tensor((128, 784), dtype=\u0026#34;float32\u0026#34;), linear_bias: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;), linear_weight_1: R.Tensor((10, 128), dtype=\u0026#34;float32\u0026#34;), linear_bias_1: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): with R.dataflow(): lv: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(linear_weight, axes=None) lv1: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.matmul(input, lv, out_dtype=\u0026#34;void\u0026#34;) lv2: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.add(lv1, linear_bias) lv3: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.nn.relu(lv2) lv4: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(linear_weight_1, axes=None) lv5: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.matmul(lv3, lv4, out_dtype=\u0026#34;void\u0026#34;) lv6: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.add(lv5, linear_bias_1) gv: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = lv6 R.output(gv) return gv Custom Operator Support åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¦è¡¨ç¤ºçš„æ¨¡å‹åŒ…å«ä¸€äº›è‡ªå®šä¹‰è¿ç®—ç¬¦ï¼Œè€Œè¿™äº›è¿ç®—ç¬¦æ²¡æœ‰è¢«æä¾›çš„ Relax è¿ç®—ç¬¦è¦†ç›–ï¼ˆå¦‚ LLaMA ä¸­çš„ Rotary Embeddingï¼‰ï¼Œæˆ–è€…æˆ‘ä»¬è¦è¿›è¡Œåº•å±‚ä¼˜åŒ–ä»¥åŠ é€Ÿå•ä¸ªå†…æ ¸ã€‚ä¸‹é¢ä»‹ç»å¦‚ä½•åœ¨ IRModule ä¸­ç¼–å†™è‡ªå®šä¹‰ç®—å­ã€‚\nTensorIR: Low-level tensor function TVM Unity åœ¨ IRModule TensorIR ä¸­æä¾›äº†åº•å±‚å¼ é‡å‡½æ•°çš„è¡¨ç¤ºæ–¹æ³•ï¼Œç”¨æˆ·å¯ä»¥åœ¨å…¶ä¸­å®šä¹‰è‡ªå®šä¹‰æ“ä½œç¬¦å¹¶æ‰§è¡Œç»†ç²’åº¦è°ƒåº¦ã€‚ ä¸‹é¢å¯¹æ¯”äº†ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ç”Ÿæˆçš„ TVMScript TensorIR ä»£ç å’Œ low-level Pytorch ä»£ç ã€‚@T.prim_funcè£…é¥°å™¨è¡¨ç¤ºä¸‹é¢çš„å‡½æ•°æ˜¯ä¸€ä¸ªåŸå§‹çš„å¼ é‡å‡½æ•°ï¼ŒåŒ…å«è¿ç®—ç¬¦å®ç°çš„åº•å±‚ç»†èŠ‚ã€‚\nDestination Passing T.prim_func é‡‡ç”¨ destination-passing çº¦å®šï¼Œå³åœ¨å‡½æ•°å¤–éƒ¨æ˜ç¡®åˆ†é…è¾“å…¥å’Œè¾“å‡ºç©ºé—´ï¼Œå¹¶å°†å…¶ä½œä¸ºå‚æ•°ä¼ å…¥ã€‚destination-passing çº¦å®šå¯ä»¥å¯¹å†…å­˜åˆ†é…è¿›è¡Œç²¾ç»†è°ƒåº¦ï¼Œä¾‹å¦‚åˆå¹¶ä¸¤ä¸ªå®æ—¶é—´éš”ä¸ç›¸äº¤çš„å˜é‡çš„å†…å­˜åˆ†é…ï¼Œè¿™æ˜¯åœ¨å†…å­˜æœ‰é™çš„è®¾å¤‡ä¸Šè¿è¡Œå¤§å‹æ¨¡å‹çš„å…³é”®ã€‚ from tvm.script import tir as T @T.prim_func def matmul(rxplaceholder: T.Buffer((T.int64(1), T.int64(784)), \u0026#34;float32\u0026#34;), rxplaceholder_1: T.Buffer((T.int64(784), T.int64(128)), \u0026#34;float32\u0026#34;), matmul: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: True}) # with T.block(\u0026#34;root\u0026#34;): for i0, i1, k in T.grid(T.int64(1), T.int64(128), T.int64(784)): with T.block(\u0026#34;matmul\u0026#34;): v_i0, v_i1, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, i1, k]) T.reads(rxplaceholder[v_i0, v_k], rxplaceholder_1[v_k, v_i1]) T.writes(matmul[v_i0, v_i1]) with T.init(): matmul[v_i0, v_i1] = T.float32(0) matmul[v_i0, v_i1] = matmul[v_i0, v_i1] + rxplaceholder[v_i0, v_k] * rxplaceholder_1[v_k, v_i1] def torch_matmul(X: torch.Tensor, W: torch.Tensor): Y = torch.zeros(1, 128, dtype=\u0026#34;float32\u0026#34;) for i in range(1): for j in range(128): for k in range(784): Y[i, j] = Y[i, j] + X[i, k] * W[k, j] return Y Interaction between Relax function and TensorIR ä¸ºäº†æ”¯æŒ T.prim_funcï¼ˆåº•å±‚éƒ¨åˆ†ï¼‰å’Œ R.functionï¼ˆé«˜å±‚éƒ¨åˆ†ï¼‰ä¹‹é—´çš„äº¤äº’ï¼ŒTVM å¼•å…¥äº† call_tir, Relax ä¸­çš„ä¸€ä¸ªç‰¹æ®Šè¿ç®—ç¬¦ï¼Œç”¨äºæè¿°è®¡ç®—å›¾ä¸­çš„èŠ‚ç‚¹åŠå…¶å¼ é‡å‡½æ•°çš„å®ç°ã€‚ torch_call_tir æ˜¯ä¸€ä¸ªå‚è€ƒå®ç°ï¼Œç”¨æ¥è¯´æ˜ call_tir çš„å«ä¹‰ã€‚å®é™…ä¸Šï¼Œå¯ä»¥æœ‰ä¸åŒçš„åº•å±‚æ–¹æ³•æ¥ä¼˜åŒ–æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé€‰æ‹©æå‰åˆ†é…æ‰€æœ‰è¾“å‡ºå†…å­˜ï¼Œç„¶åå†è¿è¡Œæ‰§è¡Œã€‚\ndef torch_call_tir(prim_func, inputs, out_sinfo): res = torch.zeros(*out_sinfo.shape, dtype=out_sinfo.dtype) prim_func(*inputs, res) return res ä¸‹é¢æ˜¯ 2 å±‚ MLP çš„ IRModuleï¼Œæˆ‘ä»¬ä½¿ç”¨ call_tir å’Œå¼ é‡åŸè¯­å‡½æ•° matmul æ¥æ›¿æ¢ Relax è¿ç®—ç¬¦ R.matmul\n@I.ir_module class Module: @T.prim_func def tir_matmul(rxplaceholder: T.Buffer((T.int64(1), T.int64(784)), \u0026#34;float32\u0026#34;), rxplaceholder_1: T.Buffer((T.int64(784), T.int64(128)), \u0026#34;float32\u0026#34;), matmul: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: True}) # with T.block(\u0026#34;root\u0026#34;): for i0, i1, k in T.grid(T.int64(1), T.int64(128), T.int64(784)): with T.block(\u0026#34;matmul\u0026#34;): v_i0, v_i1, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i0, i1, k]) T.reads(rxplaceholder[v_i0, v_k], rxplaceholder_1[v_k, v_i1]) T.writes(matmul[v_i0, v_i1]) with T.init(): matmul[v_i0, v_i1] = T.float32(0) matmul[v_i0, v_i1] = matmul[v_i0, v_i1] + rxplaceholder[v_i0, v_k] * rxplaceholder_1[v_k, v_i1] @R.function def main(inp_0: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;), weight1: R.Tensor((128, 784), dtype=\u0026#34;float32\u0026#34;), bias1: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;), weight2: R.Tensor((10, 128), dtype=\u0026#34;float32\u0026#34;), bias2: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv: R.Tensor((784, 128), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(weight1, axes=None) lv1 = R.call_tir(cls.tir_matmul, [inp_0, lv], out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv2: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.add(lv1, bias1) lv3: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.nn.relu(lv2) lv4: R.Tensor((128, 10), dtype=\u0026#34;float32\u0026#34;) = R.permute_dims(weight2, axes=None) lv5: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.matmul(lv3, lv4, out_dtype=\u0026#34;float32\u0026#34;) lv6: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = R.add(lv5, bias2) gv: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = lv6 R.output(gv) return gv Implement Custom TensorIR Function nn.Module ä¸ä»…æ”¯æŒé«˜çº§ Relax è¿ç®—ç¬¦ï¼Œè¿˜æ”¯æŒè‡ªå®šä¹‰ TensorIR å‡½æ•°ã€‚ è¦æ„å»º TensorIR å‡½æ•°å¹¶åœ¨ Relax å›¾ä¸­è°ƒç”¨å®ƒï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ nn.emit_te(f_te_expr,*args)ã€‚\nf_te_expr æ˜¯ä¸€ä¸ªè¿”å›å¼ é‡è¡¨è¾¾å¼ï¼ˆTensor Expressionï¼ŒTEï¼‰çš„å‡½æ•°ï¼Œæ˜¯æè¿°å¼ é‡è®¡ç®—çš„ DSL. args æ˜¯ f_te_expr çš„å‚æ•°ã€‚ åˆ›å»º TE è¡¨è¾¾å¼çš„æ–¹æ³•å¦‚ä¸‹\nte.compute(out_shape, f_compute) å®ƒæè¿°å¦‚ä¸‹çš„è®¡ç®—æ¨¡å¼ itertools.product åœ¨ Python çš„ itertools æ¨¡å—ä¸­ï¼Œproduct å‡½æ•°ç”¨äºç”Ÿæˆå¯è¿­ä»£å¯¹è±¡çš„ç¬›å¡å°”ç§¯ã€‚\nproduct å‡½æ•°æ¥å—ä¸€ä¸ªæˆ–å¤šä¸ªå¯è¿­ä»£å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œè¯¥è¿­ä»£å™¨ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç»„åˆï¼Œå…¶ä¸­æ¯ä¸ªç»„åˆåŒ…å«æ¥è‡ªæ¯ä¸ªè¾“å…¥å¯è¿­ä»£å¯¹è±¡çš„å•ä¸ªå…ƒç´ ã€‚\nimport itertools letters = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] numbers = [1, 2, 3] for item in itertools.product(letters, numbers): print(item) # outputï¼š # (\u0026#39;a\u0026#39;, 1) # (\u0026#39;a\u0026#39;, 2) # (\u0026#39;a\u0026#39;, 3) # (\u0026#39;b\u0026#39;, 1) # (\u0026#39;b\u0026#39;, 2) # (\u0026#39;b\u0026#39;, 3) product å‡½æ•°è¿˜æ”¯æŒé‡å¤å…ƒç´ ï¼Œå¯ä»¥ä½¿ç”¨ repeat å‚æ•°æŒ‡å®šæ¯ä¸ªå¯è¿­ä»£å¯¹è±¡éœ€è¦é‡å¤çš„æ¬¡æ•°ã€‚\nletters = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] for item in itertools.product(letters, repeat=3): print(item) # outputï¼š # (\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;) # (\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;) # (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;) # (\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;) # (\u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;) # (\u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;) # (\u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;) # (\u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;) product åº”ç”¨åœºæ™¯\nç»„åˆç”Ÿæˆ: ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç»„åˆï¼Œä¾‹å¦‚å¯†ç ç”Ÿæˆã€å½©ç¥¨å·ç ç”Ÿæˆç­‰ã€‚ å¤šç»´æ•°ç»„éå†: éå†å¤šç»´æ•°ç»„çš„æ‰€æœ‰å…ƒç´ ã€‚ æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ: ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–æ‰€æœ‰å¯èƒ½çš„è¾“å…¥ç»„åˆã€‚ from itertools import product for indices in product(range(s) for s in out_shape): out_tensor[*indices] = f_compute(*indices) ç”¨ emit_te å®ç° Linear å±‚æ¥æ„å»º IRModule çš„ä»£ç å¦‚ä¸‹\nfrom tvm import te class RelaxLinearWithEmitTE(nn.Module): def __init__(self, in_features, out_features, dtype=\u0026#34;float32\u0026#34;, bias=True) -\u0026gt; None: super(RelaxLinearWithEmitTE, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = nn.Parameter((out_features, in_features), dtype, name=\u0026#34;linear_weight\u0026#34;) if bias: self.bias = nn.Parameter((1, out_features), dtype, name=\u0026#34;linear_bias\u0026#34;) else: self.bias = None def forward(self, x: relax.Expr) -\u0026gt; relax.Var: def my_linear(x, w, b=None): out_sinfo = x.shape[:-1] + [self.out_features,] k = te.reduce_axis((0, self.out_features), name=\u0026#34;k\u0026#34;) out = te.compute(out_sinfo, fcompute=lambda i, j: te.sum(x[i, k] * w[j, k], axis=k), name=\u0026#34;matmul\u0026#34;) if b is not None: return out else: return te.compute(out_sinfo, fcompute=lambda i, j: out[i, j] + b[0, j], name=\u0026#34;add_bias\u0026#34;) return nn.emit_te(my_linear, x, self.weight, self.bias) class RelaxMLPwithEmitTE(nn.Module): def __init__(self, in_features, hidden_num, out_features, dtype=\u0026#34;float32\u0026#34;): self.linear1 = RelaxLinearWithEmitTE(in_features, hidden_num, dtype=dtype) self.linear2 = RelaxLinearWithEmitTE(hidden_num, out_features, dtype=dtype) def forward(self, input: relax.Expr) -\u0026gt; relax.Var: hidden = self.linear1(input) hidden = nn.emit(relax.op.nn.relu(hidden)) out = self.linear2(hidden) return out build_relax(RelaxMLPwithEmitTE).show() #---------------------------------------------------- @I.ir_module class Module: @T.prim_func(private=True) def my_linear(input: T.Buffer((T.int64(1), T.int64(784)), \u0026#34;float32\u0026#34;), linear_weight: T.Buffer((T.int64(128), T.int64(784)), \u0026#34;float32\u0026#34;), linear_bias: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;), matmul: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i, j, k in T.grid(T.int64(1), T.int64(128), T.int64(128)): with T.block(\u0026#34;matmul\u0026#34;): v_i, v_j, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) T.reads(input[v_i, v_k], linear_weight[v_j, v_k]) T.writes(matmul[v_i, v_j]) with T.init(): matmul[v_i, v_j] = T.float32(0.0) matmul[v_i, v_j] = matmul[v_i, v_j] + input[v_i, v_k] * linear_weight[v_j, v_k] @T.prim_func(private=True) def my_linear1(lv1: T.Buffer((T.int64(1), T.int64(128)), \u0026#34;float32\u0026#34;), linear_weight: T.Buffer((T.int64(10), T.int64(128)), \u0026#34;float32\u0026#34;), linear_bias: T.Buffer((T.int64(1), T.int64(10)), \u0026#34;float32\u0026#34;), matmul: T.Buffer((T.int64(1), T.int64(10)), \u0026#34;float32\u0026#34;)): T.func_attr({\u0026#34;tir.noalias\u0026#34;: T.bool(True)}) # with T.block(\u0026#34;root\u0026#34;): for i, j, k in T.grid(T.int64(1), T.int64(10), T.int64(10)): with T.block(\u0026#34;matmul\u0026#34;): v_i, v_j, v_k = T.axis.remap(\u0026#34;SSR\u0026#34;, [i, j, k]) T.reads(lv1[v_i, v_k], linear_weight[v_j, v_k]) T.writes(matmul[v_i, v_j]) with T.init(): matmul[v_i, v_j] = T.float32(0.0) matmul[v_i, v_j] = matmul[v_i, v_j] + lv1[v_i, v_k] * linear_weight[v_j, v_k] @R.function def main(input: R.Tensor((1, 784), dtype=\u0026#34;float32\u0026#34;), linear_weight: R.Tensor((128, 784), dtype=\u0026#34;float32\u0026#34;), linear_bias: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;), linear_weight_1: R.Tensor((10, 128), dtype=\u0026#34;float32\u0026#34;), linear_bias_1: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) -\u0026gt; R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;): cls = Module with R.dataflow(): lv = R.call_tir(cls.my_linear, (input, linear_weight, linear_bias), out_sinfo=R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;)) lv1: R.Tensor((1, 128), dtype=\u0026#34;float32\u0026#34;) = R.nn.relu(lv) lv2 = R.call_tir(cls.my_linear1, (lv1, linear_weight_1, linear_bias_1), out_sinfo=R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;)) gv: R.Tensor((1, 10), dtype=\u0026#34;float32\u0026#34;) = lv2 R.output(gv) return gv ","permalink":"http://localhost:1313/blogs/courselearning/tvm/tvm-ch9/","summary":"Add Model Architeture in MLC LLM","title":"TVM Learning (11)-Add Model Architeture in MLC LLM"},{"content":"Hello, I am WITHER.\nğŸ”¬ Research Interests Training and Inference Acceleration LLM Reasoning High Performance Computing ğŸ§‘â€ğŸ“ Education 2019.09 - 2023.06: Bachelor of Communication Engineering, China University of Geoscience, Wuhan, China. 2023.09 - Now: Shanghai Jiao Tong University, Shanghai, China. ğŸ’» Work Experience 2024.06 - Present: Full-time Intern, Shanghai AI Laboratory, Shanghai, China. Research on inference acceleration and graph optimization of Large Language Models. Research on knowlege injection and reasoning of Large Language Models. ğŸ‰ Achievements ğŸ“° Publications ğŸ¤ª Hobbies ğŸ§™ Animations, Comics and Games ","permalink":"http://localhost:1313/about_me/","summary":"\u003cp\u003eHello, I am WITHER.\u003c/p\u003e\n\u003ch2 id=\"-research-interests\"\u003eğŸ”¬ Research Interests\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eTraining and Inference Acceleration\u003c/li\u003e\n\u003cli\u003eLLM Reasoning\u003c/li\u003e\n\u003cli\u003eHigh Performance Computing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-education\"\u003eğŸ§‘â€ğŸ“ Education\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e2019.09 - 2023.06\u003c/strong\u003e: Bachelor of Communication Engineering, China University of Geoscience, Wuhan, China.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e2023.09 - Now\u003c/strong\u003e: Shanghai Jiao Tong University, Shanghai, China.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-work-experience\"\u003eğŸ’» Work Experience\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e2024.06 - Present\u003c/strong\u003e: Full-time Intern, Shanghai AI Laboratory, Shanghai, China.\n\u003cul\u003e\n\u003cli\u003eResearch on inference acceleration and graph optimization of Large Language Models.\u003c/li\u003e\n\u003cli\u003eResearch on knowlege injection and reasoning of Large Language Models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"-achievements\"\u003eğŸ‰ Achievements\u003c/h2\u003e\n\u003ch2 id=\"-publications\"\u003eğŸ“° Publications\u003c/h2\u003e\n\u003ch2 id=\"-hobbies\"\u003eğŸ¤ª Hobbies\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eğŸ§™ Animations, Comics and Games\u003c/li\u003e\n\u003c/ul\u003e","title":"About Me"},{"content":" luminolt\u0026#39;s Page Cryptography Learner Jonathan523\u0026#39;s Page æ¯ä¸€ä¸ªä¸æ›¾èµ·èˆçš„æ—¥å­, éƒ½æ˜¯å¯¹ç”Ÿå‘½çš„è¾œè´Ÿ ","permalink":"http://localhost:1313/friends/","summary":"\u003cp\u003e\u003ca target=\"_blank\" href=https://luminolt.cn/ title=luminolt\u0026#39;s\u0026#32;Page class=\"friendurl\"\u003e\n  \u003cdiv class=\"frienddiv\"\u003e\n    \u003cdiv class=\"frienddivleft\"\u003e\n      \u003cimg class=\"myfriend\" src=https://luminolt.cn/author/chenghao-chen/avatar_hu15811225952467136947.jpg /\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"frienddivright\"\u003e\n      \u003cdiv class=\"friendname\"\u003eluminolt\u0026#39;s Page\u003c/div\u003e\n      \u003cdiv class=\"friendinfo\"\u003eCryptography Learner\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/a\u003e\n\u003ca target=\"_blank\" href=https://www.cestlavie.moe/ title=Jonathan523\u0026#39;s\u0026#32;Page class=\"friendurl\"\u003e\n  \u003cdiv class=\"frienddiv\"\u003e\n    \u003cdiv class=\"frienddivleft\"\u003e\n      \u003cimg class=\"myfriend\" src=/imgs/people/Jonathan523.png /\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"frienddivright\"\u003e\n      \u003cdiv class=\"friendname\"\u003eJonathan523\u0026#39;s Page\u003c/div\u003e\n      \u003cdiv class=\"friendinfo\"\u003eæ¯ä¸€ä¸ªä¸æ›¾èµ·èˆçš„æ—¥å­, éƒ½æ˜¯å¯¹ç”Ÿå‘½çš„è¾œè´Ÿ\u003c/div\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/a\u003e\u003c/p\u003e","title":"Friends"}]