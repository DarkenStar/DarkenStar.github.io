---
title: "LLaDA"
date: 2025-06-12T13:43:16+08:00
lastmod: 2025-06-12T13:43:16+08:00
author: ["WITHER"]

categories:
- Paper Reading

tags:
- DiffusionLLM

keywords:
- DiffusionLLM

description: "Paper Reading of LLaDA" # æ–‡ç« æè¿°ï¼Œä¸æœç´¢ä¼˜åŒ–ç›¸å…³
summary: "Paper Reading of LLaDA" # æ–‡ç« ç®€å•æè¿°ï¼Œä¼šå±•ç¤ºåœ¨ä¸»é¡µ
weight: # è¾“å…¥1å¯ä»¥é¡¶ç½®æ–‡ç« ï¼Œç”¨æ¥ç»™æ–‡ç« å±•ç¤ºæ’åºï¼Œä¸å¡«å°±é»˜è®¤æŒ‰æ—¶é—´æ’åº
slug: ""
draft: false # æ˜¯å¦ä¸ºè‰ç¨¿
comments: true
showToc: true # æ˜¾ç¤ºç›®å½•
TocOpen: true # è‡ªåŠ¨å±•å¼€ç›®å½•
autonumbering: true # ç›®å½•è‡ªåŠ¨ç¼–å·
hidemeta: false # æ˜¯å¦éšè—æ–‡ç« çš„å…ƒä¿¡æ¯ï¼Œå¦‚å‘å¸ƒæ—¥æœŸã€ä½œè€…ç­‰
disableShare: true # åº•éƒ¨ä¸æ˜¾ç¤ºåˆ†äº«æ 
searchHidden: false # è¯¥é¡µé¢å¯ä»¥è¢«æœç´¢åˆ°
showbreadcrumbs: true #é¡¶éƒ¨æ˜¾ç¤ºå½“å‰è·¯å¾„
mermaid: true
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---

# Introduction


LLM ä¸»è¦çš„æ€æƒ³æ˜¯ *generative modeling* çš„æ€æƒ³æ˜¯é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥ä¼˜åŒ–æ¨¡å‹çš„åˆ†å¸ƒ $\log p_\theta(\cdot)$ æ¥é€¼è¿‘æ•°æ®çš„åˆ†å¸ƒ $\log p_{\text{data}}(\cdot)$
$$
\underbrace{\max_\theta\mathbb{E}_{p_{\text{data}}(x)}\log p_\theta(x)\Leftrightarrow\min_\theta\operatorname{KL}(p_{\text{data}}(x)||p_\theta(x)).}_{\text{Generative modeling principles}} \tag{1}
$$

å½“å‰ï¼Œå‡ ä¹æ‰€æœ‰æˆ‘ä»¬ç†ŸçŸ¥çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä»GPTç³»åˆ—åˆ°LLaMAç³»åˆ—ï¼Œéƒ½åŸºäº*autoregressice modeling* æ¥å®ç°ã€‚è¿™ç§èŒƒå¼çš„æ ¸å¿ƒæ˜¯ **next-token prediction** ï¼Œå³æ ¹æ®å·²ç»ç”Ÿæˆçš„æ–‡æœ¬åºåˆ—ï¼Œé€ toekn åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€æœ‰å¯èƒ½å‡ºç°çš„ token. 

$$
\underbrace{p_\theta(x)=p_\theta(x^1)\prod_{i=2}^Lp_\theta(x^i\mid x^1,\ldots,x^{i-1})}_{\text{Autoregressive formulation}} \tag{2}
$$

è¿™ç§å•å‘ã€é¡ºåºçš„ç”Ÿæˆæ–¹å¼åœ¨å¤„ç†éœ€è¦åŒå‘æ¨ç†çš„ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ï¼Œä¸€ä¸ªå…¸å‹çš„ä¾‹å­å°±æ˜¯ **Reversal Curse** â€”â€”æ¨¡å‹çŸ¥é“ A is Bï¼Œå´å¾€å¾€æ— æ³•æ¨æ–­å‡º B is A.

LLM èƒ½åŠ›çš„æ ¸å¿ƒåŸºçŸ³æ˜¯ç”Ÿæˆå¼å»ºæ¨¡åŸç†æœ¬èº«ï¼Œå³é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡è®©æ¨¡å‹å­¦ä¹ çœŸå®ä¸–ç•Œçš„æ•°æ®åˆ†å¸ƒ ï¼Œè€Œéè‡ªå›å½’è¿™ä¸€å…·ä½“çš„å®ç°å½¢å¼ã€‚

{{< quote >}}
**It is the generative modeling principles (i.e., Eq. (1)), rather than the autoregressive formulation (i.e., Eq. (2)) itself, that fundamentally underpin the essential properties of LLMs.**
{{< /quote >}}

1. å¤§è¯­è¨€æ¨¡å‹çš„å¯æ‰©å±•æ€§ (scalability) â€”â€”å³æ¨¡å‹è¶Šå¤§ã€æ•°æ®è¶Šå¤šã€æ•ˆæœè¶Šå¥½çš„ç‰¹æ€§â€”â€”å¹¶éè‡ªå›å½’æ¨¡å‹æ‰€ç‹¬æœ‰ ã€‚ç›¸åï¼Œè¿™ç§å¯æ‰©å±•æ€§æ¥æºäºæ›´åº•å±‚çš„ç”Ÿæˆå¼å»ºæ¨¡åŸç†ï¼Œè€Œè¿™äº›åŸç†æ°å¥½ä¿è¯äº†*fisher consistency*[^1] 

2. *instruction-following* å’Œ *in-context learning*[^2] å¹¶éè‡ªå›å½’æ¨¡å‹æ‰€ç‹¬æœ‰ï¼Œè€Œæ˜¯æ‰€æœ‰è®¾è®¡å¾—å½“çš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹ (conditional generative models) åœ¨å¤„ç†ç»“æ„åŒ–è¯­è¨€ä»»åŠ¡æ—¶éƒ½åº”å…·å¤‡çš„å†…åœ¨å±æ€§ ã€‚

å› æ­¤ä½œè€…æå‡ºäº†**LLaDA** (**L**arge **L**anguage **D**iffusion with m**A**sking)ï¼Œä¸€ä¸ªä»é›¶å¼€å§‹è®­ç»ƒçš„ã€å‚æ•°é‡è¾¾åˆ° 8B çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ã€‚

![Zero&Few-Shot Benchmarks](https://share.note.youdao.com/yws/api/personal/file/WEB0c215954f8c354f24d2d478a8eb89fab?method=download&shareKey=94170299ede39d5102cf1cf6e397c5c7 "Zero&Few-Shot Benchmarks")

LLaDA ä½¿ç”¨äº† Masked Diffusion Model (MDM)ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç¦»æ•£éšæœºæ©è”½è¿‡ç¨‹ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªæ©ç é¢„æµ‹å™¨æ¥è¿‘ä¼¼å…¶åå‘è¿‡ç¨‹ã€‚

# 2 Approach

![A Conceptual Overview of LLaDA](https://share.note.youdao.com/yws/api/personal/file/WEBe77426aa5b23c3364ad557f96d735ff7?method=download&shareKey=0293b80db53bfd7b8a9ba03f15a6f802 "A Conceptual Overview of LLaDA")

## 2.1 Probabilistic Formulation
ä¸å…¬å¼(2)ä¸­çš„è‡ªå›å½’æ¨¡å‹ä¸åŒï¼ŒLLaDAé€šè¿‡**å‰å‘è¿‡ç¨‹ (forward process)** å’Œ **åå‘è¿‡ç¨‹ (reverse process)** æ¥å®šä¹‰æ¨¡å‹åˆ†å¸ƒ $p_{\theta}(x_{0})$ã€‚

### Forward Process

é€æ­¥åœ°ã€ç‹¬ç«‹åœ° mask $x_{0}$ ä¸­çš„ tokenï¼Œç›´åˆ°åœ¨ $t=1$ æ—¶åºåˆ—è¢«å®Œå…¨ mask. 

ç»™å®š $x_{0}$ æ—¶ $x_{t}$ çš„æ¡ä»¶åˆ†å¸ƒå¯ä»¥è¢«åˆ†è§£ä¸ºï¼š

$$
q_{t|0}(x_{t}|x_{0}) = \prod_{i=1}^{L} q_{t|0}(x_{t}^{i}|x_{0}^{i})
$$ 


å¯¹äº $t \in (0,1)$ï¼Œåºåˆ— $x_{t}$ æ˜¯éƒ¨åˆ†è¢«æ©ç çš„ï¼Œå…¶ä¸­æ¯ä¸ª token æœ‰ $t$ çš„æ¦‚ç‡è¢«maskï¼Œæˆ–æœ‰ $1-t$ çš„æ¦‚ç‡ä¿æŒä¸å˜ã€‚

$$
q_{t|0}(x_{t}^{i}|x_{0}^{i}) = \begin{cases} 1-t, & x_{t}^{i} = x_{0}^{i} \\ t, & x_{t}^{i} = M \end{cases}
$$


å…¶ä¸­ M è¡¨ç¤ºæ©ç  token. ç›´è§‚ä¸Šï¼Œæ¯ä¸ª token è¦ä¹ˆä¿æŒä¸å˜ï¼Œè¦ä¹ˆè¢«æ©ç ï¼Œ**è¢«æ©ç çš„æ¦‚ç‡éšç€ t ä» 0 åˆ° 1 çº¿æ€§å¢åŠ **ã€‚åœ¨ $t=1$ æ—¶ï¼Œæ‰€æœ‰ token éƒ½è¢« mask. çº¿æ€§å˜åŒ–çš„è¢«æ©ç æ¦‚ç‡å’ŒåŸå…ˆæ‰©æ•£æ¨¡å‹çš„åŠ å™ªæµç¨‹ä¸ä¸€æ ·ï¼Œæ˜¯åŸºäºæ–‡æœ¬ä¿¡æ¯å’Œ token é•¿åº¦æˆæ­£æ¯”çš„å‡è®¾ã€‚

## Reverse Process
åå‘è¿‡ç¨‹åˆ™é€šè¿‡åœ¨ $t=1\rightarrow 0$ ä»å®Œå…¨è¢«æ©ç çš„åºåˆ—ä¸­ç”Ÿæˆæ–°æ•°æ®ã€‚

å¯¹äº $0 \le s < t \le 1$ï¼Œåå‘è¿‡ç¨‹çš„æ¡ä»¶åˆ†å¸ƒåˆ†è§£ä¸ºï¼š

$$
q_{s|t}(x_{s}|x_{t}) = \prod_{i=1}^{L} q_{s|t}(x_{s}^{i}|x_{t})
$$

å…¶ä¸­æ¯ä¸ª token çš„æ¡ä»¶åˆ†å¸ƒä¸ºï¼š

$$
q_{s|t}(x_{s}^{i}|x_{t}^{i}) = \begin{cases} 1, & x_{t}^{i} \ne M, x_{s}^{i} = x_{t}^{i} \\ \frac{s}{t}, & x_{t}^{i} = M, x_{s}^{i} = M \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & x_{t}^{i} = M, x_{s}^{i} \ne M \\ 0, & \text{otherwise} \end{cases}
$$

éœ€è¦ä¼°è®¡çš„å…³é”®å‡½æ•°æ˜¯æ¡ä»¶åˆ†å¸ƒ $q_{0|t}(x_{s}^{i}|x_{t})$ï¼Œå®ƒåœ¨è¾“å…¥ $x_{t}$ ä¸­å¯¹åº”ä½ç½®è¢«æ©ç çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹å‡ºåŸå§‹çš„ token. ç±»ä¼¼äºè¿ç»­æ‰©æ•£æ¨¡å‹ä¸­çš„æ•°æ®é¢„æµ‹å½¢å¼ã€‚å¦‚ (Ou et al., 2024) æ‰€è¯æ˜ï¼Œå¯ä»¥æ¨å¯¼å‡ºä¸€ä¸ªç­‰ä»·ä½†æ— æ—¶é—´ä¾èµ–çš„å‚æ•°åŒ–å½¢å¼

$$
q_{0|t}(x_s^i|x_t)=p_{\text{data}}(x_0^i|x_t^\text{UM}),\quad\forall i\text{ such that }x_t^i=\mathbf{M}
$$

å…¶ä¸­ $x_{t}^{\text{UM}}$ è¡¨ç¤º $x_{t}$ ä¸­æœªè¢«æ©ç  token çš„é›†åˆï¼Œå®ƒä¸åŸå§‹æ•°æ® $x_{0}$ ä¸­å¯¹åº”çš„ token ç›¸åŒï¼Œå› ä¸ºæœªæ©ç çš„ token ä»…ç”± $x_{0}$ å†³å®šä¸”ä¸æ—¶é—´ t æ— å…³ ã€‚ç›´è§‚ä¸Šï¼Œè¿™æ„å‘³ç€ä¼°è®¡æ•°æ®é¢„æµ‹å‡½æ•°ç­‰åŒäºä¼°è®¡åœ¨å¹²å‡€æ•°æ®ä¸Šçš„æ¡ä»¶åˆ†å¸ƒï¼Œè€Œåè€…æ˜¯æ—¶ä¸å˜çš„ã€‚å› æ­¤ï¼Œæ—¶é—´ t ä¸éœ€è¦ä½œä¸ºè¾“å…¥æä¾›ç»™å‚æ•°åŒ–æ¨¡å‹ ã€‚

å°½ç®¡ MDM çš„æ¨å¯¼è¿‡ç¨‹ä¸ç®€å•ï¼Œä½†å…¶å®ç°æ˜¯ç›´æ¥çš„ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥**æ©ç é¢„æµ‹å™¨**ï¼Œä¸€ä¸ªå‚æ•°åŒ–æ¨¡å‹ $p_{\theta}(\cdot|x_{t})$ (ä¾‹å¦‚ä¸€ä¸ªæ²¡æœ‰å› æœæ©ç çš„ Transformer)ï¼Œå®ƒå°†ä»»æ„ t æ—¶åˆ»çš„ $x_{t}$ ä½œä¸ºè¾“å…¥ï¼Œå¹¶åŒæ—¶é¢„æµ‹æ‰€æœ‰è¢« mask çš„ token. ç„¶åï¼Œæˆ‘ä»¬å¦‚ä¸‹å®šä¹‰æ¨¡å‹åˆ†å¸ƒ $p_{\theta}(x_{0})$ï¼šä»ä¸€ä¸ªè¢«å®Œå…¨ mask åºåˆ—çš„ $x_{1}$ å¼€å§‹ï¼Œä» $t=1$ åˆ° 0 æ¨¡æ‹Ÿä¸€ä¸ªç”± $p_{\theta}(\cdot|x_{t})$ å‚æ•°åŒ–çš„è¿‘ä¼¼åå‘è¿‡ç¨‹ã€‚åœ¨ $t=0$ æ—¶åˆ»æ¨å¯¼å‡ºçš„è¾¹ç¼˜åˆ†å¸ƒå³ä»£è¡¨äº†æ¨¡å‹åˆ†å¸ƒ $p_{\theta}(x_{0})$ ã€‚

æ©ç é¢„æµ‹å™¨å°† $x_{t}$ ä½œä¸ºè¾“å…¥å¹¶åŒæ—¶é¢„æµ‹æ‰€æœ‰è¢«æ©ç çš„ token (è¡¨ç¤ºä¸º M). å®ƒé€šè¿‡ä¸€ä¸ªä»…åœ¨è¢«æ©ç  token ä¸Šè®¡ç®—çš„äº¤å‰ç†µæŸå¤±è¿›è¡Œè®­ç»ƒï¼š

$$
\mathcal{L}(\theta)\triangleq-\mathbb{E}_{t,x_{0},x_{t}}[\frac{1}{t}\sum_{i=1}^{L}I[x_{t}^{i}=M]log~p_{\theta}(x_{0}^{i}|x_{t})], \tag{3}
$$ 

å…¶ä¸­ï¼Œ$x_{0}$ ä»è®­ç»ƒæ•°æ®ä¸­é‡‡æ ·ï¼Œ$t$ ä»`[0, 1]`ä¸­å‡åŒ€é‡‡æ ·{{< sidenote >}} Notably, LLaDA employs a masking ratio that *varies randomly* between 0 and 1 while masked language models (Devlin, 2018) use a fixed ratio. {{< /sidenote >}}ï¼Œ$x_{t}$ ä»å‰å‘è¿‡ç¨‹ä¸­é‡‡æ ·ã€‚æŒ‡ç¤ºå‡½æ•° $I[\cdot]$ ç¡®ä¿æŸå¤±ä»…é’ˆå¯¹è¢«æ©ç çš„ token è®¡ç®—ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä¾¿å¯ä»¥æ¨¡æ‹Ÿä¸€ä¸ªç”±è¯¥æ©ç é¢„æµ‹å™¨å‚æ•°åŒ–çš„åå‘è¿‡ç¨‹ï¼ˆè¯¦è§2.4èŠ‚ï¼‰ï¼Œå¹¶å°†æ¨¡å‹åˆ†å¸ƒ $p_{\theta}(x_{0})$ å®šä¹‰ä¸ºè¯¥è¿‡ç¨‹çš„è¾¹ç¼˜åˆ†å¸ƒã€‚

å…¬å¼(3)å·²è¢«è¯æ˜æ˜¯æ¨¡å‹åˆ†å¸ƒè´Ÿå¯¹æ•°ä¼¼ç„¶çš„ä¸Šç•Œ

$$
-\mathbb{E}_{p_{\text{data}}(x_{0})}\left[\log p_{\theta}(x_{0})\right]\leq\mathcal{L}(\theta) \tag{4}
$$

è¯¥æ–¹æ³•é€šè¿‡åœ¨æ­£å‘è¿‡ç¨‹ä¸­é€æ­¥å±è”½ token å¹¶åœ¨åå‘è¿‡ç¨‹ä¸­å­¦ä¹ æ¢å¤æ•°æ®åˆ†å¸ƒæ¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œæ‰€æœ‰è¿™äº›éƒ½åœ¨ï¼ˆè¿‘ä¼¼ï¼‰æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¡†æ¶ä¸‹ã€‚

## Pretraining

- LLaDA 8B æ¨¡å‹åœ¨ä¸€ä¸ªåŒ…å« 2.3T tokens çš„é«˜è´¨é‡ã€å¤šæºæ•°æ®é›†ä¸Šä»é›¶å¼€å§‹è¿›è¡Œé¢„è®­ç»ƒã€‚è¯¥æ•°æ®é›†è¦†ç›–äº†é€šç”¨æ–‡æœ¬ã€ä»£ç ã€æ•°å­¦å’Œå¤šè¯­è¨€å†…å®¹ ã€‚
- è®­ç»ƒæ€»å…±æ¶ˆè€—äº† 0.13M H800 GPU hours. è®­ç»ƒåºåˆ—é•¿åº¦å›ºå®šä¸º4096. å…¶æ ¸å¿ƒè®­ç»ƒæ­¥éª¤æ˜¯ï¼šå¯¹æ¯ä¸ªåºåˆ—éšæœºé‡‡æ ·ä¸€ä¸ªæ©ç ç‡ tï¼Œå¹¶ç‹¬ç«‹åœ°ä»¥è¯¥æ¦‚ç‡æ©ç æ¯ä¸ª tokenï¼Œç„¶åè®©æ¨¡å‹å»é¢„æµ‹è¢«æ©ç çš„éƒ¨åˆ† ã€‚

- **æ¶æ„è°ƒæ•´** ç›¸è¾ƒäºLLaMA3 8Bï¼ŒLLaDA 8Båœ¨æ¶æ„ä¸Šåšäº†ä¸€äº›å¿…è¦è°ƒæ•´ï¼Œå¦‚ä½¿ç”¨æ ‡å‡†çš„ MHA è€Œé GQAï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´äº† FFN çš„ç»´åº¦ä»¥ä¿æŒæ¨¡å‹æ€»å‚æ•°é‡ç›¸å½“ ã€‚
- **ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡** è®­ç»ƒä½¿ç”¨äº† AdamW ä¼˜åŒ–å™¨å’Œä¸€ä¸ªç‰¹æ®Šçš„ Warmup-Stable-Decay å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ã€‚æ•´ä¸ª8Bæ¨¡å‹çš„è®­ç»ƒå®éªŒåªæ‰§è¡Œäº†ä¸€æ¬¡ï¼Œæ²¡æœ‰è¿›è¡Œä»»ä½•è¶…å‚æ•°è°ƒä¼˜ã€‚

| | Our ARM Baseline 1B | LLaDA IB | Our ARM Baseline 7B | LLaDA 8B | LLaMA3 8B |
|:---|:---:|:---:|:---:|:---:|:---:|
| **Layers** | 22 | 22 | 28 | 32 | 32 |
| **Model dimension** | 2048 | 2048 | 4096 | 4096 | 4096 |
| **Attention heads** | 32 | 32 | 32 | 32 | 32 |
| **Vocabulary size** | 126,464 | 126,464 | 126,464 | 126.464 | 128,000 |
| **FFN dimension** | 5634 | 5634 | 13.440 | 12,288 | 14,336 |
| **Key/Value heads** | 4 | 4 | 8 | 32 | 8 |
| **Total parameters** | 1.49 B | 1.49 B | 6.83 B | 8.02 B | 8.03 B |
| **Non-embedding parameters** | 0.97 B | 0.97 B | 5.80 B | 6.98 B | 6.98 B |

## Supervised Fine-Tuning
 
æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é…å¯¹æ•°æ® $(p_{0}, r_{0})$ è¿›è¡Œ **ç›‘ç£å¾®è°ƒ (SFT)** æ¥å¢å¼ºLLaDAéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå…¶ä¸­ $p_{0}$ æ˜¯ promptï¼Œ$r_{0}$ è¡¨ç¤ºå“åº”(response). è¿™æ˜¯é’ˆå¯¹LLMæœ€ç®€å•ã€æœ€åŸºç¡€çš„ post-training æ–¹æ³•ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œè¿™è¦æ±‚æ¨¡å‹å¯¹æ¡ä»¶åˆ†å¸ƒ $p_{\theta}(r_{0}|p_{0})$ï¼Œè€Œéé¢„è®­ç»ƒä¸­çš„ $p_{\theta}(x_{0})$ è¿›è¡Œå»ºæ¨¡ã€‚

å…¶å®ç°æ–¹å¼ä¸é¢„è®­ç»ƒç±»ä¼¼ã€‚å¦‚å›¾2(b)æ‰€ç¤ºï¼Œä¿æŒ prompt éƒ¨åˆ†ä¸å˜ï¼Œå¹¶åƒå¤„ç† $x_{0}$ ä¸€æ ·ï¼Œç‹¬ç«‹åœ° mask response ä¸­çš„ token. ç„¶åï¼Œå°†æç¤ºå’Œè¢«æ©ç çš„å“åº” $r_{t}$ ä¸€åŒé€å…¥é¢„è®­ç»ƒå¥½çš„æ©ç é¢„æµ‹å™¨ï¼Œä»¥è®¡ç®—ç”¨äº SFT çš„æŸå¤±

$$
-\mathbb{E}_{t,p_{0},r_{0},r_{t}}[\frac{1}{t}\sum_{i=1}^{L^{\prime}}I[r_{t}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{t})] \tag{5}
$$

å…¶ä¸­ï¼Œ$L^{\prime}$ è¡¨ç¤ºç¨åæŒ‡å®šçš„åŠ¨æ€é•¿åº¦ã€‚è¿™ç§æ–¹æ³•ä¸é¢„è®­ç»ƒæ˜¯å®Œå…¨å…¼å®¹çš„ã€‚æœ¬è´¨ä¸Šï¼Œå°† $p_{0}$ å’Œ $r_{0}$ æ‹¼æ¥èµ·æ¥å¯ä»¥è¢«è§†ä¸ºå¹²å‡€çš„é¢„è®­ç»ƒæ•°æ® $x_{0} $ï¼Œè€Œå°† $p_{0}$ å’Œ $r_{t}$ æ‹¼æ¥èµ·æ¥åˆ™å¯ä½œä¸ºå…¶è¢«æ©ç åçš„ç‰ˆæœ¬ $x_{t}$. è¿™ä¸ªè¿‡ç¨‹ä¸é¢„è®­ç»ƒå®Œå…¨ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äºæ‰€æœ‰è¢«æ©ç çš„ token æ°å¥½éƒ½å‡ºç°åœ¨ $r_{0}$ éƒ¨åˆ†ã€‚

LLaDA 8B æ¨¡å‹åœ¨ä¸€ä¸ªåŒ…å« 4.5M å¯¹æ ·æœ¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº† SFT. ä¸é¢„è®­ç»ƒè¿‡ç¨‹ä¸€è‡´ï¼Œæ•°æ®å‡†å¤‡å’Œè®­ç»ƒéƒ½éµå¾ªäº†ç°æœ‰LLM (Chu et al., 2024; Yang et al., 2024) ä¸­ä½¿ç”¨çš„ SFT åè®®ï¼Œæ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„æŠ€æœ¯æ¥ä¼˜åŒ– LLaDA çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬ä»£ç ã€æ•°å­¦ã€æŒ‡ä»¤éµå¾ªå’Œç»“æ„åŒ–æ•°æ®ç†è§£ã€‚æˆ‘ä»¬åœ¨æ¯ä¸ª mini-batch ä¸­çš„çŸ­æ ·æœ¬å¯¹æœ«å°¾é™„åŠ  EOS tokenï¼Œä»¥ç¡®ä¿æ‰€æœ‰æ•°æ®é•¿åº¦ç›¸ç­‰ã€‚åœ¨è®­ç»ƒæœŸé—´å°† EOSè§†ä¸ºä¸€ä¸ªæ™®é€š token ï¼Œå¹¶åœ¨é‡‡æ ·æ—¶å°†å…¶ç§»é™¤ï¼Œä½¿å¾—LLaDAèƒ½å¤Ÿè‡ªåŠ¨æ§åˆ¶å“åº”çš„é•¿åº¦ã€‚

æˆ‘ä»¬åœ¨SFTæ•°æ®ä¸Šè®­ç»ƒäº† 3 ä¸ª epochï¼Œå…¶è°ƒåº¦ç­–ç•¥ä¸é¢„è®­ç»ƒé˜¶æ®µç›¸ä¼¼ã€‚å­¦ä¹ ç‡åœ¨æœ€åˆ 50 æ¬¡è¿­ä»£ä¸­ä» 0 çº¿æ€§å¢åŠ åˆ° $2.5 \times 10^{-5}$ï¼Œç„¶åä¿æŒä¸å˜ã€‚åœ¨æœ€å 10\% çš„è¿­ä»£ä¸­ï¼Œå­¦ä¹ ç‡æ€§é™ä½åˆ° $2.5 \times 10^{-6}$. æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æƒé‡è¡°å‡è®¾ç½®ä¸º 0.1ï¼Œå…¨å±€ batch size è®¾ç½®ä¸º 256ï¼Œæ¯ä¸ª GPU çš„æœ¬åœ° batch size è®¾ç½®ä¸º 2. SFTå®éªŒåªæ‰§è¡Œäº†ä¸€æ¬¡ï¼Œæ²¡æœ‰è¿›è¡Œä»»ä½•è¶…å‚æ•°è°ƒä¼˜ã€‚

## Inference 

ä½œä¸ºä¸€ä¸ªç”Ÿæˆå¼æ¨¡å‹ï¼ŒLLaDAæ—¢èƒ½ **é‡‡æ · (sampling)** æ–°æ–‡æœ¬ï¼Œä¹Ÿèƒ½ **è¯„ä¼° (evaluating)** å€™é€‰æ–‡æœ¬çš„ä¼¼ç„¶ã€‚

å…ˆä»é‡‡æ ·è¯´èµ·ã€‚å¦‚å›¾ 2(c) æ‰€ç¤ºï¼Œç»™å®šä¸€ä¸ª prompt $p_{0}$ï¼Œæˆ‘ä»¬é€šè¿‡ç¦»æ•£åŒ–åå‘è¿‡ç¨‹æ¥ä»æ¨¡å‹åˆ†å¸ƒ $p_{\theta}(r_{0}|p_{0})$ ä¸­è¿›è¡Œé‡‡æ ·ï¼Œè¿™ä¸ªè¿‡ç¨‹ä»ä¸€ä¸ªè¢«å®Œå…¨æ©ç çš„ response å¼€å§‹ã€‚

**æ€»çš„é‡‡æ ·æ­¥æ•°æ˜¯ä¸€ä¸ªè¶…å‚æ•°**ï¼Œä¸º LLaDA æä¾›äº†ä¸€ä¸ªåœ¨æ•ˆç‡å’Œæ ·æœ¬è´¨é‡ä¹‹é—´çš„æƒè¡¡ï¼ˆè¯¦è§3.3èŠ‚åˆ†æï¼‰ã€‚æˆ‘ä»¬é»˜è®¤ä½¿ç”¨å‡åŒ€åˆ†å¸ƒçš„æ—¶é—´æ­¥ã€‚
æ­¤å¤–ï¼Œ**ç”Ÿæˆé•¿åº¦ä¹Ÿè¢«è§†ä¸ºè¶…å‚æ•°**ï¼Œå®ƒæŒ‡å®šäº†é‡‡æ ·è¿‡ç¨‹å¼€å§‹æ—¶å®Œå…¨è¢«æ©ç å¥å­çš„é•¿åº¦ã€‚å¦‚é™„å½•B.4æ‰€è¿°ï¼Œç”±äºé¢„è®­ç»ƒå’ŒSFTéƒ½æ˜¯åœ¨å¯å˜é•¿åº¦çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„ï¼Œæœ€ç»ˆç»“æœå¯¹è¿™ä¸ªé•¿åº¦è¶…å‚æ•°ä¸æ•æ„Ÿã€‚

åœ¨ä¸€ä¸ªä»æ—¶é—´ $t \in (0, 1]$ åˆ° $s \in [0, t)$çš„ä¸­é—´æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å°† $p_{0}$ å’Œ $r_{t}$ åŒæ—¶é€å…¥æ©ç é¢„æµ‹å™¨ï¼Œå¹¶ä¸€æ¬¡æ€§é¢„æµ‹æ‰€æœ‰è¢«æ©ç çš„ token. éšå *remask* $\frac{s}{t}$ æ¯”ä¾‹çš„å·²é¢„æµ‹ token å¾—åˆ°$r_{s}$ï¼Œä»è€Œç¡®ä¿åå‘è¿‡ç¨‹çš„è½¬æ¢ä¸å‰å‘è¿‡ç¨‹ä¿æŒä¸€è‡´ï¼Œä»¥å®ç°å‡†ç¡®é‡‡æ ·ã€‚

å— LLM é‡‡æ ·ä¸­é€€ç«æŠ€å·§çš„å¯å‘ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸¤ç§ç¡®å®šæ€§ä½†æœ‰æ•ˆçš„é‡æ©ç ç­–ç•¥ã€‚
- **low-confidence remasking**: remask é‚£äº›åŸºäºé¢„æµ‹ç½®ä¿¡åº¦æœ€ä½çš„ $\frac{s}{t}$ æ¯”ä¾‹çš„ token. 
- **semi-autoregressive remasking**: å¯¹äºç»è¿‡ SFT çš„ LLaDA æ¨¡å‹ï¼Œå°†åºåˆ—åˆ†æˆå‡ ä¸ªå—ï¼Œå¹¶ä»å·¦åˆ°å³åœ°ç”Ÿæˆ. åœ¨æ¯ä¸ªå—å†…éƒ¨ï¼Œé‡‡ç”¨åå‘è¿‡ç¨‹è¿›è¡Œé‡‡æ ·ã€‚

![A Conceptual Overview of the Semi-autoregressive Sampling](https://share.note.youdao.com/yws/api/personal/file/WEB13df3bff501e46425bb65c2defedecde?method=download&shareKey=838350c5b31c7e78112324263cdf5621 "A Conceptual Overview of the Semi-autoregressive Sampling")

å¯¹äºæ¡ä»¶ä¼¼ç„¶è¯„ä¼°ï¼Œæˆ‘ä»¬è‡ªç„¶å¯ä»¥åˆ©ç”¨å…¬å¼(5)ä¸­çš„ä¸Šç•Œã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°ä¸‹é¢è¿™ä¸ªç­‰ä»·å½¢å¼ï¼ˆå…¬å¼6ï¼‰è¡¨ç°å‡ºæ›´ä½çš„æ–¹å·®ï¼Œåœ¨è¯„ä¼°æ—¶æ›´ä¸ºç¨³å®šï¼š

$$
-\mathbb{E}_{l,r_{0},r_{l}}[\frac{L}{l}\sum_{i=1}^{L}I[r_{l}^{i}=M]log~p_{\theta}(r_{0}^{i}|p_{0},r_{l})] \tag{6}
$$

å…¶ä¸­ï¼Œ$l$ ä» ${1, 2, ..., L}$ ä¸­å‡åŒ€é‡‡æ ·ï¼Œ$r_{l}$ æ˜¯é€šè¿‡ä» $r_{0}$ ä¸­ä¸æ”¾å›åœ°å‡åŒ€é‡‡æ · $l$ ä¸ªæ²¡è¢« mask çš„ token å¾—åˆ°çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº† unsupervised classifier-free guidance.

**è™½ç„¶è¿™ä¸¤ä¸ªå½¢å¼çš„æœŸæœ›å€¼ç›¸åŒï¼Œä½†å®ƒä»¬çš„æ–¹å·®ä¸åŒ**ã€‚ç›´è§‚ä¸Šï¼Œåœ¨å…¬å¼ (5) ä¸­ï¼Œæˆ‘ä»¬æœŸæœ› $x_{t}=[p_0,r_t]$ æœ‰ $t$ æ¯”ä¾‹çš„ token è¢«æ©ç ã€‚ç„¶è€Œï¼Œå‰å‘è¿‡ç¨‹çš„éšæœºæ€§å¸¸å¸¸ä¼šå¯¼è‡´åå·®ï¼Œå°¤å…¶å½“ $x_{t}$ åŒ…å«çš„ token å¾ˆå°‘æ—¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨å…¬å¼ (6) ä¸­ï¼Œ$r_{l}$ ä¸­è¢«æ©ç  token çš„æ¯”ä¾‹ $\frac{l}{L}$ æ˜¯ç¡®å®šçš„ã€‚

è™½ç„¶ç†è®ºåˆ†æå–å†³äºæ•°æ®åˆ†å¸ƒï¼Œä½†ç»éªŒç»“æœè¡¨æ˜ï¼Œå…¬å¼ (5) éœ€è¦è¶…è¿‡ 1000 æ¬¡è’™ç‰¹å¡æ´›ä¼°è®¡æ‰èƒ½å¾—åˆ°ç¨³å®šç»“æœï¼Œè€Œå…¬å¼ (6) ä»…éœ€ 128 æ¬¡ä¼°è®¡å³å¯è¾¾åˆ°ç¨³å®šã€‚

Any-order autoregressive models (AO-ARM)  é€šè¿‡å¯¹ L ä¸ªå˜é‡æ‰€æœ‰å¯èƒ½çš„æ’åˆ—é¡ºåºè¿›è¡Œè‡ªå›å½’æ¥æè¿°è”åˆåˆ†å¸ƒã€‚ä¸ºäº†å­¦ä¹ è¿™æ ·çš„åˆ†å¸ƒï¼ŒAO-ARM åˆ©ç”¨ä¸€ä¸ªæƒé‡å…±äº«çš„ç¥ç»ç½‘ç»œæ¥ä¸ºæ‰€æœ‰å•å˜é‡æ¡ä»¶æ¦‚ç‡å»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨æ©ç  token æ¥è¡¨ç¤ºç¼ºå¤±çš„å˜é‡ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œæ¨¡å‹ä¼šæœ€å°åŒ–åœ¨æ‰€æœ‰é¡ºåºçš„å‡åŒ€åˆ†å¸ƒ $U_{\pi}$ ä¸Šçš„æœŸæœ›è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š

$$
-\mathbb{E}_{x_{0},\pi \sim U_{\pi}}[\sum_{i=1}^{L}log~p_{\theta}(x_{0}^{\pi(i)}|x_{0}^{\pi(<i)}; \pi)]
$$ (15)

ç›´è§‚ä¸Šï¼Œ$x_{0}^{\pi(<i)}$ å¯ä»¥è¢«ç†è§£ä¸ºä¸€ä¸ªè¢«æ©ç çš„ token åºåˆ— $x_{t}$ï¼Œå…¶ä¸­ç´¢å¼•åœ¨ $\pi(\ge i)$ çš„ token è¢«æ©ç  ã€‚å¯ä»¥è¿›ä¸€æ­¥è¯æ˜ï¼Œå…¬å¼ (15) ç­‰ä»·äºå…¬å¼ (12) ã€‚è¿™ç§è”ç³»è§£é‡Šäº† LLaDA çš„åŒå‘æ¨ç†èƒ½åŠ›ï¼Œå³ä½¿å®ƒåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»æœªè¢«æ˜¾å¼ä½¿ç”¨ ã€‚

Nie et al. (2024) å¼•å…¥äº†æ— ç›‘ç£çš„æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æŠ€æœ¯ï¼Œå¯ä»¥å¹³è¡¡ä¸æç¤ºçš„å¯¹é½åº¦å’Œæ–‡æœ¬å¤šæ ·æ€§ ã€‚å…·ä½“æ¥è¯´ï¼Œæ— ç›‘ç£çš„æ— åˆ†ç±»å™¨æŒ‡å¯¼åœ¨æ¨ç†æ—¶é‡‡ç”¨ä»¥ä¸‹ä¿®æ”¹è¿‡çš„æ©ç é¢„æµ‹å™¨ ï¼š

$$
\tilde{p}_{\theta}(r_{0}|p_{0},r_{t}) \propto \frac{p_{\theta}(r_{0}|p_{0},r_{t})^{1+w}}{p_{\theta}(r_{0}|m,r_{t})^{w}}
$$ (16)

å…¶ä¸­ï¼Œ$m$ æ˜¯ä¸€ä¸ªä¸ $p_{0}$ é•¿åº¦ç›¸åŒçš„æ©ç åºåˆ—ï¼Œ$w$ æ˜¯ä¸€ä¸ªæ§åˆ¶ $p_{0}$ å¼ºåº¦çš„è¶…å‚æ•° ã€‚æˆ‘ä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­é‡‡ç”¨äº†æ— ç›‘ç£çš„æ— åˆ†ç±»å™¨æŒ‡å¯¼ï¼Œè¯¦è§é™„å½• B.5 ã€‚


# 3 Experiment

å®éªŒä¸»è¦å›´ç»•ä»¥ä¸‹ä¸‰ä¸ªæ ¸å¿ƒæ–¹é¢å±•å¼€ï¼š

1. å¯æ‰©å±•æ€§ (Scalability)ï¼šç ”ç©¶ LLaDA çš„æ€§èƒ½æ˜¯å¦éšç€è®¡ç®—èµ„æºå’Œæ¨¡å‹è§„æ¨¡çš„å¢åŠ è€Œç¨³å®šæå‡ã€‚é€šè¿‡ä¸è‡ªå»ºçš„è‡ªå›å½’æ¨¡å‹ (ARM) åŸºçº¿åœ¨ç›¸åŒæ•°æ®ä¸Šè¿›è¡Œå¯¹æ¯”ï¼Œç»“æœæ˜¾ç¤º LLaDA è¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ï¼Œå…¶æ€§èƒ½å¢é•¿è¶‹åŠ¿ä¸ ARM ç›¸å½“ï¼Œç”šè‡³åœ¨ MMLU å’Œ GSM8K ç­‰ä»»åŠ¡ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚


2. åŸºå‡†æµ‹è¯•ç»“æœ (Benchmark Results)ï¼šå°† 8B è§„æ¨¡çš„ LLaDA ä¸ LLaMA3 8Bã€LLaMA2 7B ç­‰ä¸»æµæ¨¡å‹åœ¨æ¶µç›–é€šç”¨ï¼Œæ•°å­¦ï¼Œä»£ç å’Œä¸­æ–‡å››å¤§ç±»çš„ 15 ä¸ªæ ‡å‡†åŸºå‡†ä¸Šè¿›è¡Œå¯¹æ¯”ã€‚
    - é¢„è®­ç»ƒæ¨¡å‹ï¼šLLaDA 8B Base æ¨¡å‹çš„æ€§èƒ½å…¨é¢è¶…è¶Š LLaMA2 7Bï¼Œå¹¶ä¸ LLaMA3 8B æ•´ä½“ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œå°¤å…¶åœ¨æ•°å­¦å’Œä¸­æ–‡ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚

    - å¾®è°ƒæ¨¡å‹ï¼šä»…ç»è¿‡ SFT çš„ LLaDA 8B Instruct æ¨¡å‹ï¼Œåœ¨æœªè¿›è¡Œå¼ºåŒ–å­¦ä¹ å¯¹é½çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½åœ¨å¤šæ•°ä»»åŠ¡ä¸Šå¾—åˆ°æå‡ ï¼Œå¹¶å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ Instruction Follow èƒ½åŠ›ã€‚


3. åå‘æ¨ç† (Reversal Reasoning)ï¼šä¸ºäº†é‡åŒ–æ¨¡å‹å…‹æœâ€œåè½¬è¯…å’’â€çš„èƒ½åŠ›ï¼Œå®éªŒåœ¨ä¸€ä¸ªä¸­æ–‡å¤è¯—è¡¥å…¨ä»»åŠ¡ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚ç»“æœè¡¨æ˜ï¼ŒLLaDA åœ¨æ­£å‘å’Œåå‘ä»»åŠ¡ä¸Šè¡¨ç°å‡è¡¡ï¼Œä¸€è‡´æ€§å¼ºï¼Œè€Œ GPT-4o ç­‰æ¨¡å‹åˆ™åœ¨åå‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚

# Generation code

1.  **åˆå§‹åŒ– (The Canvas) ğŸ¨**

    å‡½æ•°é¦–å…ˆä¼šåˆ›å»ºä¸€ä¸ªå¦‚ä¸‹æ‰€ç¤ºçš„åºåˆ—ï¼š
    `[<start_token>, <prompt_tokens>, [MASK], [MASK], ..., [MASK]]`
    generate çš„ç›®æ ‡å°±æ˜¯ç”¨ä¸€ä¸ªè¿è´¯çš„ç­”æ¡ˆæ¥æ›¿æ¢æ‰æ‰€æœ‰çš„ `[MASK]` æ ‡è®°ã€‚

2.  **åˆ†å— (Semi-Autoregressive) ğŸ§±**

    ç®—æ³•å¹¶ä¸ä¼šä¸€æ¬¡æ€§å¡«å……æ‰€æœ‰ `gen_length` ä¸ªæ©ç ï¼Œè€Œæ˜¯å°†æ•´ä¸ªè¿‡ç¨‹åˆ†è§£ä¸º `num_blocks` ä¸ªå—ã€‚å®ƒä¼šå…ˆå®Œå…¨å¡«æ»¡ç¬¬ä¸€ä¸ª `block_length` é•¿åº¦çš„æ©ç ï¼Œç„¶åå†å¼€å§‹å¤„ç†ä¸‹ä¸€ä¸ªå—ã€‚è¿™ç§æ–¹å¼åœ¨å®è§‚å±‚é¢å¼•å…¥äº†ä»å·¦åˆ°å³çš„ç”Ÿæˆé¡ºåºã€‚

3.  **è¿­ä»£å¼ç²¾ç‚¼ (æ ¸å¿ƒå¾ªç¯) ğŸ”„**

    å¯¹äºæ¯ä¸€ä¸ªå—ï¼Œä»£ç éƒ½ä¼šè¿›å…¥ä¸€ä¸ªå†…éƒ¨å¾ªç¯ï¼Œè¯¥å¾ªç¯è¿è¡Œ `steps_per_block` æ¬¡ã€‚å¾ªç¯çš„æ¯ä¸€æ­¥ä¸­ï¼š

    * **A. é¢„æµ‹ï¼š** å°†å½“å‰çš„ `x` åŒ…å«å…¶ä¸­å‰©ä½™çš„æ©ç  è¾“å…¥åˆ° `LLaDA` æ¨¡å‹ä¸­ã€‚æ¨¡å‹ä¼šä¸ºåºåˆ—ä¸­çš„*æ¯ä¸€ä¸ª*ä½ç½®é¢„æµ‹æœ€å¯èƒ½çš„ tokenï¼Œå³ä½¿æ˜¯é‚£äº›æ²¡æœ‰è¢«æ©ç çš„ä½ç½®ä¹Ÿä¼šè¿›è¡Œé¢„æµ‹ã€‚

    * **B. ç”Ÿæˆå€™é€‰ tokenï¼š** ç®—æ³•é€šè¿‡å¯¹æ¨¡å‹çš„è¾“å‡º `logits` æ‰§è¡Œ `argmax` æ“ä½œï¼Œä¸ºæ¯ä¸ªä½ç½®ç¡®å®šä¸€ä¸ªå€™é€‰ token. åœ¨è¿™é‡Œå¯ä»¥åŠ å…¥ Gumbel å™ªå£°æ¥å¼•å…¥éšæœºæ€§ï¼Œå…¶ä½œç”¨ç±»ä¼¼äºè‡ªå›å½’é‡‡æ ·ä¸­çš„ `temperature`ã€‚è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªå®Œæ•´çš„å€™é€‰åºåˆ— `x0`ã€‚

    * **C. ç½®ä¿¡åº¦è¯„åˆ†ï¼š** ç®—æ³•éœ€ä¸ºæ¯ä¸ª `[MASK]` ä½ç½®ä¸Šé¢„æµ‹å‡ºçš„ token è®¡ç®—ä¸€ä¸ª**ç½®ä¿¡åº¦åˆ†æ•°**ã€‚`low_confidence` ç­–ç•¥ï¼ˆå°½ç®¡å…¶åœ¨ä»£ç é€»è¾‘ä¸­çš„å‘½åå¯èƒ½æœ‰ç‚¹è¯¯å¯¼ï¼‰ä½¿ç”¨é¢„æµ‹ token çš„ softmax æ¦‚ç‡ä½œä¸ºå…¶ç½®ä¿¡åº¦ã€‚æ¦‚ç‡è¶Šé«˜ï¼Œä»£è¡¨æ¨¡å‹è¶Šè‡ªä¿¡ã€‚

    * **D. token é€‰æ‹©ï¼š** åŸºäºç½®ä¿¡åº¦åˆ†æ•°ï¼Œç®—æ³•ä¼šä¿ç•™**ç½®ä¿¡åº¦æœ€é«˜çš„ K ä¸ª**é¢„æµ‹ç»“æœã€‚æ¯ä¸€æ­¥è¦ä¿ç•™çš„ token æ•°é‡ (K) ç”± `get_num_transfer_tokens` å‡½æ•°é¢„å…ˆè®¡ç®—å¥½ï¼Œä»¥ç¡®ä¿çº¿æ€§çš„ unksk é€Ÿç‡ã€‚

    * **E. çŠ¶æ€æ›´æ–°ï¼š** åœ¨é‚£äº›è¢«é€‰ä¸­çš„é«˜ç½®ä¿¡åº¦ä½ç½®ï¼Œ`[MASK]`  token ä¼šè¢«æ›¿æ¢æˆ `x0` ä¸­å¯¹åº”çš„é¢„æµ‹ token. è€Œå…¶ä»–çš„ `[MASK]` ä½ç½®åˆ™ä¿æŒä¸å˜ï¼Œç•™å¾…ä¸‹ä¸€æ¬¡è¿­ä»£ã€‚

4.  **é‡å¤ä¸æ¨è¿› â¡ï¸**
    å†…éƒ¨å¾ªç¯ä¸æ–­é‡å¤ã€‚åœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œæ¨¡å‹ä¼šçœ‹åˆ°æ›´æ–°åçš„ `x`ï¼Œå…¶ä¸­åŒ…å«äº†æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯å’Œæ›´å°‘çš„æ©ç ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨åç»­æ­¥éª¤ä¸­èƒ½åšå‡ºæ›´å¥½çš„é¢„æµ‹ã€‚è¿™ä¸ªç²¾ç‚¼è¿‡ç¨‹ä¼šä¸€ç›´æŒç»­ï¼Œç›´åˆ°å½“å‰å—ä¸­æ‰€æœ‰çš„ `[MASK]` éƒ½è¢«å»é™¤ã€‚

5.  **ä¸‹ä¸€åŒºå—ä¸å®Œæˆ âœ…**
    å½“ä¸€ä¸ªå—å®Œæˆåï¼Œå¤–éƒ¨å¾ªç¯ä¼šç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª `[MASK]` å—ï¼Œå¹¶é‡å¤æ•´ä¸ªè¿­ä»£å¼ç²¾ç‚¼è¿‡ç¨‹ï¼Œç›´åˆ°ç”Ÿæˆäº†å®Œæ•´çš„ `gen_length` é•¿åº¦ã€‚æœ€åï¼Œè¿”å›æœ€ç»ˆè¢«å®Œå…¨å¡«å……çš„åºåˆ—ã€‚

```python
import torch
import numpy as np
import torch.nn.functional as F

from transformers import AutoTokenizer, AutoModel


def add_gumbel_noise(logits, temperature):
    '''
    The Gumbel max is a method for sampling categorical distributions.
    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.
    Thus, we use float64.
    '''
    if temperature == 0:
        return logits
    logits = logits.to(torch.float64)
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = (- torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise


def get_num_transfer_tokens(mask_index, steps):
    '''
    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.
    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),
    the expected number of tokens transitioned at each step should be consistent.

    This function is designed to precompute the number of tokens that need to be transitioned at each step.
    '''
    mask_num = mask_index.sum(dim=1, keepdim=True)

    base = mask_num // steps
    remainder = mask_num % steps

    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base

    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, :remainder[i]] += 1

    return num_transfer_tokens


@ torch.no_grad()
def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,
             cfg_scale=0., remasking='low_confidence', mask_id=126336):
    '''
    Args:
        model: Mask predictor.
        prompt: A tensor of shape (1, L).
        steps: Sampling steps, less than or equal to gen_length.
        gen_length: Generated answer length.
        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
        temperature: Categorical distribution sampling temperature.
        cfg_scale: Unsupervised classifier-free guidance scale.
        remasking: Remasking strategy. 'low_confidence' or 'random'.
        mask_id: The toke id of [MASK] is 126336.
    '''
    # 1. Initialization: Create the full sequence tensor 'x'.
    # It starts with the prompt, followed by `gen_length` [MASK] tokens.
    # This is the "canvas" that will be filled in.
    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    # Keep track of where the original prompt is, so we don't modify it.
    prompt_index = (x != mask_id)

    # 2. Semi-Autoregressive Setup (Blocking)
    # The generation is split into 'num_blocks' chunks. This handles long generation
    # by generating `block_length` tokens at a time before moving to the next chunk.
    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length
    
    # The total number of refinement steps is distributed among the blocks.
    assert steps % num_blocks == 0
    steps_per_block = steps // num_blocks

    # 3. Outer Loop: Process each block sequentially.
    for num_block in range(num_blocks):
        # Define the current working area (the block to be filled).
        # Note: The original code has a small typo `...:]`, corrected here for clarity.
        start_pos = prompt.shape[1] + num_block * block_length
        end_pos = prompt.shape[1] + (num_block + 1) * block_length
        block_mask_index = (x[:, start_pos:end_pos] == mask_id)

        # Calculate how many tokens to "unmask" or "confirm" in each refinement step for this block.
        # This ensures a steady, linear progression of unmasking.
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)

        # 4. Inner Loop: Iteratively refine the current block.
        for i in range(steps_per_block):
            # Get the indices of all currently masked tokens in the entire sequence.
            mask_index = (x == mask_id)

            # --- 4a. Prediction with optional Classifier-Free Guidance (CFG) ---
            if cfg_scale > 0.:
                # Create an unconditional version of the input by masking the prompt.
                un_x = x.clone()
                un_x[prompt_index] = mask_id
                
                # Run the model on both the conditional (x) and unconditional (un_x) inputs.
                x_ = torch.cat([x, un_x], dim=0)
                logits_cat = model(x_).logits
                logits, un_logits = torch.chunk(logits_cat, 2, dim=0)
                
                # Combine logits to steer the generation towards the prompt.
                # The formula is: unconditional + scale * (conditional - unconditional)
                # An algebraic simplification gives the line below.
                logits = un_logits + (cfg_scale + 1) * (logits - un_logits)
            else:
                # If no CFG, just do a single forward pass.
                logits = model(x).logits

            # --- 4b. Candidate Generation ---
            # Add Gumbel noise for stochastic sampling. If temperature is 0, this is a simple argmax.
            logits_with_noise = add_gumbel_noise(logits, temperature=temperature)
            # Get the most likely token prediction for every position in the sequence.
            x0 = torch.argmax(logits_with_noise, dim=-1)

            # --- 4c. Confidence Scoring for Remasking ---
            # Determine which of the new predictions to keep for the next step.
            if remasking == 'low_confidence':
                # Calculate the softmax probabilities of the predicted tokens.
                p = F.softmax(logits, dim=-1)
                # Get the probability of the chosen token `x0` at each position. This is the "confidence".
                x0_p = torch.squeeze(torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1)
            elif remasking == 'random':
                # Use random scores as confidence for random unmasking.
                x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
            else:
                raise NotImplementedError(remasking)
            
            # --- 4d. Selecting Tokens to Update ---
            # We are only interested in updating tokens inside the current block.
            # Set confidence outside the current active generation area to -infinity to ignore them.
            x0_p[:, end_pos:] = -np.inf

            # Only consider predictions for positions that are currently masked.
            # Original tokens (prompt and previously confirmed tokens) should have -infinity confidence.
            confidence = torch.where(mask_index, x0_p, -np.inf)
            
            # Replace the content of `x` at masked positions with the new predictions (`x0`).
            x0 = torch.where(mask_index, x0, x)
            
            # This will hold the indices of the tokens we decide to "confirm" in this step.
            transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
            
            # For each item in the batch (here, just 1)...
            for j in range(confidence.shape[0]):
                # ...select the `k` tokens with the HIGHEST confidence scores among the masked positions.
                # `k` is determined by `num_transfer_tokens` for the current step `i`.
                _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])
                # Mark these high-confidence positions to be updated.
                transfer_index[j, select_index] = True

            # --- 4e. State Update ---
            # Update the main tensor 'x' by replacing [MASK] tokens with the selected high-confidence predictions.
            # The other [MASK]s remain for the next refinement iteration.
            x[transfer_index] = x0[transfer_index]

    # 5. Return Final Generation
    # Once all blocks and steps are complete, return the generated part of the sequence.
    return x
```

# Reference 

[^1]: ç®€å•æ¥è¯´å°±æ˜¯æ‹¥æœ‰æ— é™æ•°æ®ã€ä¸€ä¸ªè¶³å¤Ÿå¤§çš„ç½‘ç»œå’Œæœ€ä¼˜è®­ç»ƒçš„ç†æƒ³æ¡ä»¶ä¸‹ï¼Œæ¨¡å‹æœ‰èƒ½åŠ›æ¢å¤å‡ºçœŸå®çš„æ•°æ®åˆ†å¸ƒã€‚


[^2]: åœ¨ä¸æ›´æ–°å…¶è‡ªèº«å‚æ•°çš„æƒ…å†µä¸‹ï¼Œä»…é€šè¿‡åœ¨ Prompt ä¸­æä¾›å°‘é‡ç¤ºä¾‹ (few-shot) æˆ–ä»»åŠ¡æè¿° (zero-shot)ï¼Œå°±èƒ½å½“åœºå­¦ä¼šå¹¶æ‰§è¡Œä¸€ä¸ªæ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚