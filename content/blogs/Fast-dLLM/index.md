---
title: "Fast-dLLM"
date: 2025-06-12T23:01:49+08:00
lastmod: 2025-06-12T23:01:49+08:00
author: ["WITHER"]

categories:
- Paper Reading

tags:
- DiffusionLLM

keywords:
- DiffusionLLM

description: "Paper Reading of Fast-dLLM" # æ–‡ç« æè¿°ï¼Œä¸æœç´¢ä¼˜åŒ–ç›¸å…³
summary: "Paper Reading of Fast-dLLM" # æ–‡ç« ç®€å•æè¿°ï¼Œä¼šå±•ç¤ºåœ¨ä¸»é¡µ
weight: # è¾“å…¥1å¯ä»¥é¡¶ç½®æ–‡ç« ï¼Œç”¨æ¥ç»™æ–‡ç« å±•ç¤ºæ’åºï¼Œä¸å¡«å°±é»˜è®¤æŒ‰æ—¶é—´æ’åº
slug: ""
draft: false # æ˜¯å¦ä¸ºè‰ç¨¿
comments: true
showToc: true # æ˜¾ç¤ºç›®å½•
TocOpen: true # è‡ªåŠ¨å±•å¼€ç›®å½•
autonumbering: true # ç›®å½•è‡ªåŠ¨ç¼–å·
hidemeta: false # æ˜¯å¦éšè—æ–‡ç« çš„å…ƒä¿¡æ¯ï¼Œå¦‚å‘å¸ƒæ—¥æœŸã€ä½œè€…ç­‰
disableShare: true # åº•éƒ¨ä¸æ˜¾ç¤ºåˆ†äº«æ 
searchHidden: false # è¯¥é¡µé¢å¯ä»¥è¢«æœç´¢åˆ°
showbreadcrumbs: true #é¡¶éƒ¨æ˜¾ç¤ºå½“å‰è·¯å¾„
mermaid: true
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---

# Introduction

Diffusion LLMs è¢«è§†ä¸ºä¸‹ä¸€ä»£æ–‡æœ¬ç”ŸæˆæŠ€æœ¯çš„æœ‰åŠ›ç«äº‰è€…ï¼Œå…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºç†è®ºä¸Šå¯ä»¥å¹¶è¡Œç”Ÿæˆå¤šä¸ª tokenï¼Œä»è€Œæœ‰æœ›å®ç°æ¯”è‡ªå›å½’æ¨¡å‹å¿«å‡ ä¸ªæ•°é‡çº§çš„æ¨ç†é€Ÿåº¦ã€‚è°·æ­Œçš„ Gemini Diffusion å’Œ Inception Labs çš„Mercuryç­‰æ¨¡å‹å·²ç»å±•ç¤ºäº†å…¶æƒŠäººçš„æ½œåŠ›ï¼Œå®£ç§°èƒ½è¾¾åˆ°æ¯ç§’ä¸Šåƒ token çš„ç”Ÿæˆé€Ÿåº¦ã€‚

å½“å‰å¼€æºçš„æ‰©æ•£LLM (LLaDAã€Dream) åœ¨å®é™…åº”ç”¨ä¸­çš„é€Ÿåº¦è¿œè¿œè¾¾ä¸åˆ°é¢„æœŸï¼Œç”šè‡³æ¯”ä¼˜åŒ–è‰¯å¥½çš„è‡ªå›å½’æ¨¡å‹è¿˜è¦æ…¢ã€‚è¿™ç¯‡è®ºæ–‡çš„å·¥ä½œï¼Œå°±æ˜¯è¦æ‹†æ‰é˜»ç¢æ‰©æ•£ LLM èµ·é£çš„ä¸¤åº§å¤§å±±ã€‚

1. æ— æ³•ä½¿ç”¨ KV Cache

æ‰©æ•£LLMçš„æ³¨æ„åŠ›æœºåˆ¶æ˜¯åŒå‘çš„ï¼Œå³ä¸€ä¸ª token çš„ç”Ÿæˆä¸ä»…ä¾èµ–äºå®ƒå‰é¢çš„å†…å®¹ï¼Œä¹Ÿä¾èµ–äºå®ƒåé¢çš„å†…å®¹ (å°½ç®¡åé¢å¯èƒ½æ˜¯æœªçŸ¥çš„ MASK token ) ã€‚è¿™ç§ç‰¹æ€§ä½¿å¾—è¿‡å»çš„ä¿¡æ¯å’Œæœªæ¥çš„ä¿¡æ¯ç›¸äº’çº ç¼ ï¼Œæ— æ³•åƒè‡ªå›å½’æ¨¡å‹é‚£æ ·ç®€å•åœ°ç¼“å­˜å’Œå¤ç”¨è¿‡å»çš„ä¿¡æ¯ã€‚å¯¼è‡´æ‰©æ•£LLMåœ¨æ¯ä¸€æ­¥æ¨ç†ä¸­éƒ½éœ€è¦è¿›è¡Œå¤§é‡çš„é‡å¤è®¡ç®—ï¼Œä¸¥é‡æ‹–æ…¢äº†é€Ÿåº¦ã€‚

Fast-dLLM çš„ç¬¬ä¸€ä¸ªæ ¸å¿ƒè´¡çŒ®ï¼Œå°±æ˜¯æå‡ºäº†ä¸€ç§åˆ†å—è¿‘ä¼¼ (block-wise approximate) KV Cache æœºåˆ¶ã€‚

{{< quote >}}
While the bidirectional nature of attention in Diffusion LLMs precludes a fully equivalent KV Cache, our approximation closely resembles an ideal cache in practice. 
{{< /quote >}}

å®ƒå°†å¾…ç”Ÿæˆçš„æ–‡æœ¬åºåˆ—åˆ†æˆè‹¥å¹²ä¸ªå—. åœ¨ç”ŸæˆæŸä¸€ä¸ªå— (æ¯”å¦‚Block 1) æ—¶ï¼Œå®ƒä¼šæå‰è®¡ç®—å¹¶ç¼“å­˜å…¶ä»–æ‰€æœ‰å— (æ¯”å¦‚ Prompt å’Œ Block 0) çš„ KV. åœ¨è¿™ä¸ªå—çš„å†…éƒ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œè¿™äº›ç¼“å­˜è¢«åå¤åˆ©ç”¨ã€‚å½“è¿™ä¸ªå—ç”Ÿæˆå®Œæ¯•åï¼Œå†æ•´ä½“æ›´æ–°ä¸€æ¬¡æ‰€æœ‰å—çš„KVç¼“å­˜ ã€‚

è¿™ä¸ªæ–¹æ³•çš„è¿‘ä¼¼åœ¨äºï¼Œåœ¨ä¸€ä¸ªå—çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œç¼“å­˜æ˜¯å›ºå®šçš„ï¼Œè€Œå®é™…ä¸Šéšç€å—å†… token çš„ä¸æ–­å»å™ªå’Œæ¸…æ™°åŒ–ï¼Œè¿™äº›ç¼“å­˜ç†è®ºä¸Šä¹Ÿåº”è¯¥éšä¹‹å¾®è°ƒã€‚ä½†è®ºæ–‡é€šè¿‡å¯è§†åŒ–å®éªŒ (å›¾3) æœ‰åŠ›åœ°è¯æ˜ï¼Œåœ¨ç›¸é‚»çš„æ¨ç†æ­¥éª¤ä¸­ï¼ŒKV æ¿€æ´»å€¼çš„ ä½™å¼¦ç›¸ä¼¼åº¦éå¸¸é«˜ï¼Œå‡ ä¹æ¥è¿‘äº1. è¿™è¯´æ˜ä½¿ç”¨å›ºå®šçš„è¿‘ä¼¼ç¼“å­˜å¸¦æ¥çš„è¯¯å·®å¾®ä¹å…¶å¾®ï¼Œå®Œå…¨å¯ä»¥ç”¨æå°çš„ç²¾åº¦æŸå¤±æ¢å–å·¨å¤§çš„é€Ÿåº¦æå‡ã€‚

è®ºæ–‡è¿˜è¿›ä¸€æ­¥æå‡ºäº†åŒç¼“å­˜ (DualCache) ç‰ˆæœ¬ï¼Œä¸ä»…ç¼“å­˜äº†å‰é¢çš„â€œå‰ç¼€â€ (prefix) ï¼Œè¿˜ç¼“å­˜äº†åé¢çš„â€œåç¼€â€ (suffixï¼Œé€šå¸¸æ˜¯ MASK  token )  ï¼Œä»è€Œè¿›ä¸€æ­¥å‹æ¦¨äº†è®¡ç®—ä¼˜åŒ–çš„ç©ºé—´ï¼Œå®ç°äº†æ›´å¿«çš„é€Ÿåº¦ã€‚

2. å¹¶è¡Œè§£ç å¸¦æ¥çš„è´¨é‡ä¸‹é™

æ‰©æ•£LLMçš„å¦ä¸€å¤§ç†è®ºä¼˜åŠ¿æ˜¯ å¹¶è¡Œè§£ç  (Parallel Decoding)ï¼Œå³ä¸€æ¬¡æ€§é¢„æµ‹å’Œç”Ÿæˆå¤šä¸ª token  ã€‚ç„¶è€Œï¼Œå®è·µå†æ¬¡è¯æ˜ï¼Œå½“å¹¶è¡Œè§£ç çš„ token æ•°é‡å¢å¤šæ—¶ï¼Œç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ä¼šæ€¥å‰§ä¸‹é™ ã€‚

è®ºæ–‡æ·±åˆ»åœ°å‰–æäº†å…¶æ ¹æºï¼šæ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ (conditional independence assumption) çš„ç ´å ã€‚åœ¨å¹¶è¡Œè§£ç æ—¶ï¼Œæ¨¡å‹æ˜¯ç‹¬ç«‹åœ°ä¸ºæ¯ä¸ªå¾…ç”Ÿæˆçš„ MASK ä½ç½®é¢„æµ‹ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åä»ä¸­é‡‡æ ·ã€‚ä½†å®é™…ä¸Šï¼Œä¸€å¥è¯ä¸­çš„ token ä¹‹é—´å­˜åœ¨ç€å¼ºçƒˆçš„ä¾èµ–å…³ç³»ã€‚è®ºæ–‡ä¸¾äº†ä¸€ä¸ªä¾‹å­:

{{< quote >}}
Consider an example from [30]: The list of poker hands that consist of two English words are: The subsequent two words could be, for instance, "high card," "two pair," "full house," or "straight flush." [...] However, the multi-token prediction procedure in MDMs first generates a probability distribution for each token and then samples from these distributions independently. This independent sampling can lead to undesirable combinations, such as "high house."
{{< /quote >}}

æ¨¡å‹å¯èƒ½ä¼šç‹¬ç«‹åœ°é¢„æµ‹å‡º "high" å’Œ "house"è¿™ä¸¤ä¸ªè¯ï¼Œä½†æŠŠå®ƒä»¬ç»„åˆåœ¨ä¸€èµ·å°±æˆäº†æ¯«æ— æ„ä¹‰çš„ high house. è¿™æ˜¯å› ä¸ºæ¨¡å‹åœ¨å¹¶è¡Œé¢„æµ‹æ—¶å¿½ç•¥äº† token é—´çš„è”åˆæ¦‚ç‡ï¼Œè€Œé”™è¯¯åœ°ç›´æ¥ä½¿ç”¨äº†è¾¹ç¼˜æ¦‚ç‡çš„ä¹˜ç§¯ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒFast-dLLMæå‡ºäº†ç¬¬äºŒä¸ªæ ¸å¿ƒè´¡çŒ®ï¼šç½®ä¿¡åº¦æ„ŸçŸ¥å¹¶è¡Œè§£ç  (Confidence-Aware Parallel Decoding) ç­–ç•¥ ã€‚è¿™ä¸ªæƒ³æ³•éå¸¸ç›´è§‚ä¸”æœ‰æ•ˆï¼šæˆ‘ä»¬åªå¯¹é‚£äº›æ¨¡å‹éå¸¸æœ‰æŠŠæ¡çš„ token è¿›è¡Œå¹¶è¡Œè§£ç ã€‚

å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯ä¸€æ­¥è§£ç æ—¶ï¼Œæ¨¡å‹ä¼šä¸ºæ¯ä¸ªå¾…ç”Ÿæˆçš„ MASK ä½ç½®è®¡ç®—ä¸€ä¸ª ç½®ä¿¡åº¦åˆ†æ•° (æ¯”å¦‚softmaxæ¦‚ç‡çš„æœ€å¤§å€¼). ç„¶åï¼Œè®¾å®šä¸€ä¸ªå…¨å±€çš„ç½®ä¿¡åº¦é˜ˆå€¼ Ï„ï¼Œåªæœ‰é‚£äº›ç½®ä¿¡åº¦è¶…è¿‡è¿™ä¸ªé˜ˆå€¼çš„ token æ‰ä¼šè¢«æ­å¼€ï¼Œè€Œç½®ä¿¡åº¦ä¸è¶³çš„ token åˆ™ç»§ç»­ä¿æŒ MASK çŠ¶æ€ï¼Œç•™åˆ°ä¸‹ä¸€æ­¥å†åšå†³ç­–ã€‚ä¸ºäº†é¿å…æ— é™å¾ªç¯ï¼Œå¦‚æœæ²¡æœ‰ä»»ä½• token çš„ç½®ä¿¡åº¦è¾¾æ ‡ï¼Œæ¨¡å‹ä¼šå¼ºåˆ¶è§£ç ç½®ä¿¡åº¦æœ€é«˜çš„é‚£ä¸€ä¸ªã€‚

è¿™ä¸ªç­–ç•¥çš„ç²¾å¦™ä¹‹å¤„åœ¨äºï¼Œå®ƒåœ¨ç†è®ºä¸Šæ˜¯ç«™å¾—ä½è„šçš„ã€‚è®ºæ–‡é€šè¿‡å®šç†ä¸€ä»æ•°å­¦ä¸Šè¯æ˜äº†ï¼šå½“æ¨¡å‹å¯¹ä¸€ç»„ token çš„é¢„æµ‹ç½®ä¿¡åº¦è¶³å¤Ÿé«˜æ—¶ (å³ p>1âˆ’Ïµï¼Œä¸” Ïµ è¶³å¤Ÿå°)ï¼ŒåŸºäºç‹¬ç«‹è¾¹ç¼˜æ¦‚ç‡çš„â€œè´ªå¿ƒå¹¶è¡Œè§£ç â€ä¸åŸºäºçœŸå®è”åˆæ¦‚ç‡çš„â€œè´ªå¿ƒä¸²è¡Œè§£ç â€ä¼šå¾—åˆ°å®Œå…¨ç›¸åŒçš„ç»“æœã€‚

![Effectiveness of Components of Fast-dLLM across Different Approaches](https://share.note.youdao.com/yws/api/personal/file/WEBccefa918e999469a4faa3badff3c32b9?method=download&shareKey=c9e48ddb1e1f0600394ce8baa1d84426 "Effectiveness of Components of Fast-dLLM across Different Approaches")

Fast-dLLM çš„åˆ›æ–°æ€§ä½“ç°åœ¨å®ƒæ˜¯ä¸€ç§ training-free çš„åŠ é€Ÿæ¡†æ¶ã€‚å®ƒæ²¡æœ‰ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œä¹Ÿä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡ä¸¤é¡¹å³æ’å³ç”¨çš„æ¨ç†ç­–ç•¥â€”â€”â€œåˆ†å—è¿‘ä¼¼KVç¼“å­˜â€å’Œâ€œç½®ä¿¡åº¦æ„ŸçŸ¥å¹¶è¡Œè§£ç â€ï¼Œåˆ†åˆ«ä»å‡å°‘é‡å¤è®¡ç®—å’Œæå‡å¹¶è¡Œæ•ˆç‡ä¸¤ä¸ªç»´åº¦ï¼Œç²¾å‡†åœ°è§£å†³äº†å½“å‰å¼€æºæ‰©æ•£ LLM é¢ä¸´çš„æ ¸å¿ƒç“¶é¢ˆã€‚ å®éªŒç»“æœåœ¨ LLaDA å’Œ Dream ç­‰æ¨¡å‹ä¸Šï¼Œç»“åˆä¸¤ç§ç­–ç•¥ï¼Œå®ç°äº†é«˜è¾¾ 27.6 å€çš„ç«¯åˆ°ç«¯ååé‡æå‡ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå‡ ä¹æ²¡æœ‰ç²¾åº¦æŸå¤±ã€‚

# 2. Preliminary

### 2.1. Masked Diffusion Model

é’ˆå¯¹ç¦»æ•£æ•°æ®çš„æ‰©æ•£æ¨¡å‹æœ€æ—©åœ¨ Argmax Flows and Multinomial Diffusion å’Œ Deep Unsupervised Learning using
Nonequilibrium Thermodynamics ä¸­è¢«æ¢æå‡ºã€‚éšå D3PM æå‡ºäº†ä¸€ä¸ªæ›´é€šç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡ç‰¹å®šçš„è½¬ç§»çŸ©é˜µ $Q_{t}$ å®šä¹‰äº†å‰å‘åŠ å™ªè¿‡ç¨‹çš„ç¦»æ•£çŠ¶æ€é©¬å°”å¯å¤«é“¾ï¼Œå¹¶é€šè¿‡æœ€å¤§åŒ– ELBO æ¥å­¦ä¹ åå‘è¿‡ç¨‹çš„å‚æ•°åŒ–æ¨¡å‹ $p_{\theta}(x_{0}|x_{t})$. CTMC è¿›ä¸€æ­¥å°† D3PM æ‰©å±•åˆ°è¿ç»­æ—¶é—´ï¼Œå°†å…¶å½¢å¼åŒ–ä¸ºä¸€ä¸ªè¿ç»­æ—¶é—´é©¬å°”å¯å¤«é“¾ (CTMC) æ¡†æ¶ã€‚åœ¨å¦ä¸€ç§ä¸åŒçš„æ–¹æ³•ä¸­ï¼ŒSEDD é€šè¿‡å‚æ•°åŒ–ä¼¼ç„¶æ¯” $\frac{p_{t}(y)}{p_{t}(x)}$ æ¥å­¦ä¹ åå‘è¿‡ç¨‹ï¼Œå¹¶é‡‡ç”¨å»å™ªåˆ†æ•°ç†µæ¥è®­ç»ƒè¯¥æ¯”ç‡ã€‚

åœ¨å„ç§ç¦»æ•£æ‰©æ•£çš„å™ªå£°å¤„ç†æ–¹å¼ä¸­ï¼Œ**Masked Diffusion Models, MDMs**ï¼Œä¹Ÿè¢«ç§°ä¸ºå¸æ”¶çŠ¶æ€ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œè·å¾—äº†ç›¸å½“å¤§çš„å…³æ³¨ã€‚MDMs é‡‡ç”¨ä¸€ç§å‰å‘åŠ å™ªè¿‡ç¨‹ï¼Œå…¶ä¸­ token è¢«é€æ­¥æ›¿æ¢ä¸ºä¸€ä¸ªç‰¹æ®Šçš„ MASK  token  ã€‚è¿™ä¸ªè¿‡ç¨‹ç”±ä»¥ä¸‹è½¬ç§»æ¦‚ç‡å®šä¹‰ï¼š

$$
q_{t|0}(x_{t}|x_{0})=\prod_{i=1}^{n}q_{t|0}(x_{t}^{i}|x_{0}^{i})=\prod_{i=1}^{n}Cat(x_{t}^{i};(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}) \tag{1}
$$ 

* $q_{t|0}(x_t|x_0)$: è¡¨ç¤ºç»™å®šåŸå§‹åºåˆ— $x_0$ï¼Œå¾—åˆ°å™ªå£°åºåˆ— $x_t$ çš„æ¦‚ç‡ ã€‚
* $\prod_{i=1}^{n}$: è¿ä¹˜ç¬¦å·ï¼Œè¡¨ç¤ºæ•´ä¸ªåºåˆ—çš„å™ªå£°è¿‡ç¨‹æ˜¯åºåˆ—ä¸­æ¯ä¸ª token  (token) ç‹¬ç«‹è¿›è¡Œå™ªå£°è¿‡ç¨‹çš„æ¦‚ç‡ä¹˜ç§¯ ã€‚
* $Cat(\cdot)$: ä»£è¡¨**ç±»åˆ«åˆ†å¸ƒ (Categorical Distribution)** ã€‚
* $t \in [0,1]$: è¡¨ç¤º**æ‰©æ•£æ—¶é—´**æˆ–**æ©ç çº§åˆ«**ã€‚å½“ $t=0$ æ—¶ï¼Œåºåˆ—å®Œå…¨æ˜¯åŸå§‹çš„ï¼›å½“ $t=1$ æ—¶ï¼Œåºåˆ—è¢«å®Œå…¨æ›¿æ¢ä¸º `[MASK]`  token ã€‚
* $(1-t)\delta_{x_{0}^{i}}+t\delta_{[MASK]}$: åœ¨æ—¶é—´ `t`ï¼Œç¬¬ `i` ä¸ª token æœ‰ $1-t$ çš„æ¦‚ç‡ä¿æŒå…¶åŸå§‹èº«ä»½ $x_0^i$ï¼Œæœ‰ $t$ çš„æ¦‚ç‡å˜æˆ `[MASK]`  token ã€‚`$\delta$` æ˜¯å…‹ç½—å†…å…‹å‡½æ•°ï¼Œç”¨äºæŒ‡å®šæ¦‚ç‡ã€‚

æœ€è¿‘ï¼ŒMDLM å’Œ RADD çš„å·¥ä½œè¡¨æ˜ï¼Œå¯¹äº MDMs ä¸åŒçš„å‚æ•°åŒ–æ˜¯ç­‰ä»·çš„ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¯æ˜äº† MDMs çš„è®­ç»ƒç›®æ ‡å¯ä»¥è¢«ç®€åŒ–æˆ–ç›´æ¥ä»æ•°æ®ä¼¼ç„¶ä¸­æ¨å¯¼å‡ºæ¥ ã€‚è¿™å¯¼å‡ºäº†ä»¥ä¸‹ç›®æ ‡å‡½æ•°ï¼Œå³ $log~p_{\theta}(x)$ çš„ä¸€ä¸ª ELBO:

{{< details title="Reparameterized Absorbing Discrete Diffusion, RADD">}}

**å®šç† 1ï¼ˆTheorem 1ï¼‰**
$$
\frac{p_t(\hat{x}_t)}{p_t(x_t)} = \underbrace{\frac{e^{-\bar{\sigma}(t)}}{1-e^{-\bar{\sigma}(t)}}}_{\text{æ—¶é—´ç›¸å…³çš„æ ‡é‡}} \cdot \underbrace{p_0(\hat{x}_t^i | x_t^{UM})}_{\text{å¹²å‡€æ•°æ®çš„æ¡ä»¶æ¦‚ç‡}}
$$

- $p_t(x_t)$ æ˜¯æ—¶é—´æ­¥ $t$ çš„æ•°æ®åˆ†å¸ƒ
- $x_t$ æ˜¯å¸¦å™ªå£°ï¼ˆè¢«æ©ç ï¼‰çš„åºåˆ—
- $x_t^{UM}$ æ˜¯å…¶ä¸­æœªè¢«æ©ç çš„éƒ¨åˆ†
- $\hat{x}_t$ æ˜¯åœ¨ $x_t$ çš„ä¸€ä¸ªæ©ç ä½ç½®ä¸Šå¡«å…¥ä¸€ä¸ªæ–° token åçš„åºåˆ—
- $p_0$ æ˜¯åŸå§‹å¹²å‡€æ•°æ®çš„åˆ†å¸ƒï¼Œ$\bar{\sigma}(t)$ æ˜¯ä¸€ä¸ªä¸å™ªå£°æ°´å¹³ç›¸å…³çš„å‡½æ•°ã€‚

è¿™ä¸ªå…¬å¼è¡¨æ˜ï¼Œæ¨¡å‹éœ€è¦å­¦ä¹ çš„ç›®æ ‡å¯ä»¥åˆ†è§£ã€‚å…¶ä¸­ä¸€éƒ¨åˆ†æ˜¯ä¸€ä¸ªå¯ä»¥ç²¾ç¡®è®¡ç®—çš„ã€åªä¸æ—¶é—´ $t$ æœ‰å…³çš„æ ‡é‡ï¼Œè€Œå¦ä¸€éƒ¨åˆ†åˆ™æ˜¯ä¸€ä¸ª**ä¸æ—¶é—´æ— å…³**çš„ã€åœ¨ç»™å®šå…¶ä»–å¯è§ token çš„æ¡ä»¶ä¸‹ï¼Œé¢„æµ‹è¢«æ©ç  token çš„æ¡ä»¶æ¦‚ç‡ã€‚æ­£æ˜¯LLM æ‰€åšçš„äº‹æƒ…ã€‚è¿™ä¸ªçœ‹ä¼¼ç®€å•çš„æ”¹åŠ¨å¸¦æ¥äº†å·¨å¤§çš„å®é™…ä¼˜åŠ¿ï¼š

1. **æ¶æ„ç®€åŒ–**ï¼šç§»é™¤äº†æ—¶é—´ç¼–ç å’Œç›¸å…³çš„è‡ªé€‚åº”å½’ä¸€åŒ–å±‚ï¼Œä½¿å¾—æ¨¡å‹å‚æ•°æ›´å°‘ï¼Œç»“æ„æ›´ç®€æ´ ã€‚
2. **é‡‡æ ·åŠ é€Ÿ**ï¼šç”±äºæ¨¡å‹è¾“å‡ºä¸å†ä¾èµ–äºæ—¶é—´ $t$ï¼Œå½“è¾“å…¥åºåˆ— $x_t$ åœ¨æŸä¸ªé‡‡æ ·åŒºé—´å†…æ²¡æœ‰å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå¯ä»¥ç›´æ¥ç¼“å­˜ä¸Šä¸€æ­¥çš„è®¡ç®—ç»“æœï¼Œè€Œæ— éœ€å†æ¬¡è°ƒç”¨ç½‘ç»œã€‚è¿™æå¤§åœ°å‡å°‘äº†**å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNumber of Function Evaluations, NFEsï¼‰**ã€‚è®ºæ–‡ç»™å‡ºäº†åœ¨ç‰¹å®šé‡‡æ ·ç­–ç•¥ä¸‹ï¼ŒæœŸæœ›å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆE-NFEsï¼‰çš„è§£æå…¬å¼ ï¼š

$$
E\text{-}NFEs(n) = n \left( 1 - \left( 1 - \frac{1}{n} \right)^l \right)
$$

**å®šç† 2ï¼ˆTheorem 2ï¼‰**

è¯æ˜äº†å¸æ”¶æ€æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒç›®æ ‡ï¼ˆå…·ä½“æ¥è¯´æ˜¯ DSE æŸå¤±ï¼‰åœ¨æ•°å­¦ä¸Šç­‰ä»·äº**ä»»æ„é˜¶è‡ªå›å½’æ¨¡å‹ï¼ˆAny-Order Autoregressive Models, AO-ARMsï¼‰**çš„è®­ç»ƒç›®æ ‡ ã€‚

AO-ARMs æ˜¯ä¸€ç±»ç‰¹æ®Šçš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒä»¬ä¸åƒæ ‡å‡†è‡ªå›å½’æ¨¡å‹é‚£æ ·å›ºå®šä»å·¦åˆ°å³çš„ç”Ÿæˆé¡ºåºï¼Œè€Œæ˜¯å­¦ä¹ åœ¨æ‰€æœ‰å¯èƒ½çš„ $d!$ï¼ˆ$d$ ä¸ºåºåˆ—é•¿åº¦ï¼‰ç§ç”Ÿæˆé¡ºåºä¸‹å¯¹æ•°æ®è¿›è¡Œå»ºæ¨¡ã€‚è®ºæ–‡é€šè¿‡ä¸€ç³»åˆ—ç²¾å·§çš„æ•°å­¦æ¨å¯¼ï¼Œå»ºç«‹äº†å››ç§ä¸åŒæŸå¤±å‡½æ•°ä¹‹é—´çš„ç­‰ä»·å…³ç³»é“¾ ï¼š

$\mathcal{L}_{DSE} \iff \mathcal{L}_{t-DCE} \iff \mathcal{L}_{\lambda-DCE} \iff \mathcal{L}_{AO}$


å®ƒè¡¨æ˜å¸æ”¶æ€æ‰©æ•£æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯åœ¨å­¦ä¹ ä¸€ä¸ªé›†æˆäº†æ‰€æœ‰å¯èƒ½ç”Ÿæˆé¡ºåºçš„è‡ªå›å½’æ¨¡å‹çš„æœŸæœ› ã€‚è¿™å¯èƒ½è§£é‡Šäº†ä¸ºä»€ä¹ˆå®ƒä»¬åœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°å¾—éå¸¸ç¨³å¥ã€‚

{{< /details >}}

$$
-log~p_{\theta}(x)\le\int_{0}^{1}\frac{1}{t}\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})]dt:=\mathcal{L}_{MDM}. \tag{2}
$$ 

* $-log~p_{\theta}(x)$: æ¨¡å‹çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç”ŸæˆçœŸå®æ•°æ® $x$ çš„å¯¹æ•°ä¼¼ç„¶ï¼Œè¿™ç­‰ä»·äºæœ€å°åŒ–å®ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚è¿™ä¸ªå…¬å¼ç»™å‡ºäº†è´Ÿå¯¹æ•°ä¼¼ç„¶çš„ä¸€ä¸ª* ELBO.
* $\int_{0}^{1}...dt$: å¯¹æ‰€æœ‰å¯èƒ½çš„å™ªå£°çº§åˆ« `t` (ä»0åˆ°1) è¿›è¡Œç§¯åˆ†ï¼Œæ„å‘³ç€æ¨¡å‹éœ€è¦å­¦ä¼šåœ¨ä»»ä½•å™ªå£°æ°´å¹³ä¸‹éƒ½èƒ½å¾ˆå¥½åœ°å¤åŸæ•°æ® ã€‚
* $\mathbb{E}_{q_{t,0}(x_{t}|x_{0})}[...]$: è¡¨ç¤ºå¯¹æ‰€æœ‰å¯èƒ½çš„å™ªå£°æ ·æœ¬æ±‚æœŸæœ›ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬æ ¹æ®å…¬å¼(1)éšæœºç”Ÿæˆä¸€ä¸ªå¸¦ `[MASK]` çš„å™ªå£°åºåˆ— $x_t$.
* $\sum_{i:x_{t}^{i}=[MASK]}-log~p_{\theta}(x_{0}^{i}|x_{t})$: 
    * $\sum_{i:x_{t}^{i}=[MASK]}$: å¯¹æ‰€æœ‰è¢« `[MASK]` çš„ä½ç½® `i` è¿›è¡Œæ±‚å’Œ ã€‚
    * $-log~p_{\theta}(x_{0}^{i}|x_{t})$: è¿™æ˜¯äº¤å‰ç†µæŸå¤±ã€‚å®ƒçš„æ„æ€æ˜¯ï¼Œç»™å®šå¸¦æœ‰ `[MASK]` çš„åºåˆ— $x_t$ï¼Œæ¨¡å‹ $p_{\theta}$ éœ€è¦é¢„æµ‹åœ¨ä½ç½® i ä¸Šçš„åŸå§‹ token  $x_0^i$ åº”è¯¥æ˜¯ä»€ä¹ˆã€‚æ¨¡å‹é¢„æµ‹å¾—è¶Šå‡†ï¼Œè¿™ä¸ªæŸå¤±å€¼å°±è¶Šå°ã€‚

### 2.2. MDMs çš„ç”Ÿæˆè¿‡ç¨‹

å¯¹äºå…¬å¼1ä¸­å®šä¹‰çš„å‰å‘è¿‡ç¨‹ï¼Œå…¶è§£æä¸Šçš„é€†è¿‡ç¨‹åœ¨ç”Ÿæˆæ—¶è®¡ç®—æ•ˆç‡ä½ä¸‹ï¼Œå› ä¸ºå®ƒé€šå¸¸æ¯æ­¥åªä¿®æ”¹ä¸€ä¸ª token ã€‚ä¸€ä¸ªå¸¸è§çš„åŠ é€Ÿç­–ç•¥æ˜¯é‡‡ç”¨ $\tau$-leaping è¿‘ä¼¼æ³•æ¥å¤„ç†åå‘è¿‡ç¨‹ã€‚åœ¨ MDMs çš„èƒŒæ™¯ä¸‹ï¼Œè¿™å…è®¸ä¸€ä¸ªè¿­ä»£å¼çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå…¶ä¸­å¤šä¸ªè¢«æ©ç çš„ token å¯ä»¥ä»ä¸€ä¸ªå™ªå£°æ°´å¹³ t è¿‘ä¼¼åœ°å•æ­¥æ¢å¤åˆ°ä¸€ä¸ªæ›´æ—©çš„æ°´å¹³ s < t.

$$
q_{s|t}(x_s|x_t)=\prod_{i=0}^{n-1}q_{s|t}(x_{s}^{i}|x_{t})
$$

å…¶ä¸­

$$
q_{s|t}(x_{s}^{i}|x_{t})=\begin{cases}1, & \text{if } x_{t}^{i}\ne[MASK], x_{s}^{i}=x_{t}^{i} \\ \frac{s}{t}, & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}=[MASK] \\ \frac{t-s}{t}q_{0|t}(x_{s}^{i}|x_{t}), & \text{if } x_{t}^{i}=[MASK], x_{s}^{i}\ne[MASK]\end{cases} \tag{3}
$$

* $q_{s|t}(x_{s}^{i}|x_{t})$: è¡¨ç¤ºä» `t` æ—¶åˆ»çš„ token  $x_t^i$ å˜ä¸º `s` æ—¶åˆ»çš„ token  $x_s^i$ çš„æ¦‚ç‡ ã€‚
* **Case 1**: å¦‚æœä¸€ä¸ª token åœ¨ `t` æ—¶åˆ»å°±ä¸æ˜¯ `[MASK]`ï¼Œé‚£ä¹ˆå®ƒåœ¨æ›´æ—©çš„ `s` æ—¶åˆ»ä¹Ÿä¿æŒä¸å˜ ã€‚
* **Case 2**: ä¸€ä¸ªåœ¨ t æ—¶åˆ»æ˜¯ `[MASK]` çš„ token ï¼Œåœ¨æ›´æ—©çš„ s æ—¶åˆ»ä»ç„¶æ˜¯ `[MASK]`. 
* **Case 3**: è¿™æ˜¯å…³é”®çš„å»å™ªæ­¥éª¤ã€‚å¦‚æœä¸€ä¸ª token åœ¨ `t` æ—¶åˆ»æ˜¯ `[MASK]`ï¼Œæ¨¡å‹ä¼šå°è¯•åœ¨ s æ—¶åˆ»é¢„æµ‹å‡ºä¸€ä¸ªå…·ä½“çš„ token.
    * $\frac{t-s}{t}$: ä»£è¡¨ä¸€ä¸ªåœ¨ `t` æ—¶åˆ»è¢«æ©ç çš„ tokenï¼Œåœ¨ `s` æ—¶åˆ»è¢«â€œæ­ç¤ºâ€å‡ºæ¥çš„æ¦‚ç‡ ã€‚
    * $q_{0|t}(x_{s}^{i}|x_{t})$: è¿™æ˜¯ç”±ç¥ç»ç½‘ç»œæ¨¡å‹ç»™å‡ºçš„é¢„æµ‹åˆ†å¸ƒã€‚æ¨¡å‹ä¼šè§‚å¯Ÿæ•´ä¸ªå¸¦æœ‰ `[MASK]` çš„ä¸Šä¸‹æ–‡ $x_t$ï¼Œç„¶åä¸ºå½“å‰ä½ç½®é¢„æµ‹ä¸€ä¸ªæœ€æœ‰å¯èƒ½çš„åŸå§‹ token ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªåœ¨æ•´ä¸ªè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ ã€‚

åœ¨æ¶‰åŠæ¡ä»¶æ•°æ®çš„åœºæ™¯ä¸­ï¼Œä¾‹å¦‚æ ¹æ®ä¸€ä¸ª propmt p ç”Ÿæˆä¸€ä¸ªå›åº” $x_{0}$ï¼ŒMDM çš„åå‘è¿‡ç¨‹ (å…¬å¼3æ‰€å®šä¹‰) éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡å‹ç”¨äºæ­ç¤ºä¸€ä¸ª token  $x_{s}^{i}$ çš„é¢„æµ‹åˆ†å¸ƒ $q_{0|t}(x_{s}^{i}|x_{t})$ ç°åœ¨ä¹Ÿéœ€è¦ä»¥ prompt p ä¸ºæ¡ä»¶ï¼Œå³ $q_{0|t}(x_{s}^{i}|x_{t},p)$ ã€‚

**å¹¶è¡Œè§£ç çš„è¯…å’’**
ç›´æ¥é€†è½¬å…¬å¼1çš„å‰å‘è¿‡ç¨‹æ¥è¿›è¡Œç”Ÿæˆæ˜¯ç¼“æ…¢çš„ï¼Œé€šå¸¸æ¯æ­¥åªæ”¹å˜ä¸€ä¸ª token. ä¸€ä¸ªå¸¸è§çš„åŠ é€Ÿç­–ç•¥æ˜¯é‡‡ç”¨ $\tau$-leaping è¿‘ä¼¼æ³•æ¥å¤„ç†åå‘è¿‡ç¨‹ã€‚å¯¹äº MDMsï¼Œè¿™æ„å‘³ç€å¤šä¸ªè¢«æ©ç çš„ token å°†åœ¨ä¸€ä¸ªæ­¥éª¤ä¸­å¹¶è¡Œç”Ÿæˆã€‚ç„¶è€Œï¼Œç”±äºæ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ï¼Œå¤š token é¢„æµ‹ä¸­å‡ºç°äº†ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚è€ƒè™‘ä¸€ä¸ªä¾‹å­ï¼šç”±ä¸¤ä¸ªè‹±æ–‡å•è¯ç»„æˆçš„æ‰‘å…‹æ‰‹ç‰Œåˆ—è¡¨æ˜¯ï¼šéšåçš„ä¸¤ä¸ªè¯å¯èƒ½æ˜¯ï¼Œä¾‹å¦‚ï¼Œhigh cardï¼Œtwo pairï¼Œfull houseï¼Œæˆ– straight flush. å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸¤ä¸ªè¯ä¹‹é—´å­˜åœ¨ç€å…³è”ã€‚ç„¶è€Œï¼ŒMDMs ä¸­çš„å¤š token é¢„æµ‹è¿‡ç¨‹é¦–å…ˆä¸ºæ¯ä¸ª token ç”Ÿæˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åç‹¬ç«‹åœ°ä»è¿™äº›åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚è¿™ç§ç‹¬ç«‹é‡‡æ ·å¯èƒ½å¯¼è‡´ä¸å¸Œæœ›çš„ç»„åˆï¼Œä¾‹å¦‚ high house.

ä¸ºäº†å°†å…¶å½¢å¼åŒ–ï¼Œè€ƒè™‘æ­ç¤ºä¸¤ä¸ª token ä½ç½® i å’Œ j. ç”±äºæ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ï¼ŒMDMs ä» $p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t})$ ä¸­é‡‡æ ·è¿™äº› token. ç„¶è€Œï¼ŒçœŸå®çš„è”åˆæ¦‚ç‡éœ€è¦è€ƒè™‘å®ƒä»¬ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼š

$$
p(x_{s}^{i},x_{s}^{j}|x_{t})=p(x_{s}^{i}|x_{t})\cdot p(x_{s}^{j}|x_{t},x_{s}^{i})
$$

æˆ–è€…å¯¹ç§°åœ°ï¼Œé€šè¿‡å°† i ä¾èµ–äºæ¡ä»¶ j. è¿™ç§å‡è®¾çš„ç‹¬ç«‹ç”Ÿæˆä¸çœŸå®çš„ä¾èµ–æ€§æ•°æ®åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œä¼šé™ä½ç”Ÿæˆåºåˆ—çš„è´¨é‡å’Œè¿è´¯æ€§ã€‚å½“åœ¨å•ä¸€æ­¥éª¤ä¸­åŒæ—¶æ­ç¤ºå¤§é‡ token æ—¶ï¼Œè¿™ä¸ªé—®é¢˜ä¼šå˜å¾—æ›´åŠ ä¸¥é‡ã€‚

# 3. Methodology

## 3.1. Pipeline Overview

**Fast-dLLM**ï¼Œå»ºç«‹åœ¨ MDM æ¶æ„ä¹‹ä¸Šï¼Œä»¥å®ç°é«˜æ•ˆå’Œé«˜è´¨é‡çš„åºåˆ—ç”Ÿæˆã€‚ä¸ºäº†åŠ é€Ÿæ¨ç†ï¼Œæ•´ä½“æµæ°´çº¿èåˆäº†ä¸¤å¤§å…³é”®ç­–ç•¥ï¼šé€šè¿‡ KV Cache å®ç°çš„é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—ï¼Œä»¥åŠä¸€ä¸ªç”±é¢„æµ‹ç½®ä¿¡åº¦å¼•å¯¼çš„ å¹¶è¡Œè§£ç æ–¹æ¡ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åˆ†å—è§£ç è®¾è®¡çš„ KV Cacheï¼Œå®ƒå…è®¸åœ¨ä¸åŒæ­¥éª¤é—´å¤ç”¨æ³¨æ„åŠ›æ¿€æ´»å€¼ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†å†—ä½™è®¡ç®—ã€‚åœ¨æ¯ä¸ªå—å†…éƒ¨ï¼Œè¿›ä¸€æ­¥æå‡ºäº†ç½®ä¿¡åº¦æ„ŸçŸ¥çš„å¹¶è¡Œè§£ç ï¼Œå®ƒèƒ½æ ¹æ®ç½®ä¿¡åº¦åˆ†æ•°é€‰æ‹©æ€§åœ°æ›´æ–° token ï¼Œä»è€Œåœ¨ä¿æŒè¾“å‡ºè´¨é‡çš„åŒæ—¶æé«˜æ•ˆç‡ã€‚é€šè¿‡ç»“åˆè¿™äº›ç­–ç•¥ï¼ŒFast-dLLM åœ¨å¯¹ç”Ÿæˆæ€§èƒ½å½±å“æœ€å°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—åŠ å¿«äº† MDM çš„æ¨ç†é€Ÿåº¦ã€‚æ•´ä½“æµç¨‹åœ¨ç®—æ³• 1 ä¸­è¿›è¡Œäº†æ€»ç»“ã€‚ 

## 3.2. Key-Value Cache for Block-Wise Decoding

![Illustration of our Key-Value Cache for Block-Wise Decoding](https://share.note.youdao.com/yws/api/personal/file/WEBe66f192a665248e7559ffa12a0bf10c1?method=download&shareKey=8952caa17d664bd8bcc33b9ebcec321e "Illustration of our Key-Value Cache for Block-Wise Decoding")

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åˆ†å—è§£ç çš„ç­–ç•¥æ¥æ”¯æŒ KV Cache çš„ä½¿ç”¨ã€‚ä¸€å¼€å§‹è®¡ç®—å¹¶å­˜å‚¨ prompt çš„ KV ç¼“å­˜ï¼Œè¿™ä¸ªç¼“å­˜å°†åœ¨æ•´ä¸ªå— 0çš„è§£ç è¿‡ç¨‹ä¸­è¢«å¤ç”¨ã€‚åœ¨æ¯ä¸ªå—çš„å†…éƒ¨ï¼Œç›¸åŒçš„ç¼“å­˜ä¼šè¢«å¤šä¸ªè§£ç æ­¥éª¤å¤ç”¨ã€‚**åœ¨å®Œæˆä¸€ä¸ªå—çš„è§£ç ä¹‹åï¼Œæ›´æ–°æ‰€æœ‰ token (ä¸ä»…ä»…æ˜¯æ–°ç”Ÿæˆçš„ token) çš„ç¼“å­˜**ã€‚è¿™ä¸ªç¼“å­˜æ›´æ–°å¯ä»¥ä¸è§£ç æ­¥éª¤è”åˆæ‰§è¡Œï¼Œå› æ­¤ä¸ä¸ä½¿ç”¨ç¼“å­˜ç›¸æ¯”ï¼Œæ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚ç”±äºæ©ç æ‰©æ•£æ¨¡å‹ä¸­ä½¿ç”¨çš„æ˜¯å®Œå…¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™ç§æ–¹æ³•å¯¼è‡´äº†ä¸€ä¸ªè¿‘ä¼¼çš„è§£ç è¿‡ç¨‹ã€‚ 

æˆ‘ä»¬çš„è¿‘ä¼¼ KV ç¼“å­˜æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæºäºæˆ‘ä»¬è§‚å¯Ÿåˆ° KV æ¿€æ´»å€¼åœ¨ç›¸é‚»çš„æ¨ç†æ­¥éª¤ä¸­è¡¨ç°å‡ºé«˜åº¦çš„ç›¸ä¼¼æ€§ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å›¾ a ä¸­çº¢è‰²æ–¹æ¡†åŒºåŸŸçªæ˜¾äº†å—å†…çš„ç›¸ä¼¼æ€§åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°å§‹ç»ˆæ¥è¿‘äº 1. è¡¨æ˜åœ¨åˆ†å—è§£ç æœŸé—´ï¼Œå‰ç¼€ (prefix) çš„é”®å’Œå€¼çš„å·®å¼‚å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå®‰å…¨åœ°å¤ç”¨ç¼“å­˜è€Œä¸ä¼šæœ‰æ˜¾è‘—çš„å‡†ç¡®ç‡æŸå¤±ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªæˆ‘ä»¬ KV ç¼“å­˜æœºåˆ¶çš„åŒå‘ç‰ˆæœ¬ï¼Œåä¸º **DualCache**ï¼Œå®ƒä¸ä»…ç¼“å­˜å‰ç¼€ token ï¼Œè¿˜ç¼“å­˜åç¼€ (suffix)  token ï¼Œåœ¨æˆ‘ä»¬çš„åˆ†å—è§£ç æ–¹æ¡ˆä¸­ï¼Œåç¼€å®Œå…¨ç”±æ©ç  token ç»„æˆã€‚å¦‚è¡¨3æ‰€ç¤ºï¼ŒDualCache å¸¦æ¥äº†è¿›ä¸€æ­¥çš„åŠ é€Ÿã€‚å›¾ b ä¸­çš„çº¢è‰²æ–¹æ¡†åŒºåŸŸè¿›ä¸€æ­¥è¯æ˜ï¼Œåœ¨åˆ†å—è§£ç æœŸé—´ï¼Œåç¼€çš„é”®å’Œå€¼çš„å·®å¼‚ä¹Ÿå¯ä»¥å¿½ç•¥ä¸è®¡ã€‚ 

![Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA](https://share.note.youdao.com/yws/api/personal/file/WEB2030e80c11d3d306e335a2dc5931b101?method=download&shareKey=6a5005c556aaa11edb4006a48b755b4a "Heatmaps of Key-Value Activation Cosine Similarity Across Inference Steps in LLaDA")

## 3.3. Confidence-Aware Parallel Decoding


å°½ç®¡å­˜åœ¨ä¸€äº›æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨è¾…åŠ©æ¨¡å‹æ¥æ˜¾å¼åœ°æ•æ‰ä¸åŒä½ç½® token ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šå¢åŠ æ•´ä¸ªæµæ°´çº¿çš„å¤æ‚æ€§ã€‚ä¸è¿™äº›æ–¹æ³•ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„**ç½®ä¿¡åº¦æ„ŸçŸ¥è§£ç ç®—æ³•**ï¼Œæ—¨åœ¨ç¼“è§£è¿™ç§æ¡ä»¶ç‹¬ç«‹æ€§é—®é¢˜ã€‚

åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯å†’ç„¶åœ°ä½¿ç”¨å®ƒä»¬ç‹¬ç«‹çš„è¾¹ç¼˜æ¦‚ç‡æ¥æ­ç¤ºæ‰€æœ‰è¢«æ©ç çš„ token ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ª token è®¡ç®—ä¸€ä¸ªç½®ä¿¡åº¦åˆ†æ•° (ä¾‹å¦‚æœ€å¤§çš„ softmax æ¦‚ç‡). åªæœ‰é‚£äº›ç½®ä¿¡åº¦è¶…è¿‡ä¸€ä¸ªé˜ˆå€¼çš„ token æ‰ä¼šåœ¨å½“å‰æ­¥éª¤è¢«æ­ç¤ºï¼›å…¶ä½™çš„åˆ™ä¿æŒæ©ç çŠ¶æ€ï¼Œå¹¶åœ¨æœªæ¥çš„æ­¥éª¤ä¸­é‡æ–°è€ƒè™‘ã€‚å¦‚æœæ²¡æœ‰ token çš„ç½®ä¿¡åº¦è¶…è¿‡é˜ˆå€¼ï¼Œå°±æ­ç¤ºç½®ä¿¡åº¦æœ€é«˜çš„é‚£ä¸€ä¸ªï¼Œä»¥ç¡®ä¿è¿‡ç¨‹èƒ½å¤Ÿè¿›è¡Œå¹¶é˜²æ­¢æ— é™å¾ªç¯ã€‚è¿™ä¸ªç­–ç•¥åœ¨åŠ é€Ÿç”Ÿæˆçš„åŒæ—¶ï¼Œå‡å°‘äº†ç”±ä¸ç¡®å®šæˆ–æ¨¡ç³Šé¢„æµ‹å¼•èµ·çš„é”™è¯¯ã€‚ 

ä¸€ä¸ªå…³é”®é—®é¢˜æ˜¯

{{< quote >}}
*When is it theoretically justifiable to decode tokens in parallel using independent marginals, despite the true joint distribution potentially containing dependencies?*
{{< /quote >}}

ä»¥ä¸‹ç»“æœæ¥å›ç­”äº†åœ¨é«˜ç½®ä¿¡åº¦æƒ…å†µä¸‹ï¼Œgreedy parallel è§£ç ç­‰åŒäº greedy sequential è§£ç çš„æ¡ä»¶ï¼Œå¹¶é‡åŒ–äº†ä¸¤ç§åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚åœ¨ç»™å‡ºå®šç†ä¹‹å‰ï¼Œæˆ‘ä»¬å°†å®šä¹‰å…¶è¡¨è¿°ä¸­ä½¿ç”¨çš„æ•°å­¦ç¬¦å·ã€‚

è®¾ $p_{\theta}(\cdot|E)$ è¡¨ç¤ºä¸€ä¸ª MDM åœ¨ç»™å®š E (åŒ…æ‹¬ prompt $p_{0}$ å’Œå…ˆå‰ç”Ÿæˆçš„ token) çš„æ¡ä»¶ä¸‹ç»™å‡ºçš„ PMF. å‡è®¾æ¨¡å‹è¦ä¸ºä¸åœ¨ E ä¸­çš„ä½ç½® $i_{1},...,i_{n}$ é¢„æµ‹ n ä¸ª token.

ä»¤ $X=(X_{i_{1}},...,X_{i_{n}})$ æ˜¯ n ä¸ª token çš„å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ª $X_{i_{j}}$ åœ¨è¯æ±‡è¡¨ V ä¸­å–å€¼ã€‚è®¾ $p(X|E)\equiv p_{\theta}(X_{i_{1}},...,X_{i_{n}}|E)$ æ˜¯æ¨¡å‹ç»™å‡ºçš„è”åˆæ¡ä»¶ PMFã€‚è®¾ $p_{j}(X_{i_{j}}|E)\equiv p_{\theta}(X_{i_{j}}|E)$ æ˜¯ä½ç½® $i_{j}$ çš„è¾¹ç¼˜æ¡ä»¶ PMFã€‚å¹¶è¡Œè§£ç ä½¿ç”¨è¾¹ç¼˜æ¦‚ç‡çš„ä¹˜ç§¯æ¥ç”Ÿæˆ token ï¼š$q(X|E)=\tilde{\prod}_{j=1}^{n}p_{j}(X_{i_{j}}|E)$ã€‚å®šç†1çš„è¯æ˜åŠç›¸å…³è®¨è®ºè§é™„å½•Aã€‚ 

**å®šç† 1 (é«˜ç½®ä¿¡åº¦ä¸‹çš„å¹¶è¡Œè§£ç ).** å‡è®¾å­˜åœ¨ä¸€ä¸ªç‰¹å®šçš„ token åºåˆ— $x^{*}=(x_{i_{1}},...,x_{i_{n}})$ï¼Œä½¿å¾—å¯¹äºæ¯ä¸ª $j\in\{1,...,n\}$ï¼Œæ¨¡å‹å¯¹ $x_{i_{j}}$ éƒ½æœ‰å¾ˆé«˜çš„ç½®ä¿¡åº¦ï¼š$p_{j}(X_{i_{j}}=x_{i_{j}}|E)>1-\epsilon$ï¼Œå¯¹äºæŸä¸ªå¾ˆå°çš„ $\epsilon>0$. é‚£ä¹ˆï¼Œä»¥ä¸‹ç»“è®ºæˆç«‹ï¼š

1. *Equivalence of Greedy Decoding*ï¼šå¦‚æœ $(n+1)\epsilon\le1$ (å³ $\epsilon\le\frac{1}{n+1}$) ï¼Œé‚£ä¹ˆ
$$
\text{argmax}_{z} p(z|E) = \text{argmax}_{z} q(z|E) = x^{*}. \tag{4}
$$ 

è¿™æ„å‘³ç€ greedy parallel è§£ç  (é€‰æ‹© argmax q) ä¸è´ªå©ªåºè´¯è§£ç  (é€‰æ‹© argmax p) äº§ç”Ÿç›¸åŒçš„ç»“æœã€‚  è¿™ä¸ªç•Œæ˜¯ç´§çš„ï¼šå¦‚æœ $\epsilon > \frac{1}{n+1}$ï¼Œåˆ™å­˜åœ¨æ»¡è¶³é«˜ç½®ä¿¡åº¦è¾¹ç¼˜å‡è®¾çš„åˆ†å¸ƒ $p(X|E)$ï¼Œä½¿å¾— argmax $p(z|E)$ â‰  argmax $q(z|E)$ã€‚ 

2. *Distance and Divergence Bounds*ï¼šä¸ºç®€æ´èµ·è§ï¼Œå°† $p(\cdot|E)$ å’Œ $q(\cdot|E)$ è¡¨ç¤ºä¸º p å’Œ q. 

**$L_p$ Distance ($p \ge 1$)**: å¯¹äº $n>1$ï¼Œ$D_{p}(p,q)<((n-1)^{p}+2n)^{1/p}\epsilon$ã€‚ç‰¹åˆ«åœ°ï¼Œå¯¹äºæ€»å˜å·®è·ç¦» ($D_{TV}(p,q)=\frac{1}{2}D_{1}(p,q)$)ï¼Œ$D_{TV}(p,q)<\frac{3n-1}{2}\epsilon$.

è¿™ä¸ªå…¬å¼è¯´æ˜ï¼Œ**çœŸå®åˆ†å¸ƒ p å’Œè¿‘ä¼¼åˆ†å¸ƒ q ä¹‹é—´çš„æ€»å˜å·®è·ç¦»æœ‰ä¸€ä¸ªä¸Šé™**ã€‚è¿™ä¸ªä¸Šé™å–å†³äºä¸¤ä¸ªå› ç´ ï¼š
1. $n$: ç”Ÿæˆåºåˆ—çš„é•¿åº¦ã€‚åºåˆ—è¶Šé•¿ï¼Œè¿™ä¸ªä¸Šé™å°±è¶Šå¤§ã€‚è¿™æ˜¯ç¬¦åˆç›´è§‰çš„ï¼Œå› ä¸ºæ¯å¢åŠ ä¸€ä¸ª tokenï¼Œè¿‘ä¼¼æ‰€ç´¯ç§¯çš„æ½œåœ¨è¯¯å·®å°±å¯èƒ½å¢åŠ ä¸€ç‚¹ã€‚
2. $\epsilon$: æ¨¡å‹åœ¨æ¯ä¸ªä½ç½®ä¸Šçš„â€œä¸ç¡®å®šæ€§â€ã€‚$\epsilon$ è¶Šå° (å³æ¨¡å‹è¶Šè‡ªä¿¡)ï¼Œè¿™ä¸ªä¸Šé™å°±è¶Šä½ã€‚

**Forward KL Divergence**: å¯¹äº $n > 1$ï¼Œ$D_{KL}(p||q)<(n-1)(H_{b}(\epsilon)+\epsilon~ln(|\mathcal{V}|-1))$ï¼Œå…¶ä¸­ $H_{b}(\epsilon)=-\epsilon~ln~\epsilon-(1-\epsilon)ln(1-\epsilon)$ æ˜¯äºŒå…ƒç†µå‡½æ•°ï¼Œè€Œ $|\mathcal{V}|$ æ˜¯è¯æ±‡è¡¨çš„å¤§å°ã€‚ 

1. $n-1$: åŒæ ·ï¼ŒæŸå¤±ä¼šéšç€åºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ã€‚
2. $H_{b}(\epsilon)$: å®ƒè¡¡é‡äº†ä¸€ä¸ªæ¦‚ç‡ä¸º $\epsilon$ çš„äº‹ä»¶å¸¦æ¥çš„â€œæ„å¤–ç¨‹åº¦â€æˆ–ä¸ç¡®å®šæ€§ã€‚å½“ $\epsilon$ å¾ˆå°æ—¶ï¼Œ$H_b(\epsilon)$ ä¹Ÿéå¸¸å°ã€‚
3. $\epsilon~ln(|\mathcal{V}|-1)$: è¿™ä¸€é¡¹åæ˜ äº†é‚£éƒ¨åˆ†å¾®å°çš„ $\epsilon$ æ¦‚ç‡è¢«åˆ†é…åˆ°è¯æ±‡è¡¨ $\mathcal{V}$ ä¸­å…¶ä»–æ‰€æœ‰ token ä¸Šæ‰€å¸¦æ¥çš„ä¸ç¡®å®šæ€§ã€‚å³ä½¿ $\epsilon$ å¾ˆå°ï¼Œå¦‚æœè¯æ±‡è¡¨éå¸¸å·¨å¤§ ($|\mathcal{V}|$ å¾ˆå¤§)ï¼Œè¿™ä¸€é¡¹ä¹Ÿå¯èƒ½æœ‰å½±å“ã€‚

---
* $L_p$ è·ç¦»è¯´æ˜åœ¨é«˜ç½®ä¿¡åº¦ä¸‹ï¼Œä¸¤ç§æ–¹æ³•æ‰¾åˆ°çš„**æœ€ä½³ç­”æ¡ˆ**æ˜¯ç›¸åŒçš„ã€‚
* KL æ•£åº¦è¯´æ˜é«˜ç½®ä¿¡åº¦ä¸‹ï¼Œä¸ä»…æœ€ä½³ç­”æ¡ˆç›¸åŒï¼Œä¸¤ç§æ–¹æ³•æç»˜çš„æ¦‚ç‡åˆ†å¸ƒéƒ½éå¸¸ç›¸ä¼¼ã€‚è¿‘ä¼¼æ–¹æ³• q ä¸ä»…çŒœå¯¹äº†å¯èƒ½æ€§æœ€å¤§çš„ tokenï¼Œ å¯¹å…¶ä»–å¯èƒ½æ€§çš„ä¼°è®¡ï¼Œä¹Ÿå’Œç²¾ç¡®æ–¹æ³• p çš„åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚

# 4. Experiments

## 4.1 Experimental Setup

* **ç¡¬ä»¶ä¸ç¯å¢ƒ** ğŸ–¥ï¸: æ‰€æœ‰å®éªŒå‡åœ¨å•å¼  **NVIDIA A100 80GB GPU** ä¸Šè¿›è¡Œï¼Œbatch size=1.
* **è¯„æµ‹æ¨¡å‹** ğŸ§ : **LLaDA**  å’Œ **Dream**.
* **è¯„æµ‹åŸºå‡†** ğŸ“Š: é‡‡ç”¨äº†å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ï¼š**GSM8K**ã€**MATH**ã€**HumanEval** å’Œ **MBPP**.
* **æ ¸å¿ƒæŒ‡æ ‡** â±ï¸:
    * **å‡†ç¡®ç‡ (Accuracy)**: è¡¡é‡æ¨¡å‹åœ¨å…·ä½“ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚
    * **ååé‡ (Throughput)**: ä»¥ tokens/sec ä¸ºå•ä½ï¼Œåæ˜ ç«¯åˆ°ç«¯çš„çœŸå®è§£ç é€Ÿåº¦ã€‚
* **è¶…å‚æ•°** âš™ï¸:
    * **ç¼“å­˜å—å¤§å°**: åœ¨ 4 åˆ° 32 ä¹‹é—´è¿›è¡Œæ¢ç´¢ã€‚
    * **ç½®ä¿¡åº¦é˜ˆå€¼**: åœ¨ 0.5 åˆ° 1.0 ä¹‹é—´è¿›è¡Œæ¢ç´¢ã€‚
    * å®éªŒé»˜è®¤ä½¿ç”¨ **PrefixCache**ï¼Œå—å¤§å°ä¸º **32**ï¼Œç½®ä¿¡åº¦é˜ˆå€¼ä¸º **0.9**.

---
## 4.2 Main Results: Performance and Speed

å®éªŒç»“æœè¡¨æ˜ï¼ŒFast-dLLM åœ¨å„ç§ä»»åŠ¡å’Œè®¾ç½®ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶å¯¹æ¨¡å‹å‡†ç¡®ç‡çš„å½±å“å¾®ä¹å…¶å¾® ã€‚

* åŠ é€Ÿæ•ˆæœ:
    * å•ç‹¬å¼•å…¥ KV Cache æœºåˆ¶ï¼Œé€šå¸¸èƒ½å¸¦æ¥ **2x-3.6x** çš„é€Ÿåº¦æå‡ã€‚
    * å½“ KV Cache å’Œå¹¶è¡Œè§£ç ä¸¤ç§ç­–ç•¥ç»“åˆä½¿ç”¨æ—¶ï¼Œæ€§èƒ½æå‡æ›´ä¸ºæ˜¾è‘—ã€‚åœ¨ LLaDA æ¨¡å‹ä¸Šï¼Œæœ€ é«˜å¯è¾¾ **11.0x** çš„ååé‡æå‡ï¼›åœ¨ Dream æ¨¡å‹ä¸Šï¼Œæœ€é«˜å¯è¾¾ **7.8x** çš„æå‡ ã€‚
* æå°çš„ç²¾åº¦æŸå¤±: åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒåŠ é€Ÿåæ¨¡å‹çš„å‡†ç¡®ç‡ä¸åŸå§‹åŸºçº¿æ¨¡å‹çš„å·®è·åŸºæœ¬ä¿æŒåœ¨ **1-2ä¸ªç™¾åˆ†ç‚¹** ä»¥å†…ï¼Œæœ‰æ—¶ç”šè‡³ç•¥æœ‰æé«˜ã€‚
* å¯¹é•¿åºåˆ—æ›´å‹å¥½: å®éªŒè¿˜å‘ç°ï¼Œåœ¨å¤„ç†æ›´é•¿çš„æ–‡æœ¬åºåˆ—æ—¶ (ä¾‹å¦‚ few-shot åœºæ™¯æˆ–é•¿ä»£ç ç”Ÿæˆ)ï¼ŒFast-dLLM çš„åŠ é€Ÿæ•ˆæœæ›´ä¸ºæ˜æ˜¾ã€‚


ä¸‹è¡¨ä»¥ GSM8K (5-shot) ä»»åŠ¡ä¸ºä¾‹ï¼Œç›´è§‚å±•ç¤ºäº† Fast-dLLM (å³ +Cache+Parallel) ç›¸è¾ƒäº baseline æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚

| æ¨¡å‹ | ç”Ÿæˆé•¿åº¦ | é…ç½® | å‡†ç¡®ç‡ (%) | ååé‡ (tok/s) | ç›¸å¯¹åŠ é€Ÿ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **LLaDA** | 256 | Baseline | 79.3 | 6.7 | 1x |
| | | **Fast-dLLM** | **78.5** | **54.4** | **8.1x** |
| | 512 | Baseline | 77.5 | 3.2 | 1x |
| | | **Fast-dLLM** | **77.2** | **35.3** | **11.0x** |
| **Dream** | 256 | Baseline | 75.0 | 9.1 | 1x |
| | | **Fast-dLLM** | **74.8** | **48.2** | **5.3x** |
| | 512 | Baseline | 76.0 | 7.7 | 1x |
| | | **Fast-dLLM** | **74.0** | **42.9** | **5.6x** |

---
## 4.3 Ablations and Analysis

ä¸ºäº†æ·±å…¥ç†è§£å„ä¸ªç»„ä»¶çš„è´¡çŒ®ï¼Œè®ºæ–‡è¿›è¡Œäº†ä¸€ç³»åˆ—è¯¦ç»†çš„æ¶ˆèå®éªŒã€‚

* **è¾“å…¥ä¸ç”Ÿæˆé•¿åº¦çš„å½±å“**:
    * å®éªŒè¯æ˜ï¼Œæ›´é•¿çš„ä¸Šä¸‹æ–‡ (prefillï¼Œå¦‚ä» 5-shot å¢åŠ åˆ° 8-shot) å’Œæ›´é•¿çš„ç”Ÿæˆé•¿åº¦ï¼Œéƒ½èƒ½æ˜¾è‘—æ”¾å¤§åŠ é€Ÿæ•ˆæœã€‚
    * åœ¨ 8-shot å’Œ 1024 ç”Ÿæˆé•¿åº¦çš„è®¾ç½®ä¸‹ï¼Œ**DualCache** å®ç°äº† **27.6x** ç«¯åˆ°ç«¯åŠ é€Ÿã€‚

* **PrefixCache vs. DualCache**:
    * **DualCache** é€šå¸¸æ¯”åªç¼“å­˜å‰ç¼€çš„ **PrefixCache** å®ç°æ›´é«˜çš„åŠ é€Ÿæ¯”ï¼Œå°¤å…¶æ˜¯åœ¨é•¿åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­ ã€‚

* **ç¼“å­˜å—å¤§å°çš„å½±å“**:
    * **small block size**ï¼šå‡†ç¡®ç‡æœ€é«˜ï¼Œä½†å› é¢‘ç¹æ›´æ–°ç¼“å­˜å¯¼è‡´å¼€é”€è¾ƒå¤§ï¼Œé€Ÿåº¦æå‡æœ‰é™ ã€‚
    * **small block size**ï¼šé€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½å› ä¸Šä¸‹æ–‡ä¸åŒ¹é…å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™ ã€‚
    * å®éªŒå‘ç°ï¼Œå—å¤§å°ä¸º **32** æ—¶åœ¨é€Ÿåº¦å’Œç²¾åº¦ä¹‹é—´å–å¾—äº†æœ€ä½³å¹³è¡¡ã€‚

![Impact of Cache Block Size on Accuracy and Throughput](https://share.note.youdao.com/yws/api/personal/file/WEB9772b6d4b4341a7ccb12bee9eef34910?method=download&shareKey=1e3a007e630de1a9cbf8b3d9f318f307 "Impact of Cache Block Size on Accuracy and Throughput")

* **åŠ¨æ€é˜ˆå€¼ vs. å›ºå®šæ­¥æ•°ç­–ç•¥**:
    * è®ºæ–‡æå‡ºçš„ **ç½®ä¿¡åº¦æ„ŸçŸ¥å¹¶è¡Œè§£ç ** ç­–ç•¥ï¼Œåœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºæ¯æ­¥å›ºå®šè§£ç  K ä¸ª token çš„ baseline æ–¹æ³•ã€‚
    * åœ¨è¾¾åˆ°ç›¸ä¼¼ç”šè‡³æ›´é«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œè¯¥åŠ¨æ€ç­–ç•¥èƒ½å®ç°æ›´é«˜çš„å¹³å‡æ¯æ­¥è§£ç  token æ•°ï¼Œä»è€Œè·å¾—æ›´é«˜çš„ååé‡ã€‚

![Threshold VS Fxied Step](https://share.note.youdao.com/yws/api/personal/file/WEBd7916aff1aba60846ae1e971b2800e0a?method=download&shareKey=88d29eb3e40615a74c4846d278413e5b "Threshold VS Fxied Step")

# 5. Related Work

æœ¬ç« èŠ‚å›é¡¾äº†ä¸ Fast-dLLM ç›¸å…³çš„ä¸¤ä¸ªæ ¸å¿ƒé¢†åŸŸï¼šæ‰©æ•£è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œä»¥åŠå¤§è¯­è¨€æ¨¡å‹çš„é€šç”¨åŠ é€ŸæŠ€æœ¯ã€‚

---

## 5.1. Diffusion LLM

æ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§å¼ºå¤§çš„ç”ŸæˆèŒƒå¼ï¼Œæœ€åˆåœ¨å›¾åƒå’ŒéŸ³é¢‘ç­‰è¿ç»­æ•°æ®é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œéšåå…¶å½±å“åŠ›æ‰©å±•åˆ°äº† NLP. ç‰¹åˆ«æ˜¯ç¦»æ•£æ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§æ›¿ä»£è‡ªå›å½’ (AR) èŒƒå¼çš„å¯è¡Œæ–¹æ¡ˆ ã€‚

* **ç†è®ºåŸºç¡€çš„å‘å±•**:
    * ç¦»æ•£æ•°æ®çš„æ‰©æ•£æ¨¡å‹æœ€æ—©ç”± [29, 11] æ¢ç´¢ ã€‚
    * **D3PM** æå‡ºäº†ä¸€ä¸ªæ›´é€šç”¨çš„æ¡†æ¶ï¼Œå°†å‰å‘åŠ å™ªè¿‡ç¨‹å»ºæ¨¡ä¸ºç¦»æ•£çŠ¶æ€é©¬å°”å¯å¤«é“¾ï¼Œå¹¶é€šè¿‡æœ€å¤§ ELBO æ¥å­¦ä¹ åå‘è¿‡ç¨‹ã€‚
    * **CTMC** å°† D3PM æ‰©å±•åˆ°è¿ç»­æ—¶é—´è®¾å®š ã€‚
    * **SEDD** é‡‡ç”¨äº†ä¸åŒçš„æ–¹æ³•ï¼Œé€šè¿‡å‚æ•°åŒ–è¾¹é™…ä¼¼ç„¶æ¯”æ¥å­¦ä¹ åå‘è¿‡ç¨‹ ã€‚
    * **MDMs** è¿‘æœŸå—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå…¶ä¸­ **MDLM** å’Œ **RADD** çš„ç ”ç©¶è¡¨æ˜ï¼ŒMDMs çš„ä¸åŒå‚æ•°åŒ–æ–¹æ³•æ˜¯ç­‰ä»·çš„ï¼Œå¹¶ä¸”å…¶è®­ç»ƒç›®æ ‡å¯ä»¥è¢«ç®€åŒ– ã€‚

* **ä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç»“åˆ**: ä¸€ä¸ªå…³é”®çš„çªç ´æ˜¯å°†ç¦»æ•£æ‰©æ•£ä¸ç°æœ‰çš„å¤§è¯­è¨€æ¨¡å‹æ¶æ„ç›¸ç»“åˆ ã€‚
    * **Diffusion-NAT** [40] å°†ç¦»æ•£æ‰©æ•£çš„å»å™ªè¿‡ç¨‹ä¸ BART çš„éè‡ªå›å½’è§£ç ç›¸ç»“åˆï¼Œé€šè¿‡è¿­ä»£å¼åœ°ä¼˜åŒ–è¢«æ©ç çš„ token ï¼Œå®ç°äº†æ¯”åŒç±»è‡ªå›å½’ Transformer å¿«20å€çš„ç”Ÿæˆé€Ÿåº¦ ã€‚
    * **LLaDA** [21]ã€**DiffuLLaMA** [7] å’Œ **Dream** [36] ç­‰æ¡†æ¶å°†æ‰©æ•£æ¨¡å‹æ‰©å±•åˆ°äº† 7B å‚æ•°çš„è§„æ¨¡ï¼Œé€šè¿‡åœ¨æ‰©æ•£æ—¶é—´æ­¥ä¸Šè¿›è¡Œé€’å½’å¼çš„ token é¢„æµ‹ï¼Œå±•ç°äº†ä¸ LLaMA3 ç­‰ä¸»æµè‡ªå›å½’æ¨¡å‹ç›¸åŒ¹æ•Œçš„æ€§èƒ½ ã€‚


## 5.2. LLM Acceleration

- KV Cache

ç”±äº LLaDA ç­‰æ‰©æ•£è¯­è¨€æ¨¡å‹é‡‡ç”¨çš„æ˜¯ **full attention**ï¼Œå°† KV ç¼“å­˜ç›´æ¥åº”ç”¨äºè¿™ç±»æ¨¡å‹å¹¶éæ˜“äº‹ã€‚ ä¸€ç¯‡ç›¸å…³çš„ç ”ç©¶ **Block diffusion**  é€šè¿‡**åˆ†å—ç”Ÿæˆ (block-by-block)** çš„æ–¹å¼ï¼Œå…‹æœäº†å…ˆå‰æ‰©æ•£è¯­è¨€æ¨¡å‹çš„å±€é™ï¼Œä½¿å¾—ç¼“å­˜å’Œå¤ç”¨å…ˆå‰å·²è§£ç å—çš„é”®å’Œå€¼æˆä¸ºå¯èƒ½ ã€‚
 
- Non-Autoregressive Generation

éè‡ªå›å½’ (NAR) ç”Ÿæˆæ ‡å¿—ç€ä¸€ç§æ ¹æœ¬æ€§çš„è½¬å˜ï¼Œå®ƒé€šè¿‡åŒæ—¶ç”Ÿæˆå¤šä¸ª token æ¥æ˜¾è‘—åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚NAR æ–¹æ³•æœ€åˆè¢«ç”¨äºç¥ç»æœºå™¨ç¿»è¯‘ï¼Œç°å·²æ‰©å±•åˆ°è¯­æ³•çº é”™ã€æ–‡æœ¬æ‘˜è¦å’Œå¯¹è¯ç³»ç»Ÿç­‰å¤šç§ä»»åŠ¡ 
ã€‚
å°½ç®¡ NAR åœ¨é€Ÿåº¦ä¸Šä¼˜åŠ¿å·¨å¤§ï¼Œä½†å®ƒé€šå¸¸ä»¥ç‰ºç‰²ä¸€å®šçš„ç”Ÿæˆè´¨é‡ä¸ºä»£ä»·ã€‚æ‰©æ•£è¯­è¨€æ¨¡å‹æ˜¯ NAR é¢†åŸŸä¸€ä¸ªæ–°å…´çš„èŒƒå¼ï¼›ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œ (å¦‚ LLaDA) åœ¨å®è·µä¸­éš¾ä»¥å®ç°é¢„æœŸçš„åŠ é€Ÿï¼Œå› ä¸ºå¹¶è¡Œç”Ÿæˆä¼šå¯¼è‡´è¾“å‡ºè´¨é‡æ˜¾è‘—ä¸‹é™ã€‚

# Weakness

è¿‘ä¼¼ç¼“å­˜çš„è¯¯å·®ç´¯ç§¯æ•ˆåº”ï¼šè®ºæ–‡è¯æ˜äº†åœ¨ç›¸é‚»æ­¥éª¤ä¸­ï¼ŒKVæ¿€æ´»å€¼çš„å·®å¼‚å¾ˆå° ã€‚ä½†éšç€ç”Ÿæˆå—çš„å¢å¤šï¼Œè¿™ç§â€œè¿‘ä¼¼â€å¸¦æ¥çš„å¾®å°è¯¯å·®æ˜¯å¦ä¼šç´¯ç§¯ï¼Œå¹¶åœ¨ç”Ÿæˆéå¸¸é•¿çš„æ–‡æœ¬ (å¦‚æ•°ä¸‡ token çš„å°è¯´) æ—¶å¯¼è‡´è¯­ä¹‰æ¼‚ç§»æˆ–ä¸€è‡´æ€§ä¸‹é™ï¼Ÿè®ºæ–‡çš„æœ€é•¿æµ‹è¯•åºåˆ—ä¸º1024 ï¼Œå¯¹äºæ›´é•¿çš„åºåˆ—ï¼Œå…¶é²æ£’æ€§æœ‰å¾…è¿›ä¸€æ­¥éªŒè¯ã€‚

å¯¹æ¨¡å‹èƒ½åŠ›çš„ä¾èµ–ï¼šâ€œç½®ä¿¡åº¦æ„ŸçŸ¥è§£ç â€ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œéšå¼åœ°ä¾èµ–äºæ¨¡å‹æœ¬èº«å…·æœ‰è‰¯å¥½çš„â€œæ ¡å‡†åº¦â€ (calibration) ï¼Œå³æ¨¡å‹çš„ç½®ä¿¡åº¦èƒ½å¤ŸçœŸå®åæ˜ å…¶é¢„æµ‹çš„æ­£ç¡®æ€§ã€‚å¦‚æœæ¨¡å‹æœ¬èº«â€œè¿‡äºè‡ªä¿¡â€æˆ–â€œä¸å¤Ÿè‡ªä¿¡â€ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯¥ç­–ç•¥æ•ˆæœä¸ä½³ã€‚è®ºæ–‡æ²¡æœ‰å¯¹æ‰€ç”¨æ¨¡å‹çš„æ ¡å‡†åº¦è¿›è¡Œåˆ†æã€‚
å®šç†ä¸€çš„ç†è®ºä¸å®è·µå·®è·ï¼šè®ºæ–‡å¦è¯šåœ°æŒ‡å‡ºäº†å®šç†ä¸€çš„å±€é™æ€§

> In practice, while MDM may not strictly satisfy this property, its behavior typically offers a close approximation.  

ç†è®ºè¯æ˜å‡è®¾äº†ä¸€ä¸ªâ€œç†æƒ³çš„â€è”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œè€ŒçœŸå®æ¨¡å‹æ˜¯å¦ä»¥åŠåœ¨å¤šå¤§ç¨‹åº¦ä¸Šç¬¦åˆè¿™ä¸ªç†æƒ³å‡è®¾ï¼Œæ˜¯ä¸€ä¸ªéœ€è¦è¿›ä¸€æ­¥æ¢ç©¶çš„é—®é¢˜ã€‚ç†è®ºå’Œå®è·µä¹‹é—´çš„å·®è·å¯èƒ½åœ¨æŸäº›åˆé’»çš„ (adversarial) æˆ–åˆ†å¸ƒå¤– (Out-of-Distribution) çš„åœºæ™¯ä¸‹è¢«æ”¾å¤§ã€‚
è¶…å‚æ•°çš„æ•æ„Ÿæ€§ä¸è°ƒä¼˜æˆæœ¬ï¼šå°½ç®¡è®ºæ–‡åˆ†æäº†å—å¤§å°å’Œé˜ˆå€¼çš„å½±å“ï¼Œä½†å¹¶æœªæä¾›ä¸€å¥—ç³»ç»Ÿæ€§çš„æ–¹æ³•æ¥ä¸ºæ–°æ¨¡å‹æˆ–æ–°ä»»åŠ¡é€‰æ‹©æœ€ä½³è¶…å‚æ•°ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™å¯èƒ½æ„å‘³ç€éœ€è¦ä¸ºæ¯ä¸ªç‰¹å®šç”¨ä¾‹è¿›è¡Œæˆæœ¬ä¸è²çš„ç½‘æ ¼æœç´¢ (grid search) ï¼Œå¢åŠ äº†æ–¹æ³•çš„åº”ç”¨é—¨æ§›ã€‚
è¯„ä¼°ç»´åº¦çš„å±€é™æ€§ï¼šè®ºæ–‡ä¸»è¦ä½¿ç”¨äº†åŸºäºå‡†ç¡®ç‡çš„åŸºå‡†æµ‹è¯•ã€‚ä½†åœ¨å¼€æ”¾å¼ç”Ÿæˆã€å¯¹è¯ç­‰ä»»åŠ¡ä¸­ï¼Œè¯„ä¼°æŒ‡æ ‡ (å¦‚æµç•…åº¦ã€ä¸€è‡´æ€§ã€å¤šæ ·æ€§) æ›´ä¸ºå¤æ‚ã€‚Fast-dLLMæ˜¯å¦ä¼šåœ¨è¿™äº›â€œè½¯â€æŒ‡æ ‡ä¸Šå¼•å…¥ä¸æ˜“å¯Ÿè§‰çš„è´Ÿé¢å½±å“ï¼Œéœ€è¦æ›´å…¨é¢çš„è¯„ä¼°ã€‚

# Source Code

1.  **åˆå§‹åŒ–**:
    * å‡½æ•°é¦–å…ˆåˆ›å»ºä¸€ä¸ªå¼ é‡ `x`ï¼Œå…¶é•¿åº¦ä¸ºâ€œæç¤ºè¯é•¿åº¦ + å¾…ç”Ÿæˆé•¿åº¦â€ã€‚
    * æç¤ºè¯ (`prompt`) éƒ¨åˆ†è¢«å¡«å……åˆ° `x` çš„å¼€å¤´ï¼Œè€Œæ‰€æœ‰å¾…ç”Ÿæˆçš„ä½ç½®åˆ™è¢«åˆå§‹åŒ–ä¸ºç‰¹æ®Šçš„æ©ç æ ‡è®° `[MASK]` (`mask_id`) ã€‚
    * å°†æ€»ç”Ÿæˆä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå— (`num_blocks`) ï¼Œå¹¶ä¸ºæ¯ä¸ªå—åˆ†é…å›ºå®šçš„è§£ç æ­¥æ•° (`steps`)

2.  **åˆ†å—ç”Ÿæˆ (å¤–å±‚å¾ªç¯)**:
    * ä»£ç ä»¥å—ä¸ºå•ä½è¿›è¡Œå¾ªç¯ï¼Œä¾æ¬¡ç”Ÿæˆæ¯ä¸ªæ–‡æœ¬å—ã€‚

3.  **å¤„ç†å•ä¸ªå— (å†…å±‚å¾ªç¯ä¸ç¼“å­˜æœºåˆ¶)**:
    * **æ­¥éª¤ A: å…¨å±€ç¼“å­˜åˆå§‹åŒ– (ç¬¬ä¸€æ¬¡æ¨¡å‹è°ƒç”¨)**
        * åœ¨å¤„ç†ä¸€ä¸ªæ–°å—çš„å¼€å§‹ï¼Œå®ƒé¦–å…ˆå°†**æ•´ä¸ªåºåˆ— `x`** (åŒ…å«æç¤ºè¯ã€å·²ç”Ÿæˆçš„å—å’Œæ‰€æœ‰æœªæ¥å¾…ç”Ÿæˆçš„`[MASK]`å—) å®Œæ•´åœ°è¾“å…¥æ¨¡å‹ã€‚
        * è¿™æ¬¡è°ƒç”¨çš„ä¸»è¦ç›®çš„æ˜¯è®¡ç®—å¹¶å­˜å‚¨æ•´ä¸ªåºåˆ—çš„é”®å€¼å¯¹ç¼“å­˜ (`past_key_values`). è¿™æ˜¯ä¸€ä¸ªå…¨å±€ç¼“å­˜ã€‚
        * ç„¶åï¼Œæ¨¡å‹æ ¹æ®è¾“å‡ºçš„ `logits`ï¼Œä½¿ç”¨ `get_transfer_index` å‡½æ•°å†³å®šåœ¨**å½“å‰å—**ä¸­ï¼Œå“ªäº› `[MASK]` æ ‡è®°åº”è¯¥è¢«ä¼˜å…ˆæ›¿æ¢æ‰ (ä¾‹å¦‚ï¼ŒåŸºäºæœ€é«˜ç½®ä¿¡åº¦çš„é¢„æµ‹) ï¼Œå¹¶ç”¨é¢„æµ‹å‡ºçš„ token  (token) è¿›è¡Œå¡«å……ã€‚è¿™ä¸ªè¿‡ç¨‹åªå‘ç”Ÿä¸€æ¬¡ã€‚

    * **æ­¥éª¤ B: å—å†…è¿­ä»£ä¼˜åŒ– (ç¬¬äºŒæ¬¡åŠåç»­æ¨¡å‹è°ƒç”¨)**
        * æ¥ä¸‹æ¥ï¼Œè¿›å…¥ä¸€ä¸ª `while` å¾ªç¯ï¼Œå¯¹å½“å‰å—è¿›è¡Œè¿­ä»£å¼åœ°â€œç²¾ç‚¼â€ï¼Œç›´åˆ°è¿™ä¸ªå—ä¸­æ‰€æœ‰çš„ `[MASK]` æ ‡è®°éƒ½è¢«å¡«æ»¡ã€‚
        * **æ ¸å¿ƒä¼˜åŒ–ç‚¹**ï¼šåœ¨è¿™æ¬¡åŠåç»­çš„æ¨¡å‹è°ƒç”¨ä¸­ï¼Œ**ä¸å†éœ€è¦è¾“å…¥æ•´ä¸ªåºåˆ—**ã€‚å®ƒåªå°†**å½“å‰å—çš„å¼ é‡** (`x[:, current_block_start:current_block_end]`) ä½œä¸ºè¾“å…¥ï¼Œå¹¶**é‡ç”¨æ­¥éª¤ A ä¸­ç”Ÿæˆçš„å…¨å±€ç¼“å­˜ `past_key_values`**ã€‚
        * è¿™å°±æ˜¯ dual cache: ä¸€ä¸ªä¸ºä¸Šä¸‹æ–‡ (æç¤ºè¯+ä¹‹å‰å—) å‡†å¤‡çš„ã€åŸºæœ¬ä¸å˜çš„é™æ€ç¼“å­˜ï¼Œå’Œä¸€ä¸ªä¸ºå½“å‰å—æœåŠ¡çš„ã€åŠ¨æ€æ›´æ–°çš„ç¼“å­˜ã€‚è¿™é¿å…äº†å¯¹ä¸Šä¸‹æ–‡éƒ¨åˆ†çš„é‡å¤è®¡ç®—ï¼Œæå¤§åœ°æå‡äº†æ•ˆç‡ã€‚
        * æ¨¡å‹ä¼šä¸ºå½“å‰å—ä¸­å‰©ä½™çš„ `[MASK]` ä½ç½®ç”Ÿæˆæ–°çš„é¢„æµ‹ï¼Œå¹¶æ ¹æ®ç­–ç•¥ç»§ç»­å¡«å……ã€‚
        * è¿™ä¸ªè¿­ä»£è¿‡ç¨‹ä¼šæŒç»­è¿›è¡Œï¼Œç›´åˆ°å½“å‰å—ä¸å†æœ‰ `[MASK]` æ ‡è®°ä¸ºæ­¢ã€‚

4.  **å®Œæˆä¸è¿”å›**:
    * å½“æ‰€æœ‰å—éƒ½å¤„ç†å®Œæ¯•åï¼Œå‡½æ•°è¿”å›æœ€ç»ˆç”Ÿæˆçš„å®Œæ•´åºåˆ— `x` å’Œæ€»çš„æ¨¡å‹å‰å‘ä¼ æ’­æ¬¡æ•° `nfe` (ä¸€ä¸ªè¡¡é‡è®¡ç®—æˆæœ¬çš„æŒ‡æ ‡) ã€‚

```python
import torch

@torch.no_grad()
def generate_with_dual_cache(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,
            remasking='low_confidence', mask_id=126336, threshold=None):
    '''
    Generates text using a non-autoregressive, block-wise decoding strategy with a dual-cache mechanism.

    Args:
        model: The mask predictor model.
        prompt: A tensor of shape (1, L) representing the input prompt.
        steps: Total number of sampling/refinement steps for the entire generation.
        gen_length: The desired length of the generated text.
        block_length: The size of each block to be generated in parallel. gen_length must be divisible by this.
        temperature: Sampling temperature for token selection. 0 means greedy decoding.
        remasking: The strategy for choosing which masks to fill ('low_confidence' or 'random').
        mask_id: The token ID for the [MASK] token.
    '''
    # Create the full tensor 'x' with the prompt and space for generation, initialized with the mask token.
    x = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)
    # Copy the prompt into the beginning of the tensor 'x'.
    x[:, :prompt.shape[1]] = prompt.clone()

    # Ensure that the generation length can be evenly divided into blocks.
    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length
    
    # Distribute the total steps among the blocks.
    assert steps % num_blocks == 0
    steps_per_block = steps // num_blocks
    
    # nfe: Number of Forward-pass Evaluations. A counter for computational cost.
    nfe = 0
    
    # Outer loop: iterate through each block to be generated.
    for num_block in range(num_blocks):
        # Define the start and end positions of the current block within the full tensor 'x'.
        current_block_start = prompt.shape[1] + num_block * block_length
        current_block_end = current_block_start + block_length

        # Find the indices of mask tokens within the current block.
        block_mask_index = (x[:, current_block_start:current_block_end] == mask_id)
        
        # Determine the number of tokens to fill at each refinement step for this block.
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps_per_block)

        # --- First Model Call: Initialize Global Cache ---
        # A single forward pass on the ENTIRE sequence (prompt + all masked blocks) to pre-calculate
        # the Key-Value cache for all tokens. This is the "global" cache.
        output = model(x, use_cache=True)
        past_key_values = output.past_key_values
        
        # Identify all mask tokens up to the end of the current block.
        mask_index = (x == mask_id)
        # Ignore masks that are in future blocks for this step's prediction.
        mask_index[:, current_block_end:] = 0
        
        # Select which tokens to predict and fill in this initial step for the current block.
        x0, transfer_index = get_transfer_index(output.logits, temperature, remasking, mask_index, x, num_transfer_tokens[:, 0] if threshold is None else None, threshold)
        
        # Update the tensor 'x' by filling the selected mask positions with the predicted tokens.
        x[transfer_index] = x0[transfer_index]
        nfe += 1 # Increment the forward-pass counter.
        
        i = 1 # Counter for refinement steps within the block.
        
        # A boolean mask indicating the position of the current block, used to update the cache efficiently.
        replace_position = torch.zeros_like(x, dtype=torch.bool)
        replace_position[:, current_block_start:current_block_end] = 1

        # --- Inner Loop: Iterative Refinement of the Current Block ---
        # This loop continues until all masks in the current block are filled.
        while True:
            nfe += 1 # Increment the forward-pass counter for each refinement step.

            # Find the remaining masks ONLY within the current block.
            mask_index_block = (x[:, current_block_start:current_block_end] == mask_id)
            
            # --- Efficient Model Call using Dual Cache ---
            # Instead of passing the whole sequence, only pass the CURRENT BLOCK's tokens.
            # Reuse the 'past_key_values' (global cache) computed earlier. The model internally
            # uses 'replace_position' to update the cache only at the current block's location.
            # This is the "dual cache" trick, avoiding re-computation for the prompt and previous blocks.
            logits = model(x[:, current_block_start:current_block_end], past_key_values=past_key_values, use_cache=True, replace_position=replace_position).logits
            
            # Select which of the remaining masks to fill in this refinement step.
            x0, transfer_index = get_transfer_index(logits, temperature, remasking, mask_index_block, 
                                            x[:, current_block_start:current_block_end], num_transfer_tokens[:, i] if threshold is None else None, threshold)

            # Update the current block with the newly predicted tokens.
            x[:, current_block_start:current_block_end][transfer_index] = x0[transfer_index]

            # If there are no more masks in the current block, exit the refinement loop.
            if (x[:, current_block_start:current_block_end] == mask_id).sum() == 0:
                break
            
            i += 1 # Move to the next refinement step.

    # Return the fully generated sequence and the total number of model evaluations.
    return x, nfe
```